paperId,paperUrl,conferenceJournalId,conferenceJournalTitle,paperTitle,paperAbstract,paperType,proceedingsVolumesIds
p0,0c2d3b28d48426b8b72f7214a7708ba8b4efa9d6,j0,Nature Biotechnology,"Reproducible, interactive, scalable and extensible microbiome data science using QIIME 2",Abstract content,fullPaper,jv0
p1,fa5853fdef7d2f6bb68203d187ddacbbddc63a8b,c92,Advances in Soft Computing,High-Dimensional Probability: An Introduction with Applications in Data Science,"© 2018, Cambridge University Press Let us summarize our findings. A random projection of a set T in R n onto an m-dimensional subspace approximately preserves the geometry of T if m ⪆ d ( T ) . For...",poster,cp92
p2,4c6e31458b0b44c1e8bd6e58f7d7e0767f7fde44,j1,IEEE Transactions on Knowledge and Data Engineering,CRISP-DM Twenty Years Later: From Data Mining Processes to Data Science Trajectories,"CRISP-DM(CRoss-Industry Standard Process for Data Mining) has its origins in the second half of the nineties and is thus about two decades old. According to many surveys and user polls it is still the de facto standard for developing data mining and knowledge discovery projects. However, undoubtedly the field has moved on considerably in twenty years, with data science now the leading term being favoured over data mining. In this paper we investigate whether, and in what contexts, CRISP-DM is still fit for purpose for data science projects. We argue that if the project is goal-directed and process-driven the process model view still largely holds. On the other hand, when data science projects become more exploratory the paths that the project can take become more varied, and a more flexible model is called for. We suggest what the outlines of such a trajectory-based model might look like and how it can be used to categorise data science projects (goal-directed, exploratory or data management). We examine seven real-life exemplars where exploratory activities play an important role and compare them against 51 use cases extracted from the NIST Big Data Public Working Group. We anticipate this categorisation can help project planning in terms of time and cost characteristics.",fullPaper,jv1
p3,7282f5c9d84cd47c516a6a66c5a6b8f1e2cf44b6,c0,International Conference on Human Factors in Computing Systems,AutoDS: Towards Human-Centered Automation of Data Science,"Data science (DS) projects often follow a lifecycle that consists of laborious tasks for data scientists and domain experts (e.g., data exploration, model training, etc.). Only till recently, machine learning(ML) researchers have developed promising automation techniques to aid data workers in these tasks. This paper introduces AutoDS, an automated machine learning (AutoML) system that aims to leverage the latest ML automation techniques to support data science projects. Data workers only need to upload their dataset, then the system can automatically suggest ML configurations, preprocess data, select algorithm, and train the model. These suggestions are presented to the user via a web-based graphical user interface and a notebook-based programming user interface. Our goal is to offer a systematic investigation of user interaction and perceptions of using an AutoDS system in solving a data science task. We studied AutoDS with 30 professional data scientists, where one group used AutoDS, and the other did not, to complete a data science project. As expected, AutoDS improves productivity; Yet surprisingly, we find that the models produced by the AutoDS group have higher quality and less errors, but lower human confidence scores. We reflect on the findings by presenting design implications for incorporating automation techniques into human work in the data science lifecycle.",fullPaper,cp0
p4,3569c79cf90b203325dd7b8f6c30bacc60f5d30e,j2,SN Computer Science,"Data Science and Analytics: An Overview from Data-Driven Smart Computing, Decision-Making and Applications Perspective",Abstract content,fullPaper,jv2
p5,d737e2d326519ab3ba5da17441e073ba1c7a3ef5,c106,Chinese Conference on Biometric Recognition,Computational Optimal Transport: With Applications to Data Science,"The goal of Optimal Transport (OT) is to define geometric tools that are useful to compare probability distributions. Their use dates back to 1781. Recent years have witnessed a new revolution in the spread of OT, thanks to the emergence of approximate solvers that can scale to sizes and dimensions that are relevant to data sciences. Thanks to this newfound scalability, OT is being increasingly used to unlock various problems in imaging sciences (such as color or texture processing), computer vision and graphics (for shape manipulation) or machine learning (for regression, classification and density fitting). This monograph reviews OT with a bias toward numerical methods and their applications in data sciences, and sheds lights on the theoretical properties of OT that make it particularly useful for some of these applications. Computational Optimal Transport presents an overview of the main theoretical insights that support the practical effectiveness of OT before explaining how to turn these insights into fast computational schemes. Written for readers at all levels, the authors provide descriptions of foundational theory at two-levels. Generally accessible to all readers, more advanced readers can read the specially identified more general mathematical expositions of optimal transport tailored for discrete measures. Furthermore, several chapters deal with the interplay between continuous and discrete measures, and are thus targeting a more mathematically-inclined audience. This monograph will be a valuable reference for researchers and students wishing to get a thorough understanding of Computational Optimal Transport, a mathematical gem at the interface of probability, analysis and optimization.",poster,cp106
p6,4ce46acc4a40038b02014c1bbce3e451586ebe05,j3,"Shanlax International Journal of Arts, Science and Humanities",Data Science,"Data science has become the most demanding task of the 21st century. All companies are looking for candidates with knowledge of data science. This topic provides an overview of data science. Includes data science duties, data science tools, data science components, applications and more.",fullPaper,jv3
p7,7478cfec7b75970f365b3e1c540ef72b4adf3635,j4,"International Journal of Advanced Research in Science, Communication and Technology",Data Science,"Data Science refers to an arising district of work regard the accumulation, arrangement, reasoning, imagination, administration, and protection of abundant groups of facts. Although the name Data Science appears to combine most powerfully accompanying extents to a degree databases and information technology, many various types of abilities containing nonmathematical abilities are still wanted attending. Data Science is much in addition to plainly analysing data. The main aim of Data Science search out turn big sets of two together unorganized and organized data into valuable news that can help organisations to create strong data-compelled resolutions. At a extreme level, data erudition maybe defined as a set of fundamental law unavoidable for profitable ancestry of news from data. Since we accumulate data continually and about everything, its use is various. The most average request is in healthcare, travel, e - trade, sports, management, public publishing, etc. The aim concerning this paper search out present data erudition and to present allure benefits and request in differing fields.",fullPaper,jv4
p8,0b1d429110757c1a9deb27fb8568740182dc1679,j5,Genome Biology,Eleven grand challenges in single-cell data science,Abstract content,fullPaper,jv5
p9,040d94340f742f522ceddb31f31fb9c9b4c23cfd,c13,International Conference on Data Science and Advanced Analytics,Computing competencies for undergraduate data science curricula,Abstract content,poster,cp13
p10,0b5b33b7ea1dc12f3e9252ac1852170a6a6775bf,j6,Nature Methods,Nucleus segmentation across imaging experiments: the 2018 Data Science Bowl,Abstract content,fullPaper,jv6
p11,dd45dc3767230b70197420e1523dc0f1d7930f80,c63,IEEE International Software Metrics Symposium,Foundations of Data Science,"Computer science as an academic discipline began in the 1960’s. Emphasis was on programming languages, compilers, operating systems, and the mathematical theory that supported these areas. Courses in theoretical computer science covered finite automata, regular expressions, context-free languages, and computability. In the 1970’s, the study of algorithms was added as an important component of theory. The emphasis was on making computers useful. Today, a fundamental change is taking place and the focus is more on applications. There are many reasons for this change. The merging of computing and communications has played an important role. The enhanced ability to observe, collect, and store data in the natural sciences, in commerce, and in other fields calls for a change in our understanding of data and how to handle it in the modern setting. The emergence of the web and social networks as central aspects of daily life presents both opportunities and challenges for theory.",poster,cp63
p12,c13147ef0b86d5ec833c272840f8f3bdacf96e7f,j7,International Journal of Data Science and Analysis,Data science: a game changer for science and innovation,Abstract content,fullPaper,jv7
p13,1ba044d3d501dddd94b479aa9dbe55a93bfa9d5f,c110,IEEE International Conference on Automatic Face & Gesture Recognition,"QIIME 2: Reproducible, interactive, scalable, and extensible microbiome data science","We present QIIME 2, an open-source microbiome data science platform accessible to users spanning the microbiome research ecosystem, from scientists and engineers to clinicians and policy makers. QIIME 2 provides new features that will drive the next generation of microbiome research. These include interactive spatial and temporal analysis and visualization tools, support for metabolomics and shotgun metagenomics analysis, and automated data provenance tracking to ensure reproducible, transparent microbiome data science.",poster,cp110
p14,ede0a8039a561905f40777ec2ae66c2010e3f2bc,j8,Journal of Big Data,Cybersecurity data science: an overview from machine learning perspective,Abstract content,fullPaper,jv8
p15,0751d2fa3a54cbbb4d594f2ee47c3aa7e4003a24,j9,IEEE Transactions on Artificial Intelligence,Leveraging Data Science to Combat COVID-19: A Comprehensive Review,"COVID-19, an infectious disease caused by the SARS-CoV-2 virus, was declared a pandemic by the World Health Organisation (WHO) in March 2020. By mid-August 2020, more than 21 million people have tested positive worldwide. Infections have been growing rapidly and tremendous efforts are being made to fight the disease. In this paper, we attempt to systematise the various COVID-19 research activities leveraging data science, where we define data science broadly to encompass the various methods and tools—including those from artificial intelligence (AI), machine learning (ML), statistics, modeling, simulation, and data visualization—that can be used to store, process, and extract insights from data. In addition to reviewing the rapidly growing body of recent research, we survey public datasets and repositories that can be used for further work to track COVID-19 spread and mitigation strategies. As part of this, we present a bibliometric analysis of the papers produced in this short span of time. Finally, building on these insights, we highlight common challenges and pitfalls observed across the surveyed works. We also created a live resource repository at https://github.com/Data-Science-and-COVID-19/Leveraging-Data-Science-To-Combat-COVID-19-A-Comprehensive-Review that we intend to keep updated with the latest resources including new papers and datasets.",fullPaper,jv9
p16,72d3ddf1f7210d7e70144bbc09f770ec411fe909,c41,Software Product Lines Conference,"Machine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence","Smarter applications are making better use of the insights gleaned from data, having an impact on every industry and research discipline. At the core of this revolution lies the tools and the methods that are driving it, from processing the massive piles of data generated each day to learning from and taking useful action. Deep neural networks, along with advancements in classical machine learning and scalable general-purpose graphics processing unit (GPU) computing, have become critical components of artificial intelligence, enabling many of these astounding breakthroughs and lowering the barrier to adoption. Python continues to be the most preferred language for scientific computing, data science, and machine learning, boosting both performance and productivity by enabling the use of low-level libraries and clean high-level APIs. This survey offers insight into the field of machine learning with Python, taking a tour through important topics to identify some of the core hardware and software paradigms that have enabled it. We cover widely-used libraries and concepts, collected together for holistic comparison, with the goal of educating the reader and driving the field of Python machine learning forward.",poster,cp41
p17,ab8ba0f2d290a8e56eb61e10027d0b2e57d2d544,c39,International Conference on Global Software Engineering,"How do Data Science Workers Collaborate? Roles, Workflows, and Tools","Today, the prominence of data science within organizations has given rise to teams of data science workers collaborating on extracting insights from data, as opposed to individual data scientists working alone. However, we still lack a deep understanding of how data science workers collaborate in practice. In this work, we conducted an online survey with 183 participants who work in various aspects of data science. We focused on their reported interactions with each other (e.g., managers with engineers) and with different tools (e.g., Jupyter Notebook). We found that data science teams are extremely collaborative and work with a variety of stakeholders and tools during the six common steps of a data science workflow (e.g., clean data and train model). We also found that the collaborative practices workers employ, such as documentation, vary according to the kinds of tools they use. Based on these findings, we discuss design implications for supporting data science team collaborations and future research directions.",poster,cp39
p18,8bba999de25bfb288b3f7f88e1d907aab02638b6,j10,Chemical Reviews,Big-Data Science in Porous Materials: Materials Genomics and Machine Learning,"By combining metal nodes with organic linkers we can potentially synthesize millions of possible metal–organic frameworks (MOFs). The fact that we have so many materials opens many exciting avenues but also create new challenges. We simply have too many materials to be processed using conventional, brute force, methods. In this review, we show that having so many materials allows us to use big-data methods as a powerful technique to study these materials and to discover complex correlations. The first part of the review gives an introduction to the principles of big-data science. We show how to select appropriate training sets, survey approaches that are used to represent these materials in feature space, and review different learning architectures, as well as evaluation and interpretation strategies. In the second part, we review how the different approaches of machine learning have been applied to porous materials. In particular, we discuss applications in the field of gas storage and separation, the stability of these materials, their electronic properties, and their synthesis. Given the increasing interest of the scientific community in machine learning, we expect this list to rapidly expand in the coming years.",fullPaper,jv10
p19,2d6adb9636df5a8a5dbcbfaecd0c4d34d7c85034,c11,Hawaii International Conference on System Sciences,Spectral Methods for Data Science: A Statistical Perspective,"Spectral methods have emerged as a simple yet surprisingly effective approach for extracting information from massive, noisy and incomplete data. In a nutshell, spectral methods refer to a collection of algorithms built upon the eigenvalues (resp. singular values) and eigenvectors (resp. singular vectors) of some properly designed matrices constructed from data. A diverse array of applications have been found in machine learning, data science, and signal processing. Due to their simplicity and effectiveness, spectral methods are not only used as a stand-alone estimator, but also frequently employed to initialize other more sophisticated algorithms to improve performance. 
While the studies of spectral methods can be traced back to classical matrix perturbation theory and methods of moments, the past decade has witnessed tremendous theoretical advances in demystifying their efficacy through the lens of statistical modeling, with the aid of non-asymptotic random matrix theory. This monograph aims to present a systematic, comprehensive, yet accessible introduction to spectral methods from a modern statistical perspective, highlighting their algorithmic implications in diverse large-scale applications. In particular, our exposition gravitates around several central questions that span various applications: how to characterize the sample efficiency of spectral methods in reaching a target level of statistical accuracy, and how to assess their stability in the face of random noise, missing data, and adversarial corruptions? In addition to conventional $\ell_2$ perturbation analysis, we present a systematic $\ell_{\infty}$ and $\ell_{2,\infty}$ perturbation theory for eigenspace and singular subspaces, which has only recently become available owing to a powerful ""leave-one-out"" analysis framework.",poster,cp11
p20,f42c69dbd792155fee6f4d2c525971f8d43f138b,c73,Workshop on Algorithms in Bioinformatics,Finding Related Tables in Data Lakes for Interactive Data Science,"Many modern data science applications build on data lakes, schema-agnostic repositories of data files and data products that offer limited organization and management capabilities. There is a need to build data lake search capabilities into data science environments, so scientists and analysts can find tables, schemas, workflows, and datasets useful to their task at hand. We develop search and management solutions for the Jupyter Notebook data science platform, to enable scientists to augment training data, find potential features to extract, clean data, and find joinable or linkable tables. Our core methods also generalize to other settings where computational tasks involve execution of programs or scripts.",poster,cp73
p21,12f62537251cf8eb76fa11c59df68d2211008898,j11,International Journal of Digital Earth,Big Earth Data science: an information framework for a sustainable planet,"ABSTRACT The digital transformation of our society coupled with the increasing exploitation of natural resources makes sustainability challenges more complex and dynamic than ever before. These changes will unlikely stop or even decelerate in the near future. There is an urgent need for a new scientific approach and an advanced form of evidence-based decision-making towards the benefit of society, the economy, and the environment. To understand the impacts and interrelationships between humans as a society and natural Earth system processes, we propose a new engineering discipline, Big Earth Data science. This science is called to provide the methodologies and tools to generate knowledge from diverse, numerous, and complex data sources necessary to ensure a sustainable human society essential for the preservation of planet Earth. Big Earth Data science aims at utilizing data from Earth observation and social sensing and develop theories for understanding the mechanisms of how such a social-physical system operates and evolves. The manuscript introduces the universe of discourse characterizing this new science, its foundational paradigms and methodologies, and a possible technological framework to be implemented by applying an ecosystem approach. CASEarth and GEOSS are presented as examples of international implementation attempts. Conclusions discuss important challenges and collaboration opportunities.",fullPaper,jv11
p22,a4b6f802b3f416fb1af6d723e0549c5e6d34faae,c77,Networks,Data science in economics: comprehensive review of advanced machine learning and deep learning methods,"This paper provides a state-of-the-art investigation of advances in data science in emerging economic applications. The analysis was performed on novel data science methods in four individual classes of deep learning models, hybrid deep learning models, hybrid machine learning, and ensemble models. Application domains include a wide and diverse range of economics research from the stock market, marketing, and e-commerce to corporate banking and cryptocurrency. Prisma method, a systematic literature review methodology, was used to ensure the quality of the survey. The findings reveal that the trends follow the advancement of hybrid models, which, based on the accuracy metric, outperform other learning algorithms. It is further expected that the trends will converge toward the advancements of sophisticated hybrid deep learning models.",poster,cp77
p23,5b9ea2abf1c5a04b3024367409284edceb741ef2,c48,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",A new paradigm for accelerating clinical data science at Stanford Medicine,"Stanford Medicine is building a new data platform for our academic research community to do better clinical data science. Hospitals have a large amount of patient data and researchers have demonstrated the ability to reuse that data and AI approaches to derive novel insights, support patient care, and improve care quality. However, the traditional data warehouse and Honest Broker approaches that are in current use, are not scalable. We are establishing a new secure Big Data platform that aims to reduce time to access and analyze data. In this platform, data is anonymized to preserve patient data privacy and made available preparatory to Institutional Review Board (IRB) submission. Furthermore, the data is standardized such that analysis done at Stanford can be replicated elsewhere using the same analytical code and clinical concepts. Finally, the analytics data warehouse integrates with a secure data science computational facility to support large scale data analytics. The ecosystem is designed to bring the modern data science community to highly sensitive clinical data in a secure and collaborative big data analytics environment with a goal to enable bigger, better and faster science.",poster,cp48
p24,82620503cacf8ff6f8f3490e7bdf7508f1ab2021,j12,Journal of Geographical Systems,Opening practice: supporting reproducibility and critical spatial data science,Abstract content,fullPaper,jv12
p25,deb4e0c46f2e389ec5e4528f9dcee643bb6a15fa,j13,IEEE Transactions on Signal Processing,Fixed Point Strategies in Data Science,"The goal of this article is to promote the use of fixed point strategies in data science by showing that they provide a simplifying and unifying framework to model, analyze, and solve a great variety of problems. They are seen to constitute a natural environment to explain the behavior of advanced convex optimization methods as well as of recent nonlinear methods in data science which are formulated in terms of paradigms that go beyond minimization concepts and involve constructs such as Nash equilibria or monotone inclusions. We review the pertinent tools of fixed point theory and describe the main state-of-the-art algorithms for provenly convergent fixed point construction. We also incorporate additional ingredients such as stochasticity, block-implementations, and non-Euclidean metrics, which provide further enhancements. Applications to signal and image processing, machine learning, statistics, neural networks, and inverse problems are discussed.",fullPaper,jv13
p26,0669286d8d4ca8ec2fdf16b7813157c21eb690be,j14,Scientific Data,Heidelberg colorectal data set for surgical data science in the sensor operating room,Abstract content,fullPaper,jv14
p27,648ba966b63975c6859e1948ae3ddc30053884e4,c80,International Conference on Learning Representations,Making data science systems work,"How are data science systems made to work? It may seem that whether a system works is a function of its technical design, but it is also accomplished through ongoing forms of discretionary work by many actors. Based on six months of ethnographic fieldwork with a corporate data science team, we describe how actors involved in a corporate project negotiated what work the system should do, how it should work, and how to assess whether it works. These negotiations laid the foundation for how, why, and to what extent the system ultimately worked. We describe three main findings. First, how already-existing technologies are essential reference points to determine how and whether systems work. Second, how the situated resolution of development challenges continually reshapes the understanding of how and whether systems work. Third, how business goals, and especially their negotiated balance with data science imperatives, affect a system’s working. We conclude with takeaways for critical data studies, orienting researchers to focus on the organizational and cultural aspects of data science, the third-party platforms underlying data science systems, and ways to engage with practitioners’ imagination of how systems can and should work.",poster,cp80
p28,41cf91ee13a1d15983ede066ddf6b67cc94a41f4,c29,International Conference on Software Engineering,The Role of Academia in Data Science Education,"As the demand for data scientists continues to grow, universities are trying to figure out how to best contribute to the training of a workforce. However, there does not appear to be a consensus on the fundamental principles, expertise, skills, or knowledge-base needed to define an academic discipline. We argue that data science is not a discipline but rather an umbrella term used to describe a complex process involving not one data scientist possessing all the necessary expertise, but a team of data scientists with nonoverlapping complementary skills. We provide some recommendations for how to take this into account when designing data science academic programs.Keywords: applied statistics, data science, data science curriculum, data wrangling, machine learning, software engineering",poster,cp29
p29,d83a99bfb6f81565a186e0eb86858864568c1327,c114,IEEE International Conference on Robotics and Automation,Data Science: Challenges and Directions,"While data science has emerged as a contentious new scientific field, enormous debates and discussions have been made on it why we need data science and what makes it as a science. However, only a limited number of discussions are about intrinsic complexities and intelligence embedded in data science problems, and the gaps and opportunities for disciplinary directions. After a comprehensive review [5, 6, 9, 10, 12, 15, 18] of hundreds of literature that directly incorporates data science in their scopes, we make the following observations of the big data buzz and data science debate:",poster,cp114
p30,4b5505a54799d796ae94115409b01ee33a7e2b20,j15,Journal of Epidemiology and Community Health,Glossary for public health surveillance in the age of data science,"Public health surveillance is the ongoing systematic collection, analysis and interpretation of data, closely integrated with the timely dissemination of the resulting information to those responsible for preventing and controlling disease and injury. With the rapid development of data science, encompassing big data and artificial intelligence, and with the exponential growth of accessible and highly heterogeneous health-related data, from healthcare providers to user-generated online content, the field of surveillance and health monitoring is changing rapidly. It is, therefore, the right time for a short glossary of key terms in public health surveillance, with an emphasis on new data-science developments in the field.",fullPaper,jv15
p31,79be83a308a9a75ef4e64f63a938b201531c0bbf,c46,Brazilian Symposium on Software Engineering,Data Science: A Comprehensive Overview,Abstract content,poster,cp46
p32,b85ac20631159ca3e370afa9c1f81a4618242b4f,j16,American Statistician,The Democratization of Data Science Education,"Abstract Over the last three decades, data have become ubiquitous and cheap. This transition has accelerated over the last five years and training in statistics, machine learning, and data analysis has struggled to keep up. In April 2014, we launched a program of nine courses, the Johns Hopkins Data Science Specialization, which has now had more than 4 million enrollments over the past five years. Here, the program is described and compared to standard data science curricula as they were organized in 2014 and 2015. We show that novel pedagogical and administrative decisions introduced in our program are now standard in online data science programs. The impact of the Data Science Specialization on data science education in the U.S. is also discussed. Finally, we conclude with some thoughts about the future of data science education in a data democratized world.",fullPaper,jv16
p33,b017bf6879e57077b4b4e180a02747b89878d7a1,j17,Journal of Statistics and Data Science Education,A Fresh Look at Introductory Data Science,"ABSTRACT The proliferation of vast quantities of available datasets that are large and complex in nature has challenged universities to keep up with the demand for graduates trained in both the statistical and the computational set of skills required to effectively plan, acquire, manage, analyze, and communicate the findings of such data. To keep up with this demand, attracting students early on to data science as well as providing them a solid foray into the field becomes increasingly important. We present a case study of an introductory undergraduate course in data science that is designed to address these needs. Offered at Duke University, this course has no prerequisites and serves a wide audience of aspiring statistics and data science majors as well as humanities, social sciences, and natural sciences students. We discuss the unique set of challenges posed by offering such a course, and in light of these challenges, we present a detailed discussion into the pedagogical design elements, content, structure, computational infrastructure, and the assessment methodology of the course. We also offer a repository containing all teaching materials that are open-source, along with supplementary materials and the R code for reproducing the figures found in the article.",fullPaper,jv17
p34,c016852106ac787678105fd9dd22e57ba620517c,j18,Patterns,Human Data Science,Abstract content,fullPaper,jv18
p35,62a9d4f1763c5071cb2476c100614ba9741f036b,c41,Software Product Lines Conference,Data science applications to string theory,Abstract content,poster,cp41
p36,05ceac747bec286129818e9e2fbd37ef034f5ada,c94,Vision,Interrogating Data Science,"Data science provides powerful tools and methods. CSCW researchers have contributed insightfulstudies of conventional work-practices in data science - and particularly machine learning. However,recent research has shown that human skills and collaborative decision-making, play important rolesin defining data, acquiring data, curating data, designing data, and creating data. This workshopgathers researchers and practitioners together to take a collective and critical look at data sciencework-practices, and at how those work-practices make crucial and often invisible impacts on theformal work of data science. When we understand the human and social contributions to data sciencepipelines, we can constructively redesign both work and technologies for new insights, theories, andchallenges.",poster,cp94
p37,d88d39dd9c910105e7503aa43698c806d42d5198,c110,IEEE International Conference on Automatic Face & Gesture Recognition,Statistical Foundations of Data Science,Abstract content,poster,cp110
p38,579b64d2179a58a8bc586c30850ea238d3c14164,j1,IEEE Transactions on Knowledge and Data Engineering,A Survey on Data Pricing: From Economics to Data Science,"Data are invaluable. How can we assess the value of data objectively, systematically and quantitatively? Pricing data, or information goods in general, has been studied and practiced in dispersed areas and principles, such as economics, marketing, electronic commerce, data management, data mining and machine learning. In this article, we present a unified, interdisciplinary and comprehensive overview of this important direction. We examine various motivations behind data pricing, understand the economics of data pricing and review the development and evolution of pricing models according to a series of fundamental principles. We discuss both digital products and data products. We also consider a series of challenges and directions for future work.",fullPaper,jv1
p39,398b154013db9d8025bf60f910bc156dedd9b40e,c0,International Conference on Human Factors in Computing Systems,"How Data Science Workers Work with Data: Discovery, Capture, Curation, Design, Creation","With the rise of big data, there has been an increasing need for practitioners in this space and an increasing opportunity for researchers to understand their workflows and design new tools to improve it. Data science is often described as data-driven, comprising unambiguous data and proceeding through regularized steps of analysis. However, this view focuses more on abstract processes, pipelines, and workflows, and less on how data science workers engage with the data. In this paper, we build on the work of other CSCW and HCI researchers in describing the ways that scientists, scholars, engineers, and others work with their data, through analyses of interviews with 21 data science professionals. We set five approaches to data along a dimension of interventions: Data as given; as captured; as curated; as designed; and as created. Data science workers develop an intuitive sense of their data and processes, and actively shape their data. We propose new ways to apply these interventions analytically, to make sense of the complex activities around data practices.",fullPaper,cp0
p40,8ece479b5dfed4727d2d9b9763f777bb9a94096e,c112,Very Large Data Bases Conference,Human-AI Collaboration in Data Science,"The rapid advancement of artificial intelligence (AI) is changing our lives in many ways. One application domain is data science. New techniques in automating the creation of AI, known as AutoAI or AutoML, aim to automate the work practices of data scientists. AutoAI systems are capable of autonomously ingesting and pre-processing data, engineering new features, and creating and scoring models based on a target objectives (e.g. accuracy or run-time efficiency). Though not yet widely adopted, we are interested in understanding how AutoAI will impact the practice of data science. We conducted interviews with 20 data scientists who work at a large, multinational technology company and practice data science in various business settings. Our goal is to understand their current work practices and how these practices might change with AutoAI. Reactions were mixed: while informants expressed concerns about the trend of automating their jobs, they also strongly felt it was inevitable. Despite these concerns, they remained optimistic about their future job security due to a view that the future of data science work will be a collaboration between humans and AI systems, in which both automation and human expertise are indispensable.",poster,cp112
p41,a11e157cb828b800426223f0a3d79e8fb122c8cc,c72,Intelligent Systems in Molecular Biology,Process Mining for Python (PM4Py): Bridging the Gap Between Process- and Data Science,"Process mining, i.e., a sub-field of data science focusing on the analysis of event data generated during the execution of (business) processes, has seen a tremendous change over the past two decades. Starting off in the early 2000's, with limited to no tool support, nowadays, several software tools, i.e., both open-source, e.g., ProM and Apromore, and commercial, e.g., Disco, Celonis, ProcessGold, etc., exist. The commercial process mining tools provide limited support for implementing custom algorithms. Moreover, both commercial and open-source process mining tools are often only accessible through a graphical user interface, which hampers their usage in large-scale experimental settings. Initiatives such as RapidProM provide process mining support in the scientific workflow-based data science suite RapidMiner. However, these offer limited to no support for algorithmic customization. In the light of the aforementioned, in this paper, we present a novel process mining library, i.e. Process Mining for Python (PM4Py) that aims to bridge this gap, providing integration with state-of-the-art data science libraries, e.g., pandas, numpy, scipy and scikit-learn. We provide a global overview of the architecture and functionality of PM4Py, accompanied by some representative examples of its usage.",poster,cp72
p42,e2055b85dab66c922ccf25a28046e8e559074824,j19,Computer/law journal,Algorithmic Government: Automating Public Services and Supporting Civil Servants in using Data Science Technologies,"The data science technologies of artificial intelligence (AI), Internet of Things (IoT), big data and behavioral/predictive analytics, and blockchain are poised to revolutionize government and create a new generation of GovTech start-ups. The impact from the ‘smartification’ of public services and the national infrastructure will be much more significant in comparison to any other sector given government's function and importance to every institution and individual. Potential GovTech systems include Chatbots and intelligent assistants for public engagement, Robo-advisors to support civil servants, real-time management of the national infrastructure using IoT and blockchain, automated compliance/regulation, public records securely stored in blockchain distributed ledgers, online judicial and dispute resolution systems, and laws/statutes encoded as blockchain smart contracts. Government is potentially the major ‘client’ and also ‘public champion’ for these new data technologies. This review paper uses our simple taxonomy of government services to provide an overview of data science automation being deployed by governments world-wide. The goal of this review paper is to encourage the Computer Science community to engage with government to develop these new systems to transform public services and support the work of civil servants.",fullPaper,jv19
p43,3b16bcb226bb1c87a6e63e0658be30067ed03f57,c105,Biometrics and Identity Management,A Systematic Review on Supervised and Unsupervised Machine Learning Algorithms for Data Science,Abstract content,poster,cp105
p44,27b9d1182e913decc7ef6a3509245fa6b6fd509d,j20,Proceedings of the National Academy of Sciences of the United States of America,Veridical data science,"Significance Predictability, computability, and stability (PCS) are three core principles of data science. They embed the scientific principles of prediction and replication in data-driven decision making while recognizing the central role of computation. Based on these principles, we propose the PCS framework, including workflow and documentation (in R Markdown or Jupyter Notebook). The PCS framework aims at responsible, reliable, reproducible, and transparent analysis across fields of science, social science, engineering, business, and government. It can be used as a recommendation system for scientific hypothesis generation and experimental design. In particular, we propose (basic) PCS inference for reliability measures on data results, extending statistical inference to a much broader scope as current data science practice entails. Building and expanding on principles of statistics, machine learning, and scientific inquiry, we propose the predictability, computability, and stability (PCS) framework for veridical data science. Our framework, composed of both a workflow and documentation, aims to provide responsible, reliable, reproducible, and transparent results across the data science life cycle. The PCS workflow uses predictability as a reality check and considers the importance of computation in data collection/storage and algorithm design. It augments predictability and computability with an overarching stability principle. Stability expands on statistical uncertainty considerations to assess how human judgment calls impact data results through data and model/algorithm perturbations. As part of the PCS workflow, we develop PCS inference procedures, namely PCS perturbation intervals and PCS hypothesis testing, to investigate the stability of data results relative to problem formulation, data cleaning, modeling decisions, and interpretations. We illustrate PCS inference through neuroscience and genomics projects of our own and others. Moreover, we demonstrate its favorable performance over existing methods in terms of receiver operating characteristic (ROC) curves in high-dimensional, sparse linear model simulations, including a wide range of misspecified models. Finally, we propose PCS documentation based on R Markdown or Jupyter Notebook, with publicly available, reproducible codes and narratives to back up human choices made throughout an analysis. The PCS workflow and documentation are demonstrated in a genomics case study available on Zenodo.",fullPaper,jv20
p45,89f41c87c8849ce37e609c1010087291a4679a37,j21,Philosophical Transactions of the Royal Society of London. Biological Sciences,Outbreak analytics: a developing data science for informing the response to emerging pathogens,"Despite continued efforts to improve health systems worldwide, emerging pathogen epidemics remain a major public health concern. Effective response to such outbreaks relies on timely intervention, ideally informed by all available sources of data. The collection, visualization and analysis of outbreak data are becoming increasingly complex, owing to the diversity in types of data, questions and available methods to address them. Recent advances have led to the rise of outbreak analytics, an emerging data science focused on the technological and methodological aspects of the outbreak data pipeline, from collection to analysis, modelling and reporting to inform outbreak response. In this article, we assess the current state of the field. After laying out the context of outbreak response, we critically review the most common analytics components, their inter-dependencies, data requirements and the type of information they can provide to inform operations in real time. We discuss some challenges and opportunities and conclude on the potential role of outbreak analytics for improving our understanding of, and response to outbreaks of emerging pathogens. This article is part of the theme issue ‘Modelling infectious disease outbreaks in humans, animals and plants: epidemic forecasting and control‘. This theme issue is linked with the earlier issue ‘Modelling infectious disease outbreaks in humans, animals and plants: approaches and important themes’.",fullPaper,jv21
p46,1ec4d0e29455e47245edaa17368257df3efb6562,c0,International Conference on Human Factors in Computing Systems,"Practitioners Teaching Data Science in Industry and Academia: Expectations, Workflows, and Challenges","Data science has been growing in prominence across both academia and industry, but there is still little formal consensus about how to teach it. Many people who currently teach data science are practitioners such as computational researchers in academia or data scientists in industry. To understand how these practitioner-instructors pass their knowledge onto novices and how that contrasts with teaching more traditional forms of programming, we interviewed 20 data scientists who teach in settings ranging from small-group workshops to large online courses. We found that: 1) they must empathize with a diverse array of student backgrounds and expectations, 2) they teach technical workflows that integrate authentic practices surrounding code, data, and communication, 3) they face challenges involving authenticity versus abstraction in software setup, finding and curating pedagogically-relevant datasets, and acclimating students to live with uncertainty in data analysis. These findings can point the way toward better tools for data science education and help bring data literacy to more people around the world.",fullPaper,cp0
p47,f9e0e85732f0736c0d5a6f0c63df5c7f1f245dcd,j22,BMC Medicine,From hype to reality: data science enabling personalized medicine,Abstract content,fullPaper,jv22
p48,678da221aa156807bc2c191ed5f4bcbb0b25d421,j23,Ethics and Information Technology,Data science ethical considerations: a systematic literature review and proposed project framework,Abstract content,fullPaper,jv23
p49,0c86e8d19d0fc62a5f829ea625ffd3e7fa9551b9,j24,Metabolomics,Toward collaborative open data science in metabolomics using Jupyter Notebooks and cloud computing,Abstract content,fullPaper,jv24
p50,129a5609d4b00ae86f583ffbc1b3680234601b46,j25,Journal of Medical Internet Research,Health Care and Precision Medicine Research: Analysis of a Scalable Data Science Platform,"Background Health care data are increasing in volume and complexity. Storing and analyzing these data to implement precision medicine initiatives and data-driven research has exceeded the capabilities of traditional computer systems. Modern big data platforms must be adapted to the specific demands of health care and designed for scalability and growth. Objective The objectives of our study were to (1) demonstrate the implementation of a data science platform built on open source technology within a large, academic health care system and (2) describe 2 computational health care applications built on such a platform. Methods We deployed a data science platform based on several open source technologies to support real-time, big data workloads. We developed data-acquisition workflows for Apache Storm and NiFi in Java and Python to capture patient monitoring and laboratory data for downstream analytics. Results Emerging data management approaches, along with open source technologies such as Hadoop, can be used to create integrated data lakes to store large, real-time datasets. This infrastructure also provides a robust analytics platform where health care and biomedical research data can be analyzed in near real time for precision medicine and computational health care use cases. Conclusions The implementation and use of integrated data science platforms offer organizations the opportunity to combine traditional datasets, including data from the electronic health record, with emerging big data sources, such as continuous patient monitoring and real-time laboratory results. These platforms can enable cost-effective and scalable analytics for the information that will be key to the delivery of precision medicine initiatives. Organizations that can take advantage of the technical advances found in data science platforms will have the opportunity to provide comprehensive access to health care data for computational health care and precision medicine research.",fullPaper,jv25
p51,5c8b7127ad0b5257f81ce1aa70b89faa97bbc211,j26,Frontiers in Environmental Science,Data Science of the Natural Environment: A Research Roadmap,"Data science is the science of extracting meaning from potentially complex data. This is a fast moving field, drawing principles and techniques from a number of different disciplinary areas including computer science, statistics and complexity science. Data science is having a profound impact on a number of areas including commerce, health and smart cities. This paper argues that data science can have an equal if not greater impact in the area of earth and environmental sciences, offering a rich tapestry of new techniques to support both a deeper understanding of the natural environment in all its complexities, as well as the development of well-founded mitigation and adaptation strategies in the face of climate change. The paper argues that data science for the natural environment brings about new challenges for data science, particularly around complexity, spatial and temporal reasoning, and managing uncertainty. The paper also describes a case study in environmental data science which offers up insights into the promise of the area. The paper concludes with a research roadmap highlighting ten top challenges of environmental data science and also an invitation to become part of an international community working collaboratively on these problems.",fullPaper,jv26
p52,2081ed6854290a479f796f2432c7951ff24232fe,c100,ACM SIGMOD Conference,Human-Centered Study of Data Science Work Practices,"With the rise of big data, there has been an increasing need to understand who is working in data science and how they are doing their work. HCI and CSCW researchers have begun to examine these questions. In this workshop, we invite researchers to share their observations, experiences, hypotheses, and insights, in the hopes of developing a taxonomy of work practices and open issues in the behavioral and social study of data science and data science workers.",poster,cp100
p53,f31d47dcf3788f9d3fcfdddbe303f806f4a63768,c78,Neural Information Processing Systems,Data Science,"The presence of new science does not necessarily occur just like that. Every science starts from interests, discussion, and looks for a basic foundation, but in general the main foundation of science is mathematics. Data science includes structured and systematic knowledge about data. However, many other sciences that has a relationship with the data in question, ranging from statistics to computer science. This paper aims to reveal the obstacle and limitations of other science into a data science completely, on that basis the definition of data sciences needs to be elaborated, then confirm data science as new science and not depend directly on several other sciences.",poster,cp78
p54,f56425ec56586dcfd2694ab83643e9e76f314e91,c8,The Compass,50 Years of Data Science,"ABSTRACT More than 50 years ago, John Tukey called for a reformation of academic statistics. In “The Future of Data Analysis,” he pointed to the existence of an as-yet unrecognized science, whose subject of interest was learning from data, or “data analysis.” Ten to 20 years ago, John Chambers, Jeff Wu, Bill Cleveland, and Leo Breiman independently once again urged academic statistics to expand its boundaries beyond the classical domain of theoretical statistics; Chambers called for more emphasis on data preparation and presentation rather than statistical modeling; and Breiman called for emphasis on prediction rather than inference. Cleveland and Wu even suggested the catchy name “data science” for this envisioned field. A recent and growing phenomenon has been the emergence of “data science” programs at major universities, including UC Berkeley, NYU, MIT, and most prominently, the University of Michigan, which in September 2015 announced a $100M “Data Science Initiative” that aims to hire 35 new faculty. Teaching in these new programs has significant overlap in curricular subject matter with traditional statistics courses; yet many academic statisticians perceive the new programs as “cultural appropriation.” This article reviews some ingredients of the current “data science moment,” including recent commentary about data science in the popular media, and about how/whether data science is really different from statistics. The now-contemplated field of data science amounts to a superset of the fields of statistics and machine learning, which adds some technology for “scaling up” to “big data.” This chosen superset is motivated by commercial rather than intellectual developments. Choosing in this way is likely to miss out on the really important intellectual event of the next 50 years. Because all of science itself will soon become data that can be mined, the imminent revolution in data science is not about mere “scaling up,” but instead the emergence of scientific studies of data analysis science-wide. In the future, we will be able to predict how a proposal to change data analysis workflows would impact the validity of data analysis across all of science, even predicting the impacts field-by-field. Drawing on work by Tukey, Cleveland, Chambers, and Breiman, I present a vision of data science based on the activities of people who are “learning from data,” and I describe an academic field dedicated to improving that activity in an evidence-based manner. This new field is a better academic enlargement of statistics and machine learning than today’s data science initiatives, while being able to accommodate the same short-term goals. Based on a presentation at the Tukey Centennial Workshop, Princeton, NJ, September 18, 2015.",poster,cp8
p55,e564e3656395782d0ab9f801bfbe9f9f1a5d34a7,j27,Journal of Library and Information Sciences,Data science in data librarianship: Core competencies of a data librarian,"Currently, data are stored in an always-on condition, and can be globally accessed at any point, by any user. Data librarianship has its origins in the social sciences. In particular, the creation of data services and data archives, in the United Kingdom (Data Archives Services) and in the United States and Canada (Data Library Services), is a key factor for the emergence of data librarianship. The focus of data librarianship nowadays is on the creation of new library services. Data librarians are concerned with the proposition of services for data management and curation in academic libraries and other research organizations. The purpose of this paper is to understand how the complexity of the data can serve as the basis for identifying the technical skills required by data librarians. This essay is systematically divided, first introducing the concepts of data and research data in data librarianship, followed by an overview of data science as a theory, method, and technology to assess data. Next, the identification of the competencies and skills required by data scientists and data librarians are discussed. Our final remarks highlight that data librarians should understand that the complexity and novelty associated with data science praxis. Data science provides new methods and practices for data librarianship. A data librarian need not become a programmer, statistician, or database manager, but should be interested in learning about the languages and programming logic of computers, databases, and information retrieval tools. We believe that numerous kinds of scientific data research provide opportunities for a data librarian to engage with data science.",fullPaper,jv27
p56,590ead4aeddbf8fea8414998b2dc3b74576a71cb,j28,CHANCE : New Directions for Statistics and Computing,A Second Chance to Get Causal Inference Right: A Classification of Data Science Tasks,"Causal inference from observational data is the goal of many data analyses in the health and social sciences. However, academic statistics has often frowned upon data analyses with a causal objective. The introduction of the term ""data science"" provides a historic opportunity to redefine data analysis in such a way that it naturally accommodates causal inference from observational data. Like others before, we organize the scientific contributions of data science into three classes of tasks: Description, prediction, and counterfactual prediction (which includes causal inference). An explicit classification of data science tasks is necessary to discuss the data, assumptions, and analytics required to successfully accomplish each task. We argue that a failure to adequately describe the role of subject-matter expert knowledge in data analysis is a source of widespread misunderstandings about data science. Specifically, causal analyses typically require not only good data and algorithms, but also domain expert knowledge. We discuss the implications for the use of data science to guide decision-making in the real world and to train data scientists.",fullPaper,jv28
p57,e799d31e1c2d80a971c1f956d62b98c0a9f27031,j29,British Journal of Educational Technology,Big Data and data science: A critical review of issues for educational research,"Big Data refers to large and disparate volumes of data generated by people, applications and machines. It is gaining increasing attention from a variety of domains, including education. What are the challenges of engaging with Big Data research in education? This paper identifies a wide range of critical issues that researchers need to consider when working with Big Data in education. The issues identified include diversity in the conception and meaning of Big Data in education, ontological, epistemological disparity, technical challenges, ethics and privacy, digital divide and digital dividend, lack of expertise and academic development opportunities to prepare educational researchers to leverage opportunities afforded by Big Data. The goal of this paper is to raise awareness on these issues and initiate a dialogue. The paper was inspired partly by insights drawn from the literature but mostly informed by experience researching into Big Data in education. [ABSTRACT FROM AUTHOR]",fullPaper,jv29
p58,005cb0304960c5249579f3aa39a47e2cb164c0b6,j5,Genome Biology,Genomics and data science: an application within an umbrella,Abstract content,fullPaper,jv5
p59,eaa3bbe9e3c52781fd84149d8ee6e2670c90e5ec,c31,International Conference on Evaluation & Assessment in Software Engineering,Bayesian Optimization and Data Science,Abstract content,poster,cp31
p60,f670069c81466f625d1d9a315e8cb419a79f2590,c15,International Conference on Conceptual Structures,Process Mining: Data Science in Action,"This is the second edition of Wil van der Aalsts seminal book on process mining, which now discusses the field also in the broader context of data science and big data approaches. It includes several additions and updates, e.g. on inductive mining techniques, the notion of alignments, a considerably expanded section on software tools and a completely new chapter of process mining in the large. It is self-contained, while at the same time covering the entire process-mining spectrum from process discovery to predictive analytics. After a general introduction to data science and process mining in Part I, Part II provides the basics of business process modeling and data mining necessary to understand the remainder of the book. Next, Part III focuses on process discovery as the most important process mining task, while Part IV moves beyond discovering the control flow of processes, highlighting conformance checking, and organizational and time perspectives. Part V offers a guide to successfully applying process mining in practice, including an introduction to the widely used open-source tool ProM and several commercial products. Lastly, Part VI takes a step back, reflecting on the material presented and the key open challenges. Overall, this book provides a comprehensive overview of the state of the art in process mining. It is intended for business process analysts, business consultants, process managers, graduate students, and BPM researchers.",poster,cp15
p61,702cd9a7a128706b8a6ec88e7424e06c326021e5,c73,Workshop on Algorithms in Bioinformatics,Upscaling urban data science for global climate solutions,"Non-technical summary Manhattan, Berlin and New Delhi all need to take action to adapt to climate change and to reduce greenhouse gas emissions. While case studies on these cities provide valuable insights, comparability and scalability remain sidelined. It is therefore timely to review the state-of-the-art in data infrastructures, including earth observations, social media data, and how they could be better integrated to advance climate change science in cities and urban areas. We present three routes for expanding knowledge on global urban areas: mainstreaming data collections, amplifying the use of big data and taking further advantage of computational methods to analyse qualitative data to gain new insights. These data-based approaches have the potential to upscale urban climate solutions and effect change at the global scale. Technical summary Cities have an increasingly integral role in addressing climate change. To gain a common understanding of solutions, we require adequate and representative data of urban areas, including data on related greenhouse gas emissions, climate threats and of socio-economic contexts. Here, we review the current state of urban data science in the context of climate change, investigating the contribution of urban metabolism studies, remote sensing, big data approaches, urban economics, urban climate and weather studies. We outline three routes for upscaling urban data science for global climate solutions: 1) Mainstreaming and harmonizing data collection in cities worldwide; 2) Exploiting big data and machine learning to scale solutions while maintaining privacy; 3) Applying computational techniques and data science methods to analyse published qualitative information for the systematization and understanding of first-order climate effects and solutions. Collaborative efforts towards a joint data platform and integrated urban services would provide the quantitative foundations of the emerging global urban sustainability science.",poster,cp73
p62,0e23ff1f915b6af32bf1a1107ee7e15ebe10efe8,j30,"Annual review of political science (Palo Alto, Calif. Print)",The Challenge of Big Data and Data Science,"Big data and data science are transforming the world in ways that spawn new concerns for social scientists, such as the impacts of the internet on citizens and the media, the repercussions of smart cities, the possibilities of cyber-warfare and cyber-terrorism, the implications of precision medicine, and the consequences of artificial intelligence and automation. Along with these changes in society, powerful new data science methods support research using administrative, internet, textual, and sensor-audio-video data. Burgeoning data and innovative methods facilitate answering previously hard-to-tackle questions about society by offering new ways to form concepts from data, to do descriptive inference, to make causal inferences, and to generate predictions. They also pose challenges as social scientists must grasp the meaning of concepts and predictions generated by convoluted algorithms, weigh the relative value of prediction versus causal inference, and cope with ethical challenges as their methods, such as algorithms for mobilizing voters or determining bail, are adopted by policy makers.",fullPaper,jv30
p63,c0b1eedfa2031a69fbdf02a4abc8a741faf6a912,c76,International Conference on Artificial Neural Networks,INTRODUCTION TO DATA SCIENCE,データサイエンスの概説 データ獲得法 国と統計データ データの形式と分析の準備 データの変換と加工 データをグラフに表す 統計量によるデータの要約 ２次元の量的データの記述 回帰分析の基礎 質的データの分析 社会現象データの分析事例 データ解析のソフトウェア,poster,cp76
p64,a9e447d4d6b91f75ac4d8e336c609cfcbcbcfc0a,c66,Annual Conference on Innovation and Technology in Computer Science Education,Data Science,"Program of Study The technological revolution has led to an explosion of data in domains of knowledge including medicine, policy, social sciences, commerce, and the natural sciences. Petabytes of data are being collected from a myriad of instruments, like sequencing machines for genomics and mobile devices for quantifying social interactions. In addition to driving research, data are shaping the way people work, live, and communicate. Correspondingly, new methodologies have emerged to power intelligent systems, make more accurate predictions, and gain new insight using the large volumes of data generated by scientists, entrepreneurs, and analysts.",poster,cp66
p65,d6801d0ffd08ba56bccfa01884bb6a126f99de2e,c65,Formal Concept Analysis,"Our data, our society, our health: A vision for inclusive and transparent health data science in the United Kingdom and beyond","The last 6 years have seen sustained investment in health data science in the United Kingdom and beyond, which should result in a data science community that is inclusive of all stakeholders, working together to use data to benefit society through the improvement of public health and well‐being.",poster,cp65
p66,2ab2796390ac12df283e218907ed0ffef232dbc7,j31,The Journal of the Learning Sciences,Situating Data Science: Exploring How Relationships to Data Shape Learning,"The emerging field of Data Science has had a large impact on science and society. This has led to over a decade of calls to establish a corresponding field of Data Science Education. There is still a need, however, to more deeply conceptualize what a field of Data Science Education might entail in terms of scope, responsibility, and execution. This special issue explores how one distinguishing feature of Data Science—its focus on data collected from social and environmental contexts within which learners often find themselves deeply embedded—suggests serious implications for learning and education. The learning sciences is uniquely positioned to investigate how such contextual embeddings impact learners’ engagement with data including conceptual, experiential, communal, racialized, spatial, and political dimensions. This special issue demonstrates the richly layered relationships learners build with data and reveals them to be not merely utilitarian mechanisms for learning about data, but a critical part of navigating data as social text and understanding Data Science as a discipline. Together, the contributions offer a vision of how the learning sciences can contribute to a more expansive, agentive and socially aware Data Science Education.",fullPaper,jv31
p67,8d446e7af03d7c7f9fe5828b2d9939e23a3ed7b0,j32,Geographical Analysis,A Data Science Framework for Movement,"Author(s): Dodge, S | Abstract: © 2019 The Ohio State University Movement is the driving force behind the form and function of many ecological and human systems. Identification and analysis of movement patterns that may relate to the behavior of individuals and their interactions is a fundamental first step in understanding these systems. With advances in IoT and the ubiquity of smart connected sensors to collect movement and contextual data, we now have access to a wealth of geo-enriched high-resolution tracking data. These data promise new forms of knowledge and insight into movement of humans, animals, and goods, and hence can increase our understanding of complex spatiotemporal processes such as disease outbreak, urban mobility, migration, and human-species interaction. To take advantage of the evolution in our data, we need a revolution in how we visualize, model, and analyze movement as a multidimensional process that involves space, time, and context. This paper introduces a data science paradigm with the aim of advancing research on movement.",fullPaper,jv32
p68,863a35bdd1ae803491801e283c2ae79fe973cf68,j33,Journal of Biosciences,Microbiome data science,Abstract content,fullPaper,jv33
p69,747359803e9a734fa4f1338a83121a942f3da60e,j32,Geographical Analysis,Geographic Data Science,"It is widely acknowledged that the emergence of “Big Data” is having a profound and often controversial impact on the production of knowledge. In this context, Data Science has developed as an interdisciplinary approach that turns such “Big Data” into information. This article argues for the positive role that Geography can have on Data Science when being applied to spatially explicit problems; and inversely, makes the case that there is much that Geography and Geographical Analysis could learn from Data Science. We propose a deeper integration through an ambitious research agenda, including systems engineering, new methodological development, and work toward addressing some acute challenges around epistemology. We argue that such issues must be resolved in order to realize a Geographic Data Science, and that such goal would be a desirable one.",fullPaper,jv32
p70,c74844827c90ee9e575b4e3bfc2ad32e58c3e482,j34,Proceedings of the VLDB Endowment,Trinity: An Extensible Synthesis Framework for Data Science,"In this demo paper, we introduce Trinity, a general-purpose framework that can be used to quickly build domain-specific program synthesizers for automating many tedious tasks that arise in data science. We illustrate how Trinity can be used by three different users: First, we show how endusers can use Trinity’s built-in synthesizers to automate data wrangling tasks. Second, we show how advanced users can easily extend existing synthesizers to support additional functionalities. Third, we show how synthesis experts can change the underlying search engine in Trinity. Overall, this paper is intended to demonstrate how users can quickly use, modify, and extend the Trinity framework with the goal of automating many tasks that are considered to be the “janitor” work of data science. PVLDB Reference Format: Ruben Martins, Jia Chen, Yanju Chen, Yu Feng, and Isil Dillig. Trinity: An Extensible Synthesis Framework for Data Science. PVLDB, 12(12): 1914-1917, 2019. DOI: https://doi.org/10.14778/3352063.3352098",fullPaper,jv34
p71,4f0218eb9ed62d5acc03f02bfa24b388a66067e8,j35,TOP - An Official Journal of the Spanish Society of Statistics and Operations Research,Distance geometry and data science,Abstract content,fullPaper,jv35
p72,bb44d1472bb281c699ef556f6eb6ccc66889f2d3,c46,Brazilian Symposium on Software Engineering,Data Science and Machine Learning,"The purpose of Data Science and Machine Learning: Mathematical and Statistical Methods is to provide an accessible, yet comprehensive textbook intended for students interested in gaining a better understanding of the mathematics and statistics that underpin the rich variety of ideas and machine learning algorithms in data science.",poster,cp46
p73,46f1c45c62b7dbf77af405f5ddcf137b5e1ddde9,c102,International Conference on Biometrics,Data science from a library and information science perspective,"
Purpose
Data science is a relatively new field which has gained considerable attention in recent years. This new field requires a wide range of knowledge and skills from different disciplines including mathematics and statistics, computer science and information science. The purpose of this paper is to present the results of the study that explored the field of data science from the library and information science (LIS) perspective.


Design/methodology/approach
Analysis of research publications on data science was made on the basis of papers published in the Web of Science database. The following research questions were proposed: What are the main tendencies in publication years, document types, countries of origin, source titles, authors of publications, affiliations of the article authors and the most cited articles related to data science in the field of LIS? What are the main themes discussed in the publications from the LIS perspective?


Findings
The highest contribution to data science comes from the computer science research community. The contribution of information science and library science community is quite small. However, there has been continuous increase in articles from the year 2015. The main document types are journal articles, followed by conference proceedings and editorial material. The top three journals that publish data science papers from the LIS perspective are the Journal of the American Medical Informatics Association, the International Journal of Information Management and the Journal of the Association for Information Science and Technology. The top five countries publishing are USA, China, England, Australia and India. The most cited article has got 112 citations. The analysis revealed that the data science field is quite interdisciplinary by nature. In addition to the field of LIS the papers belonged to several other research areas. The reviewed articles belonged to the six broad categories: data science education and training; knowledge and skills of the data professional; the role of libraries and librarians in the data science movement; tools, techniques and applications of data science; data science from the knowledge management perspective; and data science from the perspective of health sciences.


Research limitations/implications
The limitations of this research are that this study only analyzed research papers in the Web of Science database and therefore only covers a certain amount of scientific papers published in the field of LIS. In addition, only publications with the term “data science” in the topic area of the Web of Science database were analyzed. Therefore, several relevant studies are not discussed in this paper that are not reflected in the Web of Science database or were related to other keywords such as “e-science,” “e-research,” “data service,” “data curation” or “research data management.”


Originality/value
The field of data science has not been explored using bibliographic analysis of publications from the perspective of the LIS. This paper helps to better understand the field of data science and the perspectives for information professionals.
",poster,cp102
p74,da1de6fa2d15e50c9e96811217e27a66313bd762,j36,Journal of Statistics Education,A First Course in Data Science,"Abstract Data science is a discipline that provides principles, methodology, and guidelines for the analysis of data for tools, values, or insights. Driven by a huge workforce demand, many academic institutions have started to offer degrees in data science, with many at the graduate, and a few at the undergraduate level. Curricula may differ at different institutions, because of varying levels of faculty expertise, and different disciplines (such as mathematics, computer science, and business) in developing the curriculum. The University of Massachusetts Dartmouth started offering degree programs in data science from Fall 2015, at both the undergraduate and the graduate level. Quite a few articles have been published that deal with graduate data science courses, much less so dealing with undergraduate ones. Our discussion will focus on undergraduate course structure and function, and specifically, a first course in data science. Our design of this course centers around a concept called the data science life cycle. That is, we view tasks or steps in the practice of data science as forming a process, consisting of states that indicate how it comes into life, how different tasks in data science depend on or interact with others until the birth of a data product or a conclusion. Naturally, different pieces of the data science life cycle then form individual parts of the course. Details of each piece are filled up by concepts, techniques, or skills that are popular in industry. Consequently, the design of our course is both “principled” and practical. A significant feature of our course philosophy is that, in line with activity theory, the course is based on the use of tools to transform real data to answer strongly motivated questions related to the data.",fullPaper,jv36
p75,daec8baf1740a09725b375729d95caebc42f61c8,c1,Technical Symposium on Computer Science Education,ACM Task Force on Data Science Education: Draft Report and Opportunity for Feedback,"The ACM Data Science Task Force was established by the ACM Education Council and tasked with articulating the role of computing discipline-specific contributions to this emerging field. This special session seeks to introduce the work of the ACM Data Science Task Force as well as to engage the SIGCSE community in this effort. Members of the task force will introduce key components of a draft report, including a summary of data science curricular efforts to date, results of ACM academic and industry surveys on data science, as well as the initial articulation of computing competencies for undergraduate programs in data science. This session should be of interest to all SIGCSE attendees, but especially faculty developing college-level curricula in Data Science.",fullPaper,cp1
p76,bf12943b1862cbdf556ba1ddcdbc685d4f38a6c3,j37,Communications of the ACM,Realizing the potential of data science,"Data science promises new insights, helping transform information into knowledge that can drive science and industry.",fullPaper,jv37
p77,36708c11c2fde2efb50e75d81f174b2c205082c8,j38,Big Data & Society,What is responsible and sustainable data science?,"In the expansion of health ecosystems, issues of responsibility and sustainability of the data science involved are central. The idea that these values should be central to the practice of data science is increasingly gaining traction, yet there is no agreement on what exactly makes data science responsible or sustainable because these concepts prove slippery when applied to a global field involving commercial, academic and governmental actors. This lack of clarity is causing problems in setting goals and boundaries for data scientific practice, and risks fundamental disagreement on governance principles for this emerging field. We will argue in this commentary for a commons analytical framework as one approach to this problem, since it offers useful signposts for how to establish governance principles for shared resources.",fullPaper,jv38
p78,fb566f2001e44a65433fb7cc2eb7bcf6513a7db8,c64,Experimental Software Engineering Network,The 9 Pitfalls of Data Science,"Scientific rigor and critical thinking skills are indispensable in this age of big data because machine learning and artificial intelligence are often led astray by meaningless patterns. The 9 Pitfalls of Data Science is loaded with entertaining real-world examples of both successful and misguided approaches to interpreting data, both grand successes and epic failures. Anyone can learn to distinguish between good data science and nonsense. We are confident that readers will learn how to avoid being duped by data, and make better, more informed decisions. Whether they want to be effective creators, interpreters, or users of data, they need to know the nine pitfalls of data science.",poster,cp64
p79,4aeda303fa0b9beae3f6d65e052dace9d4540116,j39,Journal of Library Administration,Data Science Support at the Academic Library,"Abstract Data science is a rapidly growing field with applications across all scientific domains. The demand for support in data science literacy is outpacing available resources at college campuses. The academic library is uniquely positioned to provide training and guidance in a number of areas relevant to data science. The University of Arizona Libraries has built a successful data science support program, focusing on computational literacy, geographic information systems, and reproducible science. Success of the program has largely been due to the strength of library personnel and strategic partnerships with units outside of the library. Academic libraries can support campus data science needs through professional development of current staff and recruitment of new personnel with expertise in data-intensive domains.",fullPaper,jv39
p80,08468bac470e5c2cbbd2b66e8e7cf2ab65f38e02,j40,Social Science Research Network,Data Science for Local Government,"The Data Science for Local Government project was about understanding how the growth of ‘data science’ is changing the way that local government works in the UK. We define data science as a dual shift which involves both bringing in new decision making and analytical techniques to local government work (e.g. machine learning and predictive analytics, artificial intelligence and A/B testing) and also expanding the types of data local government makes use of (for example, by repurposing administrative data, harvesting social media data, or working with mobile phone companies). The emergence of data science is facilitated by the growing availability of free, open-source tools for both collecting data and performing analysis. Based on extensive documentary review, a nationwide survey of local authorities, and in-depth interviews with over 30 practitioners, we have sought to produce a comprehensive guide to the different types of data science being undertaken in the UK, the types of opportunities and benefits created, and also some of the challenges and difficulties being encountered. Our aim was to provide a basis for people working in local government to start on their own data science projects, both by providing a library of dozens of ideas which have been tried elsewhere and also by providing hints and tips for overcoming key problems and challenges.",fullPaper,jv40
p81,e8547557eeea9c9a334edfc4bfa4bbe3774e24d5,j41,Journal of Physics: Condensed Matter,Machine learning and data science in soft materials engineering,"In many branches of materials science it is now routine to generate data sets of such large size and dimensionality that conventional methods of analysis fail. Paradigms and tools from data science and machine learning can provide scalable approaches to identify and extract trends and patterns within voluminous data sets, perform guided traversals of high-dimensional phase spaces, and furnish data-driven strategies for inverse materials design. This topical review provides an accessible introduction to machine learning tools in the context of soft and biological materials by ‘de-jargonizing’ data science terminology, presenting a taxonomy of machine learning techniques, and surveying the mathematical underpinnings and software implementations of popular tools, including principal component analysis, independent component analysis, diffusion maps, support vector machines, and relative entropy. We present illustrative examples of machine learning applications in soft matter, including inverse design of self-assembling materials, nonlinear learning of protein folding landscapes, high-throughput antimicrobial peptide design, and data-driven materials design engines. We close with an outlook on the challenges and opportunities for the field.",fullPaper,jv41
p82,ffdb6039a5d82f8edd70b2d177074c2f2c89e97f,j42,Journal of Social Computing,Data Science as Political Action: Grounding Data Science in a Politics of Justice,"In response to recent controversies, the field of data science has rushed to adopt codes of ethics. Such professional codes, however, are ill-equipped to address broad matters of social justice. Instead of ethics codes, I argue, the field must embrace politics. Data scientists must recognize themselves as political actors engaged in normative constructions of society and, as befits political work, evaluate their work according to its downstream material impacts on people's lives. I justify this notion in two parts: first, by articulating why data scientists must recognize themselves as political actors, and second, by describing how the field can evolve toward a deliberative and rigorous grounding in a politics of social justice. Part 1 responds to three arguments that are commonly invoked by data scientists when they are challenged to take political positions regarding their work. In confronting these arguments, I will demonstrate why attempting to remain apolitical is itself a political stance--a fundamentally conservative one--and why the field's current attempts to promote ""social good"" dangerously rely on vague and unarticulated political assumptions. Part 2 proposes a framework for what a politically-engaged data science could look like and how to achieve it, recognizing the challenge of reforming the field in this manner. I conceptualize the process of incorporating politics into data science in four stages: becoming interested in directly addressing social issues, recognizing the politics underlying these issues, redirecting existing methods toward new applications, and, finally, developing new practices and methods that orient data science around a mission of social justice. The path ahead does not require data scientists to abandon their technical expertise, but it does entail expanding their notions of what problems to work on and how to engage with society.",fullPaper,jv42
p83,305600f3cba8a63bad1bedeab34a299bf748754b,j34,Proceedings of the VLDB Endowment,Northstar: An Interactive Data Science System,"In order to democratize data science, we need to fundamentally rethink the current analytics stack, from the user interface to the ""guts."" Most importantly, enabling a broader range of users to unfold the potential of (their) data requires a change in the interface and the ""protection"" we offer them. On the one hand, visual interfaces for data science have to be intuitive, easy, and interactive to reach users without a strong background in computer science or statistics. On the other hand, we need to protect users from making false discoveries. Furthermore, it requires that technically involved (and often boring) tasks have to be automatically done by the system so that the user can focus on contributing their domain expertise to the problem. In this paper, we present Northstar, the Interactive Data Science System, which we have developed over the last 4 years to explore designs that make advanced analytics and model building more accessible.",fullPaper,jv34
p84,e1c8f86668d3e37e430f187b7fd91d1643a0a0ff,j1,IEEE Transactions on Knowledge and Data Engineering,Theory-Guided Data Science: A New Paradigm for Scientific Discovery from Data,"Data science models, although successful in a number of commercial domains, have had limited applicability in scientific problems involving complex physical phenomena. Theory-guided data science (TGDS) is an emerging paradigm that aims to leverage the wealth of scientific knowledge for improving the effectiveness of data science models in enabling scientific discovery. The overarching vision of TGDS is to introduce scientific consistency as an essential component for learning generalizable models. Further, by producing scientifically interpretable models, TGDS aims to advance our scientific understanding by discovering novel domain insights. Indeed, the paradigm of TGDS has started to gain prominence in a number of scientific disciplines such as turbulence modeling, material discovery, quantum chemistry, bio-medical science, bio-marker discovery, climate science, and hydrology. In this paper, we formally conceptualize the paradigm of TGDS and present a taxonomy of research themes in TGDS. We describe several approaches for integrating domain knowledge in different research themes using illustrative examples from different disciplines. We also highlight some of the promising avenues of novel research for realizing the full potential of theory-guided data science.",fullPaper,jv1
p85,0a4b3c33e830d8cde364443a52e673c2c07dcfe8,c2,International Symposium on Intelligent Data Analysis,Open Data Science,Abstract content,fullPaper,cp2
p86,140a6476f7b8dde9e7bbcd199d248fc629721faa,c88,Symposium on the Theory of Computing,Trust in Data Science,"The trustworthiness of data science systems in applied and real-world settings emerges from the resolution of specific tensions through situated, pragmatic, and ongoing forms of work. Drawing on research in CSCW, critical data studies, and history and sociology of science, and six months of immersive ethnographic fieldwork with a corporate data science team, we describe four common tensions in applied data science work: (un)equivocal numbers, (counter)intuitive knowledge, (in)credible data, and (in)scrutable models. We show how organizational actors establish and re-negotiate trust under messy and uncertain analytic conditions through practices of skepticism, assessment, and credibility. Highlighting the collaborative and heterogeneous nature of real-world data science, we show how the management of trust in applied corporate data science settings depends not only on pre-processing and quantification, but also on negotiation and translation. We conclude by discussing the implications of our findings for data science research and practice, both within and beyond CSCW.",poster,cp88
p87,260ef5a455afcb8d052866bdf2fb28996ab8bbad,j7,International Journal of Data Science and Analysis,Data Science: the impact of statistics,Abstract content,fullPaper,jv7
p88,577564ac25a12b37972d77a35b589f6b2270a45f,j43,Chest,Big Data and Data Science in Critical Care.,Abstract content,fullPaper,jv43
p89,2146edb37621d80f53c1261c8a53c94d3dda84c8,c3,Frontiers in Education Conference,Smart Blockchain Badges for Data Science Education,"Blockchain technology has the potential to revolutionise education in a number of ways. In this paper, we explore the applications of Smart Blockchain Badges on data science education. In particular, we investigate how Smart Blockchain Badges can support learners that want to advance their careers in data science, by offering them personalised recommendations based on their learning achievements. This work aims at enhancing data science accreditation by introducing a robust system based on the Blockchain technology. Learners will benefit from a sophisticated, open and transparent accreditation system, as well as from receiving job recommendations that match their skills and can potentially progress their careers. As a result, this work contributes towards closing the data science skills gap by linking data science education to the industry.",fullPaper,cp3
p90,a2d7efb8b174702111e713765cbf741dff2bf9b8,c62,International Conference on Software Reuse,Searching for Hidden Perovskite Materials for Photovoltaic Systems by Combining Data Science and First Principle Calculations,"Undiscovered perovskite materials for applications in capturing solar lights are explored through the implementation of data science. In particular, 15000 perovskite materials data is analyzed where visualization of the data reveals hidden trends and clustering of data. Random forest classification within machine learning is used in order to predict the band gap of perovskite materials where 18 physical descriptors are revealed to determine the band gap. With trained random forest, 9328 perovskite materials with potential for applications in solar cell materials are predicted. The selected Li and Na based perovskite materials within predicted 9328 perovskite materials are evaluated with first principle calculations where 11 undiscovered Li(Na) based perovskite materials fall into the ideal band gap and formation energy ranges for solar cell applications. Thus, the implementation of data science accelerates the discovery of hidden perovskite materials and the approach can be applied to the materials scienc...",poster,cp62
p91,6e8d94181832771bc5dca8d288c52b6ad5914029,c1,Technical Symposium on Computer Science Education,Data Science as Machinic Neoplatonism,Abstract content,poster,cp1
p92,e4c66275e46a66586365c851f0974a3c88baf3d7,c83,International Conference on Computer Graphics and Interactive Techniques,Network embedding in biomedical data science,"Owning to the rapid development of computer technologies, an increasing number of relational data have been emerging in modern biomedical research. Many network-based learning methods have been proposed to perform analysis on such data, which provide people a deep understanding of topology and knowledge behind the biomedical networks and benefit a lot of applications for human healthcare. However, most network-based methods suffer from high computational and space cost. There remain challenges on handling high dimensionality and sparsity of the biomedical networks. The latest advances in network embedding technologies provide new effective paradigms to solve the network analysis problem. It converts network into a low-dimensional space while maximally preserves structural properties. In this way, downstream tasks such as link prediction and node classification can be done by traditional machine learning methods. In this survey, we conduct a comprehensive review of the literature on applying network embedding to advance the biomedical domain. We first briefly introduce the widely used network embedding models. After that, we carefully discuss how the network embedding approaches were performed on biomedical networks as well as how they accelerated the downstream tasks in biomedical science. Finally, we discuss challenges the existing network embedding applications in biomedical domains are faced with and suggest several promising future directions for a better improvement in human healthcare.",poster,cp83
p93,6bf9d589f80823735084956f056728ae1a7bcfa8,j44,BioScience,"Situating Ecology as a Big-Data Science: Current Advances, Challenges, and Solutions","Ecology has joined a world of big data. Two complementary frameworks define big data: data that exceed the analytical capacities of individuals or disciplines or the “Four Vs” axes of volume, variety, veracity, and velocity. Variety predominates in ecoinformatics and limits the scalability of ecological science. Volume varies widely. Ecological velocity is low but growing as data throughput and societal needs increase. Ecological big-data systems include in situ and remote sensors, community data resources, biodiversity databases, citizen science, and permanent stations. Technological solutions include the development of open code- and data-sharing platforms, flexible statistical models that can handle heterogeneous data and sources of uncertainty, and cloud-computing delivery of high-velocity computing to large-volume analytics. Cultural solutions include training targeted to early and current scientific workforce and strengthening collaborations among ecologists and data scientists. The broader goal is to maximize the power, scalability, and timeliness of ecological insights and forecasting.",fullPaper,jv44
p94,f968cdd4637e7b26ca6c057a2f7f593b8cea2d18,c94,Vision,Fundamentals of Clinical Data Science,Abstract content,poster,cp94
p95,bf90a04b8c9a7752bab8cb50c8796393218627d2,j37,Communications of the ACM,Data science,"While it may not be possible to build a data brain identical to a human, data science can still aspire to imaginative machine thinking.",fullPaper,jv37
p96,3335c340c20609b4e6de481c9eaf67ecd6c960dc,c4,Annual Conference on Genetic and Evolutionary Computation,Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science,"As the field of data science continues to grow, there will be an ever-increasing demand for tools that make machine learning accessible to non-experts. In this paper, we introduce the concept of tree-based pipeline optimization for automating one of the most tedious parts of machine learning--pipeline design. We implement an open source Tree-based Pipeline Optimization Tool (TPOT) in Python and demonstrate its effectiveness on a series of simulated and real-world benchmark data sets. In particular, we show that TPOT can design machine learning pipelines that provide a significant improvement over a basic machine learning analysis while requiring little to no input nor prior knowledge from the user. We also address the tendency for TPOT to design overly complex pipelines by integrating Pareto optimization, which produces compact pipelines without sacrificing classification accuracy. As such, this work represents an important step toward fully automating machine learning pipeline design.",fullPaper,cp4
p97,a1dbdc2ce338d694a720163f591e4eb5c4070140,c63,IEEE International Software Metrics Symposium,Deep Learning in Biomedical Data Science,"Since the 1980s, deep learning and biomedical data have been coevolving and feeding each other. The breadth, complexity, and rapidly expanding size of biomedical data have stimulated the development of novel deep learning methods, and application of these methods to biomedical data have led to scientific discoveries and practical solutions. This overview provides technical and historical pointers to the field, and surveys current applications of deep learning to biomedical data organized around five subareas, roughly of increasing spatial scale: chemoinformatics, proteomics, genomics and transcriptomics, biomedical imaging, and health care. The black box problem of deep learning methods is also briefly discussed.",poster,cp63
p98,7e6bac4de2adda5f5e993644126d8cbdf6839f39,c89,Conference on Uncertainty in Artificial Intelligence,Process Mining: Data science in Action,"Data science is the profession of the future, because organizations that are unable to use (big) data in a smart way will not survive. It is not sufficient to focus on data storage and data analysis. The data scientist also needs to relate data to process analysis. Process mining bridges the gap between traditional model-based process analysis (e.g., simulation and other business process management techniques) and datacentric analysis techniques such as machine learning and data mining. Process mining seeks the confrontation between event data (i.e., observed behavior) and process models (hand-made or discovered automatically). This technology has become available only recently, but it can be applied to any type of operational processes (organizations and systems). Example applications include: analyzing treatment processes in hospitals, improving customer service processes in a multinational, understanding the browsing behavior of customers using a booking site, analyzing failures of a baggage handling system, and improving the user interface of an X-ray machine. All of these applications have in common that dynamic behavior needs to be related to process models. Hence, we refer to this as ""data science in action"".",poster,cp89
p99,9b54c9a7d2060f800961c2f9195fcf5408288f17,c30,IEEE Aerospace Conference,Data Science for Undergraduates,Abstract content,poster,cp30
p100,5a44f70130875b212452ad777ab02a4eb5cd35d9,j45,International Journal of Population Data Science,A Position Statement on Population Data Science: The Science of Data about People,"Information is increasingly digital, creating opportunities to respond to pressing issues about human populations using linked datasets that are large, complex, and diverse. The potential social and individual benefits that can come from data-intensive science are large, but raise challenges of balancing individual privacy and the public good, building appropriate socio-technical systems to support data-intensive science, and determining whether defining a new field of inquiry might help move those collective interests and activities forward. A combination of expert engagement, literature review, and iterative conversations led to our conclusion that defining the field of Population Data Science (challenge 3) will help address the other two challenges as well. We define Population Data Science succinctly as the science of data about people and note that it is related to but distinct from the fields of data science and informatics. A broader definition names four characteristics of: data use for positive impact on citizens and society; bringing together and analyzing data from multiple sources; finding population-level insights; and developing safe, privacy-sensitive and ethical infrastructure to support research. One implication of these characteristics is that few people possess all of the requisite knowledge and skills of Population Data Science, so this is by nature a multi-disciplinary field. Other implications include the need to advance various aspects of science, such as data linkage technology, various forms of analytics, and methods of public engagement. These implications are the beginnings of a research agenda for Population Data Science, which if approached as a collective field, can catalyze significant advances in our understanding of trends in society, health, and human behavior.",fullPaper,jv45
p101,843793928e308b5414d2883ac869e813ec16f65d,c85,International Conference on Graph Transformation,Progressive Data Science: Potential and Challenges,"Data science requires time-consuming iterative manual activities. In particular, activities such as data selection, preprocessing, transformation, and mining, highly depend on iterative trial-and-error processes that could be sped up significantly by providing quick feedback on the impact of changes. The idea of progressive data science is to compute the results of changes in a progressive manner, returning a first approximation of results quickly and allow iterative refinements until converging to a final result. Enabling the user to interact with the intermediate results allows an early detection of erroneous or suboptimal choices, the guided definition of modifications to the pipeline and their quick assessment. In this paper, we discuss the progressiveness challenges arising in different steps of the data science pipeline. We describe how changes in each step of the pipeline impact the subsequent steps and outline why progressive data science will help to make the process more effective. Computing progressive approximations of outcomes resulting from changes creates numerous research challenges, especially if the changes are made in the early steps of the pipeline. We discuss these challenges and outline first steps towards progressiveness, which, we argue, will ultimately help to significantly speed-up the overall data science process.",poster,cp85
p102,ea07f64ad84542e04acc41db6b171007f344efd7,c5,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,Milo: A visual programming environment for Data Science Education,"Most courses on Data Science offered at universities or online require students to have familiarity with at least one programming language. In this paper, we present, “Milo”, a web-based visual programming environment for Data Science Education, designed as a pedagogical tool that can be used by students without prior-programming experience. To that end, Milo uses graphical blocks as abstractions of language specific implementations of Data Science and Machine Learning(ML) concepts along with creation of interactive visualizations. Using block definitions created by a user, Milo generates equivalent source code in JavaScript to run entirely in the browser. Based on a preliminary user study with a focus group of undergraduate computer science students, Milo succeeds as an effective tool for novice learners in the field of Data Science.",fullPaper,cp5
p103,17e5ebfa11b1d3bf194a8e94f6ca580ce22c9c0d,j46,Computational Mechanics,Data science for finite strain mechanical science of ductile materials,Abstract content,fullPaper,jv46
p104,1d045f4f347409f0635c9d15d538dbfabb6b38fa,j47,Environmental Modelling & Software,Environmental Data Science,Abstract content,fullPaper,jv47
p105,dd1f93c3faae464d50d2e97c2bf4ac8d43681cb1,c77,Networks,Twinning data science with information science in schools of library and information science,"As an emerging discipline, data science represents a vital new current of school of library and information science (LIS) education. However, it remains unclear how it relates to information science within LIS schools. The purpose of this paper is to clarify this issue.,Mission statement and nature of both data science and information science are analyzed by reviewing existing work in the two disciplines and drawing DIKW hierarchy. It looks at the ways in which information science theories bring new insights and shed new light on fundamentals of data science.,Data science and information science are twin disciplines by nature. The mission, task and nature of data science are consistent with those of information science. They greatly overlap and share similar concerns. Furthermore, they can complement each other. LIS school should integrate both sciences and develop organizational ambidexterity. Information science can make unique contributions to data science research, including conception of data, data quality control, data librarianship and theory dualism. Document theory, as a promising direction of unified information science, should be introduced to data science to solve the disciplinary divide.,The results of this paper may contribute to the integration of data science and information science within LIS schools and iSchools. It has particular value for LIS school development and reform in the age of big data.",poster,cp77
p106,b34b9758b36c92c023c3c10f3a39aeb8f5c83927,c6,Americas Conference on Information Systems,Exploring Project Management Methodologies Used Within Data Science Teams,"There are many reasons data science teams should use a well-defined process to manage and coordinate their efforts, such as improved collaboration, efficiency and stakeholder communication. This paper explores the current methodology data science teams use to manage and coordinate their efforts. Unfortunately, based on our survey results, most data science teams currently use an ad hoc project management approach. In fact, 82% of the data scientists surveyed did not follow an explicit process. However, it is encouraging to note that 85% of the respondents thought that adopting an improved process methodology would improve the teams’ outcomes. Based on these results, we described six possible process methodologies teams could use. To conclude, we outlined plans to describe best practices for data science team processes and to develop a process evaluation framework.",fullPaper,cp6
p107,a0ef3467c09acc3106b915258b7b8db7bb663b77,c30,IEEE Aerospace Conference,Data Science Methodology for Cybersecurity Projects,"Cyber-security solutions are traditionally static and signature-based. The traditional solutions along with the use of analytic models, machine learning and big data could be improved by automatically trigger mitigation or provide relevant awareness to control or limit consequences of threats. This kind of intelligent solutions is covered in the context of Data Science for Cyber-security. Data Science provides a significant role in cyber-security by utilising the power of data (and big data), high-performance computing and data mining (and machine learning) to protect users against cyber-crimes. For this purpose, a successful data science project requires an effective methodology to cover all issues and provide adequate resources. In this paper, we are introducing popular data science methodologies and will compare them in accordance with cyber-security challenges. A comparison discussion has also delivered to explain methodologies strengths and weaknesses in case of cyber-security projects.",poster,cp30
p108,d01c02801212f823cdc11e329b3a1afa63f3a2d5,c54,International Workshop on Agent-Oriented Software Engineering,Shifting to Data Savvy: The Future of Data Science In Libraries,"The Data Science in Libraries Project is funded by the Institute for Museum and Library Services (IMLS) and led by Matt Burton and Liz Lyon, School of Computing & Information, University of Pittsburgh; Chris Erdmann, North Carolina State University; and Bonnie Tijerina, Data & Society. The project explores the challenges associated with implementing data science within diverse library environments by examining two specific perspectives framed as ‘the skills gap,’ i.e. where librarians are perceived to lack the technical skills to be effective in a data-rich research environment; and ‘the management gap,’ i.e. the ability of library managers to understand and value the benefits of in-house data science skills and to provide organizational and managerial support. 
 
This report primarily presents a synthesis of the discussions, findings, and reflections from an international, two-day workshop held in May 2017 in Pittsburgh, where community members participated in a program with speakers, group discussions, and activities to drill down into the challenges of successfully implementing data science in libraries. Participants came from funding organizations, academic and public libraries, nonprofits, and commercial organizations with most of the discussions focusing on academic libraries and library schools.",poster,cp54
p109,5de20ffb7852ae0665c382084c8a56918f23dc0b,c7,European Conference on Modelling and Simulation,Drafting a Data Science Curriculum for Secondary Schools,"Data science as the art of generating information and knowledge from data is increasingly becoming an important part of most operational processes. But up to now, data science is hardly an issue in German computer science education at secondary schools. For this reason, we are developing a data science curriculum for German secondary schools, which first guidelines and ideas we present in this paper. The curriculum is designed as interdisciplinary approach between maths and computer science education, with also a strong focus on societal aspects. After a brief discussion of important concepts and challenges in data science, a first draft of the curriculum and an outline of a data science course for upper secondary schools accompanying the development are presented.",fullPaper,cp7
p110,818c9fd2df1229f962af3c50ef493e8633433fb9,c76,International Conference on Artificial Neural Networks,Data Science Thinking,Abstract content,poster,cp76
p111,04d7b3457dc78b2d2282e6af2c787308f75c9b26,c8,The Compass,Care and the Practice of Data Science for Social Good,"Data science is an interdisciplinary field that extracts insights from data through a multi-stage process of data collection, analysis and use. When data science is applied for social good, a variety of stakeholders are introduced to the process with an intention to inform policies or programs to improve well-being. Our goal in this paper is to propose an orientation to care in the practice of data science for social good. When applied to data science, a logic of care can improve the data science process and reveal outcomes of ""good"" throughout. Consideration of care in practice has its origins in Science and Technology Studies (STS) and has recently been applied by Human Computer Interaction (HCI) researchers to understand technology repair and use in under-served environments as well as care in remote health monitoring. We bring care to the practice of data science through a detailed examination of our engaged research with a community group that uses data as a strategy to advocate for permanently affordable housing. We identify opportunities and experiences of care throughout the stages of the data science process. We bring greater detail to the notion of human-centered systems for data science and begin to describe what these look like.",fullPaper,cp8
p112,010f65dd2fa979892a8229db825954871652fb8f,c44,International Workshop on Green and Sustainable Software,Defining Data Science by a Data-Driven Quantification of the Community,"Data science is a new academic field that has received much attention in recent years. One reason for this is that our increasingly digitalized society generates more and more data in all areas of our lives and science and we are desperately seeking for solutions to deal with this problem. In this paper, we investigate the academic roots of data science. We are using data of scientists and their citations from Google Scholar, who have an interest in data science, to perform a quantitative analysis of the data science community. Furthermore, for decomposing the data science community into its major defining factors corresponding to the most important research fields, we introduce a statistical regression model that is fully automatic and robust with respect to a subsampling of the data. This statistical model allows us to define the ‘importance’ of a field as its predictive abilities. Overall, our method provides an objective answer to the question ‘What is data science?’.",poster,cp44
p113,3c51a892ce5a8fc78d57ea290c6e5144ee9db579,c1,Technical Symposium on Computer Science Education,Key Concepts for a Data Science Ethics Curriculum,"Data science is a new field that integrates aspects of computer science, statistics and information management. As a new field, ethical issues a data scientist may encounter have received little attention to date, and ethics training within a data science curriculum has received even less attention. To address this gap, this article explores the different codes of conduct and ethics frameworks related to data science. We compare this analysis with the results of a systematic literature review focusing on ethics in data science. Our analysis identified twelve key ethics areas that should be included within a data science ethics curriculum. Our research notes that none of the existing codes or frameworks covers all of the identified themes. Data science educators and program coordinators can use our results as a way to identify key ethical concepts that can be introduced within a data science program.",fullPaper,cp1
p114,798e5e09c20b0270701b194a3198427fec6a4fcd,j7,International Journal of Data Science and Analysis,What makes Data Science different? A discussion involving Statistics2.0 and Computational Sciences,Abstract content,fullPaper,jv7
p115,fd570fa0a974cd47172a06c67181caeb199a6567,j48,Nature Human Behaviour,How data science can advance mental health research,Abstract content,fullPaper,jv48
p116,e78be911203960b3b2a417465d726734367f8e30,c28,International Conference on Collaboration Technologies and Systems,Counter‐mapping data science,"Counter-mapping is a combination of critical ideas and practices for social change that offers a productive and promising approach for grassroots data science initiatives. Current information technologies collect, store, and analyze data with new degrees of size, speed, heterogeneity, and detail. While much work utilizing data science technologies is dedicated to generating profit or to national security, some data science projects explicitly attempt to facilitate new social relations, though with inconsistent results and consequences. This paper reviews counter-mapping's particular combination of theory and practice as a potential point of reference for such initiatives. Counter-mapping takes the tools of institutional map-making at government agencies and corporations and applies them in situated, bottom-up ways. Moreover, counter-mapping's multiple theoretical approaches and polyglot practices offer a variety of inspirations and avenues for future work in identifying and realizing alternative, ideally better, possibilities. This paper defines counter-mapping; outlines its multiple theorizations; briefly describes three relevant case studies, The Detroit Geographical Expedition and Institute, Mapping Police Violence, and the Counter-Cartographies Collective; and concludes with a few hard-learned considerations from counter-mapping that are directly pertinent for data-oriented projects focused on change.",poster,cp28
p117,82feed9f0f8d077046b9b8be36e664483a66e33b,c49,International Symposium on Search Based Software Engineering,Teaching Stats for Data Science,"ABSTRACT “Data science” is a useful catchword for methods and concepts original to the field of statistics, but typically being applied to large, multivariate, observational records. Such datasets call for techniques not often part of an introduction to statistics: modeling, consideration of covariates, sophisticated visualization, and causal reasoning. This article re-imagines introductory statistics as an introduction to data science and proposes a sequence of 10 blocks that together compose a suitable course for extracting information from contemporary data. Recent extensions to the mosaic packages for R together with tools from the “tidyverse” provide a concise and readable notation for wrangling, visualization, model-building, and model interpretation: the fundamental computational tasks of data science.",poster,cp49
p118,0df3d2694b41e99a30101555cc835ccae8d2bdc6,j49,ACM Computing Surveys,Data Science,"The 21st century has ushered in the age of big data and data economy, in which data DNA, which carries important knowledge, insights, and potential, has become an intrinsic constituent of all data-based organisms. An appropriate understanding of data DNA and its organisms relies on the new field of data science and its keystone, analytics. Although it is widely debated whether big data is only hype and buzz, and data science is still in a very early phase, significant challenges and opportunities are emerging or have been inspired by the research, innovation, business, profession, and education of data science. This article provides a comprehensive survey and tutorial of the fundamental aspects of data science: the evolution from data analysis to data science, the data science concepts, a big picture of the era of data science, the major challenges and directions in data innovation, the nature of data analytics, new industrialization and service opportunities in the data economy, the profession and competency of data education, and the future of data science. This article is the first in the field to draw a comprehensive big picture, in addition to offering rich observations, lessons, and thinking about data science and analytics.",fullPaper,jv49
p119,843149b649b888fdb3649b8d4852263b62356799,c9,Pacific Symposium on Biocomputing,Democratizing data science through data science training,"The biomedical sciences have experienced an explosion of data which promises to overwhelm many current practitioners. Without easy access to data science training resources, biomedical researchers may find themselves unable to wrangle their own datasets. In 2014, to address the challenges posed such a data onslaught, the National Institutes of Health (NIH) launched the Big Data to Knowledge (BD2K) initiative. To this end, the BD2K Training Coordinating Center (TCC; bigdatau.org) was funded to facilitate both in-person and online learning, and open up the concepts of data science to the widest possible audience. Here, we describe the activities of the BD2K TCC and its focus on the construction of the Educational Resource Discovery Index (ERuDIte), which identifies, collects, describes, and organizes online data science materials from BD2K awardees, open online courses, and videos from scientific lectures and tutorials. ERuDIte now indexes over 9,500 resources. Given the richness of online training materials and the constant evolution of biomedical data science, computational methods applying information retrieval, natural language processing, and machine learning techniques are required - in effect, using data science to inform training in data science. In so doing, the TCC seeks to democratize novel insights and discoveries brought forth via large-scale data science training.",fullPaper,cp9
p120,1901a26945ecb5445a9d58b4c32a0dc6dbd12f1a,j50,"Science, Technology and Human Values","STS, Meet Data Science, Once Again","Science and technology studies (STS) and the emerging field of data science share surprising elective affinities. At the growing intersections of these fields, there will be many opportunities and not a few thorny difficulties for STS scholars. First, I discuss how both fields frame the rollout of data science as a simultaneously social and technical endeavor, even if in distinct ways and for diverging purposes. Second, I discuss the logic of domains in contemporary computer, information, and data science circles. While STS is often agnostic about the borders between the sciences or with industry and state—occasionally taking those boundaries as an object of study—data science takes those boundaries as its target to overcome. These two elective affinities present analytic and practical challenges for STS but also opportunities for engagement. Overall, in addition to these typifications, I urge STS scholars to strategically position themselves to investigate and contribute to the breadth of transformations that seek to touch virtually every science and newly bind spheres of academy, industry, and state.",fullPaper,jv50
p121,db7f24f099fc5a8228f2a8c8b03d43ec4030bbdc,c77,Networks,Data Science,"Data science is the study of how analytics techniques can be applied to large and diverse data sets. This field is emerging because of the availability of massive data sets in both consumer and health sectors, new machine learning and other analytics requiring large-scale computation, and the vital need to identify risk factors, trends, and other relationships not apparent when applying traditional analytics methods to smaller structured data sets. In some organizations, the primary role of a clinical informatics professional no longer is focused on how electronic health records are used in healthcare delivery but instead is focused on how patient encounter information can be collected efficiently, aggregated with information from other encounters or sources, and analyzed to improve our understanding of how population studies can improve the care of individuals. Such an understanding is critical to improving care quality and lowering healthcare costs.",poster,cp77
p122,3b8960ba528d3c66d11c38736accef4859e2a5cb,c65,Formal Concept Analysis,Curriculum Guidelines for Undergraduate Programs in Data Science,"The Park City Math Institute (PCMI) 2016 Summer Undergraduate Faculty Program met for the purpose of composing guidelines for undergraduate programs in Data Science. The group consisted of 25 undergraduate faculty from a variety of institutions in the U.S., primarily from the disciplines of mathematics, statistics and computer science. These guidelines are meant to provide some structure for institutions planning for or revising a major in Data Science.",poster,cp65
p123,ae118a88ada51dfdb2296cbaa948eb4a467942b6,c24,Decision Support Systems,"Computer Age Statistical Inference: Algorithms, Evidence, and Data Science","The twenty-first century has seen a breathtaking expansion of statistical methodology, both in scope and in influence. 'Big data', 'data science', and 'machine learning' have become familiar terms in the news, as statistical methods are brought to bear upon the enormous data sets of modern science and commerce. How did we get here? And where are we going? This book takes us on an exhilarating journey through the revolution in data analysis following the introduction of electronic computation in the 1950s. Beginning with classical inferential theories - Bayesian, frequentist, Fisherian - individual chapters take up a series of influential topics: survival analysis, logistic regression, empirical Bayes, the jackknife and bootstrap, random forests, neural networks, Markov chain Monte Carlo, inference after model selection, and dozens more. The distinctly modern approach integrates methodology and algorithms with statistical inference. The book ends with speculation on the future direction of statistics and data science.",poster,cp24
p124,cb5816d87598cc64ba39c6e4a23e49cbd769a54e,j20,Proceedings of the National Academy of Sciences of the United States of America,Science and data science,"Data science has attracted a lot of attention, promising to turn vast amounts of data into useful predictions and insights. In this article, we ask why scientists should care about data science. To answer, we discuss data science from three perspectives: statistical, computational, and human. Although each of the three is a critical component of data science, we argue that the effective combination of all three components is the essence of what data science is about.",fullPaper,jv20
p125,8d89159249e0faf5deae508cc8533010898bbda5,c90,Computer Vision and Pattern Recognition,Data Science in Action,Abstract content,poster,cp90
p126,89535aa63bc5dac6f3beb60b813abb77aa4309d1,c10,Big Data,Critique and Contribute: A Practice-Based Framework for Improving Critical Data Studies and Data Science,"Abstract What would data science look like if its key critics were engaged to help improve it, and how might critiques of data science improve with an approach that considers the day-to-day practices of data science? This article argues for scholars to bridge the conversations that seek to critique data science and those that seek to advance data science practice to identify and create the social and organizational arrangements necessary for a more ethical data science. We summarize four critiques that are commonly made in critical data studies: data are inherently interpretive, data are inextricable from context, data are mediated through the sociomaterial arrangements that produce them, and data serve as a medium for the negotiation and communication of values. We present qualitative research with academic data scientists, “data for good” projects, and specialized cross-disciplinary engineering teams to show evidence of these critiques in the day-to-day experience of data scientists as they acknowledge and grapple with the complexities of their work. Using ethnographic vignettes from two large multiresearcher field sites, we develop a set of concepts for analyzing and advancing the practice of data science and improving critical data studies, including (1) communication is central to the data science endeavor; (2) making sense of data is a collective process; (3) data are starting, not end points, and (4) data are sets of stories. We conclude with two calls to action for researchers and practitioners in data science and critical data studies alike. First, creating opportunities for bringing social scientific and humanistic expertise into data science practice simultaneously will advance both data science and critical data studies. Second, practitioners should leverage the insights from critical data studies to build new kinds of organizational arrangements, which we argue will help advance a more ethical data science. Engaging the insights of critical data studies will improve data science. Careful attention to the practices of data science will improve scholarly critiques. Genuine collaborative conversations between these different communities will help push for more ethical, and better, ways of knowing in increasingly datum-saturated societies.",fullPaper,cp10
p127,796d70a6eb0428ae19f1187ae1c81185d4ae6701,c66,Annual Conference on Innovation and Technology in Computer Science Education,Automating Biomedical Data Science Through Tree-Based Pipeline Optimization,Abstract content,poster,cp66
p128,da63f30bd5b3a1b16c261f75ca1b1daddfc5b44d,c49,International Symposium on Search Based Software Engineering,Big Data and Data Science Methods for Management Research,"The recent advent of remote sensing, mobile technologies, novel transaction systems, and highperformance computing offers opportunities to understand trends, behaviors, and actions in a manner that has not been previously possible. Researchers can thus leverage “big data” that are generated from a plurality of sources including mobile transactions, wearable technologies, social media, ambient networks, andbusiness transactions.An earlierAcademy of Management Journal (AMJ) editorial explored the potential implications for data science inmanagement research and highlighted questions for management scholarship as well as the attendant challenges of data sharing and privacy (George, Haas, & Pentland, 2014). This nascent field is evolving rapidly and at a speed that leaves scholars and practitioners alike attempting to make sense of the emergent opportunities that big datahold.With thepromiseof bigdata comequestions about the analytical value and thus relevance of these data for theory development—including concerns over the context-specific relevance, its reliability and its validity. To address this challenge, data science is emerging as an interdisciplinary field that combines statistics, data mining, machine learning, and analytics to understand and explainhowwecan generate analytical insights and prediction models from structured and unstructured big data. Data science emphasizes the systematic study of the organization, properties, and analysis of data and their role in inference, including our confidence in the inference (Dhar, 2013).Whereas both big data and data science terms are often used interchangeably, “big data” refer to large and varied data that can be collected and managed, whereas “data science” develops models that capture, visualize, andanalyze theunderlyingpatterns in thedata. In this editorial, we address both the collection and handling of big data and the analytical tools provided by data science for management scholars. At the current time, practitioners suggest that data science applications tackle the three core elements of big data: volume, velocity, and variety (McAfee & Brynjolfsson, 2012; Zikopoulos & Eaton, 2011). “Volume” represents the sheer size of the dataset due to the aggregation of a large number of variables and an even larger set of observations for each variable. “Velocity” reflects the speed atwhich these data are collected and analyzed, whether in real time or near real time from sensors, sales transactions, social media posts, and sentiment data for breaking news and social trends. “Variety” in big data comes from the plurality of structured and unstructured data sources such as text, videos, networks, and graphics among others. The combinations of volume, velocity, and variety reveal the complex task of generating knowledge from big data, which often runs into millions of observations, and deriving theoretical contributions from such data. In this editorial, we provide a primer or a “starter kit” for potential data science applications inmanagement research. We do so with a caveat that emerging fields outdate and improve uponmethodologies while often supplanting them with new applications. Nevertheless, this primer can guide management scholars who wish to use data science techniques to reach better answers to existing questions or explore completely new research questions.",poster,cp49
p129,57c82a005ae353f4683938b15a52e1b0561f6e43,c58,Australian Software Engineering Conference,"R for Data Science: Import, Tidy, Transform, Visualize, and Model Data","Learn how to use R to turn raw data into insight, knowledge, and understanding. This book introduces you to R, RStudio, and the tidyverse, a collection of R packages designed to work together to make data science fast, fluent, and fun. Suitable for readers with no previous programming experience, R for Data Science is designed to get you doing data science as quickly as possible. Authors Hadley Wickham and Garrett Grolemund guide you through the steps of importing, wrangling, exploring, and modeling your data and communicating the results. Youll get a complete, big-picture understanding of the data science cycle, along with basic tools you need to manage the details. Each section of the book is paired with exercises to help you practice what youve learned along the way. Youll learn how to: Wrangletransform your datasets into a form convenient for analysisProgramlearn powerful R tools for solving data problems with greater clarity and easeExploreexamine your data, generate hypotheses, and quickly test themModelprovide a low-dimensional summary that captures true ""signals"" in your datasetCommunicatelearn R Markdown for integrating prose, code, and results",poster,cp58
p130,fde0b586e3bc9e5139a14493044bce9ff61706d4,c7,European Conference on Modelling and Simulation,Inverse statistical problems: from the inverse Ising problem to data science,"Inverse problems in statistical physics are motivated by the challenges of ‘big data’ in different fields, in particular high-throughput experiments in biology. In inverse problems, the usual procedure of statistical physics needs to be reversed: Instead of calculating observables on the basis of model parameters, we seek to infer parameters of a model based on observations. In this review, we focus on the inverse Ising problem and closely related problems, namely how to infer the coupling strengths between spins given observed spin correlations, magnetizations, or other data. We review applications of the inverse Ising problem, including the reconstruction of neural connections, protein structure determination, and the inference of gene regulatory networks. For the inverse Ising problem in equilibrium, a number of controlled and uncontrolled approximate solutions have been developed in the statistical mechanics community. A particularly strong method, pseudolikelihood, stems from statistics. We also review the inverse Ising problem in the non-equilibrium case, where the model parameters must be reconstructed based on non-equilibrium statistics.",poster,cp7
p131,afe79672aa99b7f606cbff234ec2454cf2295554,j51,Ethnicity & Disease,Big Data Science: Opportunities and Challenges to Address Minority Health and Health Disparities in the 21st Century.,"Addressing minority health and health disparities has been a missing piece of the puzzle in Big Data science. This article focuses on three priority opportunities that Big Data science may offer to the reduction of health and health care disparities. One opportunity is to incorporate standardized information on demographic and social determinants in electronic health records in order to target ways to improve quality of care for the most disadvantaged populations over time. A second opportunity is to enhance public health surveillance by linking geographical variables and social determinants of health for geographically defined populations to clinical data and health outcomes. Third and most importantly, Big Data science may lead to a better understanding of the etiology of health disparities and understanding of minority health in order to guide intervention development. However, the promise of Big Data needs to be considered in light of significant challenges that threaten to widen health disparities. Care must be taken to incorporate diverse populations to realize the potential benefits. Specific recommendations include investing in data collection on small sample populations, building a diverse workforce pipeline for data science, actively seeking to reduce digital divides, developing novel ways to assure digital data privacy for small populations, and promoting widespread data sharing to benefit under-resourced minority-serving institutions and minority researchers. With deliberate efforts, Big Data presents a dramatic opportunity for reducing health disparities but without active engagement, it risks further widening them.",fullPaper,jv51
p132,88761dffd173cd0e75e88c02d68f866f8cc43c14,j52,Data Science,Knowledge-based biomedical Data Science,"Computational manipulation of knowledge is an important, and often under-appreciated, aspect of biomedical Data Science. The first Data Science initiative from the US National Institutes of Health was entitled “Big Data to Knowledge (BD2K).” The main emphasis of the more than $200M allocated to that program has been on “Big Data;” the “Knowledge” component has largely been the implicit assumption that the work will lead to new biomedical knowledge. However, there is long-standing and highly productive work in computational knowledge representation and reasoning, and computational processing of knowledge has a role in the world of Data Science. Knowledge-based biomedical Data Science involves the design and implementation of computer systems that act as if they knew about biomedicine. There are many ways in which a computational approach might act as if it knew something: for example, it might be able to answer a natural language question about a biomedical topic, or pass an exam; it might be able to use existing biomedical knowledge to rank or evaluate hypotheses; it might explain or interpret data in light of prior knowledge, either in a Bayesian or other sort of framework. These are all examples of automated reasoning that act on computational representations of knowledge. After a brief survey of existing approaches to knowledge-based data science, this position paper argues that such research is ripe for expansion, and expanded application.",fullPaper,jv52
p133,fdadb61f774a6da2cecee53e65b399dbad628163,c82,Workshop on Interdisciplinary Software Engineering Research,Data Science,Abstract content,poster,cp82
p134,e420259fd53d15c2b6cb8906027d5a100ca356d7,c69,International Conference on Parallel Processing,Data science for building energy management: A review,Abstract content,poster,cp69
p135,f4e66bd035e195f539f1b65a5aaec0e873cdee29,j53,Computer Applications in Engineering Education,Data science in education: Big data and learning analytics,This paper considers the data science and the summaries significance of Big Data and Learning Analytics in education. The widespread platform of making high‐quality benefits that could be achieved by exhausting big data techniques in the field of education is considered. One principal architecture framework to support education research is proposed.,fullPaper,jv53
p136,c9ed1ad1a3a08bf5ebebe8105805dd102546b8f3,c60,IEEE International Conference on Software Engineering and Formal Methods,Process-Structure Linkages Using a Data Science Approach: Application to Simulated Additive Manufacturing Data,Abstract content,poster,cp60
p137,cdd5d0a3e2ba0e1f4b5bcd3115e5f5b6536e24f9,c72,Intelligent Systems in Molecular Biology,The Data Science Design Manual,Abstract content,poster,cp72
p138,dcecbf916b9e2c61042f0dc992bdfd8ac1c99b8d,c78,Neural Information Processing Systems,Materials Knowledge Systems in Python—a Data Science Framework for Accelerated Development of Hierarchical Materials,Abstract content,poster,cp78
p139,0d2fb0c5c326bf649d6903cfb7305786328b4667,j54,Nature Biomedical Engineering,Surgical data science for next-generation interventions,Abstract content,fullPaper,jv54
p140,972edbd8bd19486a37c0a9f34508634fc8733529,c88,Symposium on the Theory of Computing,Our path to better science in less time using open data science tools,Abstract content,poster,cp88
p141,38fadf7c21c32b183fa3dcf32da1044e8441b813,c59,British Computer Society Conference on Human-Computer Interaction,The Data Science Handbook,"Data frameworks, modules, and toolkits are for doing data but they’re also a good way to dive into the without actually understanding data In this you’ll how many of the most fundamental data science tools and algorithms work by implementing them from scratch. If you an aptitude for mathematics and some programming you get with the math and at the core of data and with hacking skills you need to get started as a data scientist. Today’s messy glut of data holds answers to questions no one’s even thought to ask. This book provides you with the know-how to dig those answers out. Get a crash course in Python Learn the basics of linear algebra, statistics, and probability—and understand how and when they're used in data science Collect, explore, clean, munge, and manipulate data Dive into the fundamentals of machine learning Implement models such as k-nearest Neighbors, Naive Bayes, linear and logistic regression, decision trees, neural networks, and clustering Explore recommender systems, natural language processing, network analysis, MapReduce, and databases of microbial community dynamics, Support Vector Machines, a robust prediction method with applications in bioinformatics, Bayesian Model Selection for Data with High Dimension, High dimensional statistical inference: theoretical development to data analytics, Big data challenges in genomics, Analysis of microarray gene expression data using information theory and stochastic algorithm, Hybrid Models, Markov Chain Monte Carlo Methods: Theory and Practice, and more. Provides the authority and expertise of leading contributors from an international board of authors Presents the latest release in the Handbook of Statistics series Updated release includes the latest information on Principles and Methods for Data Science file organization with UNIX/Linux shell, version control with Git and GitHub, and reproducible document preparation. This book is a textbook for a first course in data science. No previous knowledge of R is necessary, although some experience with programming may be helpful. The book is divided into six parts: R, data visualization, statistics with R, data wrangling, machine learning, and productivity tools. Each part has several chapters meant to be presented as one lecture. The author uses motivating case studies that realistically mimic a data scientist’s experience. He starts by asking specific questions and answers these through data analysis so concepts are learned as a means to answering the questions. Examples of the case studies included are: US murder rates by state, self-reported student heights, trends in world health and economics, the impact of vaccines on infectious disease rates, the financial crisis of 2007-2008, election forecasting, building a baseball team, image processing of hand-written digits, and movie recommendation systems. The statistical concepts used to answer the case study questions are only briefly introduced, so complementing with a probability and statistics textbook is highly recommended for in-depth understanding of these concepts. If you read and understand the chapters and complete the exercises, you will be prepared to learn the more advanced concepts and skills needed to become an expert. The statistics profession is at a unique point in history. The need for valid statistical tools is greater than ever; data sets are massive, often measuring hundreds of thousands of measurements for a single subject. The field is ready to move towards clear objective benchmarks under which tools can be evaluated. Targeted learning allows (1) the full generalization and utilization of cross-validation as an estimator selection tool so that the subjective choices made by humans are now made by the machine, and (2) targeting the fitting of the probability distribution of the data toward the target parameter representing the scientific question of interest. This book is aimed at both statisticians and applied researchers interested in causal inference and general effect estimation for observational and experimental data. Part I is an accessible introduction to super learning and the targeted maximum likelihood estimator, including related concepts necessary to understand and apply these methods. Parts II-IX handle complex data structures and topics applied researchers will immediately recognize from their own research, including time-to-event outcomes, direct and indirect effects, positivity violations, case-control studies, and algorithmic foundations of data science, including machine learning, high-dimensional geometry, and analysis of large networks. Topics include the counterintuitive nature of data in high dimensions, important linear algebraic techniques such as singular value decomposition, the theory of random walks and Markov chains, the fundamentals of and important algorithms for machine learning, algorithms and analysis for clustering, probabilistic models for large networks, representation learning including topic modelling and non-negative matrix factorization, wavelets and compressed sensing. Important probabilistic techniques are developed including the law of large numbers, tail inequalities, analysis of random projections, generalization guarantees in machine learning, and moment methods for analysis of phase transitions in large random graphs. Additionally, important structural and complexity measures are discussed such as matrix norms and VC-dimension. This book is suitable for both undergraduate and graduate courses in the design and analysis of algorithms for data. As telescopes, detectors, and computers grow ever more powerful, the volume of data at the disposal of astronomers and astrophysicists will enter the petabyte domain, providing accurate measurements for billions of celestial objects. This book provides a comprehensive and accessible introduction to the cutting-edge statistical methods needed to efficiently analyze complex data sets from astronomical surveys such as the Panoramic Survey Telescope and Rapid Response System, the Dark Energy Survey, and the upcoming Large Synoptic Survey Telescope. It serves as a practical handbook for graduate students and advanced undergraduates in physics and astronomy, and as an indispensable reference for researchers. Statistics, Data Mining, and Machine Learning in Astronomy presents a wealth of practical analysis problems, evaluates techniques for solving them, and explains how to use various approaches for different types and sizes of data sets. For all applications described in the book, Python code and example data sets are provided. The supporting data sets have been carefully selected from contemporary astronomical surveys (for example, the Sloan Digital Sky Survey) and are easy to download and use. The accompanying Python code is publicly available, well documented, and follows uniform coding standards. Together, the data sets and code enable readers to reproduce all the figures and examples, evaluate the methods, and adapt them to their own fields of interest. Describes the most useful statistical and data-mining methods for extracting knowledge from huge and complex astronomical data sets Features real-world data sets from contemporary astronomical surveys Uses a freely available Python codebase throughout Ideal for students and working astronomers competitive by analyzing the data their support their decision-making an of the concepts, tools, and techniques behind the fields of data and artificial intelligence (AI) applied to business and industries. of Applied Data and Artificial in and all stages of data to AI their application real across industries—from and and together practice and science to successful from both and offline",poster,cp59
p142,00b1fa3c7170563567fb22a9bb6ff4c7b2e8853e,c11,Hawaii International Conference on System Sciences,Comparing Data Science Project Management Methodologies via a Controlled Experiment,"Data Science is an emerging field with a significant research focus on improving the techniques available to analyze data. However, there has been much less focus on how people should work together on a data science project. In this paper, we report on the results of an experiment comparing four different methodologies to manage and coordinate a data science project. We first introduce a model to compare different project management methodologies and then report on the results of our experiment. The results from our experiment demonstrate that there are significant differences based on the methodology used, with an Agile Kanban methodology being the most effective and surprisingly, an Agile Scrum methodology being the least effective.",fullPaper,cp11
p143,12e17fa5dd5715c563aadf705427da84817f100f,c74,IEEE International Conference on Tools with Artificial Intelligence,Data science: Data science tutorials,Abstract content,poster,cp74
p144,14cf58288f88d8a3ef1f710769d3f4c9ae63abe5,c84,The Web Conference,Data Science,Abstract content,poster,cp84
p145,589ebdd0d7b4a58f7fdfb07f116f62681bb9a915,j20,Proceedings of the National Academy of Sciences of the United States of America,Hack weeks as a model for data science education and collaboration,"Significance As scientific disciplines grapple with more datasets of rapidly increasing complexity and size, new approaches are urgently required to introduce new statistical and computational tools into research communities and improve the cross-disciplinary exchange of ideas. In this paper, we introduce a type of scientific workshop, called a hack week, which allows for fast dissemination of new methodologies into scientific communities and fosters exchange and collaboration within and between disciplines. We present implementations of this concept in astronomy, neuroscience, and geoscience and show that hack weeks produce positive learning outcomes, foster lasting collaborations, yield scientific results, and promote positive attitudes toward open science. Across many scientific disciplines, methods for recording, storing, and analyzing data are rapidly increasing in complexity. Skillfully using data science tools that manage this complexity requires training in new programming languages and frameworks as well as immersion in new modes of interaction that foster data sharing, collaborative software development, and exchange across disciplines. Learning these skills from traditional university curricula can be challenging because most courses are not designed to evolve on time scales that can keep pace with rapidly shifting data science methods. Here, we present the concept of a hack week as an effective model offering opportunities for networking and community building, education in state-of-the-art data science methods, and immersion in collaborative project work. We find that hack weeks are successful at cultivating collaboration and facilitating the exchange of knowledge. Participants self-report that these events help them in both their day-to-day research as well as their careers. Based on our results, we conclude that hack weeks present an effective, easy-to-implement, fairly low-cost tool to positively impact data analysis literacy in academic disciplines, foster collaboration, and cultivate best practices.",fullPaper,jv20
p146,04437c6fd15567182221d43e58b4fbea59bcaef1,c32,International Conference on Software Technology: Methods and Tools,"Introduction to Data Science - A Python Approach to Concepts, Techniques and Applications",Abstract content,poster,cp32
p147,61b3ce156347a7f107df75924a45f81f12a0ef14,c34,IEEE Working Conference on Mining Software Repositories,Surgical data science: the new knowledge domain,"Abstract Healthcare in general, and surgery/interventional care in particular, is evolving through rapid advances in technology and increasing complexity of care, with the goal of maximizing the quality and value of care. Whereas innovations in diagnostic and therapeutic technologies have driven past improvements in the quality of surgical care, future transformation in care will be enabled by data. Conventional methodologies, such as registry studies, are limited in their scope for discovery and research, extent and complexity of data, breadth of analytical techniques, and translation or integration of research findings into patient care. We foresee the emergence of surgical/interventional data science (SDS) as a key element to addressing these limitations and creating a sustainable path toward evidence-based improvement of interventional healthcare pathways. SDS will create tools to measure, model, and quantify the pathways or processes within the context of patient health states or outcomes and use information gained to inform healthcare decisions, guidelines, best practices, policy, and training, thereby improving the safety and quality of healthcare and its value. Data are pervasive throughout the surgical care pathway; thus, SDS can impact various aspects of care, including prevention, diagnosis, intervention, or postoperative recovery. The existing literature already provides preliminary results, suggesting how a data science approach to surgical decision-making could more accurately predict severe complications using complex data from preoperative, intraoperative, and postoperative contexts, how it could support intraoperative decision-making using both existing knowledge and continuous data streams throughout the surgical care pathway, and how it could enable effective collaboration between human care providers and intelligent technologies. In addition, SDS is poised to play a central role in surgical education, for example, through objective assessments, automated virtual coaching, and robot-assisted active learning of surgical skill. However, the potential for transforming surgical care and training through SDS may only be realized through a cultural shift that not only institutionalizes technology to seamlessly capture data but also assimilates individuals with expertise in data science into clinical research teams. Furthermore, collaboration with industry partners from the inception of the discovery process promotes optimal design of data products as well as their efficient translation and commercialization. As surgery continues to evolve through advances in technology that enhance delivery of care, SDS represents a new knowledge domain to engineer surgical care of the future.",poster,cp34
p148,427a613d349d305726e1c4c7935b33c79de5850a,c13,International Conference on Data Science and Advanced Analytics,Python Data Science Handbook: Essential Tools for Working with Data,"For many researchers, Python is a first-class tool mainly because of its libraries for storing, manipulating, and gaining insight from data. Several resources exist for individual pieces of this data science stack, but only with the Python Data Science Handbook do you get them all IPython, NumPy, Pandas, Matplotlib, Scikit-Learn, and other related tools. Working scientists and data crunchers familiar with reading and writing Python code will find this comprehensive desk reference ideal for tackling day-to-day issues: manipulating, transforming, and cleaning data; visualizing different types of data; and using data to build statistical or machine learning models. Quite simply, this is the must-have reference for scientific computing in Python. With this handbook, youll learn how to use:IPython and Jupyter: provide computational environments for data scientists using PythonNumPy: includes the ndarray for efficient storage and manipulation of dense data arrays in PythonPandas: features the DataFrame for efficient storage and manipulation of labeled/columnar data in PythonMatplotlib: includes capabilities for a flexible range of data visualizations in PythonScikit-Learn: for efficient and clean Python implementations of the most important and established machine learning algorithms",poster,cp13
p149,87a7e55b4c3116751edb4b0f74e0484eaf7a853d,j55,Information systems research,"Editorial - Big Data, Data Science, and Analytics: The Opportunity and Challenge for IS Research","We address key questions related to the explosion of interest in the emerging fields of big data, analytics, and data science. We discuss the novelty of the fields and whether the underlying questions are fundamentally different, the strengths that the information systems IS community brings to this discourse, interesting research questions for IS scholars, the role of predictive and explanatory modeling, and how research in this emerging area should be evaluated for contribution and significance.",fullPaper,jv55
p150,ce0b7ee60920f9b37f88cab785cb8b4dc337e89f,c110,IEEE International Conference on Automatic Face & Gesture Recognition,Educational data science in massive open online courses,"The current massive open online course (MOOC) euphoria is revolutionizing online education. Despite its expediency, there is considerable skepticism over various concerns. In order to resolve some of these problems, educational data science (EDS) has been used with success. MOOCs provide a wealth of information about the way in which a large number of learners interact with educational platforms and engage with the courses offered. This extensive amount of data provided by MOOCs concerning students' usage information is a gold mine for EDS. This paper aims to provide the reader with a complete and comprehensive review of the existing literature that helps us understand the application of EDS in MOOCs. The main works in this area are described and grouped by task or issue to be solved, along with the techniques used. WIREs Data Mining Knowl Discov 2017, 7:e1187. doi: 10.1002/widm.1187",poster,cp110
p151,224eb3407b50533668b6c1caa55a720688b8b532,j56,International Journal of Information Management,"A review and future direction of agile, business intelligence, analytics and data science",Abstract content,fullPaper,jv56
p152,31485e1213dd886fa2b668eefcd9b13533d8a9fe,c101,International Conference on Automatic Face and Gesture Recognition,Big data and data science: what should we teach?,"The era of big data has arrived. Big data bring us the data‐driven paradigm and enlighten us to challenge new classes of problems we were not able to solve in the past. We are beginning to see the impacts of big data in every aspect of our lives and society. We need a science that can address these big data problems. Data science is a new emerging discipline that was termed to address challenges that we are facing and going to face in the big data era. Thus, education in data science is the key to success, and we need concrete strategies and approaches to better educate future data scientists. In this paper, we discuss general concepts on big data, data science, and data scientists and show the results of an extensive survey on current data science education in United States. Finally, we propose various approaches that data science education should aim to accomplish.",poster,cp101
p153,a1dc9a3df54ac24712fc47ac5f0b116f1043b95e,c66,Annual Conference on Innovation and Technology in Computer Science Education,R – Data Science,Abstract content,poster,cp66
p154,7bd38f01029f12ca51b91de5d2164eb65a0b1d9d,c109,International Conference on Mobile Data Management,Surgical Data Science: Enabling Next-Generation Surgery,Abstract content,poster,cp109
p155,9c1b9598f82f9ed7d75ef1a9e627496759aa2387,c108,International Conference on Information Integration and Web-based Applications & Services,"Data Science, Predictive Analytics, and Big Data: A Revolution that Will Transform Supply Chain Design and Management","We illuminate the myriad of opportunities for research where supply chain management intersects with data science, predictive analytics, and big data, collectively referred to as DPB. We show that these terms are not only becoming popular but are also relevant to supply chain research and education. Data science requires both domain knowledge and a broad set of quantitative skills, but there is a dearth of literature on the topic and many questions. We call for research on skills that are needed by SCM data scientists and discuss how such skills and domain knowledge affect the effectiveness of a SCM data scientist. Such knowledge is crucial to developing future supply chain leaders. We propose definitions of data science and predictive analytics as applied to supply chain management. We examine possible applications of DPB in practice and provide examples of research questions from these applications, as well as examples of research questions employing DPB that stem from management theories. Finally, we propose specific steps interested researchers can take to respond to our call for research on the intersection of supply chain management and DPB.",poster,cp108
p156,3b963487cbf944d51f33c2a0b41eb2aed7c68b89,c37,Asia-Pacific Software Engineering Conference,Locating ethics in data science: responsibility and accountability in global and distributed knowledge production systems,"The distributed and global nature of data science creates challenges for evaluating the quality, import and potential impact of the data and knowledge claims being produced. This has significant consequences for the management and oversight of responsibilities and accountabilities in data science. In particular, it makes it difficult to determine who is responsible for what output, and how such responsibilities relate to each other; what ‘participation’ means and which accountabilities it involves, with regard to data ownership, donation and sharing as well as data analysis, re-use and authorship; and whether the trust placed on automated tools for data mining and interpretation is warranted (especially as data processing strategies and tools are often developed separately from the situations of data use where ethical concerns typically emerge). To address these challenges, this paper advocates a participative, reflexive management of data practices. Regulatory structures should encourage data scientists to examine the historical lineages and ethical implications of their work at regular intervals. They should also foster awareness of the multitude of skills and perspectives involved in data science, highlighting how each perspective is partial and in need of confrontation with others. This approach has the potential to improve not only the ethical oversight for data science initiatives, but also the quality and reliability of research outputs. This article is part of the themed issue ‘The ethical impact of data science’.",poster,cp37
p157,dc86a7295d737fc2f195b67a273e90b549bd6272,j57,Business & Information Systems Engineering,Business Analytics and Data Science: Once Again?,Abstract content,fullPaper,jv57
p158,c740a6816155fd123081d2f78926a0d3819926e7,c95,IEEE International Conference on Computer Vision,LibGuides: *Data Science: Data Science Resources,"Data science resources, from finding ebooks and blogs, to finding raw datasets and analysis. Learn about data science resources, analysis, communities and data management. Also learn about hte datasets openly available and dataset purchase program.",poster,cp95
p159,bfd6caddec8a98d531ee9f1f7ebf5833797cd5e3,c1,Technical Symposium on Computer Science Education,Introducing Data Science to School Kids,"Data-driven decision making is fast becoming a necessary skill in jobs across the board. The industry today uses analytics and machine learning to get useful insights from a wealth of digital information in order to make decisions. With data science becoming an important skill needed in varying degrees of complexity by the workforce of the near future, we felt the need to expose school-goers to its power through a hands-on exercise. We organized a half-day long data science tutorial for kids in grades 5 through 9 (10-15 years old). Our aim was to expose them to the full cycle of a typical supervised learning approach - data collection, data entry, data visualization, feature engineering, model building, model testing and data permissions. We discuss herein the design choices made while developing the dataset, the method and the pedagogy for the tutorial. These choices aimed to maximize student engagement while ensuring minimal pre-requisite knowledge. This was a challenging task given that we limited the pre-requisites for the kids to the knowledge of counting, addition, percentages, comparisons and a basic exposure to operating computers. By designing an exercise with the stated principles, we were able to provide to kids an exciting, hands-on introduction to data science, as confirmed by their experiences. To the best of the authors' knowledge, the tutorial was the first of its kind. Considering the positive reception of such a tutorial, we hope that educators across the world are encouraged to introduce data science in their respective curricula for high-schoolers and are able to use the principles laid out in this work to build full-fledged courses.",fullPaper,cp1
p160,bd1c1d5540f246090e740c0d5a0fa7f2c64059d1,c10,Big Data,Data Science and its Relationship to Big Data and Data-Driven Decision Making,"Companies have realized they need to hire data scientists, academic institutions are scrambling to put together data-science programs, and publications are touting data science as a hot-even ""sexy""-career choice. However, there is confusion about what exactly data science is, and this confusion could lead to disillusionment as the concept diffuses into meaningless buzz. In this article, we argue that there are good reasons why it has been hard to pin down exactly what is data science. One reason is that data science is intricately intertwined with other important concepts also of growing importance, such as big data and data-driven decision making. Another reason is the natural tendency to associate what a practitioner does with the definition of the practitioner's field; this can result in overlooking the fundamentals of the field. We believe that trying to define the boundaries of data science precisely is not of the utmost importance. We can debate the boundaries of the field in an academic setting, but in order for data science to serve business effectively, it is important (i) to understand its relationships to other important related concepts, and (ii) to begin to identify the fundamental principles underlying data science. Once we embrace (ii), we can much better understand and explain exactly what data science has to offer. Furthermore, only once we embrace (ii) should we be comfortable calling it data science. In this article, we present a perspective that addresses all these concepts. We close by offering, as examples, a partial list of fundamental principles underlying data science.",fullPaper,cp10
p161,cd247b7830fb58c6f019a79ae9679251176e8342,c48,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",Game Theory for Data Science: Eliciting Truthful Information,"Intelligent systems often depend on data provided by information agents, for example, sensor data or crowdsourced human computation. Providing accurate and relevant data requires costly effort that agents may not always be willing to provide. Thus, it becomes important not only to verify the correctness of data, but also to provide incentives so that agents that provide high-quality data are rewarded while those that do not are discouraged by low rewards. We cover different settings and the assumptions they admit, including sensing, human computation, peer grading, reviews, and predictions. We survey different incentive mechanisms, including proper scoring rules, prediction markets and peer prediction, Bayesian Truth Serum, Peer Truth Serum, Correlated Agreement, and the settings where each of them would be suitable. As an alternative, we also consider reputation mechanisms. We complement the game-theoretic analysis with practical examples of applications in prediction platforms, community sensing, and peer grading.",poster,cp48
p162,021865bb9fcc59814d2ce84d086554e5e0259779,j58,Journal of Data and Information Science,"Big Metadata, Smart Metadata, and Metadata Capital: Toward Greater Synergy Between Data Science and Metadata","Abstract Purpose The purpose of the paper is to provide a framework for addressing the disconnect between metadata and data science. Data science cannot progress without metadata research. This paper takes steps toward advancing the synergy between metadata and data science, and identifies pathways for developing a more cohesive metadata research agenda in data science. Design/methodology/approach This paper identifies factors that challenge metadata research in the digital ecosystem, defines metadata and data science, and presents the concepts big metadata, smart metadata, and metadata capital as part of a metadata lingua franca connecting to data science. Findings The “utilitarian nature” and “historical and traditional views” of metadata are identified as two intersecting factors that have inhibited metadata research. Big metadata, smart metadata, and metadata capital are presented as part of a metadata lingua franca to help frame research in the data science research space. Research limitations There are additional, intersecting factors to consider that likely inhibit metadata research, and other significant metadata concepts to explore. Practical implications The immediate contribution of this work is that it may elicit response, critique, revision, or, more significantly, motivate research. The work presented can encourage more researchers to consider the significance of metadata as a research worthy topic within data science and the larger digital ecosystem. Originality/value Although metadata research has not kept pace with other data science topics, there is little attention directed to this problem. This is surprising, given that metadata is essential for data science endeavors. This examination synthesizes original and prior scholarship to provide new grounding for metadata research in data science.",fullPaper,jv58
p163,f447afeccbdb9ed5df15c44011aec9c018d4b2c4,j58,Journal of Data and Information Science,Big Data and Data Science: Opportunities and Challenges of iSchools,"Abstract Due to the recent explosion of big data, our society has been rapidly going through digital transformation and entering a new world with numerous eye-opening developments. These new trends impact the society and future jobs, and thus student careers. At the heart of this digital transformation is data science, the discipline that makes sense of big data. With many rapidly emerging digital challenges ahead of us, this article discusses perspectives on iSchools’ opportunities and suggestions in data science education. We argue that iSchools should empower their students with “information computing” disciplines, which we define as the ability to solve problems and create values, information, and knowledge using tools in application domains. As specific approaches to enforcing information computing disciplines in data science education, we suggest the three foci of user-based, tool-based, and application-based. These three foci will serve to differentiate the data science education of iSchools from that of computer science or business schools. We present a layered Data Science Education Framework (DSEF) with building blocks that include the three pillars of data science (people, technology, and data), computational thinking, data-driven paradigms, and data science lifecycles. Data science courses built on the top of this framework should thus be executed with user-based, tool-based, and application-based approaches. This framework will help our students think about data science problems from the big picture perspective and foster appropriate problem-solving skills in conjunction with broad perspectives of data science lifecycles. We hope the DSEF discussed in this article will help fellow iSchools in their design of new data science curricula.",fullPaper,jv58
p164,4a6d46962d3f58d278cfb46d3ddebbb30bf275f5,j59,IEEE Computer Graphics and Applications,Geographic Data Science,"Data science methods and approaches address all stages of transition from data to knowledge and action. Visualization of this data is essential for human understanding of the subject under study, analytical reasoning about it, and generating new knowledge. Geographic data science deals with data that incorporates spatial and, often, temporal elements. The articles selected for this special issue represent a mix of theoretical approaches and novel applications of geographic data science.",fullPaper,jv59
p165,4676cf6b5e97bfcdb7654487ed76ebf2ef6a4035,c54,International Workshop on Agent-Oriented Software Engineering,An Introduction to Data Science,"An Introduction to Data Scienceby Jeffrey S. Saltz and Jeffrey M. Stanton is an easy-to-read, gentle introduction for people with a wide range of backgrounds into the world of data science. Needing no prior coding experience or a deep understanding of statistics, this book uses the R programming language and RStudio platform to make data science welcoming and accessible for all learners. After introducing the basics of data science, the book builds on each previous concept to explain R programming from the ground up. Readers will learn essential skills in data science through demonstrations of how to use data to construct models, predict outcomes, and visualize data.",poster,cp54
p166,5b42e8ab6542fbbc11d84b07b34443a6853f96f1,j57,Business & Information Systems Engineering,Responsible Data Science,Abstract content,fullPaper,jv57
p167,96f5a9360ccfd1c5c4210dc62948baac234c372d,c99,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,Predicting data science sociotechnical execution challenges by categorizing data science projects,"The challenge in executing a data science project is more than just identifying the best algorithm and tool set to use. Additional sociotechnical challenges include items such as how to define the project goals and how to ensure the project is effectively managed. This paper reports on a set of case studies where researchers were embedded within data science teams and where the researcher observations and analysis was focused on the attributes that can help describe data science projects and the challenges faced by the teams executing these projects, as opposed to the algorithms and technologies that were used to perform the analytics. Based on our case studies, we identified 14 characteristics that can help describe a data science project. We then used these characteristics to create a model that defines two key dimensions of the project. Finally, by clustering the projects within these two dimensions, we identified four types of data science projects, and based on the type of project, we identified some of the sociotechnical challenges that project teams should expect to encounter when executing data science projects.",poster,cp99
p168,62806c60226d54ba1a4455bb1d7d2f034ef7c29a,c61,Jahrestagung der Gesellschaft für Informatik,"Introducing Data Science: Big Data, Machine Learning, and more, using Python tools","Summary Introducing Data Science teaches you how to accomplish the fundamental tasks that occupy data scientists. Using the Python language and common Python libraries, you'll experience firsthand the challenges of dealing with data at scale and gain a solid foundation in data science. Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications. About the Technology Many companies need developers with data science skills to work on projects ranging from social media marketing to machine learning. Discovering what you need to learn to begin a career as a data scientist can seem bewildering. This book is designed to help you get started. About the BookIntroducing Data Science Introducing Data Science explains vital data science concepts and teaches you how to accomplish the fundamental tasks that occupy data scientists. Youll explore data visualization, graph databases, the use of NoSQL, and the data science process. Youll use the Python language and common Python libraries as you experience firsthand the challenges of dealing with data at scale. Discover how Python allows you to gain insights from data sets so big that they need to be stored on multiple machines, or from data moving so quickly that no single machine can handle it. This book gives you hands-on experience with the most popular Python data science libraries, Scikit-learn and Stats Models. After reading this book, youll have the solid foundation you need to start a career in data science. Whats Inside Handling large data Introduction to machine learning Using Python to work with data Writing data science algorithms About the ReaderThis book assumes you're comfortable reading code in Python or a similar language, such as C, Ruby, or JavaScript. No prior experience with data science is required. About the Authors Davy Cielen, Arno D. B. Meysman, and Mohamed Ali are the founders and managing partners of Optimately and Maiton, where they focus on developing data science projects and solutions in various sectors.",poster,cp61
p169,4d01ea7a22f869ee888725d5f42f856eeeb22da0,c61,Jahrestagung der Gesellschaft für Informatik,Ushering in a New Frontier in Geospace Through Data Science,"Our understanding and specification of solar‐terrestrial interactions benefit from taking advantage of comprehensive data‐intensive approaches. These data‐driven methods are taking on new importance in light of the shifting data landscape of the geospace system, which extends from the near Earth space environment, through the magnetosphere and interplanetary space, to the Sun. The space physics community faces both an exciting opportunity and an important imperative to create a new frontier built at the intersection of traditional approaches and state‐of‐the‐art data‐driven sciences and technologies. This brief commentary addresses the current paradigm of geospace science and the emerging need for data science innovation, discusses the meaning of data science in the context of geospace, and highlights community efforts to respond to the changing landscape.",poster,cp61
p170,9141efc0d91ab0bda9b264ff6d1df5f20fd1dbb0,c57,IEEE International Conference on Engineering of Complex Computer Systems,Transdisciplinary Foundations of Geospatial Data Science,"Recent developments in data mining and machine learning approaches have brought lots of excitement in providing solutions for challenging tasks (e.g., computer vision). However, many approaches have limited interpretability, so their success and failure modes are difficult to understand and their scientific robustness is difficult to evaluate. Thus, there is an urgent need for better understanding of the scientific reasoning behind data mining and machine learning approaches. This requires taking a transdisciplinary view of data science and recognizing its foundations in mathematics, statistics, and computer science. Focusing on the geospatial domain, we apply this crucial transdisciplinary perspective to five common geospatial techniques (hotspot detection, colocation detection, prediction, outlier detection and teleconnection detection). We also describe challenges and opportunities for future advancement.",poster,cp57
p171,dd340315c44a9c68391d8d2f600a0adc76b70c09,c12,International Conference on Statistical and Scientific Database Management,Fides: Towards a Platform for Responsible Data Science,"Issues of responsible data analysis and use are coming to the forefront of the discourse in data science research and practice, with most significant efforts to date on the part of the data mining, machine learning, and security and privacy communities. In these fields, the research has been focused on analyzing the fairness, accountability and transparency (FAT) properties of specific algorithms and their outputs. Although these issues are most apparent in the social sciences where fairness is interpreted in terms of the distribution of resources across protected groups, management of bias in source data affects a variety of fields. Consider climate change studies that require representative data from geographically diverse regions, or supply chain analyses that require data that represents the diversity of products and customers. Any domain that involves sparse or sampled data has exposure to potential bias. In this vision paper, we argue that FAT properties must be considered as database system issues, further upstream in the data science lifecycle: bias in source data goes unnoticed, and bias may be introduced during pre-processing (fairness), spurious correlations lead to reproducibility problems (accountability), and assumptions made during pre-processing have invisible but significant effects on decisions (transparency). As machine learning methods continue to be applied broadly by non-experts, the potential for misuse increases. We see a need for a data sharing and collaborative analytics platform with features to encourage (and in some cases, enforce) best practices at all stages of the data science lifecycle. We describe features of such a platform, which we term Fides, in the context of urban analytics, outlining a systems research agenda in responsible data science.",fullPaper,cp12
p172,19bb52bec8b5ced3175f4c3ef1b8fb7027cc5ff1,c97,Interspeech,Applications of Python to evaluate environmental data science problems,"There is a significant convergence of interests in the research community efforts to advance the development and application of software resources (capable of handling the relevant mathematical algorithms to provide scalable information) for solving data science problems. Anaconda is one of the many open source platforms that facilitate the use of open source programming languages (R, Python) for large‐scale data processing, predictive analytics, and scientific computing. The environmental research community may choose to adapt the use of either of the R or the Python programming languages for analyzing the data science problems on the Anaconda platform. This study demonstrated the applications of using Scikit‐learn (a Python machine learning library package) on Anaconda platform for analyzing the in‐bus carbon dioxide concentrations by (i) importing the data into Spyder (Python 3.6) in Anaconda, (ii) performing an exploratory data analysis, (iii) performing dimensionality reduction through RandomForestRegressor feature selection, (iv) developing statistical regression models, and (v) generating regression decision tree models with DecisionTreeRegressor feature. The readers may adopt the methods (inclusive of the Python coding) discussed in this article to successfully address their own data science problems. © 2017 American Institute of Chemical Engineers Environ Prog, 36: 1580–1586, 2017",poster,cp97
p173,3a8da09a87f06273c19fb61573b299388f8d1673,c89,Conference on Uncertainty in Artificial Intelligence,Data science vs. statistics: two cultures?,Abstract content,poster,cp89
p174,37095b714dad5895d946b1f8435a3a38dee1be8b,c108,International Conference on Information Integration and Web-based Applications & Services,"Data quality for data science, predictive analytics, and big data in supply chain management: An introduction to the problem and suggestions for research and applications",Abstract content,poster,cp108
p175,7c614fe86cc11c0430dd12b44e018e16e5dcf742,j16,American Statistician,A Guide to Teaching Data Science,"ABSTRACT Demand for data science education is surging and traditional courses offered by statistics departments are not meeting the needs of those seeking training. This has led to a number of opinion pieces advocating for an update to the Statistics curriculum. The unifying recommendation is that computing should play a more prominent role. We strongly agree with this recommendation, but advocate the main priority is to bring applications to the forefront as proposed by Nolan and Speed in 1999. We also argue that the individuals tasked with developing data science courses should not only have statistical training, but also have experience analyzing data with the main objective of solving real-world problems. Here, we share a set of general principles and offer a detailed guide derived from our successful experience developing and teaching a graduate-level, introductory data science course centered entirely on case studies. We argue for the importance of statistical thinking, as defined by Wild and Pfannkuch in 1999 and describe how our approach teaches students three key skills needed to succeed in data science, which we refer to as creating, connecting, and computing. This guide can also be used for statisticians wanting to gain more practical knowledge about data science before embarking on teaching an introductory course. Supplementary materials for this article are available online.",fullPaper,jv16
p176,0a9b30386408595ff0b3155d4de4a56dad80a97b,c54,International Workshop on Agent-Oriented Software Engineering,The ambiguity of data science team roles and the need for a data science workforce framework,"This paper first reviews the benefits of well-defined roles and then discusses the current lack of standardized roles within the data science community, perhaps due to the newness of the field. Specifically, the paper reports on five case studies exploring five different attempts to define a standard set of roles. These case studies explore the usage of roles from an industry perspective as well as from national standard big data committee efforts. The paper then leverages the results of these case studies to explore the use of data science roles within online job postings. While some roles appeared frequently, such as data scientist and data engineer, no role was consistently used across all five case studies. Hence, the paper concludes by noting the need to create a data science workforce framework that could be used by students, employers, and academic institutions. This framework would enable organizations to staff their data science teams more accurately with the desired skillsets.",poster,cp54
p177,3af056b2aed8724dcddea074eb68aff6dd11c926,j60,PLoS Biology,Building the biomedical data science workforce,"This article describes efforts at the National Institutes of Health (NIH) from 2013 to 2016 to train a national workforce in biomedical data science. We provide an analysis of the Big Data to Knowledge (BD2K) training program strengths and weaknesses with an eye toward future directions aimed at any funder and potential funding recipient worldwide. The focus is on extramurally funded programs that have a national or international impact rather than the training of NIH staff, which was addressed by the NIH’s internal Data Science Workforce Development Center. From its inception, the major goal of BD2K was to narrow the gap between needed and existing biomedical data science skills. As biomedical research increasingly relies on computational, mathematical, and statistical thinking, supporting the training and education of the workforce of tomorrow requires new emphases on analytical skills. From 2013 to 2016, BD2K jump-started training in this area for all levels, from graduate students to senior researchers.",fullPaper,jv60
p178,c694c6a685a067393204f36e21c8917a49f02a9b,c3,Frontiers in Education Conference,Big data at work : the data science revolution and organizational psychology,"Foreword by Richard Klimoski 1. Building Understanding of the Data Science Revolution and IO Psychology Eden B. King, Scott Tonidandel, Jose M. Cortina, & Alexis A. Fink Part I: Big Issues for Big Data Methods 2. Big Data Platform Jacqueline Ryan 3. Statistical Methods for Big Data: A Scenic Tour Frederick L. Oswald & Dan J. Putka 4. Twitter Analysis: Methods for Data Management and a Word Count Dictionary to Measure City-Level Job Satisfaction Ivan Hernandez, Daniel A. Newman, & Gahyun Jeon 5. Data Visualization Evan F. Sinar 6. Sensing Big Data: Multimodal Information Interfaces for Exploration of Large Data Sets Jeffrey Stanton Part II: Big Ideas for Big Data in Organization 7. Implications of the Big Data Movement for the Advancement I-O Science and Practice Dan J. Putka & Frederick L. Oswald 8. Big Data in Talent Selection and Assessment A. James Illingworth, Michael Lippstreu, & Anne-Sophie Deprez-Sims 9. Big Data in Turnover/Retention John P. Hausknecht & Huisi (Jessica) Li 10. Using Big Data to Advance the Science of Team Effectiveness Steve W. J. Kozlowski, Georgia T. Chao, Chu-Hsiang (Daisy) Chang, & Rosemarie Fernandez 11. Using Big Data to Create Diversity and Inclusion in Organizations Whitney Botsford Morgan, Eric Dunleavy, & Peter D. DeVries 12. How Big Data Matters Richard A. Guzzo",poster,cp3
p179,6d5f2d663246c4b598309a13da78db7646996f67,c31,International Conference on Evaluation & Assessment in Software Engineering,Data science: Accelerating innovation and discovery in chemical engineering,"A ll of science and engineering, including chemical engineering, is being transformed by new sources of data from high-throughput experiments, observational studies, and simulation. In this new era of data-enabled science and engineering, discovery is no longer limited by the collection and processing of data but by data management, knowledge extraction, and the visualization of information. The term data science has become increasingly popular across industry, and academic disciplines to refer to the combination of strategies and tools for addressing the oncoming deluge of data. The term data scientist is a common descriptor of an engineer or scientist from any disciplinary background who is equipped to seamlessly process, analyze, and communicate in this data-intensive context. The core areas of data science are often identified as data management, statistical and machine learning, and visualization. In this Perspective, we present an overview of these core areas, discuss application areas from within chemical engineering research, and conclude with perspectives on how data science principles can be included in our training. As has been noted for several years, chemical engineers of all varieties, from the practicing process engineer to the academic researcher, are being asked more and more often to manipulate, transform, and analyze complex data sets. The complexity often stems from the size of the data set itself, but this is not the only factor. For example, the stream of information available to an engineer in a modern plant is tremendous because of the proliferation of inexpensive instrumentation and the nearly ubiquitous high bandwidth and low-latency connectivity. In the area of research and discovery, a student or researcher conducting data-intensive experiments, such as high-resolution particle tracking, might generate more data in an afternoon than a student from a previous decade in the entire time spent earning his or her Ph.D. For those conducting mathematical modeling and computer simulations, advanced algorithms and hardware now give simulators unprecedented resolution but at the cost of massive increases in the data set. Underlying all of these examples is the cheap (near free) cost of data storage and the ubiquitous availability of our data from cloud-based services. The aforementioned examples may appear to be vastly different from the outset. However, common themes in the limitation of our current approaches quickly emerge. Because our training of new chemical engineers (at all levels) has not kept pace with the explosion of data, each chemical engineer in the previous examples will likely approach her or his work in the same manner: manual searching for relationships in the data, classical visualization of monovariate or bivariate correlations in features, and a hypothesis-driven approach to science reminiscent of a data-poor era when the researcher or engineer could essentially manipulate relevant data in their mind. Simply put, without knowledge about and training on how to handle data skillfully, most of the information from our plants and refineries, our data-intensive experiments, and our computer simulations is thrown away, simply because we do not know how to extract knowledge from it. Fortunately, there is a potential solution on the horizon. Through the lens of the nascent field of data science, we can see an emergent (and limited) set of tasks needed by all of the previous chemical engineers: (1) to manage a huge data set consisting of ensembles of spatiotemporal data, (2) to sensibly read the data in a computationally scalable manner, and (3) to extract knowledge from this pile of information with robust techniques whose statistical reliability can be quantified. It also goes without saying that data science itself is not a panacea. Chemical engineering fundamentals are of the utmost importance, and Correspondence concerning this article should be addressed to D. A. C. Beck at dacb@uw.edu and J. Pfaendtner at jpfaendt@uw.edu.",poster,cp31
p180,97a3726b3f9395c8919c6271540d87d1c44e10ac,c13,International Conference on Data Science and Advanced Analytics,Deep feature synthesis: Towards automating data science endeavors,"In this paper, we develop the Data Science Machine, which is able to derive predictive models from raw data automatically. To achieve this automation, we first propose and develop the Deep Feature Synthesis algorithm for automatically generating features for relational datasets. The algorithm follows relationships in the data to a base field, and then sequentially applies mathematical functions along that path to create the final feature. Second, we implement a generalizable machine learning pipeline and tune it using a novel Gaussian Copula process based approach. We entered the Data Science Machine in 3 data science competitions that featured 906 other data science teams. Our approach beats 615 teams in these data science competitions. In 2 of the 3 competitions we beat a majority of competitors, and in the third, we achieved 94% of the best competitor's score. In the best case, with an ongoing competition, we beat 85.6% of the teams and achieved 95.7% of the top submissions score.",fullPaper,cp13
p181,45cc47c1beaad4e08a85c7dfc69cb10913f824ed,c34,IEEE Working Conference on Mining Software Repositories,On Model Discovery For Hosted Data Science Projects,"Alongside developing systems for scalable machine learning and collaborative data science activities, there is an increasing trend toward publicly shared data science projects, hosted in general or dedicated hosting services, such as GitHub and DataHub. The artifacts of the hosted projects are rich and include not only text files, but also versioned datasets, trained models, project documents, etc. Under the fast pace and expectation of data science activities, model discovery, i.e., finding relevant data science projects to reuse, is an important task in the context of data management for end-to-end machine learning. In this paper, we study the task and present the ongoing work on ModelHub Discovery, a system for finding relevant models in hosted data science projects. Instead of prescribing a structured data model for data science projects, we take an information retrieval approach by decomposing the discovery task into three major steps: project query and matching, model comparison and ranking, and processing and building ensembles with returned models. We describe the motivation and desiderata, propose techniques, and present opportunities and challenges for model discovery for hosted data science projects.",poster,cp34
p182,23a57b1e2beb4235d2020ed57f484c947e3d0816,c10,Big Data,The Quantified Self: Fundamental Disruption in Big Data Science and Biological Discovery,"A key contemporary trend emerging in big data science is the quantified self (QS)-individuals engaged in the self-tracking of any kind of biological, physical, behavioral, or environmental information as n=1 individuals or in groups. There are opportunities for big data scientists to develop new models to support QS data collection, integration, and analysis, and also to lead in defining open-access database resources and privacy standards for how personal data is used. Next-generation QS applications could include tools for rendering QS data meaningful in behavior change, establishing baselines and variability in objective metrics, applying new kinds of pattern recognition techniques, and aggregating multiple self-tracking data streams from wearable electronics, biosensors, mobile phones, genomic data, and cloud-based services. The long-term vision of QS activity is that of a systemic monitoring approach where an individual's continuous personal information climate provides real-time performance optimization suggestions. There are some potential limitations related to QS activity-barriers to widespread adoption and a critique regarding scientific soundness-but these may be overcome. One interesting aspect of QS activity is that it is fundamentally a quantitative and qualitative phenomenon since it includes both the collection of objective metrics data and the subjective experience of the impact of these data. Some of this dynamic is being explored as the quantified self is becoming the qualified self in two new ways: by applying QS methods to the tracking of qualitative phenomena such as mood, and by understanding that QS data collection is just the first step in creating qualitative feedback loops for behavior change. In the long-term future, the quantified self may become additionally transformed into the extended exoself as data quantification and self-tracking enable the development of new sense capabilities that are not possible with ordinary senses. The individual body becomes a more knowable, calculable, and administrable object through QS activity, and individuals have an increasingly intimate relationship with data as it mediates the experience of reality.",fullPaper,cp10
p183,f6705e68c71bc0b51ddb8d1e4f986c894ba8f34f,c99,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,"Data Science, Predictive Analytics, and Big Data in Supply Chain Management: Current State and Future Potential","While data science, predictive analytics, and big data have been frequently used buzzwords, rigorous academic investigations into these areas are just emerging. In this forward thinking article, we discuss the results of a recent large-scale survey on these topics among supply chain management (SCM) professionals, complemented with our experiences in developing, implementing, and administering one of the first master's degree programs in predictive analytics. As such, we effectively provide an assessment of the current state of the field via a large-scale survey, and offer insight into its future potential via the discussion of how a research university is training next-generation data scientists. Specifically, we report on the current use of predictive analytics in SCM and the underlying motivations, as well as perceived benefits and barriers. In addition, we highlight skills desired for successful data scientists, and provide illustrations of how predictive analytics can be implemented in the curriculum. Relying on one of the largest data sets of predictive analytics users in SCM collected to date and our experiences with one of the first master's degree programs in predictive analytics, it is our intent to provide a timely assessment of the field, illustrate its future potential, and motivate additional research and pedagogical advancements in this domain.",poster,cp99
p184,47134cbdfa7c44a55de5697a35b6652d0fcfee30,c22,International Conference on Data Technologies and Applications,Data Science in Libraries,"EDITOR'S SUMMARY 
 
The new field of data science involves advanced knowledge in statistics and computer science, combined with copious amounts of data. A report from the Big Data and Research Initiative under the Obama Administration, The Federal Big Data Research and Development Strategic Plan, calls attention to the roles that librarians will play in the future of data science. However, there are skills and management gaps librarians face that inhibit their ability to move forward in data science. A number of educational programs are now offered to remedy this problem, such as the Data and Visualization Institute for Librarians from North Carolina State University, the volunteer-led Library Carpentry program, and most recently, the Data Sciences in Libraries Project, funded by the IMLS. This project aims to get librarians and library managers together to discuss the world of data science and create a roadmap for strategic planning.",poster,cp22
p185,0ec1992151e28c5678832c0923e56aeb58caad53,c14,International Conference on Exploring Services Science,From Data Science to Value Creation,Abstract content,fullPaper,cp14
p186,6b705d7ef453d42d87a9099b31344adad2367f40,c86,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,EDISON Data Science Framework: A Foundation for Building Data Science Profession for Research and Industry,"Data Science is an emerging field of science, which requires a multi-disciplinary approach and should be built with a strong link to emerging Big Data and data driven technologies, and consequently needs re-thinking and re-design of both traditional educational models and existing courses. The education and training of Data Scientists currently lacks a commonly accepted, harmonized instructional model that reflects by design the whole lifecycle of data handling in modern, data driven research and the digital economy. This paper presents the EDISON Data Science Framework (EDSF) that is intended to create a foundation for the Data Science profession definition. The EDSF includes the following core components: Data Science Competence Framework (CF-DS), Data Science Body of Knowledge (DS-BoK), Data Science Model Curriculum (MC-DS), and Data Science Professional profiles (DSP profiles). The MC-DS is built based on CF-DS and DS-BoK, where Learning Outcomes are defined based on CF-DS competences and Learning Units are mapped to Knowledge Units in DS-BoK. In its own turn, Learning Units are defined based on the ACM Classification of Computer Science (CCS2012) and reflect typical courses naming used by universities in their current programmes. The paper provides example how the proposed EDSF can be used for designing effective Data Science curricula and reports the experience of implementing EDSF by the Champion Universities that cooperate with the EDISON project.",poster,cp86
p187,8ffe80d758a78810c7d5a33a088cd4529b8a6a4b,j61,Journal of Decision Systems,Data science: supporting decision-making,"Abstract Data science is a new academic trans-discipline that builds on 60 years of research about supporting decision-making in organisations. It is an important and potentially significant concept and practice. Contemplating the need for data scientists encourages academics and managers to examine issues of decision-maker rationality, data and data analysis needs, analytical tools, job skills and academic preparation. This article explores data science and the data professionals who will use new data streams and analytics to support decision-making. It also examines the dimensions that are changing in the data stream and the skills needed by data scientists to analyse the new data streams. Organisations need data scientists, but academics need to understand the new data science jobs to prepare more people to support decision-making.",fullPaper,jv61
p188,071036abe55e7247d7e6ec28a4afc8ef2670f479,c31,International Conference on Evaluation & Assessment in Software Engineering,A Comparison of Open Source Tools for Data Science,"The next decade of competitive advantage revolves around the ability to make predictions and discover patterns in data. Data science is at the center of this revolution. Data science has been termed the sexiest job of the 21st century. Data science combines data mining, machine learning, and statistical methodologies to extract knowledge and leverage predictions from data. Given the need for data science in organizations, many small or medium organizations are not adequately funded to acquire expensive data science tools. Open source tools may provide the solution to this issue. While studies comparing open source tools for data mining or business intelligence exist, an update on the current state of the art is necessary. This work explores and compares common open source data science tools. Implications include an overview of the state of the art and knowledge for practitioners and academics to select an open source data science tool that suits the requirements of specific data science projects.",poster,cp31
p189,c04aaf36c8587e40747212e316d9bf44186ef64a,c54,International Workshop on Agent-Oriented Software Engineering,Developing a Research Agenda for Human-Centered Data Science,"The study and analysis of large and complex data sets offer a wealth of insights in a variety of applications. Computational approaches provide researchers access to broad assemblages of data, but the insights extracted may lack the rich detail that qualitative approaches have brought to the understanding of sociotechnical phenomena. How do we preserve the richness associated with traditional qualitative methods while utilizing the power of large data sets? How do we uncover social nuances or consider ethics and values in data use? These and other questions are explored by human-centered data science, an emerging field at the intersection of human-computer interaction (HCI), computer-supported cooperative work (CSCW), human computation, and the statistical and computational techniques of data science. This workshop, the first of its kind at CSCW, seeks to bring together researchers interested in human-centered approaches to data science to collaborate, define a research agenda, and form a community.",poster,cp54
p190,259d81ced1837bb74f3eeeb30ca3217d535e0c31,c15,International Conference on Conceptual Structures,Introduction to HPC with MPI for Data Science,Abstract content,poster,cp15
p191,b0150dd118ebedbc3ece68726e065f9afaaf3b18,c52,Workshop on Learning from Authoritative Security Experiment Results,Big data analytics and big data science: a survey,"Big data has attracted much attention from academia and industry. But the discussion of big data is disparate, fragmented and distributed among different outlets. This paper conducts a systematic and extensive review on 186 journal publications about big data from 2011 to 2015 in the Science Citation Index (SCI) and the Social Science Citation Index (SSCI) database aiming to provide scholars and practitioners with a comprehensive overview and big picture about research on big data. The selected papers are grouped into 20 research categories. The contents of the paper(s) in each research category are summarized. Research directions for each category are outlined as well. The results in this study indicate that the selected papers were mainly published between 2013 and 2015 and focus on technological issues regarding big data. Diverse new approaches, methods, frameworks and systems are proposed for data collection, storage, transport, processing and analysis in the selected papers. Possible directions for f...",poster,cp52
p192,8ea48934b6f6a0717efb4e5355be3b008fc5b1bd,c76,International Conference on Artificial Neural Networks,Coding the biodigital child: the biopolitics and pedagogic strategies of educational data science,"Abstract Educational data science is an emerging transdisciplinary field formed from an amalgamation of data science and elements of biological, psychological and neuroscientific knowledge about learning, or learning science. This article conceptualises educational data science as a biopolitical strategy focused on the evaluation and management of the corporeal, emotional and embrained lives of children. Such strategies are enacted through the development of new kinds of digitally-mediated ‘biopedagogies’ of body optimisation, ‘psychopedagogies’ of emotional maximisation, and ‘neuropedagogies’ of brain empowerment. The data practices, scientific knowledges, digital devices and pedagogies that constitute educational data science produce new systems of knowledge about the child that are consequential to their formation as ‘biodigital’ subjects, whose assumed qualities and capacities are defined through expert practices of biosensing, emotion analytics, and neurocomputation, combined with associated scientific knowledges. The article develops the concept of transcoding to account for the processes involved in the formation of the biodigital child.",poster,cp76
p193,b9888bb70d6f246c7ffb53dcb9498bfafe113d8f,c66,Annual Conference on Innovation and Technology in Computer Science Education,Thinking by classes in data science: the symbolic data analysis paradigm,"Data Science, considered as a science by itself, is in general terms, the extraction of knowledge from data. Symbolic data analysis (SDA) gives a new way of thinking in Data Science by extending the standard input to a set of classes of individual entities. Hence, classes of a given population are considered to be units of a higher level population to be studied. Such classes often represent the real units of interest. In order to take variability between the members of each class into account, classes are described by intervals, distributions, set of categories or numbers sometimes weighted and the like. In that way, we obtain new kinds of data, called ‘symbolic’ as they cannot be reduced to numbers without losing much information. The first step in SDA is to build the symbolic data table where the rows are classes and the variables can take symbolic values. The second step is to study and extract new knowledge from these new kinds of data by at least an extension of Computer Statistics and Data Mining to symbolic data. SDA is a new paradigm which opens up a vast domain of research and applications by giving complementary results to classical methods applied to standard data. SDA also gives answers to big data and complex data challenges as big data can be reduced and summarized by classes and as complex data with multiple unstructured data tables and unpaired variables can be transformed into a structured data table with paired symbolic‐valued variables. WIREs Comput Stat 2016, 8:172–205. doi: 10.1002/wics.1384",poster,cp66
p194,12f3b97d76e2e07c3bf2914606d26bbfbbe85bd1,c101,International Conference on Automatic Face and Gesture Recognition,Role of materials data science and informatics in accelerated materials innovation,"The goal of the Materials Genome Initiative is to substantially reduce the time and cost of materials design and deployment. Achieving this goal requires taking advantage of the recent advances in data and information sciences. This critical need has impelled the emergence of a new discipline, called materials data science and informatics. This emerging new discipline not only has to address the core scientific/technological challenges related to datafication of materials science and engineering, but also, a number of equally important challenges around data-driven transformation of the current culture, practices, and workflows employed for materials innovation. A comprehensive effort that addresses both of these aspects in a synergistic manner is likely to succeed in realizing the vision of scaled-up materials innovation. Key toolsets needed for the successful adoption of materials data science and informatics in materials innovation are identified and discussed in this article. Prototypical examples of emerging novel toolsets and their functionality are described along with select case studies.",poster,cp101
p195,f152a4008f114ac19076ee6b98d431268f4aea9e,c1,Technical Symposium on Computer Science Education,A Practical and Sustainable Model for Learning and Teaching Data Science,"This paper details our experiences with design and implementation of data science curriculum at University at Buffalo (UB). We discuss (i) briefly the history of project, (ii) a certificate program that we created, (iii) a data-intensive computing course that forms the core of the curriculum and (iv) some of the challenges we faced and how we addressed them. Major goal of the project was to improve the preparedness of our workforce for the emerging data-intensive computing area. We measured this through assessment of student learning on various concepts and topics related to data-intensive computing. We also discuss the best practices in building a data science program. We highlight the importance of external funding support and multi-disciplinary collaborations in the success of the project. The pedagogical resources created for the project are freely available to help educators and other learners navigate the path to learning data science. We expect this paper about our experience will provide a road map for educators who desire to introduce data science in their curriculum.",fullPaper,cp1
p196,ca9f74a1a7b69214c670202bb4f66eb16194f836,c1,Technical Symposium on Computer Science Education,Datathons: An Experience Report of Data Hackathons for Data Science Education,Large amounts of data are becoming increasingly available through open data repositories as well as companies and governments collecting data to improve decision making and efficiencies. Consequently there is a need to increase the data literacy of computer science students. Data science is a relatively new area within computer science and the curriculum is rapidly evolving along with the tools required to perform analytics which students need to learn how to effectively use. To address the needs of students learning key data science and analytics skills we propose augmenting existing data science curriculums with hackathon events that focus on data also known as datathons. In this paper we present our experience at hosting and running four datathons that involved students and members from the community coming together to solve challenging problems with data from not-for-profit social good organizations and publicly open data. Our reported experience from our datathons will help inform other academics and community groups who also wish to host datathons to help facilitate their students and members to learn key data science and analytics skills.,fullPaper,cp1
p197,2081508c05ebe4fc0b7b2a1fd6a356a0e933186b,c15,International Conference on Conceptual Structures,Teaching Data Science,Abstract content,fullPaper,cp15
p198,07c9fb38b47ce320895f08930fabbb607a363c59,j62,Nature,Deep learning and process understanding for data-driven Earth system science,Abstract content,fullPaper,jv62
p199,469fb7c7178c8370a93fb27dadc9c5c839a9b8ec,j58,Journal of Data and Information Science,Information Science Roles in the Emerging Field of Data Science,"There has long been discussion about the distinctions of library science, information science, and informatics, and how these areas differ and overlap with computer science. Today the term data science is emerging that generates excitement and questions about how it relates to and differs from these other areas of study. For our purposes here, I consider information science to be the general term that subsumes library science and informatics and focuses on distinctions and similarities among these disciplines that each informs data science. At the most general levels, information science deals with the genesis, flow, use, and preservation of information; computer science deals with algorithms and techniques for computational processes. Data science as a concept emerges from the applications of existing studies of measurement, representation, interpretation, and management to problems in Citation: Gary Marchionini (2016). Information Science Roles in the Emerging Field of Data Science. Received: Mar. 10, 2016 Accepted: Mar. 22, 2016",fullPaper,jv58
p200,5cc0ebcc25ca96fd2a66240a3a897268e1ff2868,j63,Expert Opinion on Drug Discovery,Providing data science support for systems pharmacology and its implications to drug discovery,"ABSTRACT Introduction: The conventional one-drug-one-target-one-disease drug discovery process has been less successful in tracking multi-genic, multi-faceted complex diseases. Systems pharmacology has emerged as a new discipline to tackle the current challenges in drug discovery. The goal of systems pharmacology is to transform huge, heterogeneous, and dynamic biological and clinical data into interpretable and actionable mechanistic models for decision making in drug discovery and patient treatment. Thus, big data technology and data science will play an essential role in systems pharmacology. Areas covered: This paper critically reviews the impact of three fundamental concepts of data science on systems pharmacology: similarity inference, overfitting avoidance, and disentangling causality from correlation. The authors then discuss recent advances and future directions in applying the three concepts of data science to drug discovery, with a focus on proteome-wide context-specific quantitative drug target deconvolution and personalized adverse drug reaction prediction. Expert opinion: Data science will facilitate reducing the complexity of systems pharmacology modeling, detecting hidden correlations between complex data sets, and distinguishing causation from correlation. The power of data science can only be fully realized when integrated with mechanism-based multi-scale modeling that explicitly takes into account the hierarchical organization of biological systems from nucleic acid to proteins, to molecular interaction networks, to cells, to tissues, to patients, and to populations.",fullPaper,jv63
p201,e02ecae774d70f8163804306c3544eaaaa10f796,c80,International Conference on Learning Representations,A Case for Data Commons: Toward Data Science as a Service,"Data commons collocate data, storage, and computing infrastructure with core services and commonly used tools and applications for managing, analyzing, and sharing data to create an interoperable resource for the research community. An architecture for data commons is described, as well as some lessons learned from operating several large-scale data commons.",poster,cp80
p202,52ff64f7f26b28447af255fedeb2216a70b48d66,c16,Knowledge Discovery and Data Mining,Large Scale Distributed Data Science using Apache Spark,"Apache Spark is an open-source cluster computing framework for big data processing. It has emerged as the next generation big data processing engine, overtaking Hadoop MapReduce which helped ignite the big data revolution. Spark maintains MapReduce's linear scalability and fault tolerance, but extends it in a few important ways: it is much faster (100 times faster for certain applications), much easier to program in due to its rich APIs in Python, Java, Scala (and shortly R), and its core data abstraction, the distributed data frame, and it goes far beyond batch applications to support a variety of compute-intensive tasks, including interactive queries, streaming, machine learning, and graph processing. This tutorial will provide an accessible introduction to Spark and its potential to revolutionize academic and commercial data science practices.",fullPaper,cp16
p203,e78d7fa72a5dbe5f3bc93f6e200826004f23530b,j64,IEEE Intelligent Systems,Data Science: Nature and Pitfalls,Data science is creating exciting trends as well as significant controversy. A critical matter for the healthy development of data science in its early stages is to deeply understand the nature of data and data science and discuss the various pitfalls. These important issues motivate the discussions in this article.,fullPaper,jv64
p204,bff0d1d3a3251cb7bcbeb424ae0580c3085649f7,j65,International Journal of System Dynamics Applications,Integrating Systems Modelling and Data Science: The Joint Future of Simulation and 'Big Data' Science,"Although System Dynamics modelling is sometimes referred to as data-poor modelling, it often is -or could be-applied in a data-rich manner. However, more can be done in the era of 'big data'. Big data refers here to situations with much more available data than was until recently manageable. The field of data science makes bigger data manageable. This paper provides a perspective on the future of System Dynamics with a prominent place for bigger data and data science. It discusses different approaches for dealing with bigger data. It reviews methods, techniques and tools for dealing with bigger data in System Dynamics, and sheds light on the modelling phases for which data science is most useful. Finally, it provides several examples of current applications in which big data, data science, and System Dynamics modelling and simulation are being merged.",fullPaper,jv65
p205,217905fe7b549d5f3d45896072518b06adb8dd98,c89,Conference on Uncertainty in Artificial Intelligence,Data Science from Scratch: First Principles with Python,"Data science libraries, frameworks, modules, and toolkits are great for doing data science, but they're also a good way to dive into the discipline without actually understanding data science. In this book, you'll learn how many of the most fundamental data science tools and algorithms work by implementing them from scratch. If you have an aptitude for mathematics and some programming skills, author Joel Grus will help you get comfortable with the math and statistics at the core of data science, and with hacking skills you need to get started as a data scientist. Today's messy glut of data holds answers to questions no one's even thought to ask. This book provides you with the know-how to dig those answers out.",poster,cp89
p206,abe269076b072e14918921a25f10757747fa3493,c17,International Conference on Enterprise Information Systems,"Responsible Data Science: Using Event Data in a ""People Friendly"" Manner",Abstract content,fullPaper,cp17
p207,49522df4fab1ebbeb831fc265196c2c129bf6087,c56,European Conference on Software Process Improvement,Survey on data science with population-based algorithms,Abstract content,poster,cp56
p208,fa15d626d8905d08953abe646a75a31417ad61fa,c43,ACM Symposium on Applied Computing,"Data science on the ground: Hype, criticism, and everyday work","Modern organizations often employ data scientists to improve business processes using diverse sets of data. Researchers and practitioners have both touted the benefits and warned of the drawbacks associated with data science and big data approaches, but few studies investigate how data science is carried out “on the ground.” In this paper, we first review the hype and criticisms surrounding data science and big data approaches. We then present the findings of semistructured interviews with 18 data analysts from various industries and organizational roles. Using qualitative coding techniques, we evaluated these interviews in light of the hype and criticisms surrounding data science in the popular discourse. We found that although the data analysts we interviewed were sensitive to both the allure and the potential pitfalls of data science, their motivations and evaluations of their work were more nuanced. We conclude by reflecting on the relationship between data analysts' work and the discourses around data science and big data, suggesting how future research can better account for the everyday practices of this profession.",poster,cp43
p209,e1a1ad4025e2c7a82882c7389d937cbdfd10b799,j66,Data Science Journal,Towards Data Science,"Currently, a huge amount of data is being rapidly generated in cyberspace. Datanature (all data in cyberspace) is forming due to a data explosion. Exploring the patterns and rules in datanature is necessary but difficult. A new discipline called Data Science is coming. It provides a type of novel research method (a data-intensive method) for natural and social sciences and goes beyond computer science in researching data. This paper presents the challenges presented by data and discusses what differentiates data science from the established sciences, data technologies, and big data. Our goal is to encourage data related researchers to transfer their focus towards this new science.",fullPaper,jv66
p210,aeede2d75d7cb3e10bc3b732a897ca1a7bfc12c5,c105,Biometrics and Identity Management,"Data Science and Big Data Analytics: Discovering, Analyzing, Visualizing and Presenting Data","Data Science and Big Data Analytics: Discovering, Analyzing, Visualizing and Presenting Data By EMC Education Services Data Science and Big Data Analytics is about harnessing the power of data for new insights. The book covers the breadth of activities and methods and tools that Data Scientists use. The content focuses on concepts, principles and practical applications that are applicable to any industry and technology environment, and the learning is supported and explained with examples that you can replicate using open-source software. This book will help you: Become a contributor on a data science team ●",poster,cp105
p211,d343c9823bcacf31ea4aca105d0366f3f18a75e5,j64,IEEE Intelligent Systems,The Role of Data Science in Web Science,"Web science relies on an interdisciplinary approach that seeks to go beyond what any one subject can say about the World Wide Web. By incorporating numerous disciplinary perspectives and relying heavily on domain knowledge and expertise, data science has emerged as an important new area that integrates statistics with computational knowledge, data collection, cleaning and processing, analysis methods, and visualization to produce actionable insights from big data. As a discipline to use within Web science research, data science offers significant opportunities for uncovering trends in large Web-based datasets. A Web science observatory exemplifies this relationship by offering an online platform of tools for carrying out Web science research, allowing users to carry out data science techniques to produce insights into Web science issues such as community development, online behavior, and information propagation. The authors outline the similarities and differences of these two growing subject areas to demonstrate the important relationship developing between them.",fullPaper,jv64
p212,def43235dba7eb98659fb8879fa9d27695029df2,j67,IEEE Geoscience and Remote Sensing Magazine,Recent Activities in Earth Data Science [Technical Committees],"Recent trends on big Earth-observing (EO) data lead to some questions that the Earth science community needs to address. Are we experiencing a paradigm shift in Earth science research now? How can we better utilize the explosion of technology maturation to create new forms of EO data processing? Can we summarize the existing methodologies and technologies scaling to big EO data as a new field named earth data science? Big data technologies are being widely practiced in Earth sciences and remote sensing communities to support EO data access, processing, and knowledge discovery. The data-intensive scientific discovery, named the fourth paradigm, leads to data science in the big data era [1]. According to the definition by the U.S. National Institute of Standards and Technology, the data science paradigm is the ""extraction of actionable knowledge directly from data through a process of discovery, hypothesis, and hypothesis testing"" [2]. Earth data science is the art and science of applying the data science paradigm to EO data.",fullPaper,jv67
p213,b70cfcc6bbb764728f8aa55aa173cc692eb77bdf,j68,Frontiers in Genetics,The Process of Analyzing Data is the Emergent Feature of Data Science,"In recent years the term “data science” gained considerable attention worldwide. In a A Very Short History Of Data Science by Press (2013), the first appearance of the term is ascribed to Peter Naur in 1974 (Concise Survey of Computer Methods). Regardless who used the term first and in what context it has been used, we think that data science is a good term to indicate that data are the focus of scientific research. This is in analogy to computer science, where the first department of computer science in the USA had been established in 1962 at Purdue University, at a time when the first electronic computers became available and it was still not clear enough what computers can do, one created therefore a new field where the computer was the focus of the study. In this paper, we want to address a couple of questions in order to demystify the meaning and the goals of data science in general.",fullPaper,jv68
p214,682b105746238d3c39bd4f6cd0baa375dc0c2534,c108,International Conference on Information Integration and Web-based Applications & Services,Perspectives on Data Science for Software Engineering,Abstract content,poster,cp108
p215,15992848d2e9b242bd89561ebef911a8c6a9c698,j7,International Journal of Data Science and Analysis,Data science and analytics: a new era,Abstract content,fullPaper,jv7
p216,0e42458f6c98929a01e105363a6f0531d9812b98,c86,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,A Data Science Course for Undergraduates: Thinking With Data,"Data science is an emerging interdisciplinary field that combines elements of mathematics, statistics, computer science, and knowledge in a particular application domain for the purpose of extracting meaningful information from the increasingly sophisticated array of data available in many settings. These data tend to be nontraditional, in the sense that they are often live, large, complex, and/or messy. A first course in statistics at the undergraduate level typically introduces students to a variety of techniques to analyze small, neat, and clean datasets. However, whether they pursue more formal training in statistics or not, many of these students will end up working with data that are considerably more complex, and will need facility with statistical computing techniques. More importantly, these students require a framework for thinking structurally about data. We describe an undergraduate course in a liberal arts environment that provides students with the tools necessary to apply data science. The course emphasizes modern, practical, and useful skills that cover the full data analysis spectrum, from asking an interesting question to acquiring, managing, manipulating, processing, querying, analyzing, and visualizing data, as well communicating findings in written, graphical, and oral forms. Supplementary materials for this article are available online. [Received June 2014. Revised July 2015.]",poster,cp86
p217,0442b04b4e8741900b65de0721f0c3e152e044ef,c45,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",Materials Data Science: Current Status and Future Outlook,"The field of materials science and engineering is on the cusp of a digital data revolution. After reviewing the nature of data science and Big Data, we discuss the features of materials data that distinguish them from data in other fields. We introduce the concept of process-structure-property (PSP) linkages and illustrate how the determination of PSPs is one of the main objectives of materials data science. Then we review a selection of materials databases, as well as important aspects of materials data management, such as storage hardware, archiving strategies, and data access strategies. We introduce the emerging field of materials data analytics, which focuses on data-driven approaches to extract and curate materials knowledge from available data sets. The critical need for materials e-collaboration platforms is highlighted, and we conclude the article with a number of suggestions regarding the near-term future of the materials data science field.",poster,cp45
p218,57039a11d9fb423595a4e16129f7cc7f3ff2cac7,c56,European Conference on Software Process Improvement,Data science ethics in government,"Data science can offer huge opportunities for government. With the ability to process larger and more complex datasets than ever before, it can provide better insights for policymakers and make services more tailored and efficient. As with all new technologies, there is a risk that we do not take up its opportunities and miss out on its enormous potential. We want people to feel confident to innovate with data. So, over the past 18 months, the Government Data Science Partnership has taken an open, evidence-based and user-centred approach to creating an ethical framework. It is a practical document that brings all the legal guidance together in one place, and is written in the context of new data science capabilities. As part of its development, we ran a public dialogue on data science ethics, including deliberative workshops, an experimental conjoint survey and an online engagement tool. The research supported the principles set out in the framework as well as provided useful insight into how we need to communicate about data science. It found that people had a low awareness of the term ‘data science’, but that showing data science examples can increase broad support for government exploring innovative uses of data. But people's support is highly context driven. People consider acceptability on a case-by-case basis, first thinking about the overall policy goals and likely intended outcome, and then weighing up privacy and unintended consequences. The ethical framework is a crucial start, but it does not solve all the challenges it highlights, particularly as technology is creating new challenges and opportunities every day. Continued research is needed into data minimization and anonymization, robust data models, algorithmic accountability, and transparency and data security. It also has revealed the need to set out a renewed deal between the citizen and state on data, to maintain and solidify trust in how we use people's data for social good. This article is part of the themed issue ‘The ethical impact of data science’.",poster,cp56
p219,71ef37f557f324a08b76b895947933bc3c1e40e6,j19,Computer/law journal,Data Science,"The Bachelor of Science in Data Science studies the collection, manipulation, storage, retrieval, and computational analysis of data in its various forms, including numeric, textual, image, and video data from small to large volumes. The program combines computer science, information science, mathematics, statistics, and probability theory into an integrated curriculum that prepares students for careers or graduate studies in big data analysis, data science, and data analytics. The course work covers exploratory data analysis, data manipulation in a variety of programming languages, large-scale data storage, predictive analytics, machine learning, data mining, and information visualization and presentation. Data science has emerged as a discipline due to the confluence of two major events:",fullPaper,jv19
p220,3e209c705350761fe676ac330503e8662279fbf2,j69,IEEE Transactions on Services Computing,Processes Meet Big Data: Connecting Data Science with Process Science,"As more and more companies are embracing Big data, it has become apparent that the ultimate challenge is to relate massive amounts of event data to processes that are highly dynamic. To unleash the value of event data, events need to be tightly connected to the control and management of operational processes. However, the primary focus of Big data technologies is currently on storage, processing, and rather simple analytical tasks. Big data initiatives rarely focus on the improvement of end-to-end processes. To address this mismatch, we advocate a better integration of data science, data technology and process science. Data science approaches tend to be process agonistic whereas process science approaches tend to be model-driven without considering the “evidence” hidden in the data. Process mining aims to bridge this gap. This editorial discusses the interplay between data science and process science and relates process mining to Big data technologies, service orientation, and cloud computing.",fullPaper,jv69
p221,659890e52fe234cde0e02a2305e213d3e8cb14b2,c6,Americas Conference on Information Systems,Data science and cyberinfrastructure: critical enablers for accelerated development of hierarchical materials,"Abstract The slow pace of new/improved materials development and deployment has been identified as the main bottleneck in the innovation cycles of most emerging technologies. Much of the continuing discussion in the materials development community is therefore focused on the creation of novel materials innovation ecosystems designed to dramatically accelerate materials development efforts, while lowering the overall cost involved. In this paper, it is argued that the recent advances in data science can be leveraged suitably to address this challenge by effectively mediating between the seemingly disparate, inherently uncertain, multiscale and multimodal measurements and computations involved in the current materials’ development efforts. Proper utilisation of modern data science in the materials’ development efforts can lead to a new generation of data-driven decision support tools for guiding effort investment (for both measurements and computations) at various stages of the materials development. It should also be recognised that the success of such ecosystems is predicated on the creation and utilisation of integration platforms for promoting intimate, synchronous collaborations between cross-disciplinary and distributed team members (i.e. cyberinfrastructure). Indeed, data sciences and cyberinfrastructure form the two main pillars of the emerging new discipline broadly referred to as materials informatics (MI). This paper provides a summary of current capabilities in this emerging new field as they relate to the accelerated development of advanced hierarchical materials (the internal structure plays a dominant role in controlling overall properties/performance in these materials) and identifies specific directions of research that offer the most promising avenues.",poster,cp6
p222,5a56bbd762e9dd70dd20afe8740a6d09ec85ffed,c39,International Conference on Global Software Engineering,Data science from scratch,"This is a first-principles-based, practical introduction to the fundamentals of data science aimed at the mathematically-comfortable reader with some programming skills. The book covers: The important parts of Python to know The important parts of Math / Probability / Statistics to know The basics of data science How commonly-used data science techniques work (learning by implementing them) What is Map-Reduce and how to do it in Python Other applications such as NLP, Network Analysis, and more",poster,cp39
p223,499ffc99eeee63ceff6fc33f732b590e4d3352b9,c10,Big Data,Mining the Quantified Self: Personal Knowledge Discovery as a Challenge for Data Science,"The last several years have seen an explosion of interest in wearable computing, personal tracking devices, and the so-called quantified self (QS) movement. Quantified self involves ordinary people recording and analyzing numerous aspects of their lives to understand and improve themselves. This is now a mainstream phenomenon, attracting a great deal of attention, participation, and funding. As more people are attracted to the movement, companies are offering various new platforms (hardware and software) that allow ever more aspects of daily life to be tracked. Nearly every aspect of the QS ecosystem is advancing rapidly, except for analytic capabilities, which remain surprisingly primitive. With increasing numbers of qualified self participants collecting ever greater amounts and types of data, many people literally have more data than they know what to do with. This article reviews the opportunities and challenges posed by the QS movement. Data science provides well-tested techniques for knowledge discovery. But making these useful for the QS domain poses unique challenges that derive from the characteristics of the data collected as well as the specific types of actionable insights that people want from the data. Using a small sample of QS time series data containing information about personal health we provide a formulation of the QS problem that connects data to the decisions of interest to the user.",fullPaper,cp10
p224,0c7a2f274f7e55609e3084f05471afbdb1abba8a,j70,Statistics and computing,Statistics and computing: the genesis of data science,Abstract content,fullPaper,jv70
p225,e12b5363078a6d435bfba80e9d5cbab6b2cac897,c14,International Conference on Exploring Services Science,Data Science and Digital Art History,"I present a number of core concepts from data science that are relevant to digital art history and the use of quantitative methods to study any cultural artifacts or processes in general. These concepts are objects, features, data, feature space, and dimension reduction. These concepts enable computational exploration of both large and small visual cultural data. We can analyze relations between works on a single artist, many artists, all digitized production from a whole historical period, holdings in museum collections, collection metadata, or writings about art. The same concepts allow us to study contemporary vernacular visual media using massive social media content. (In our lab, we analyzed works by van Gogh, Mondrian, and Rothko, 6000 paintings by French Impressionists, 20,000 photographs from MoMA photo­graphy collection, one million manga pages from manga books, one million artworks of contemporary non-professional artists, and over 13 million Instagram images from 16 global cities.) While data science techniques do not replace other art historical methods, they allow us to see familiar art historical material in new ways, and also to study contemporary digital visual culture.",poster,cp14
p226,1a95f1e8ff32488f228a25764af64531cb758ff0,c51,Conference of the Centre for Advanced Studies on Collaborative Research,Exploration of data science techniques to predict fatigue strength of steel from composition and processing parameters,Abstract content,poster,cp51
p227,8bae989e056e60cd2d450911e81b65e6c78b5513,j71,Communications in Computer and Information Science,Data Science,Abstract content,fullPaper,jv71
p228,f6b3505e06b1b19806e35524adbafa70c79851d9,c112,Very Large Data Bases Conference,Data Science in Statistics Curricula: Preparing Students to “Think with Data”,"A growing number of students are completing undergraduate degrees in statistics and entering the workforce as data analysts. In these positions, they are expected to understand how to use databases and other data warehouses, scrape data from Internet sources, program solutions to complex problems in multiple languages, and think algorithmically as well as statistically. These data science topics have not traditionally been a major component of undergraduate programs in statistics. Consequently, a curricular shift is needed to address additional learning outcomes. The goal of this article is to motivate the importance of data science proficiency and to provide examples and resources for instructors to implement data science in their own statistics curricula. We provide case studies from seven institutions. These varied approaches to teaching data science demonstrate curricular innovations to address new needs. Also included here are examples of assignments designed for courses that foster engagement of undergraduates with data and data science. [Received November 2014. Revised July 2015.]",poster,cp112
p229,9ba08d45d60130c7e5880f63a980b185a86e177c,c10,Big Data,A Big Data Guide to Understanding Climate Change: The Case for Theory-Guided Data Science,"Global climate change and its impact on human life has become one of our era's greatest challenges. Despite the urgency, data science has had little impact on furthering our understanding of our planet in spite of the abundance of climate data. This is a stark contrast from other fields such as advertising or electronic commerce where big data has been a great success story. This discrepancy stems from the complex nature of climate data as well as the scientific questions climate science brings forth. This article introduces a data science audience to the challenges and opportunities to mine large climate datasets, with an emphasis on the nuanced difference between mining climate data and traditional big data approaches. We focus on data, methods, and application challenges that must be addressed in order for big data to fulfill their promise with regard to climate science applications. More importantly, we highlight research showing that solely relying on traditional big data techniques results in dubious findings, and we instead propose a theory-guided data science paradigm that uses scientific theory to constrain both the big data techniques as well as the results-interpretation process to extract accurate insight from large climate data.",fullPaper,cp10
p230,8f6a4609531ca9ff35915c32dae5cd146fc57c40,c104,IEEE International Conference on Multimedia and Expo,HEALTH BANK - A Workbench for Data Science Applications in Healthcare,"The enormous amounts of data that are generated in the healthcare process and stored in electronic health record (EHR) systems are an underutilized resource that, with the use of data science applica- tions, can be exploited to improve healthcare. To foster the development and use of data science applications in healthcare, there is a fundamen- tal need for access to EHR data, which is typically not readily available to researchers and developers. A relatively rare exception is the large EHR database, the Stockholm EPR Corpus, comprising data from more than two million patients, that has been been made available to a lim- ited group of researchers at Stockholm University. Here, we describe a number of data science applications that have been developed using this database, demonstrating the potential reuse of EHR data to support healthcare and public health activities, as well as facilitate medical re- search. However, in order to realize the full potential of this resource, it needs to be made available to a larger community of researchers, as well as to industry actors. To that end, we envision the provision of an in- frastructure around this database called HEALTH BANK – the Swedish Health Record Research Bank. It will function both as a workbench for the development of data science applications and as a data explo- ration tool, allowing epidemiologists, pharmacologists and other medical researchers to generate and evaluate hypotheses. Aggregated data will be fed into a pipeline for open e-access, while non-aggregated data will be provided to researchers within an ethical permission framework. We believe that HEALTH BANK has the potential to promote a growing industry around the development of data science applications that will ultimately increase the efficiency and effectiveness of healthcare.",poster,cp104
p231,de84e808462b8240c75987364a6d518eff7d8813,c10,Big Data,Statistics: a data science for the 21st century,"The rise of data science could be seen as a potental threat to the long‐term status of the statistics discipline. I first argue that, although there is a threat, there is also a much greater opportunity to re‐emphasize the universal relevance of statistical method to the interpretation of data, and I give a short historical outline of the increasingly important links between statistics and information technology. The core of the paper is a summary of several recent research projects, through which I hope to demonstrate that statistics makes an essential, but incomplete, contribution to the emerging field of ‘electronic health’ research. Finally, I offer personal thoughts on how statistics might best be organized in a research‐led university, on what we should teach our students and on some issues broadly related to data science where the Royal Statistical Society can take a lead.",poster,cp10
p232,b885916e9af51010ca7ebafbc9270f0e5e207b38,j72,Nursing Administration Quarterly,Nursing Knowledge: Big Data Science—Implications for Nurse Leaders,"The integration of Big Data from electronic health records and other information systems within and across health care enterprises provides an opportunity to develop actionable predictive models that can increase the confidence of nursing leaders' decisions to improve patient outcomes and safety and control costs. As health care shifts to the community, mobile health applications add to the Big Data available. There is an evolving national action plan that includes nursing data in Big Data science, spearheaded by the University of Minnesota School of Nursing. For the past 3 years, diverse stakeholders from practice, industry, education, research, and professional organizations have collaborated through the “Nursing Knowledge: Big Data Science” conferences to create and act on recommendations for inclusion of nursing data, integrated with patient-generated, interprofessional, and contextual data. It is critical for nursing leaders to understand the value of Big Data science and the ways to standardize data and workflow processes to take advantage of newer cutting edge analytics to support analytic methods to control costs and improve patient quality and safety.",fullPaper,jv72
p233,bc88849f48e04a642e4cdedabe921039af837c03,c37,Asia-Pacific Software Engineering Conference,Taking a Chance in the Classroom: Setting the Stage for Data Science: Integration of Data Management Skills in Introductory and Second Courses in Statistics,"Many have argued that statistics students need additional facility to express statistical computations. By introducing students to commonplace tools for data management, visualization, and reproducible analysis in data science and applying these to real-world scenarios, we prepare them to think statistically. In an era of increasingly big data, it is imperative that students develop data-related capacities, beginning with the introductory course. We believe that the integration of these precursors to data science into our curricula-early and often-will help statisticians be part of the dialogue regarding ""Big Data"" and ""Big Questions"".",poster,cp37
p234,9d653160d048eecf1a8138407994bfc69952324b,c79,International Conference on Medical Image Computing and Computer-Assisted Intervention,Practical Data Science with R,"Summary Practical Data Science with R lives up to its name. It explains basic principles without the theoretical mumbo-jumbo and jumps right to the real use cases you'll face as you collect, curate, and analyze the data crucial to the success of your business. You'll apply the R programming language and statistical analysis techniques to carefully explained examples based in marketing, business intelligence, and decision support. Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications. About the Book Business analysts and developers are increasingly collecting, curating, analyzing, and reporting on crucial business data. The R language and its associated tools provide a straightforward way to tackle day-to-day data science tasks without a lot of academic theory or advanced mathematics. Practical Data Science with R shows you how to apply the R programming language and useful statistical techniques to everyday business situations. Using examples from marketing, business intelligence, and decision support, it shows you how to design experiments (such as A/B tests), build predictive models, and present results to audiences of all levels. This book is accessible to readers without a background in data science. Some familiarity with basic statistics, R, or another scripting language is assumed. What's Inside Data science for the business professional Statistical analysis using the R language Project lifecycle, from planning to delivery Numerous instantly familiar use cases Keys to effective data presentations About the Authors Nina Zumel and John Mount are cofounders of a San Francisco-based data science consulting firm. Both hold PhDs from Carnegie Mellon and blog on statistics, probability, and computer science at win-vector.com.",poster,cp79
p235,92efba7c622f54b8cd7b0d70d7cc09063e17b4f3,c1,Technical Symposium on Computer Science Education,An undergraduate degree in data science: curriculum and a decade of implementation experience,"We describe Data Science, a four-year undergraduate program in predictive analytics, machine learning, and data mining implemented at the College of Charleston, Charleston, South Carolina, USA. We present a ten-year status report detailing the program's origins, successes, and challenges. Our experience demonstrates that education and training for big data concepts are possible and practical at the undergraduate level. The development of this program parallels the growing demand for finding utility in data sets and streaming data. The curriculum is a seventy-seven credit-hour program that has been successfully implemented in a liberal arts and sciences institution by the faculties of computer science and mathematics.",fullPaper,cp1
p236,c7d7d579d94b7fc67c75b68361e01ba8f59b1d40,j73,Future generations computer systems,Intelligent services for Big Data science,Abstract content,fullPaper,jv73
p237,516a53c59a53b0a471cd8a277b229925e0582114,c18,Conference on Innovative Data Systems Research,DataHub: Collaborative Data Science & Dataset Version Management at Scale,"Relational databases have limited support for data collaboration, where teams collaboratively curate and analyze large datasets. Inspired by software version control systems like git, we propose (a) a dataset version control system, giving users the ability to create, branch, merge, difference and search large, divergent collections of datasets, and (b) a platform, DATAHUB, that gives users the ability to perform collaborative data analysis building on this version control system. We outline the challenges in providing dataset version control at scale.",fullPaper,cp18
p238,915cd8e2b39eb02723553913d592b2237d4d9960,j74,Statistical analysis and data mining,Data science: An action plan for expanding the technical areas of the field of statistics,"An action plan to expand the technical areas of statistics focuses on the data analyst. The plan sets out six technical areas of work for a university department and advocates a specific allocation of resources devoted to research in each area and to courses in each area. The value of technical work is judged by the extent to which it benefits the data analyst, either directly or indirectly. The plan is also applicable to government research labs and corporate research organizations.",fullPaper,jv74
p239,0040c830969302a8c88c0c083aee5051e405bfe5,c29,International Conference on Software Engineering,"Big Data, Big Problems: Emerging Issues in the Ethics of Data Science and Journalism","As big data techniques become widespread in journalism, both as the subject of reporting and as newsgathering tools, the ethics of data science must inform and be informed by media ethics. This article explores emerging problems in ethical research using big data techniques. It does so using the duty-based framework advanced by W.D. Ross, who has significantly influenced both research science and media ethics. A successful framework must provide stability and flexibility. Without stability, ethical precommitments will vanish as technology rapidly shifts costs. Without flexibility, traditional approaches will rapidly become obsolete in the face of technological change. The article concludes that Ross's duty-based approach both provides stability in the face of rapid technological change and flexibility to innovate to achieve the original purpose of basic ethical principles.",poster,cp29
p240,520515cfffcd2f439469398d7c959f8baa9ccc8b,c61,Jahrestagung der Gesellschaft für Informatik,Philosophy of Big Data: Expanding the Human-Data Relation with Big Data Science Services,"Big data is growing as an area of information technology, service, and science, and so too is the need for its intellectual understanding and interpretation from a theoretical, philosophical, and societal perspective. The Philosophy of Big Data is the branch of philosophy concerned with the foundations, methods, and implications of big data, the definitions, meaning, conceptualization, knowledge possibilities, truth standards, and practices in situations involving very-large data sets that are big in volume, velocity, variety, veracity, and variability. The Philosophy of Big Data is evolving into a discipline at two levels, one internal to the field as a generalized articulation of the concepts, theory, and systems that comprise the overall conduct of big data science. The other is external to the field, as a consideration of the impact of big data science more broadly on individuals, society, and the world. Methods, tools, and concepts are evaluated at both the level of industry practice theory and social impact. Three aspects are considered: what might constitute a Philosophy of Big Data, how the disciplines of the Philosophy of Information and the Philosophy of Big Data are developing, and an example of the Philosophy of Big Data in application in the data-intensive science field of Synthetic Biology. Overall a Philosophy of Big Data might helpful in conceptualizing and realizing big data science as a service practice, and also in transitioning to data-rich futures with human and data entities more productively co-existing in mutual growth and collaboration.",poster,cp61
p241,f707f6d7c3f874cb1a8aa961a50e50706731cd2d,c19,ACM Conference on Economics and Computation,Mechanism design for data science,"The promise of data science is that if data from a system can be recorded and understood then this understanding can potentially be utilized to improve the system. Behavioral and economic data, however, is different from scientific data in that it is subjective to the system. Behavior changes when the system changes, and to predict behavior for any given system change or to optimize over system changes, the behavioral model that generates the data must be inferred from the data. The ease with which this inference can be performed generally also depends on the system. Trivially, a system that ignores behavior does not admit any inference of a behavior generating model that can be used to predict behavior in a system that is responsive to behavior. To realize the promise of data science in economic systems, a theory for the design of such systems must also incorporate the desired inference properties. Consider as an example the revenue-maximizing auctioneer. If the auctioneer has knowledge of the distribution of bidder values then she can run the first-price auction with a reserve price that is tuned to the distribution. Under some mild distributional assumptions, with the appropriate reserve price the first-price auction is revenue optimal [Myerson 1981]. Notice that the historical bid data for the first-price auction with a reserve price will in most cases not have bids for bidders whose values are below the reserve. Therefore, there is no data analysis that the auctioneer can perform that will enable properties of the distribution of bidder values below the reserve price to be inferred. It could be, nonetheless, that over time the population of potential bidders evolves and the optimal reserve price lowers. This change could go completely unnoticed in the auctioneer's data. The two main tools for optimizing revenue in an auction are reserve prices (as above) and ironing. Both of these tools cause pooling behavior (i.e., bidders with distinct values take the same action) and economic inference cannot thereafter differentiate these pooled bidders. In order to maintain the distributional knowledge necessary to be able to run a good auction in the long term, the auctioneer must sacrifice the short-term revenue by running a non-revenue-optimal auction.",fullPaper,cp19
p242,b24798473712101660e8a47f5b8fc300a982ab42,c25,International Conference on Contemporary Computing,Analytics in a Big Data World: The Essential Guide to Data Science and its Applications,"The guide to targeting and leveraging business opportunities using big data & analytics By leveraging big data & analytics, businesses create the potential to better understand, manage, and strategically exploiting the complex dynamics of customer behavior. Analytics in a Big Data World reveals how to tap into the powerful tool of data analytics to create a strategic advantage and identify new business opportunities. Designed to be an accessible resource, this essential book does not include exhaustive coverage of all analytical techniques, instead focusing on analytics techniques that really provide added value in business environments. The book draws on author Bart Baesens' expertise on the topics of big data, analytics and its applications in e.g. credit risk, marketing, and fraud to provide a clear roadmap for organizations that want to use data analytics to their advantage, but need a good starting point. Baesens has conducted extensive research on big data, analytics, customer relationship management, web analytics, fraud detection, and credit risk management, and uses this experience to bring clarity to a complex topic. * Includes numerous case studies on risk management, fraud detection, customer relationship management, and web analytics * Offers the results of research and the author's personal experience in banking, retail, and government * Contains an overview of the visionary ideas and current developments on the strategic use of analytics for business * Covers the topic of data analytics in easy-to-understand terms without an undo emphasis on mathematics and the minutiae of statistical analysis For organizations looking to enhance their capabilities via data analytics, this resource is the go-to reference for leveraging data to enhance business capabilities.",poster,cp25
p243,78a1e55ffb2ca07a612c033bb1b359f56f4ca9c3,j75,Lecture Notes in Computer Science,Data Science,Abstract content,fullPaper,jv75
p244,875bfb0016b0f6bd806e1c74ff078eb0044d45e7,j76,Education and Information Technologies : Official Journal of the IFIP technical committee on Education,Data science in educational assessment,Abstract content,fullPaper,jv76
p245,04831fedd16110da4cbd0798d16e21fbbc34ad06,j77,International Journal of Intelligent Computing and Cybernetics,A survey of open source data science tools,"Purpose – Data science is the study of the generalizable extraction of knowledge from data. It includes a variety of components and develops on methods and concepts from many domains, containing mathematics, probability models, machine learning, statistical learning, computer programming, data engineering, pattern recognition and learning, visualization and data warehousing aiming to extract value from data. The purpose of this paper is to provide an overview of open source (OS) data science tools, proposing a classification scheme that can be used to study OS data science software. Design/methodology/approach – The proposed classification scheme is based on general characteristics, project activity, operational characteristics and data mining characteristics. The authors then use the proposed scheme to examine 70 identified Open Source Software. From this the authors provide insight about the current status of OS data science tools and reveal the state-of-the-art tools. Findings – The features of 70 OS t...",fullPaper,jv77
p246,5a92ccd20e551c191ff19bdd8e75bf1b64faa54b,j78,College and Research Libraries,Dealing with Data: Science Librarians' Participation in Data Management at Association of Research Libraries Institutions,"As long as empirical research has existed, researchers have been doing “data management” in one form or another. However, funding agency mandates for doing formal data management are relatively recent, and academic libraries’ involvement has been concentrated mainly in the last few years. The National Science Foundation implemented a new mandate in January 2011, requiring researchers to include a data management plan with their proposals for funding. This has prompted many academic libraries to work more actively than before in data management, and science librarians in particular are uniquely poised to step into new roles to meet researchers’ data management needs. This study, a survey of science librarians at institutions affiliated with the Association of Research Libraries, investigates science librarians’ awareness of and involvement in institutional repositories, data repositories, and data management support services at their institutions. The study also explores the roles and responsibilities, both new and traditional, that science librarians have assumed related to data management, and the skills that science librarians believe are necessary to meet the demands of data management work. The results reveal themes of both uncertainty and optimism—uncertainty about the roles of librarians, libraries, and other campus entities; uncertainty about the skills that will be required; but also optimism about applying “traditional” librarian skills to this emerging field of academic librarianship.",fullPaper,jv78
p247,2eeb50f48905a3d4adc6cfcf185ffb0f94bd6006,c44,International Workshop on Green and Sustainable Software,Editor’s comments: the business of business data science in IS journals,"Data science is at the core of a host of ongoing business transformations and disruptive technologies. The application of data science methods to new and old business problems presents a wealth of research opportunities upon which the information systems (IS) data science community is uniquely positioned to focus. Strong demand for business data science education by both students and recruiters has also given rise to new business analytics programs, often lead by IS groups. This mission will be well served by an active business data science research within IS. While contributions by IS data science scholars are being published in the premier reference discipline journals, and although such a relationship with the reference disciplines is immensely important to maintain, it may be difficult to sustain IS data science research without a healthy presence in the leading IS journals. Furthermore, IS journals are arguably best positioned to publish research at the nexus of data science, business, and society. The objective of this editorial is to inspire a discussion on opportunities to facilitate IS data science reviewing and publication in top IS journals, and to capitalize on these opportunities effectively as a community. This discussion is based on the understanding that both writing and reviewing practices are fundamental to our ability to assess the quality of data science contributions and to draw upon and publish high-quality and impactful research. These practices also affect the potential to sustain and grow an impactful data science community. Toward that end, this editorial also hopes to initiate a discussion on sustaining and growing a data science community within IS.",poster,cp44
p248,3402835f33e3e1342eb86b4d13907e3c9121c82b,c21,Grid Computing Environments,Data Science for Business: What You Need to Know about Data Mining and Data-Analytic Thinking,"Written by renowned data science experts Foster Provost and Tom Fawcett, Data Science for Business introduces the fundamental principles of data science, and walks you through the ""data-analytic thinking"" necessary for extracting useful knowledge and business value from the data you collect. This guide also helps you understand the many data-mining techniques in use today. Based on an MBA course Provost has taught at New York University over the past ten years, Data Science for Business provides examples of real-world business problems to illustrate these principles. Youll not only learn how to improve communication between business stakeholders and data scientists, but also how participate intelligently in your companys data science projects. Youll also discover how to think data-analytically, and fully appreciate how data science methods can support business decision-making.Understand how data science fits in your organizationand how you can use it for competitive advantage Treat data as a business asset that requires careful investment if youre to gain real value Approach business problems data-analytically, using the data-mining process to gather good data in the most appropriate way Learn general concepts for actually extracting knowledge from data Apply data science principles when interviewing data science job candidates",poster,cp21
p249,f69e1ba2b7cb1ddd866688e6d6007bc9e79b17fd,j79,Computer,Theory-Guided Data Science for Climate Change,"To adequately address climate change, we need novel data-science methods that account for the spatiotemporal and physical nature of climate phenomena. Only then will we be able to move from statistical analysis to scientific insights.",fullPaper,jv79
p250,8419f3ae96341e7a2d4012acc9848284feddfd3a,c20,Research Conference of the South African Institute of Computer Scientists and Information Technologists,Data Science For Dummies,"Discover how data science can help you gain in-depth insight into your business the easy way! Jobs in data science abound, but few people have the data science skills needed to fill these increasingly important roles in organizations. Data Science For Dummies is the perfect starting point for IT professionals and students interested in making sense of their organization s massive data sets and applying their findings to real-world business scenarios. From uncovering rich data sources to managing large amounts of data within hardware and software limitations, ensuring consistency in reporting, merging various data sources, and beyond, you ll develop the know-how you need to effectively interpret data and tell a story that can be understood by anyone in your organization. * Provides a background in data science fundamentals before moving on to working with relational databases and unstructured data and preparing your data for analysis * Details different data visualization techniques that can be used to showcase and summarize your data * Explains both supervised and unsupervised machine learning, including regression, model validation, and clustering techniques * Includes coverage of big data processing tools like MapReduce, Hadoop, Dremel, Storm, and Spark It s a big, big data world out there let Data Science For Dummies help you harness its power and gain a competitive edge for your organization.",poster,cp20
p251,0a7dd279ee312c9ef9c6fe04cd6f4f5e974abae3,c1,Technical Symposium on Computer Science Education,A Data Science Solution for Mining Interesting Patterns from Uncertain Big Data,"Nowadays, high volumes of valuable uncertain data can be easily collected or generated at high velocity in many real-life applications. Mining these uncertain Big data is computationally intensive due to the presence of existential probability values associated with items in every transaction in the uncertain data. Each existential probability value expresses the likelihood of that item to be present in a particular transaction in the Big data. In some situations, users may be interested in mining all frequent patterns from these uncertain Big data, in other situations, users may be interested in only a tiny portion of these mined patterns. To reduce the computation and to focus the mining for the latter situations, we propose a tree-based algorithm that (i) allows users to express the patterns to be mined according to their intention via the use of constraints and (ii) uses MapReduce to mine uncertain Big data for only those frequent patterns that satisfy user-specified constraints. Experimental results show the effectiveness of our algorithm in mining interesting patterns from uncertain Big data.",poster,cp1
p252,5bbb90ae23803b8bb115d5d7f60c8defc5376e2a,c20,Research Conference of the South African Institute of Computer Scientists and Information Technologists,"Intrinsic Relations between Data Science, Big Data, Business Analytics and Datafication","Data recording and storage have evolved over the past decades from manual gathering of data by using simple writing materials to the automation of data collection. Data storage has evolved significantly in the past decades and today databases no longer suffice as the only medium for the storage and management of data. This is due to the emergence of the Big Data and Data Science concepts. Previous studies have indicated that the multiplication of processing power of computers and the availability of larger data storage at reduced cost are part of the catalysts for the volume and rate at which data is now made available and captured.
 In this paper, the concepts of Big Data, Data Science and Business Analytics are reviewed. This paper discusses datafication of different aspects of life as the fundamental concept behind the growth of Big Data and Data Science. A review of the characteristics and value of Big Data and Data Science suggests that these emerging concepts will bring a paradigm change to a number of areas. Big Data was described as the basis for Data Science and Business Analytics which are tools employed in Data Science. Because these fields are still developing, there are diverse opinions, especially on the definition of Data Science. This paper provides a revised definition of Data Science, based on the review of available literature and proposes a schematic representation of the concepts.",fullPaper,cp20
p253,c27b057880c13df5cdc8a4c0efff85d272457d4e,j74,Statistical analysis and data mining,Divide and recombine (D&R): Data science for large complex data,"The need for deep analysis of large complex data has brought a focus to data science. The reasoning is simple. Data science consists of all technical areas that come into play in the analysis of data, and deep analysis of large complex data challenges all of the technical areas, from statistical theory to the architecture of clusters designed specifically for data. What is more, research in the technical areas needs to be tightly integrated.",fullPaper,jv74
p254,c2fb0ded7b21a23cd0931558b52ddbc98fc4f934,c34,IEEE Working Conference on Mining Software Repositories,Doing Data Science: Straight Talk from the Frontline,"Now that people are aware that data can make the difference in an election or a business model, data science as an occupation is gaining ground. But how can you get started working in a wide-ranging, interdisciplinary field thats so clouded in hype? This insightful book, based on Columbia Universitys Introduction to Data Science class, tells you what you need to know. In many of these chapter-long lectures, data scientists from companies such as Google, Microsoft, and eBay share new algorithms, methods, and models by presenting case studies and the code they use. If youre familiar with linear algebra, probability, and statistics, and have programming experience, this book is an ideal introduction to data science. Topics include:Statistical inference, exploratory data analysis, and the data science process Algorithms Spam filters, Naive Bayes, and data wrangling Logistic regression Financial modeling Recommendation engines and causality Data visualization Social networks and data journalism Data engineering, MapReduce, Pregel, and Hadoop Doing Data Science is collaboration between course instructor Rachel Schutt, Senior VP of Data Science at News Corp, and data science consultant Cathy ONeil, a senior data scientist at Johnson Research Labs, who attended and blogged about the course.",poster,cp34
p255,694516601ffb21d876d4e3967d3e81dd813efc6e,c1,Technical Symposium on Computer Science Education,Data science as an undergraduate degree,The purpose of this panel is to discuss the creation and implementation of a data science degree program at the undergraduate level. The panel includes representatives from three different universities that each offers an undergraduate degree in Data Science as of fall 2013. We plan to share information on the logistics of how the data science programs came to exist at each of our schools as well as encourage a robust interactive discussion about the future of data science education at the undergraduate level.,fullPaper,cp1
p256,cf3e96dbc50eadbbfb27328f2b9b528105764eeb,c77,Networks,Data science for business,"Written by renowned data science experts Foster Provost and Tom Fawcett, Data Science for Business introduces the fundamental principles of data science, and walks you through the ""data-analytic thinking"" necessary for extracting useful knowledge and business value from the data you collect. This guide also helps you understand the many data-mining techniques in use today. Based on an MBA course Provost has taught at New York University over the past ten years, Data Science for Business provides examples of real-world business problems to illustrate these principles. You'll not only learn how to improve communication between business stakeholders and data scientists, but also how participate intelligently in your company's data science projects. You'll also discover how to think data-analytically, and fully appreciate how data science methods can support business decision-making. Understand how data science fits in your organization - and how you can use it for competitive advantage Treat data as a business asset that requires careful investment if you're to gain real value Approach business problems data-analytically, using the data-mining process to gather good data in the most appropriate way Learn general concepts for actually extracting knowledge from data Apply data science principles when interviewing data science job candidates",poster,cp77
p257,3d7fcd1399573fb5cb455de6f85f149e0ab53828,c10,Big Data,The Science of Data Science,Abstract content,fullPaper,cp10
p258,1163c2996dfd0a46639b094e34ad783e969a0692,c1,Technical Symposium on Computer Science Education,Data science and prediction,Big data promises automated actionable knowledge creation and predictive models for use by both humans and computers.,poster,cp1
p259,e9931ea8ae9b8db38b519ef9ae32ec41a06d8445,c24,Decision Support Systems,Doing Data Science,"Now that people are aware that data can make the difference in an election or a business model, data science as an occupation is gaining ground. But how can you get started working in a wide-ranging, interdisciplinary field that's so clouded in hype? This insightful book, based on Columbia University's Introduction to Data Science class, tells you what you need to know. In many of these chapter-long lectures, data scientists from companies such as Google, Microsoft, and eBay share new algorithms, methods, and models by presenting case studies and the code they use. If you're familiar with linear algebra, probability, and statistics, and have programming experience, this book is an ideal introduction to data science. Topics include: Statistical inference, exploratory data analysis, and the data science process Algorithms Spam filters, Naive Bayes, and data wrangling Logistic regression Financial modeling Recommendation engines and causality Data visualization Social networks and data journalism Data engineering, MapReduce, Pregel, and Hadoop Doing Data Science is collaboration between course instructor Rachel Schutt, Senior VP of Data Science at News Corp, and data science consultant Cathy O'Neil, a senior data scientist at Johnson Research Labs, who attended and blogged about the course.",poster,cp24
p260,0e341b2a181f71dea088dbba800e70262f91a79e,c101,International Conference on Automatic Face and Gesture Recognition,"Color Science: Concepts and Methods, Quantitative Data and Formulae, 2nd Edition",Physical Data. The Eye. Colorimetry. Photometry. Visual Equivalence and Visual Matching. Uniform Color Scales. Visual Thresholds. Theories and Models of Color Vision. Appendix. References. Author and Subject Indexes.,poster,cp101
p261,379e9576dea9690cf88d9132287edbefb7626232,c58,Australian Software Engineering Conference,Data Smart: Using Data Science to Transform Information into Insight,"Data Science gets thrown around in the press like it's magic. Major retailers are predicting everything from when their customers are pregnant to when they want a new pair of Chuck Taylors. It's a brave new world where seemingly meaningless data can be transformed into valuable insight to drive smart business decisions.But how does one exactly do data science? Do you have to hire one of these priests of the dark arts, the ""data scientist,"" to extract this gold from your data? Nope.Data science is little more than using straight-forward steps to process raw data into actionable insight. And inData Smart, author and data scientist John Foreman will show you how that's done within the familiar environment of a spreadsheet.",poster,cp58
p262,6a324214a73610d8819e004e7ebd7dd23107d1f8,j62,Nature,Computing: A vision for data science,Abstract content,fullPaper,jv62
p263,b9111489ec08b50bc573982ede11f5bc2d7a4e88,c40,IEEE International Conference on Software Maintenance and Evolution,Sjplot - Data Visualization For Statistics In Social Science.,"New functions


 tab_model() as replacement for sjt.lm() , sjt.glm() , sjt.lmer() and sjt.glmer() . Furthermore, tab_model() is designed to work with the same model-objects as plot_model() .
 New colour scales for ggplot-objects: scale_fill_sjplot() and scale_color_sjplot() . These provide predifined colour palettes from this package.
 show_sjplot_pals() to show all predefined colour palettes provided by this package.
 sjplot_pal() to return colour values of a specific palette.


Deprecated

Following functions are now deprecated:


 sjp.lm() , sjp.glm() , sjp.lmer() , sjp.glmer() and sjp.int() . Please use plot_model() instead.
 sjt.frq() . Please use sjmisc::frq(out = ""v"") instead.


Removed / Defunct

Following functions are now defunct:


 sjt.grpmean() , sjt.mwu() and sjt.df() . The replacements are sjstats::grpmean() , sjstats::mwu() and tab_df() resp. tab_dfs() .


Changes to functions


 plot_model() and plot_models() get a prefix.labels -argument, to prefix automatically retrieved term labels with either the related variable name or label.
 plot_model() gets a show.zeroinf -argument to show or hide the zero-inflation-part of models in the plot.
 plot_model() gets a jitter -argument to add some random variation to data points for those plot types that accept show.data = TRUE .
 plot_model() gets a legend.title -argument to define the legend title for plots that display a legend.
 plot_model() now passes more arguments in ... down to ggeffects::plot() for marginal effects plots.
 plot_model() now plots the zero-inflated part of the model for brmsfit -objects.
 plot_model() now plots multivariate response models, i.e. models with multiple outcomes.
 Diagnostic plots in plot_model() ( type = ""diag"" ) can now also be used with brmsfit -objects.
 Axis limits of diagnostic plots in plot_model() ( type = ""diag"" ) for Stan-models ( brmsfit or stanreg resp. stanfit ) can now be set with the axis.lim -argument.
 The grid.breaks -argument for plot_model() and plot_models() now also takes a vector of values to directly define the grid breaks for the plot.
 Better default calculation for grid breaks in plot_model() and plot_models() when the grid.breaks -argument is of length one.
 The terms -argument for plot_model() now also allows the specification of a range of numeric values in square brackets for marginal effects plots, e.g. terms = ""age [30:50]"" or terms = ""age [pretty]"" .
 For coefficient-plots, the terms - and rm.terms -arguments for plot_model() now also allows specification of factor levels for categorical terms. Coefficients for the indicted factor levels are kept resp. removed (see ?plot_model for details).
 plot_model() now supports clmm -objects (package ordinal).
 plot_model(type = ""diag"") now also shows random-effects QQ-plots for glmmTMB -models, and also plots random-effects QQ-plots for all random effects (if model has more than one random effect term).


Bug fixes


 plot_model(type = ""re"") now supports standard errors and confidence intervals for glmmTMB -objects.
 Fixed typo for glmmTMB -tidier, which may have returned wrong data for zero-inflation part of model.
 Multiple random intercepts for multilevel models fitted with brms area now shown in each own facet per intercept.
 Remove unnecessary warning in sjp.likert() for uneven category count when neutral category is specified.
 plot_model(type = ""int"") could not automatically select mdrt.values properly for non-integer variables.
 sjp.grpfrq() now correctly uses the complete space in facets when facet.grid = TRUE .
 sjp.grpfrq(type = ""boxplot"") did not correctly label the x-axis when one category had no elements in a vector.
 Problems with German umlauts when printing HTML tables were fixed.",poster,cp40
p264,3858f600d0187c28f381b034a70226213e82a54e,j80,Nature Reviews Methods Primers,Network analysis of multivariate data in psychological science,Abstract content,fullPaper,jv80
p265,f3eda875e14bf933759f3b777131a4a9973537b4,c114,IEEE International Conference on Robotics and Automation,"Data-driven science and engineering: machine learning, dynamical systems, and control",Abstract content,poster,cp114
p266,0636653b82e152ba99b1d921b0aa2798aa845d1e,j81,Quantitative Science Studies,"Scopus as a curated, high-quality bibliometric data source for academic research in quantitative science studies","Scopus is among the largest curated abstract and citation databases, with a wide global and regional coverage of scientific journals, conference proceedings, and books, while ensuring only the highest quality data are indexed through rigorous content selection and re-evaluation by an independent Content Selection and Advisory Board. Additionally, extensive quality assurance processes continuously monitor and improve all data elements in Scopus. Besides enriched metadata records of scientific articles, Scopus offers comprehensive author and institution profiles, obtained from advanced profiling algorithms and manual curation, ensuring high precision and recall. The trustworthiness of Scopus has led to its use as bibliometric data source for large-scale analyses in research assessments, research landscape studies, science policy evaluations, and university rankings. Scopus data have been offered for free for selected studies by the academic research community, such as through application programming interfaces, which have led to many publications employing Scopus data to investigate topics such as researcher mobility, network visualizations, and spatial bibliometrics. In June 2019, the International Center for the Study of Research was launched, with an advisory board consisting of bibliometricians, aiming to work with the scientometric research community and offering a virtual laboratory where researchers will be able to utilize Scopus data.",fullPaper,jv81
p267,010a8ed71c6a80c2c02c7f55e1718151f91ff35a,j81,Quantitative Science Studies,Web of Science as a data source for research on scientific and scholarly activity,"Web of Science (WoS) is the world’s oldest, most widely used and authoritative database of research publications and citations. Based on the Science Citation Index, founded by Eugene Garfield in 1964, it has expanded its selective, balanced, and complete coverage of the world’s leading research to cover around 34,000 journals today. A wide range of use cases are supported by WoS from daily search and discovery by researchers worldwide through to the supply of analytical data sets and the provision of specialized access to raw data for bibliometric partners. A long- and well-established network of such partners enables the Institute for Scientific Information (ISI) to continue to work closely with bibliometric groups around the world to the benefit of both the community and the services that the company provides to researchers and analysts.",fullPaper,jv81
p268,cbf6a6d8fff87b74f36c5e4ede09f55e7a71506c,c42,Joint Conference of International Workshop on Software Measurement and International Conference on Software Process and Product Measurement,Numerical data and functional relationships in science and technology,Abstract content,poster,cp42
p269,e257edf34abd9a191fea1023a423abb497cca70f,c82,Workshop on Interdisciplinary Software Engineering Research,The data science education dilemma,"The need for people fluent in working with data is growing rapidly and enormously, but U.S. K–12 education does not provide meaningful learning experiences designed to develop understanding of data science concepts or a fluency with data science skills. Data science is inherently inter- disciplinary, so it makes sense to integrate it with existing content areas, but difficulties abound. Consideration of the work involved in doing data science and the habits of mind that lie behind it leads to a way of thinking about integrating data science with mathematics and science. Examples drawn from current activity development in the Data Games project shed some light on what technology-based, data-driven might be like. The project’s ongoing research on learners’ conceptions of organizing data and the relevance to data science education is explained.",poster,cp82
p270,46d55edae7cf5f065bb037462a7951c220f42618,j20,Proceedings of the National Academy of Sciences of the United States of America,"Active learning increases student performance in science, engineering, and mathematics","Significance The President’s Council of Advisors on Science and Technology has called for a 33% increase in the number of science, technology, engineering, and mathematics (STEM) bachelor’s degrees completed per year and recommended adoption of empirically validated teaching practices as critical to achieving that goal. The studies analyzed here document that active learning leads to increases in examination performance that would raise average grades by a half a letter, and that failure rates under traditional lecturing increase by 55% over the rates observed under active learning. The analysis supports theory claiming that calls to increase the number of students receiving STEM degrees could be answered, at least in part, by abandoning traditional lecturing in favor of active learning. To test the hypothesis that lecturing maximizes learning and course performance, we metaanalyzed 225 studies that reported data on examination scores or failure rates when comparing student performance in undergraduate science, technology, engineering, and mathematics (STEM) courses under traditional lecturing versus active learning. The effect sizes indicate that on average, student performance on examinations and concept inventories increased by 0.47 SDs under active learning (n = 158 studies), and that the odds ratio for failing was 1.95 under traditional lecturing (n = 67 studies). These results indicate that average examination scores improved by about 6% in active learning sections, and that students in classes with traditional lecturing were 1.5 times more likely to fail than were students in classes with active learning. Heterogeneity analyses indicated that both results hold across the STEM disciplines, that active learning increases scores on concept inventories more than on course examinations, and that active learning appears effective across all class sizes—although the greatest effects are in small (n ≤ 50) classes. Trim and fill analyses and fail-safe n calculations suggest that the results are not due to publication bias. The results also appear robust to variation in the methodological rigor of the included studies, based on the quality of controls over student quality and instructor identity. This is the largest and most comprehensive metaanalysis of undergraduate STEM education published to date. The results raise questions about the continued use of traditional lecturing as a control in research studies, and support active learning as the preferred, empirically validated teaching practice in regular classrooms.",fullPaper,jv20
p271,dd4f9aa21cf34994c07d2d74fc7f633194564224,j81,Quantitative Science Studies,"Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic","We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.",fullPaper,jv81
p272,425744cb05e854071d06af0da2b8ef2d677f33d5,j82,EOS,Harnessing the GPS Data Explosion for Interdisciplinary Science,"More GPS stations, faster data delivery, and better data processing provide an abundance of information for all kinds of Earth scientists.",fullPaper,jv82
p273,30c9e3fcb1ead2a827f91ff5cd203aa0d8058bff,c35,EUROMICRO Conference on Software Engineering and Advanced Applications,Data-Driven Science and Engineering,Abstract content,poster,cp35
p274,abc0a9eb3ae901ece2f532f504c336fbb6ba81ca,j83,Advancement of science,"Data‐Driven Materials Science: Status, Challenges, and Perspectives","Data‐driven science is heralded as a new paradigm in materials science. In this field, data is the new resource, and knowledge is extracted from materials datasets that are too big or complex for traditional human reasoning—typically with the intent to discover new or improved materials or materials phenomena. Multiple factors, including the open science movement, national funding, and progress in information technology, have fueled its development. Such related tools as materials databases, machine learning, and high‐throughput methods are now established as parts of the materials research toolset. However, there are a variety of challenges that impede progress in data‐driven materials science: data veracity, integration of experimental and computational data, data longevity, standardization, and the gap between industrial interests and academic efforts. In this perspective article, the historical development and current state of data‐driven materials science, building from the early evolution of open science to the rapid expansion of materials data infrastructures are discussed. Key successes and challenges so far are also reviewed, providing a perspective on the future development of the field.",fullPaper,jv83
p275,1110da1c238a7b09258136e7a2e7d558fb16f272,j84,Nuclear Data Sheets,TENDL: Complete Nuclear Data Library for Innovative Nuclear Science and Technology,Abstract content,fullPaper,jv84
p276,ee5825861645ec9b9d11a2882f3aa15ec9e6e4dd,c48,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication","ENDF/B-VII.1 Nuclear Data for Science and Technology: Cross Sections, Covariances, Fission Product Yields and Decay Data",Abstract content,poster,cp48
p277,00a4bdc5158945a0b9463a29da4810838e474875,c13,International Conference on Data Science and Advanced Analytics,Perspective: Materials informatics and big data: Realization of the “fourth paradigm” of science in materials science,"Our ability to collect “big data” has greatly surpassed our capability to analyze it, underscoring the emergence of the fourth paradigm of science, which is data-driven discovery. The need for data informatics is also emphasized by the Materials Genome Initiative (MGI), further boosting the emerging field of materials informatics. In this article, we look at how data-driven techniques are playing a big role in deciphering processing-structure-property-performance relationships in materials, with illustrative examples of both forward models (property prediction) and inverse models (materials discovery). Such analytics can significantly reduce time-to-insight and accelerate cost-effective materials discovery, which is the goal of MGI.",poster,cp13
p278,ee013b1477e8f81cb5c66a9a93a342281f740042,c1,Technical Symposium on Computer Science Education,Assessing data quality in citizen science,"Ecological and environmental citizen-science projects have enormous potential to advance scientific knowledge, influence policy, and guide resource management by producing datasets that would otherwise be infeasible to generate. However, this potential can only be realized if the datasets are of high quality. While scientists are often skeptical of the ability of unpaid volunteers to produce accurate datasets, a growing body of publications clearly shows that diverse types of citizen-science projects can produce data with accuracy equal to or surpassing that of professionals. Successful projects rely on a suite of methods to boost data accuracy and account for bias, including iterative project development, volunteer training and testing, expert validation, replication across volunteers, and statistical modeling of systematic error. Each citizen-science dataset should therefore be judged individually, according to project design and application, and not assumed to be substandard simply because volunteers generated it.",poster,cp1
p279,8f63eed1c6aef4e96a08281563c2305ff55e7ab9,j85,BMC Bioinformatics,ImageJ2: ImageJ for the next generation of scientific image data,Abstract content,fullPaper,jv85
p280,ae5cd38f62db7892177de7dded2ac25e6981f87e,c41,Software Product Lines Conference,What Is Data Science,"Program of Study The technological revolution has led to an explosion of data in domains of knowledge including medicine, policy, social sciences, commerce, and the natural sciences. Petabytes of data are being collected from a myriad of instruments, like sequencing machines for genomics and mobile devices for quantifying social interactions. In addition to driving research, data are shaping the way people work, live, and communicate. Correspondingly, new methodologies have emerged to power intelligent systems, make more accurate predictions, and gain new insight using the large volumes of data generated by scientists, entrepreneurs, and analysts.",poster,cp41
p281,40b2324cde863db7670178f0151fae400a9a2b93,j86,American Political Science Review,Analyzing Incomplete Political Science Data: An Alternative Algorithm for Multiple Imputation,"We propose a remedy for the discrepancy between the way political scientists analyze data with missing values and the recommendations of the statistics community. Methodologists and statisticians agree that “multiple imputation” is a superior approach to the problem of missing data scattered through one’s explanatory and dependent variables than the methods currently used in applied data analysis. The discrepancy occurs because the computational algorithms used to apply the best multiple imputation models have been slow, difficult to implement, impossible to run with existing commercial statistical packages, and have demanded considerable expertise. We adapt an algorithm and use it to implement a general-purpose, multiple imputation model for missing data. This algorithm is considerably faster and easier to use than the leading method recommended in the statistics literature. We also quantify the risks of current missing data practices, illustrate how to use the new procedure, and evaluate this alternative through simulated data as well as actual empirical examples. Finally, we offer easy-to-use software that implements all methods discussed.",fullPaper,jv86
p282,81e1dbe1e8152d0ddbf89e861468d799dbebe367,j87,Biological Conservation,Social media data for conservation science: A methodological overview,Abstract content,fullPaper,jv87
p283,7281fb2c44f4d73c86ffefd1fde7e4f8a1f5e75c,j88,Science Education,Engagement in science through citizen science: Moving beyond data collection,"""To date, most studies of citizen science engagement focus on quantifiable measures related to the contribution of data or other output measures. Few studies have attempted to qualitatively characterize citizen science engagement across multiple projects and from the perspective of the participants. Building on pertinent literature and sociocultural learning theories, this study operationalizes engagement in citizen science through an analysis of interviews of 72 participants from six different environmentally based projects. We document engagement in citizen science through an examination of cognitive, affective, social, behavioral, and motivational dimensions. We assert that engagement in citizen science is enhanced by acknowledging these multiple dimensions and creating opportunities for volunteers to find personal relevance in their work with scientists. A Dimensions of Engagement framework is presented that can facilitate the innovation of new questions and methodologies for studying engagement in citizen science and other forms of informal science education.""",fullPaper,jv88
p284,3fe3924a5315fbb5b5cd0edf98533b8c61a3bbdf,j89,ICES Journal of Marine Science,Machine intelligence and the data-driven future of marine science,"
 Oceans constitute over 70% of the earth's surface, and the marine environment and ecosystems are central to many global challenges. Not only are the oceans an important source of food and other resources, but they also play a important roles in the earth's climate and provide crucial ecosystem services. To monitor the environment and ensure sustainable exploitation of marine resources, extensive data collection and analysis efforts form the backbone of management programmes on global, regional, or national levels. Technological advances in sensor technology, autonomous platforms, and information and communications technology now allow marine scientists to collect data in larger volumes than ever before. But our capacity for data analysis has not progressed comparably, and the growing discrepancy is becoming a major bottleneck for effective use of the available data, as well as an obstacle to scaling up data collection further. Recent years have seen rapid advances in the fields of artificial intelligence and machine learning, and in particular, so-called deep learning systems are now able to solve complex tasks that previously required human expertise. This technology is directly applicable to many important data analysis problems and it will provide tools that are needed to solve many complex challenges in marine science and resource management. Here we give a brief review of recent developments in deep learning, and highlight the many opportunities and challenges for effective adoption of this technology across the marine sciences.",fullPaper,jv89
p285,2ff6d7e05b1f74e0b17dbf97a59ac0d75ef65efc,j90,Data Intelligence,FAIR Data and Services in Biodiversity Science and Geoscience,"We examine the intersection of the FAIR principles (Findable, Accessible, Interoperable and Reusable), the challenges and opportunities presented by the aggregation of widely distributed and heterogeneous data about biological and geological specimens, and the use of the Digital Object Architecture (DOA) data model and components as an approach to solving those challenges that offers adherence to the FAIR principles as an integral characteristic. This approach will be prototyped in the Distributed System of Scientific Collections (DiSSCo) project, the pan-European Research Infrastructure which aims to unify over 110 natural science collections across 21 countries. We take each of the FAIR principles, discuss them as requirements in the creation of a seamless virtual collection of bio/geo specimen data, and map those requirements to Digital Object components and facilities such as persistent identification, extended data typing, and the use of an additional level of abstraction to normalize existing heterogeneous data structures. The FAIR principles inform and motivate the work and the DO Architecture provides the technical vision to create the seamless virtual collection vitally needed to address scientific questions of societal importance.",fullPaper,jv90
p286,6a697a4b3bdbbfb7681d9f9a518fc0be73744037,j91,Physical Review Letters,Big data of materials science: critical role of the descriptor.,"Statistical learning of materials properties or functions so far starts with a largely silent, nonchallenged step: the choice of the set of descriptive parameters (termed descriptor). However, when the scientific connection between the descriptor and the actuating mechanisms is unclear, the causality of the learned descriptor-property relation is uncertain. Thus, a trustful prediction of new promising materials, identification of anomalies, and scientific advancement are doubtful. We analyze this issue and define requirements for a suitable descriptor. For a classic example, the energy difference of zinc blende or wurtzite and rocksalt semiconductors, we demonstrate how a meaningful descriptor can be found systematically.",fullPaper,jv91
p287,e27acaf97f5b2eae4257bb5d8278fbe0e6405c39,c21,Grid Computing Environments,Creating the CIPRES Science Gateway for inference of large phylogenetic trees,"Understanding the evolutionary history of living organisms is a central problem in biology. Until recently the ability to infer evolutionary relationships was limited by the amount of DNA sequence data available, but new DNA sequencing technologies have largely removed this limitation. As a result, DNA sequence data are readily available or obtainable for a wide spectrum of organisms, thus creating an unprecedented opportunity to explore evolutionary relationships broadly and deeply across the Tree of Life. Unfortunately, the algorithms used to infer evolutionary relationships are NP-hard, so the dramatic increase in available DNA sequence data has created a commensurate increase in the need for access to powerful computational resources. Local laptop or desktop machines are no longer viable for analysis of the larger data sets available today, and progress in the field relies upon access to large, scalable high-performance computing resources. This paper describes development of the CIPRES Science Gateway, a web portal designed to provide researchers with transparent access to the fastest available community codes for inference of phylogenetic relationships, and implementation of these codes on scalable computational resources. Meeting the needs of the community has included developing infrastructure to provide access, working with the community to improve existing community codes, developing infrastructure to insure the portal is scalable to the entire systematics community, and adopting strategies that make the project sustainable by the community. The CIPRES Science Gateway has allowed more than 1800 unique users to run jobs that required 2.5 million Service Units since its release in December 2009. (A Service Unit is a CPU-hour at unit priority).",fullPaper,cp21
p288,8e981ddb4877615f7d5f944a8d64789d1388ee87,j92,Astrophysical Journal,LSST: From Science Drivers to Reference Design and Anticipated Data Products,"We describe here the most ambitious survey currently planned in the optical, the Large Synoptic Survey Telescope (LSST). The LSST design is driven by four main science themes: probing dark energy and dark matter, taking an inventory of the solar system, exploring the transient optical sky, and mapping the Milky Way. LSST will be a large, wide-field ground-based system designed to obtain repeated images covering the sky visible from Cerro Pachón in northern Chile. The telescope will have an 8.4 m (6.5 m effective) primary mirror, a 9.6 deg2 field of view, a 3.2-gigapixel camera, and six filters (ugrizy) covering the wavelength range 320–1050 nm. The project is in the construction phase and will begin regular survey operations by 2022. About 90% of the observing time will be devoted to a deep-wide-fast survey mode that will uniformly observe a 18,000 deg2 region about 800 times (summed over all six bands) during the anticipated 10 yr of operations and will yield a co-added map to r ∼ 27.5. These data will result in databases including about 32 trillion observations of 20 billion galaxies and a similar number of stars, and they will serve the majority of the primary science programs. The remaining 10% of the observing time will be allocated to special projects such as Very Deep and Very Fast time domain surveys, whose details are currently under discussion. We illustrate how the LSST science drivers led to these choices of system parameters, and we describe the expected data products and their characteristics.",fullPaper,jv92
p289,8a9f26a4cee210e51c96f4016737605e31d490ee,j93,MRS Communications,A data ecosystem to support machine learning in materials science,"Facilitating the application of machine learning to materials science problems will require enhancing the data ecosystem to enable discovery and collection of data from many sources, automated dissemination of new data across the ecosystem, and the connecting of data with materials-specific machine learning models. Here, we present two projects, the Materials Data Facility (MDF) and the Data and Learning Hub for Science (DLHub), that address these needs. We use examples to show how MDF and DLHub capabilities can be leveraged to link data with machine learning models and how users can access those capabilities through web and programmatic interfaces.",fullPaper,jv93
p290,4e2f43dab69d690dc86422949e410ebf37f522d4,c34,IEEE Working Conference on Mining Software Repositories,Bayesian data analysis.,"Bayesian methods have garnered huge interest in cognitive science as an approach to models of cognition and perception. On the other hand, Bayesian methods for data analysis have not yet made much headway in cognitive science against the institutionalized inertia of 20th century null hypothesis significance testing (NHST). Ironically, specific Bayesian models of cognition and perception may not long endure the ravages of empirical verification, but generic Bayesian methods for data analysis will eventually dominate. It is time that Bayesian data analysis became the norm for empirical methods in cognitive science. This article reviews a fatal flaw of NHST and introduces the reader to some benefits of Bayesian data analysis. The article presents illustrative examples of multiple comparisons in Bayesian analysis of variance and Bayesian approaches to statistical power. Copyright © 2010 John Wiley & Sons, Ltd. For further resources related to this article, please visit the WIREs website.",poster,cp34
p291,7ec947261f5a3eabdaddb8e53d58a36b986c4e71,c59,British Computer Society Conference on Human-Computer Interaction,ENDF/B-VII.0: Next Generation Evaluated Nuclear Data Library for Nuclear Science and Technology,Abstract content,poster,cp59
p292,22737046fbbe822deaaffddddb8f16be076d3f95,c88,Symposium on the Theory of Computing,"Open Science, Open Data, and Open Scholarship: European Policies to Make Science Fit for the Twenty-First Century","Open science will make science more efficient, reliable, and responsive to societal challenges. The European Commission has sought to advance open science policy from its inception in a holistic and integrated way, covering all aspects of the research cycle from scientific discovery and review to sharing knowledge, publishing, and outreach. We present the steps taken with a forward-looking perspective on the challenges laying ahead, in particular the necessary change of the rewards and incentives system for researchers (for which various actors are co-responsible and which goes beyond the mandate of the European Commission). Finally, we discuss the role of artificial intelligence (AI) within an open science perspective.",poster,cp88
p293,fff51615943e08d05080682009c9c656321ef0b2,j94,MRS bulletin,NOMAD: The FAIR concept for big data-driven materials science,"<jats:p><jats:fig position=""anchor""><jats:graphic xmlns:xlink=""http://www.w3.org/1999/xlink"" orientation=""portrait"" mime-subtype=""jpeg"" mimetype=""image"" position=""float"" xlink:type=""simple"" xlink:href=""S0883769418002087_figAb"" /></jats:fig></jats:p>",fullPaper,jv94
p294,652c77a90d84df639622efdc9cd7475e96a248c9,j95,Comptes rendus. Mecanique,Data-driven modeling and learning in science and engineering,Abstract content,fullPaper,jv95
p295,f4c01d8780c86abdcfdd52c60843a2499fd5c1b6,j96,Perspectives on Psychological Science,Using Smartphones to Collect Behavioral Data in Psychological Science,"Smartphones now offer the promise of collecting behavioral data unobtrusively, in situ, as it unfolds in the course of daily life. Data can be collected from the onboard sensors and other phone logs embedded in today’s off-the-shelf smartphone devices. These data permit fine-grained, continuous collection of people’s social interactions (e.g., speaking rates in conversation, size of social groups, calls, and text messages), daily activities (e.g., physical activity and sleep), and mobility patterns (e.g., frequency and duration of time spent at various locations). In this article, we have drawn on the lessons from the first wave of smartphone-sensing research to highlight areas of opportunity for psychological research, present practical considerations for designing smartphone studies, and discuss the ongoing methodological and ethical challenges associated with research in this domain. It is our hope that these practical guidelines will facilitate the use of smartphones as a behavioral observation tool in psychological science.",fullPaper,jv96
p296,2660fbc3b666145a87f05de10066fc2a3e7467dd,c37,Asia-Pacific Software Engineering Conference,The Science Of Real Time Data Capture Self Reports In Health Research,"The National Cancer Institute (NCI) has designated the topic of real-time data capture as an important and innovative research area. As such, the NCI sponsored a national meeting of distinguished research scientists to discuss the state of the science in this emerging and burgeoning field. This book reflects the findings of the conference and discusses the state of the science of real-time data capture and its application to health and cancer research. It provides a conceptual framework for minute-by-minute data captureecological momentary assessments (EMA)and discusses health-related topics where these assessements have been applied. In addition, future directions in real-time data capture assessment, interventions, methodology, and technology are discussed.",poster,cp37
p297,0563dfcd6ebc910e5580b89bcd73f2b0cb82a21d,j97,Science,Data-driven predictions in the science of science,"The desire to predict discoveries—to have some idea, in advance, of what will be discovered, by whom, when, and where—pervades nearly all aspects of modern science, from individual scientists to publishers, from funding agencies to hiring committees. In this Essay, we survey the emerging and interdisciplinary field of the “science of science” and what it teaches us about the predictability of scientific discovery. We then discuss future opportunities for improving predictions derived from the science of science and its potential impact, positive and negative, on the scientific community.",fullPaper,jv97
p298,b48a917258f4e7e2b78a41289d005513db1de8c9,c22,International Conference on Data Technologies and Applications,Earth Observation Open Science: Enhancing Reproducible Science Using Data Cubes,"Earth Observation Data Cubes (EODC) have emerged as a promising solution to efficiently and effectively handle Big Earth Observation (EO) Data generated by satellites and made freely and openly available from different data repositories. The aim of this Special Issue, “Earth Observation Data Cube”, in Data, is to present the latest advances in EODC development and implementation, including innovative approaches for the exploitation of satellite EO data using multi-dimensional (e.g., spatial, temporal, spectral) approaches. This Special Issue contains 14 articles covering a wide range of topics such as Synthetic Aperture Radar (SAR), Analysis Ready Data (ARD), interoperability, thematic applications (e.g., land cover, snow cover mapping), capacity development, semantics, processing techniques, as well as national implementations and best practices. These papers made significant contributions to the advancement of a more Open and Reproducible Earth Observation Science, reducing the gap between users’ expectations for decision-ready products and current Big Data analytical capabilities, and ultimately unlocking the information power of EO data by transforming them into actionable knowledge.",fullPaper,cp22
p299,3e02906da7498b5fdbc6f0eea4b6bb9f2d86dd00,c41,Software Product Lines Conference,ON PATIENT FLOW IN HOSPITALS: A DATA-BASED QUEUEING-SCIENCE PERSPECTIVE,"Hospitals are complex systems with essential societal benefits and huge mounting costs. These costs are exacerbated by inefficiencies in hospital processes, which are often manifested by congestion and long delays in patient care. Thus, a queueing-network view of patient flow in hospitals is natural for studying and improving its performance. The goal of our research is to explore patient flow data through the lens of a queueing scientist. The means is exploratory data analysis (EDA) in a large Israeli hospital, which reveals important features that are not readily explainable by existing models. Questions raised by our EDA include: Can a simple (parsimonious) queueing model usefully capture the complex operational reality of the Emergency Department (ED)? What time scales and operational regimes are relevant for modeling patient length of stay in the Internal Wards (IWs)? How do protocols of patient transfer between the ED and the IWs influence patient delay, workload division and fairness? EDA also unde...",poster,cp41
p300,d1db1c83c64f556fd4005cc12bddf7963f82a77f,j14,Scientific Data,Open science resources for the discovery and analysis of Tara Oceans data,Abstract content,fullPaper,jv14
p301,54d9fc3ed4937ee546ed45aee7bef16b4ae3775d,c34,IEEE Working Conference on Mining Software Repositories,Statistics for citizen science: extracting signals of change from noisy ecological data,"Policy‐makers increasingly demand robust measures of biodiversity change over short time periods. Long‐term monitoring schemes provide high‐quality data, often on an annual basis, but are taxonomically and geographically restricted. By contrast, opportunistic biological records are relatively unstructured but vast in quantity. Recently, these data have been applied to increasingly elaborate science and policy questions, using a range of methods. At present, we lack a firm understanding of which methods, if any, are capable of delivering unbiased trend estimates on policy‐relevant time‐scales. We identified a set of candidate methods that employ data filtering criteria and/or correction factors to deal with variation in recorder activity. We designed a computer simulation to compare the statistical properties of these methods under a suite of realistic data collection scenarios. We measured the Type I error rates of each method–scenario combination, as well as the power to detect genuine trends. We found that simple methods produce biased trend estimates, and/or had low power. Most methods are robust to variation in sampling effort, but biases in spatial coverage, sampling effort per visit, and detectability, as well as turnover in community composition, all induced some methods to fail. No method was wholly unaffected by all forms of variation in recorder activity, although some performed well enough to be useful. We warn against the use of simple methods. Sophisticated methods that model the data collection process offer the greatest potential to estimate timely trends, notably Frescalo and occupancy–detection models. The potential of these methods and the value of opportunistic data would be further enhanced by assessing the validity of model assumptions and by capturing small amounts of information about sampling intensity at the point of data collection.",poster,cp34
p302,d4825585c5b036bb789ad183635cc2d4d89ff394,c53,International Conference on Software Engineering and Knowledge Engineering,Opening the archive: How free data has enabled the science and monitoring promise of Landsat,Abstract content,poster,cp53
p303,bd8a307efcffbf57d2e5c3c23577de44d883d865,c23,International Conference on Open and Big Data,MedRec: Using Blockchain for Medical Data Access and Permission Management,"Years of heavy regulation and bureaucratic inefficiency have slowed innovation for electronic medical records (EMRs). We now face a critical need for such innovation, as personalization and data science prompt patients to engage in the details of their healthcare and restore agency over their medical data. In this paper, we propose MedRec: a novel, decentralized record management system to handle EMRs, using blockchain technology. Our system gives patients a comprehensive, immutable log and easy access to their medical information across providers and treatment sites. Leveraging unique blockchain properties, MedRec manages authentication, confidentiality, accountability and data sharing- crucial considerations when handling sensitive information. A modular design integrates with providers' existing, local data storage solutions, facilitating interoperability and making our system convenient and adaptable. We incentivize medical stakeholders (researchers, public health authorities, etc.) to participate in the network as blockchain “miners”. This provides them with access to aggregate, anonymized data as mining rewards, in return for sustaining and securing the network via Proof of Work. MedRec thus enables the emergence of data economics, supplying big data to empower researchers while engaging patients and providers in the choice to release metadata. The purpose of this short paper is to expose, prior to field tests, a working prototype through which we analyze and discuss our approach.",fullPaper,cp23
p304,87f7c170aecf8f3465b26a11b9a384fef934337b,c4,Annual Conference on Genetic and Evolutionary Computation,Measurement and Data Analysis for Engineering and Science,"Fundamentals of Experimentation Introduction Experiments Chapter Overview Experimental Approach Role of Experiments The Experiment Classification of Experiments Plan for Successful Experimentation Hypothesis Testing* Design of Experiments* Factorial Design* Problems Bibliography Fundamental Electronics Chapter Overview Concepts and Definitions Circuit Elements RLC Combinations Elementary DC Circuit Analysis Elementary AC Circuit Analysis Equivalent Circuits* Meters* Impedance Matching and Loading Error* Electrical Noise* Problems Bibliography Measurement Systems: Sensors and Transducers Chapter Overview Measurement System Overview Sensor Domains Sensor Characteristics Physical Principles of Sensors Electric Piezoelectric Fluid Mechanic Optic Photoelastic Thermoelectric Electrochemical Sensor Scaling* Problems Bibliography Measurement Systems: Other Components Chapter Overview Signal Conditioning, Processing, and Recording Amplifiers Filters Analog-to-Digital Converters Smart Measurement Systems Other Example Measurement Systems Problems Bibliography Measurement Systems: Calibration and Response Chapter Overview Static Response Characterization by Calibration Dynamic Response Characterization Zero-Order System Dynamic Response First-Order System Dynamic Response Second-Order System Dynamic Response Measurement System Dynamic Response Problems Bibliography Measurement Systems: Design-Stage Uncertainty Chapter Overview Design-Stage Uncertainty Analysis Design-Stage Uncertainty Estimate of a Measurand Design-Stage Uncertainty Estimate of a Result Problems Bibliography Signal Characteristics Chapter Overview Signal Classification Signal Variables Signal Statistical Parameters Problems Bibliography The Fourier Transform Chapter Overview Fourier Series of a Periodic Signal Complex Numbers and Waves Exponential Fourier Series Spectral Representations Continuous Fourier Transform Continuous Fourier Transform Properties* Discrete Fourier Transform Fast Fourier Transform Problems Bibliography Digital Signal Analysis Chapter Overview Digital Sampling Digital Sampling Errors Windowing* Determining a Sample Period Problems Bibliography Probability Chapter Overview Relation to Measurements Basic Probability Concepts Sample versus Population Plotting Statistical Information Probability Density Function Various Probability Density Functions Central Moments Probability Distribution Function Problems Bibliography Statistics Chapter Overview Normal Distribution Normalized Variables Student's t Distribution Rejection of Data Standard Deviation of the Means Chi-Square Distribution Pooling Samples* Problems Bibliography Uncertainty Analysis Chapter Overview Modeling and Experimental Uncertainties Probabilistic Basis of Uncertainty Identifying Sources of Error Systematic and Random Errors Quantifying Systematic and Random Errors Measurement Uncertainty Analysis Uncertainty Analysis of a Multiple-Measurement Result Uncertainty Analyses for Other Measurement Situations Uncertainty Analysis Summary Finite-Difference Uncertainties* Uncertainty Based upon Interval Statistics* Problems Bibliography Regression and Correlation Chapter Overview Least-Squares Approach Least-Squares Regression Analysis Linear Analysis Higher-Order Analysis* Multi-Variable Linear Analysis* Determining the Appropriate Fit Regression Confidence Intervals Regression Parameters Linear Correlation Analysis Signal Correlations in Time* Problems Bibliography Units and Significant Figures Chapter Overview English and Metric Systems Systems of Units SI Standards Technical English and SI Conversion Factors Prefixes Significant Figures Problems Bibliography Technical Communication Chapter Overview Guidelines for Writing Technical Memo Technical Report Oral Technical Presentation Problems Bibliography A Glossary B Symbols C Review Problem Answers Index",poster,cp4
p305,7579330e89bffd736fee19d25359ab3ae65bf5f7,c51,Conference of the Centre for Advanced Studies on Collaborative Research,rioja: Analysis of Quaternary Science Data,Abstract content,poster,cp51
p306,3f809897b51d824846cf5a56f2a7b4292f7bc4a4,j62,Nature,Materials science: Share corrosion data,Abstract content,fullPaper,jv62
p307,16f4135a229c79e60fa25259100c8cdcedfab8cc,c45,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",Patent citation data in social science research: Overview and best practices,"The last 2 decades have witnessed a dramatic increase in the use of patent citation data in social science research. Facilitated by digitization of the patent data and increasing computing power, a community of practice has grown up that has developed methods for using these data to: measure attributes of innovations such as impact and originality; to trace flows of knowledge across individuals, institutions and regions; and to map innovation networks. The objective of this article is threefold. First, it takes stock of these main uses. Second, it discusses 4 pitfalls associated with patent citation data, related to office, time and technology, examiner, and strategic effects. Third, it highlights gaps in our understanding and offers directions for future research.",poster,cp45
p308,30d6f200f8b4bae78dbb4f69f1730bcad131d523,c59,British Computer Society Conference on Human-Computer Interaction,The Materials Data Facility: Data Services to Advance Materials Science Research,Abstract content,poster,cp59
p309,969f983d000ac68ca77548b5bba2e8d1b89086c4,c7,European Conference on Modelling and Simulation,Materials science with large-scale data and informatics: Unlocking new opportunities,"Universal access to abundant scientific data, and the software to analyze the data at scale, could fundamentally transform the field of materials science. Today, the materials community faces serious challenges to bringing about this data-accelerated research paradigm, including diversity of research areas within materials, lack of data standards, and missing incentives for sharing, among others. Nonetheless, the landscape is rapidly changing in ways that should benefit the entire materials research enterprise. We provide an overview of the current state of the materials data and informatics landscape, highlighting a few selected efforts that make more data freely available and useful to materials researchers.",poster,cp7
p310,b5fb74dfc71c92113c84a0e8f0502e0e76b4dbda,j98,Social Science Research,The role of administrative data in the big data revolution in social science research.,Abstract content,fullPaper,jv98
p311,0db731c99879bb74c3850c53923d1df2c510f8c3,j99,IEEE Transactions on Geoscience and Remote Sensing,"AIRS/AMSU/HSB on the Aqua mission: design, science objectives, data products, and processing systems","The Atmospheric Infrared Sounder (AIRS), the Advanced Microwave Sounding Unit (AMSU), and the Humidity Sounder for Brazil (HSB) form an integrated cross-track scanning temperature and humidity sounding system on the Aqua satellite of the Earth Observing System (EOS). AIRS is an infrared spectrometer/radiometer that covers the 3.7-15.4-/spl mu/m spectral range with 2378 spectral channels. AMSU is a 15-channel microwave radiometer operating between 23 and 89 GHz. HSB is a four-channel microwave radiometer that makes measurements between 150 and 190 GHz. In addition to supporting the National Aeronautics and Space Administration's interest in process study and climate research, AIRS is the first hyperspectral infrared radiometer designed to support the operational requirements for medium-range weather forecasting of the National Ocean and Atmospheric Administration's National Centers for Environmental Prediction (NCEP) and other numerical weather forecasting centers. AIRS, together with the AMSU and HSB microwave radiometers, will achieve global retrieval accuracy of better than 1 K in the lower troposphere under clear and partly cloudy conditions. This paper presents an overview of the science objectives, AIRS/AMSU/HSB data products, retrieval algorithms, and the ground-data processing concepts. The EOS Aqua was launched on May 4, 2002 from Vandenberg AFB, CA, into a 705-km-high, sun-synchronous orbit. Based on the excellent radiometric and spectral performance demonstrated by AIRS during prelaunch testing, which has by now been verified during on-orbit testing, we expect the assimilation of AIRS data into the numerical weather forecast to result in significant forecast range and reliability improvements.",fullPaper,jv99
p312,a41bd1872fa81874a4057e303a1dcc5716554f81,j100,Astronomy & Astrophysics,Gaia Data Release 2,"Context. The Gaia second Data Release (DR2) presents a first mapping of full-sky RR Lyrae stars and Cepheids observed by the spacecraft during the initial 22 months of science operations.
Aims. The Specific Objects Study (SOS) pipeline, developed to validate and fully characterise Cepheids and RR Lyrae stars (SOS Cep&RRL) observed by Gaia, has been presented in the documentation and papers accompanying the Gaia first Data Release. Here we describe how the SOS pipeline was modified to allow for processing the Gaia multi-band (G, GBP, and GRP) time-series photometry of all-sky candidate variables and produce specific results for confirmed RR Lyrae stars and Cepheids that are published in the DR2 catalogue.
Methods. The SOS Cep&RRL processing uses tools such as the period–amplitude and the period–luminosity relations in the G band. For the analysis of the Gaia DR2 candidates we also used tools based on the GBP and GRP photometry, such as the period–Wesenheit relation in (G, GRP).
Results. Multi-band time-series photometry and characterisation by the SOS Cep&RRL pipeline are published in Gaia DR2 for 150 359 such variables (9575 classified as Cepheids and 140 784 as RR Lyrae stars) distributed throughout the sky. The sample includes variables in 87 globular clusters and 14 dwarf galaxies (the Magellanic Clouds, 5 classical and 7 ultra-faint dwarfs). To the best of our knowledge, as of 25 April 2018, the variability of 50 570 of these sources (350 Cepheids and 50 220 RR Lyrae stars) has not been reported before in the literature, therefore they are likely new discoveries by Gaia. An estimate of the interstellar absorption is published for 54 272 fundamental-mode RR Lyrae stars from a relation based on the G-band amplitude and the pulsation period. Metallicities derived from the Fourier parameters of the light curves are also released for 64 932 RR Lyrae stars and 3738 fundamental-mode classical Cepheids with periods shorter than 6.3 days.",fullPaper,jv100
p313,7bd598f6a7c6eb4265fe5a9ca64504d1d639684a,c78,Neural Information Processing Systems,Educational data mining and learning analytics: An updated survey,"This survey is an updated and improved version of the previous one published in 2013 in this journal with the title “data mining in education”. It reviews in a comprehensible and very general way how Educational Data Mining and Learning Analytics have been applied over educational data. In the last decade, this research area has evolved enormously and a wide range of related terms are now used in the bibliography such as Academic Analytics, Institutional Analytics, Teaching Analytics, Data‐Driven Education, Data‐Driven Decision‐Making in Education, Big Data in Education, and Educational Data Science. This paper provides the current state of the art by reviewing the main publications, the key milestones, the knowledge discovery cycle, the main educational environments, the specific tools, the free available datasets, the most used methods, the main objectives, and the future trends in this research area.",poster,cp78
p314,16cecb0173adc68762b6e70daecb25089a5a6b6a,j0,Nature Biotechnology,ProteomeXchange provides globally co-ordinated proteomics data submission and dissemination,Abstract content,fullPaper,jv0
p315,2809d4876e34b8c64fc1783fe6a0a278770505b0,c79,International Conference on Medical Image Computing and Computer-Assisted Intervention,A survey of data provenance in e-science,"Data management is growing in complexity as large-scale applications take advantage of the loosely coupled resources brought together by grid middleware and by abundant storage capacity. Metadata describing the data products used in and generated by these applications is essential to disambiguate the data and enable reuse. Data provenance, one kind of metadata, pertains to the derivation history of a data product starting from its original sources.In this paper we create a taxonomy of data provenance characteristics and apply it to current research efforts in e-science, focusing primarily on scientific workflow approaches. The main aspect of our taxonomy categorizes provenance systems based on why they record provenance, what they describe, how they represent and store provenance, and ways to disseminate it. The survey culminates with an identification of open research problems in the field.",poster,cp79
p316,33aeb033401ec748633bdd5b806db4f58288ee69,c85,International Conference on Graph Transformation,The Accuracy of Citizen Science Data: A Quantitative Review,"Author(s): Aceves-Bueno, Erendira; Adeleye, Adeyemi S; Feraud, Marina; Huang, Yuxiong; Tao, Mengya; Yang, Yi; Anderson, Sarah E",poster,cp85
p317,ad3d83248eae66580d4deada76e72e3be9a9b44c,c20,Research Conference of the South African Institute of Computer Scientists and Information Technologists,Named data networking,"Named Data Networking (NDN) is one of five projects funded by the U.S. National Science Foundation under its Future Internet Architecture Program. NDN has its roots in an earlier project, Content-Centric Networking (CCN), which Van Jacobson first publicly presented in 2006. The NDN project investigates Jacobson's proposed evolution from today's host-centric network architecture (IP) to a data-centric network architecture (NDN). This conceptually simple shift has far-reaching implications for how we design, develop, deploy, and use networks and applications. We describe the motivation and vision of this new architecture, and its basic components and operations. We also provide a snapshot of its current design, development status, and research challenges. More information about the project, including prototype implementations, publications, and annual reports, is available on named-data.net.",poster,cp20
p318,1e4709c0b8fe3bf759cd64dc1ede695d6e5316f0,j8,Journal of Big Data,Deep learning applications and challenges in big data analytics,Abstract content,fullPaper,jv8
p319,0870c1ea2b7d5a515c7b5b954f1433b379fe1e02,j101,Earth-Science Reviews,Principles and methods of scaling geospatial Earth science data,Abstract content,fullPaper,jv101
p320,439ede62248e5f6202982afead02b33d3feffae7,j102,Nucleic Acids Research,TCGAbiolinks: an R/Bioconductor package for integrative analysis of TCGA data,"The Cancer Genome Atlas (TCGA) research network has made public a large collection of clinical and molecular phenotypes of more than 10 000 tumor patients across 33 different tumor types. Using this cohort, TCGA has published over 20 marker papers detailing the genomic and epigenomic alterations associated with these tumor types. Although many important discoveries have been made by TCGA's research network, opportunities still exist to implement novel methods, thereby elucidating new biological pathways and diagnostic markers. However, mining the TCGA data presents several bioinformatics challenges, such as data retrieval and integration with clinical data and other molecular data types (e.g. RNA and DNA methylation). We developed an R/Bioconductor package called TCGAbiolinks to address these challenges and offer bioinformatics solutions by using a guided workflow to allow users to query, download and perform integrative analyses of TCGA data. We combined methods from computer science and statistics into the pipeline and incorporated methodologies developed in previous TCGA marker studies and in our own group. Using four different TCGA tumor types (Kidney, Brain, Breast and Colon) as examples, we provide case studies to illustrate examples of reproducibility, integrative analysis and utilization of different Bioconductor packages to advance and accelerate novel discoveries.",fullPaper,jv102
p321,b970f9c088beee99666a40374dd5ccb06eeda112,c24,Decision Support Systems,Understanding the paradigm shift to computational social science in the presence of big data,Abstract content,fullPaper,cp24
p322,bee570503aaa0ed5bc5dd4cf6aa742df0b5cef87,c39,International Conference on Global Software Engineering,"The Elements of Statistical Learning: Data Mining, Inference, and Prediction, 2nd Edition",Abstract content,poster,cp39
p323,a1bee4683c64f0e74771e095c1bd952e4d39f735,j103,Conservation Biology,Emerging problems of data quality in citizen science,"The role of citizen science in research and natural resource monitoring and management is increasing, as evidenced by the growing number of peer-reviewed publications (including a special section in this journal) and calls for involving citizens in monitoring and governance (through, for example, “participatory research” [Danielsen et al. 2014] and “participatory monitoring” [Kennett et al. 2015]). Citizen science projects can be targeted to a specific research question (and thus involve very specific data-collection protocols) or can be more open-ended (giving rise to a need to collect data for which the uses may be unknown or changing) (Wiersma 2010). Advances in online content production and sharing technologies (i.e., Web 2.0), mobile computing, and sensor-equipped devices have contributed to a dramatic rise in online citizen science projects, in which citizens contribute sightings (e.g., eBird [Sullivan et al. 2009]), transcribe data (e.g., Old Weather [Eveleigh et al. 2013]), or classify phenomena (e.g., Galaxy Zoo [Hopkin 2007]). It is these online projects, also referred to as crowdsourcing (Franzoni & Sauermann 2014), which have been the focus of our research and that inform the opinions presented here. Galaxy Zoo exemplifies an initiative that began as a targeted project in which citizens were engaged in the relatively simple task of classifying images of galaxies as one of 3 shapes (Hopkin 2007). The goal was to distribute a large workload among a large number of people. Citizen participation grew quickly, which led project sponsors to create an online forum to accommodate the large volume of comments and questions. Through this forum, a number of unanticipated categories of celestial bodies arose, including 2 from Dutch school teacher Hanny Van Arkel, who noted the “green peas” phenomena (Cardamone et al. 2009) and a new body that became known as “Hanny’s Voorwerp” (Lintott et al. 2009). The Galaxy Zoo story provides an example of the different dimensions of data quality in citizen science. The researchers anticipated a small, fixed set of categories of galaxy shapes and designed the data-collection interface accordingly. One dimension of data quality (Lewandowski & Specht 2015) is data accuracy; others include data completeness and timeliness. (For a complete discussion of the many dimensions of data quality, see Wang and Strong [1996]). In the case of Galaxy Zoo, data accuracy is measured as the proportion of images correctly classified by galaxy shape. Had it not been for the attentiveness of one person who went beyond the task of classifying galaxies into predetermined categories and was able to communicate this to the researchers via the online forum, what turned out to be important new phenomena might have gone undiscovered. Failure to discover these phenomena would have affected the data-quality dimension of completeness because not all celestial bodies in the images would have been cataloged. Thus, the data quality would be diminished. Lewandowski and Specht (2015) describe 4 dimensions of data quality in their broad review of biologythemed citizen science: data accuracy and precision; sufficient sample size; and standardized sampling procedures (including sufficient spatial and temporal representation). These dimensions are congruent with good scientific practice and thus suggest that the criteria used to measure the quality of citizens’ data should fit the standards of professional science. In this sense, citizen science amounts to asking citizens to fill in the blanks in a story written by scientists. Although it is helpful for citizen scientists to adhere to standards of scientific practice, the process of doing science includes more than simply collecting and processing data. As Stevens et al. (2014:21) admitted: “Often . . . participants might be viewed as sensors or data collectors, but they’re rarely invited to decide what data to collect or to contribute to the data analysis or interpretation, even though they . . . might have valuable insights,” a view echoed in a recent Nature commentary by Kennett et al. (2015). The online forum created by the Galaxy Zoo project manifests a design decision that allowed for participants to provide valuable new insights and contribute beyond simply classifying images. Because discoveries resulted from one individual going beyond the assigned task, an open question is how many discoveries went undetected because other participants failed to notice particular features (given the prescribed task) or noticed but failed to post on the site’s forum. Based on examples such as this one, we argue that data quality in citizen science is much more than data accuracy. Because citizens generally lack formal scientific training, they view problems and issues in light of their own knowledge and interests, creating fertile ground for discoveries. This perspective – that citizen scientists view problems differently than scientists – means that the quality of data should be defined as more than simply",fullPaper,jv103
p324,08a2ef1648fa5ea539ebe1718da577dc79124a21,j26,Frontiers in Environmental Science,Prospects and challenges for social media data in conservation science,"Social media data have been extensively used in numerous fields of science, but examples of their use in conservation science are still very limited. In this paper, we propose a framework on how social media data could be useful for conservation science and practice. We present the commonly used social media platforms and discuss how their content could be providing new data and information for conservation science. Based on this, we discuss how future work in conservation science and practice would benefit from social media data.",fullPaper,jv26
p325,362f50f59a280d7cc526fb626fdf44ad382cee57,j104,Scientometrics,The journal coverage of Web of Science and Scopus: a comparative analysis,Abstract content,fullPaper,jv104
p326,e0634f2945b43d4c13a0aa2ff31f2c1c5fe597b9,c32,International Conference on Software Technology: Methods and Tools,The Role of Anomalous Data in Knowledge Acquisition: A Theoretical Framework and Implications for Science Instruction,"Understanding how science students respond to anomalous data is essential to understanding knowledge acquisition in science classrooms. This article presents a detailed analysis of the ways in which scientists and science students respond to such data. We postulate that there are seven distinct forms of response to anomalous data, only one of which is to accept the data and change theories. The other six responses involve discounting the data in various ways in order to protect the preinstructional theory. We analyze the factors that influence which of these seven forms of response a scientist or student will choose, giving special attention to the factors that make theory change more likely. Finally, we discuss the implications of our framework for science instruction.",poster,cp32
p327,53a7e8d57128adc10817c1db98ff946c7060124c,j105,Behavior Research Methods,TurkPrime.com: A versatile crowdsourcing data acquisition platform for the behavioral sciences,Abstract content,fullPaper,jv105
p328,51995dc568874ea34911833355234b1f696dacfc,j58,Journal of Data and Information Science,Science Mapping: A Systematic Review of the Literature,"Abstract Purpose We present a systematic review of the literature concerning major aspects of science mapping to serve two primary purposes: First, to demonstrate the use of a science mapping approach to perform the review so that researchers may apply the procedure to the review of a scientific domain of their own interest, and second, to identify major areas of research activities concerning science mapping, intellectual milestones in the development of key specialties, evolutionary stages of major specialties involved, and the dynamics of transitions from one specialty to another. Design/methodology/approach We first introduce a theoretical framework of the evolution of a scientific specialty. Then we demonstrate a generic search strategy that can be used to construct a representative dataset of bibliographic records of a domain of research. Next, progressively synthesized co-citation networks are constructed and visualized to aid visual analytic studies of the domain’s structural and dynamic patterns and trends. Finally, trajectories of citations made by particular types of authors and articles are presented to illustrate the predictive potential of the analytic approach. Findings The evolution of the science mapping research involves the development of a number of interrelated specialties. Four major specialties are discussed in detail in terms of four evolutionary stages: conceptualization, tool construction, application, and codification. Underlying connections between major specialties are also explored. The predictive analysis demonstrates citations trajectories of potentially transformative contributions. Research limitations The systematic review is primarily guided by citation patterns in the dataset retrieved from the literature. The scope of the data is limited by the source of the retrieval, i.e. the Web of Science, and the composite query used. An iterative query refinement is possible if one would like to improve the data quality, although the current approach serves our purpose adequately. More in-depth analyses of each specialty would be more revealing by incorporating additional methods such as citation context analysis and studies of other aspects of scholarly publications. Practical implications The underlying analytic process of science mapping serves many practical needs, notably bibliometric mapping, knowledge domain visualization, and visualization of scientific literature. In order to master such a complex process of science mapping, researchers often need to develop a diverse set of skills and knowledge that may span multiple disciplines. The approach demonstrated in this article provides a generic method for conducting a systematic review. Originality/value Incorporating the evolutionary stages of a specialty into the visual analytic study of a research domain is innovative. It provides a systematic methodology for researchers to achieve a good understanding of how scientific fields evolve, to recognize potentially insightful patterns from visually encoded signs, and to synthesize various information so as to capture the state of the art of the domain.",fullPaper,jv58
p329,c36991759325bedd19f69264f72d1cbf59a6158c,c50,International Conference on Automated Software Engineering,Data Mining: Concepts and Techniques,"The increasing volume of data in modern business and science calls for more complex and sophisticated tools. Although advances in data mining technology have made extensive data collection much easier, it's still always evolving and there is a constant need for new techniques and tools that can help us transform this data into useful information and knowledge. Since the previous edition's publication, great advances have been made in the field of data mining. Not only does the third of edition of Data Mining: Concepts and Techniques continue the tradition of equipping you with an understanding and application of the theory and practice of discovering patterns hidden in large data sets, it also focuses on new, important topics in the field: data warehouses and data cube technology, mining stream, mining social networks, and mining spatial, multimedia and other complex data. Each chapter is a stand-alone guide to a critical topic, presenting proven algorithms and sound implementations ready to be used directly or with strategic modification against live data. This is the resource you need if you want to apply today's most powerful data mining techniques to meet real business challenges. * Presents dozens of algorithms and implementation examples, all in pseudo-code and suitable for use in real-world, large-scale data mining projects. * Addresses advanced topics such as mining object-relational databases, spatial databases, multimedia databases, time-series databases, text databases, the World Wide Web, and applications in several fields. *Provides a comprehensive, practical look at the concepts and techniques you need to get the most out of real business data",poster,cp50
p330,6802bbeea45ea9c44b8e9f69ee1d775f5af0717f,j106,International Journal of Exercise Science,Ethical Issues Relating to Scientific Discovery in Exercise Science.,"This work aims to present concepts related to ethical issues in conducting and reporting scientific research in a clear and straightforward manner. Considerations around research design including authorship, sound research practices, non-discrimination in subject recruitment, objectivity, respect for intellectual property, and financial interests are detailed. Further, concepts relating to the conducting of research including the competency of the researcher, conflicts of interest, accurately representing data, and ethical practices in human and animal research are presented. Attention pertaining to the dissemination of research including plagiarism, duplicate submission, redundant publication, and figure manipulation is offered. Other considerations including responsible mentoring, respect for colleagues, and social responsibility are set forth. The International Journal of Exercise Science will now require a statement in all subsequent published manuscripts that the authors have complied with each of the ethics statements contained in this work.",fullPaper,jv106
p331,e150ddf65a610d636c78a17fa7ca32f0c304184a,c113,International Conference on Image Analysis and Processing,General data protection regulation,"Presentacio sobre l'Oficina de Proteccio de Dades Personals de la UAB i la politica Open Science. Va formar part de la conferencia ""Les politiques d'Open Data / Open Acces: Implicacions a la recerca"" orientada a investigadors i gestors de projectes europeus que va tenir lloc el 20 de setembre de 2018 a la Universitat Autonoma de Barcelona",poster,cp113
p332,eaf5a5e0b32a055e288d5edcc5cd39f9f4d335ad,j107,Nature Communications,The misuse of colour in science communication,Abstract content,fullPaper,jv107
p333,4d12b00963aa6e0d9b9b84a62f0543de608fccb5,j108,PLoS ONE,"If We Share Data, Will Anyone Use Them? Data Sharing and Reuse in the Long Tail of Science and Technology","Research on practices to share and reuse data will inform the design of infrastructure to support data collection, management, and discovery in the long tail of science and technology. These are research domains in which data tend to be local in character, minimally structured, and minimally documented. We report on a ten-year study of the Center for Embedded Network Sensing (CENS), a National Science Foundation Science and Technology Center. We found that CENS researchers are willing to share their data, but few are asked to do so, and in only a few domain areas do their funders or journals require them to deposit data. Few repositories exist to accept data in CENS research areas.. Data sharing tends to occur only through interpersonal exchanges. CENS researchers obtain data from repositories, and occasionally from registries and individuals, to provide context, calibration, or other forms of background for their studies. Neither CENS researchers nor those who request access to CENS data appear to use external data for primary research questions or for replication of studies. CENS researchers are willing to share data if they receive credit and retain first rights to publish their results. Practices of releasing, sharing, and reusing of data in CENS reaffirm the gift culture of scholarship, in which goods are bartered between trusted colleagues rather than treated as commodities.",fullPaper,jv108
p334,24931dc3ddedfc2db5405af236e3ca84944d66d7,c83,International Conference on Computer Graphics and Interactive Techniques,Big Data and Social Science: A Practical Guide to Methods and Tools,"Both Traditional Students and Working Professionals Acquire the Skills to Analyze Social Problems. Big Data and Social Science: A Practical Guide to Methods and Tools shows how to apply data science to real-world problems in both research and the practice. The book provides practical guidance on combining methods and tools from computer science, statistics, and social science. This concrete approach is illustrated throughout using an important national problem, the quantitative study of innovation. The text draws on the expertise of prominent leaders in statistics, the social sciences, data science, and computer science to teach students how to use modern social science research principles as well as the best analytical and computational tools. It uses a real-world challenge to introduce how these tools are used to identify and capture appropriate data, apply data science models and tools to that data, and recognize and respond to data errors and limitations. For more information, including sample chapters and news, please visit the author's website.",poster,cp83
p335,cf83811d697dc3419a52c9853807afb410eb3943,j109,American Journal of Political Science,Tree-Based Models for Political Science Data,"Political scientists often find themselves analyzing data sets with a large number of observations, a large number of variables, or both. Yet, traditional statistical techniques fail to take full advantage of the opportunities inherent in “big data,” as they are too rigid to recover nonlinearities and do not facilitate the easy exploration of interactions in high-dimensional data sets. In this article, we introduce a family of tree-based nonparametric techniques that may, in some circumstances, be more appropriate than traditional methods for confronting these data challenges. In particular, tree models are very effective for detecting nonlinearities and interactions, even in data sets with many (potentially irrelevant) covariates. We introduce the basic logic of tree-based models, provide an overview of the most prominent methods in the literature, and conduct three analyses that illustrate how the methods can be implemented while highlighting both their advantages and limitations. Replication Materials: The data, code, and any additional materials required to replicate all analyses in this article are available on the American Journal of Political Science Dataverse within the Harvard Dataverse Network at: https://doi.org/10.7910/DVN/8ZJBLI. Social science scholars often work with data sets containing a large number of observations, many potential covariates, or (increasingly) both. Indeed, political scientists now regularly analyze data with levels of complexity unimaginable just two decades ago. Widely used surveys, for instance, interview tens of thousands of respondents about hundreds of topics. Scholars of institutions can quickly assemble data sets with thousands of observations using resources like the Comparative Agendas Project. Moreover, new measurement methods, such as text analysis, have combined with data sources, such as Twitter, to generate databases of almost unmanageable sizes. It is clear that political science, like all areas of the social sciences, will increasingly have access to a deluge of data so vast that it will dwarf everything that has come before. What statistical methods are needed in this datasaturated world? Surely, there is no one correct answer. Yet, just as surely, traditional statistical models are not always equipped to take full advantage of new data sources. Traditional models—largely variants of linear regressions—are ideal for evaluating theories that imply specific functional forms relating outcomes to predictors. In particular, they excel in their ability to leverage assumptions about the data-generating process, or DGP (additivity, linearity in the parameters, homoskedasticity, Jacob M. Montgomery is Associate Professor, Department of Political Science, Washington University in St. Louis, Campus Box 1063, One Brookings Drive, St. Louis, MO 63130 (jacob.montgomery@wustl.edu). Santiago Olivella is Assistant Professor, Department of Political Science, University of North Carolina at Chapel Hill, Hamilton Hall 361, CB 3265, Chapel Hill, NC 27599 (olivella@unc.edu). etc.) to make valid inferences despite inherent data limitations. Although appropriate when testing theories that conform with these assumptions, standard models are often insufficiently flexible to capture nuances in the data—such as complex nonlinear functional forms and deep interactions—when no clear a priori expectations exist. In this article, we introduce a family of tree-based nonparametric techniques from the machine learning literature. We argue that, under specific circumstances, regression and classification tree models are an appropriate standard choice for analyzing high-dimensional data sets. In particular, past research has shown tree-based methods to be very useful for making accurate predictions when the underlying DGP includes nonlinearities, discontinuities, and interactions among many covariates. Further, tree models require few assumptions. Rather than imposing a presumed structure on the DGP, tree-based methods allow the data to “speak for themselves.” Thus, our goal in this article is to introduce political scientists to this promising family of methods, which are well suited for today’s data analysis demands. In the next sections, we discuss the promise and perils of high-dimensional, “large”-N data sets and introduce the basic logic of tree models. We then provide an overview of the most prominent methods in the literature. American Journal of Political Science, Vol. 62, No. 3, July 2018, Pp. 729–744 C ©2018, Midwest Political Science Association DOI: 10.1111/ajps.12361",fullPaper,jv109
p336,b9921fb4d1448058642897797e77bdaf8f444404,j110,Political Analysis,Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts,"Politics and political conflict often occur in the written and spoken word. Scholars have long recognized this, but the massive costs of analyzing even moderately sized collections of texts have hindered their use in political science research. Here lies the promise of automated text analysis: it substantially reduces the costs of analyzing large collections of text. We provide a guide to this exciting new area of research and show how, in many instances, the methods have already obtained part of their promise. But there are pitfalls to using automated methods—they are no substitute for careful thought and close reading and require extensive and problem-specific validation. We survey a wide range of new methods, provide guidance on how to validate the output of the models, and clarify misconceptions and errors in the literature. To conclude, we argue that for automated text methods to become a standard tool for political scientists, methodologists must contribute new methods and new methods of validation.",fullPaper,jv110
p337,d2a595c5efb4b26245c4353d5d85cbe6c7ecac0f,j97,Science,Machine learning for data-driven discovery in solid Earth geoscience,"Automating geoscience analysis Solid Earth geoscience is a field that has very large set of observations, which are ideal for analysis with machine-learning methods. Bergen et al. review how these methods can be applied to solid Earth datasets. Adopting machine-learning techniques is important for extracting information and for understanding the increasing amount of complex data collected in the geosciences. Science, this issue p. eaau0323 BACKGROUND The solid Earth, oceans, and atmosphere together form a complex interacting geosystem. Processes relevant to understanding Earth’s geosystem behavior range in spatial scale from the atomic to the planetary, and in temporal scale from milliseconds to billions of years. Physical, chemical, and biological processes interact and have substantial influence on this complex geosystem, and humans interact with it in ways that are increasingly consequential to the future of both the natural world and civilization as the finiteness of Earth becomes increasingly apparent and limits on available energy, mineral resources, and fresh water increasingly affect the human condition. Earth is subject to a variety of geohazards that are poorly understood, yet increasingly impactful as our exposure grows through increasing urbanization, particularly in hazard-prone areas. We have a fundamental need to develop the best possible predictive understanding of how the geosystem works, and that understanding must be informed by both the present and the deep past. This understanding will come through the analysis of increasingly large geo-datasets and from computationally intensive simulations, often connected through inverse problems. Geoscientists are faced with the challenge of extracting as much useful information as possible and gaining new insights from these data, simulations, and the interplay between the two. Techniques from the rapidly evolving field of machine learning (ML) will play a key role in this effort. ADVANCES The confluence of ultrafast computers with large memory, rapid progress in ML algorithms, and the ready availability of large datasets place geoscience at the threshold of dramatic progress. We anticipate that this progress will come from the application of ML across three categories of research effort: (i) automation to perform a complex prediction task that cannot easily be described by a set of explicit commands; (ii) modeling and inverse problems to create a representation that approximates numerical simulations or captures relationships; and (iii) discovery to reveal new and often unanticipated patterns, structures, or relationships. Examples of automation include geologic mapping using remote-sensing data, characterizing the topology of fracture systems to model subsurface transport, and classifying volcanic ash particles to infer eruptive mechanism. Examples of modeling include approximating the viscoelastic response for complex rheology, determining wave speed models directly from tomographic data, and classifying diverse seismic events. Examples of discovery include predicting laboratory slip events using observations of acoustic emissions, detecting weak earthquake signals using similarity search, and determining the connectivity of subsurface reservoirs using groundwater tracer observations. OUTLOOK The use of ML in solid Earth geosciences is growing rapidly, but is still in its early stages and making uneven progress. Much remains to be done with existing datasets from long-standing data sources, which in many cases are largely unexplored. Newer, unconventional data sources such as light detection and ranging (LiDAR), fiber-optic sensing, and crowd-sourced measurements may demand new approaches through both the volume and the character of information that they present. Practical steps could accelerate and broaden the use of ML in the geosciences. Wider adoption of open-science principles such as open source code, open data, and open access will better position the solid Earth community to take advantage of rapid developments in ML and artificial intelligence. Benchmark datasets and challenge problems have played an important role in driving progress in artificial intelligence research by enabling rigorous performance comparison and could play a similar role in the geosciences. Testing on high-quality datasets produces better models, and benchmark datasets make these data widely available to the research community. They also help recruit expertise from allied disciplines. Close collaboration between geoscientists and ML researchers will aid in making quick progress in ML geoscience applications. Extracting maximum value from geoscientific data will require new approaches for combining data-driven methods, physical modeling, and algorithms capable of learning with limited, weak, or biased labels. Funding opportunities that target the intersection of these disciplines, as well as a greater component of data science and ML education in the geosciences, could help bring this effort to fruition. Digital geology. Digital representation of the geology of the conterminous United States. [Geology of the Conterminous United States at 1:2,500,000 scale; a digital representation of the 1974 P. B. King and H. M. Beikman map by P. G. Schruben, R. E. Arndt, W. J. Bawiec] The list of author affiliations is available in the full article online. Understanding the behavior of Earth through the diverse fields of the solid Earth geosciences is an increasingly important task. It is made challenging by the complex, interacting, and multiscale processes needed to understand Earth’s behavior and by the inaccessibility of nearly all of Earth’s subsurface to direct observation. Substantial increases in data availability and in the increasingly realistic character of computer simulations hold promise for accelerating progress, but developing a deeper understanding based on these capabilities is itself challenging. Machine learning will play a key role in this effort. We review the state of the field and make recommendations for how progress might be broadened and accelerated.",fullPaper,jv97
p338,da619a6c524f5ab800b44c8728db3cef3d3b25d9,c1,Technical Symposium on Computer Science Education,"Big Data, new epistemologies and paradigm shifts","This article examines how the availability of Big Data, coupled with new data analytics, challenges established epistemologies across the sciences, social sciences and humanities, and assesses the extent to which they are engendering paradigm shifts across multiple disciplines. In particular, it critically explores new forms of empiricism that declare ‘the end of theory’, the creation of data-driven rather than knowledge-driven science, and the development of digital humanities and computational social sciences that propose radically different ways to make sense of culture, history, economy and society. It is argued that: (1) Big Data and new data analytics are disruptive innovations which are reconfiguring in many instances how research is conducted; and (2) there is an urgent need for wider critical reflection within the academy on the epistemological implications of the unfolding data revolution, a task that has barely begun to be tackled despite the rapid changes in research practices presently taking place. After critically reviewing emerging epistemological positions, it is contended that a potentially fruitful approach would be the development of a situated, reflexive and contextually nuanced epistemology.",poster,cp1
p339,3aa1b70fdc97ae96091c5fb39cd911015ac5253e,c97,Interspeech,Novel methods improve prediction of species' distributions from occurrence data,"Prediction of species' distributions is central to diverse applications in ecology, evolution and conservation science. There is increasing electronic access to vast sets of occurrence records in museums and herbaria, yet little effective guidance on how best to use this information in the context of numerous approaches for modelling distributions. To meet this need, we compared 16 modelling methods over 226 species from 6 regions of the world, creating the most comprehensive set of model comparisons to date. We used presence-only data to fit models, and independent presence-absence data to evaluate the predictions. Along with well-established modelling methods such as generalised additive models and GARP and BIOCLIM, we explored methods that either have been developed recently or have rarely been applied to modelling species' distributions. These include machine-learning methods and community models, both of which have features that may make them particularly well suited to noisy or sparse information, as is typical of species' occurrence data. Presence-only data were effective for modelling species' distributions for many species and regions. The novel methods consistently outperformed more established methods. The results of our analysis are promising for the use of data from museums and herbaria, especially as methods suited to the noise inherent in such data improve.",poster,cp97
p340,efa5558bddd68abe4adc81adbbef6f739e648392,j60,PLoS Biology,Big Data: Astronomical or Genomical?,"Genomics is a Big Data science and is going to get much bigger, very soon, but it is not known whether the needs of genomics will exceed other Big Data domains. Projecting to the year 2025, we compared genomics with three other major generators of Big Data: astronomy, YouTube, and Twitter. Our estimates show that genomics is a “four-headed beast”—it is either on par with or the most demanding of the domains analyzed here in terms of data acquisition, storage, distribution, and analysis. We discuss aspects of new technologies that will need to be developed to rise up and meet the computational challenges that genomics poses for the near future. Now is the time for concerted, community-wide planning for the “genomical” challenges of the next decade.",fullPaper,jv60
p341,48fc9c42522184c652742255fdf31f7b9ed7ebae,j111,Journal of Evidence-Based Medicine,Brief introduction of medical database and data mining technology in big data era,"Data mining technology can search for potentially valuable knowledge from a large amount of data, mainly divided into data preparation and data mining, and expression and analysis of results. It is a mature information processing technology and applies database technology. Database technology is a software science that researches manages, and applies databases. The data in the database are processed and analyzed by studying the underlying theory and implementation methods of the structure, storage, design, management, and application of the database. We have introduced several databases and data mining techniques to help a wide range of clinical researchers better understand and apply database technology.",fullPaper,jv111
p342,0b510ee69a507407008661aacb2345f73c70f8cb,c107,British Machine Vision Conference,Strategies Employed by Citizen Science Programs to Increase the Credibility of Their Data,"The success of citizen science in producing important and unique data is attracting interest from scientists and resource managers. Nonetheless, questions remain about the credibility of citizen science data. Citizen science programs desire to meet the same standards of credibility as academic science, but they usually work within a different context, for example, training and managing significant numbers of volunteers with limited resources. We surveyed the credibility-building strategies of 30 citizen science programs that monitor environmental aspects of the California coast. We identified a total of twelve strategies: Three that are applied during training and planning; four that are applied during data collection; and five that are applied during data analysis and program evaluation. Variation in the application of these strategies by program is related to factors such as the number of participants, the focus on group or individual work, and the time commitment required of volunteers. The structure of each program and available resources require program designers to navigate tradeoffs in the choices of their credibility strategies. Our results illustrate those tradeoffs and provide a framework for the necessary discussions between citizen science programs and potential users of their data—including scientists and decision makers—about shared expectations for credibility and practical approaches for meeting those expectations. This article has been corrected here: http://dx.doi.org/10.5334/cstp.91",poster,cp107
p343,f2b66923db74a16169d040a51ada555d5b6f8851,c37,Asia-Pacific Software Engineering Conference,Data Mining and Analysis: Fundamental Concepts and Algorithms,"The fundamental algorithms in data mining and analysis form the basis for the emerging field of data science, which includes automated methods to analyze patterns and models for all kinds of data, with applications ranging from scientific discovery to business intelligence and analytics. This textbook for senior undergraduate and graduate data mining courses provides a broad yet in-depth overview of data mining, integrating related concepts from machine learning and statistics. The main parts of the book include exploratory data analysis, pattern mining, clustering, and classification. The book lays the basic foundations of these tasks, and also covers cutting-edge topics such as kernel methods, high-dimensional data analysis, and complex graphs and networks. With its comprehensive coverage, algorithmic perspective, and wealth of examples, this book offers solid guidance in data mining for students, researchers, and practitioners alike. Key features: Covers both core methods and cutting-edge research Algorithmic approach with open-source implementations Minimal prerequisites: all key mathematical concepts are presented, as is the intuition behind the formulas Short, self-contained chapters with class-tested examples and exercises allow for flexibility in designing a course and for easy reference Supplementary website with lecture slides, videos, project ideas, and more",poster,cp37
p344,5703617b9d9d40e90b6c8ffa21a52734d9822d60,c39,International Conference on Global Software Engineering,Defining Computational Thinking for Mathematics and Science Classrooms,Abstract content,poster,cp39
p345,46d71d947231f86e1f9d4581e61212385debbe14,c3,Frontiers in Education Conference,OpenML: networked science in machine learning,"Many sciences have made significant breakthroughs by adopting online tools that help organize, structure and mine information that is too detailed to be printed in journals. In this paper, we introduce OpenML, a place for machine learning researchers to share and organize data in fine detail, so that they can work more effectively, be more visible, and collaborate with others to tackle harder problems. We discuss how OpenML relates to other examples of networked science and what benefits it brings for machine learning research, individual scientists, as well as students and practitioners.",poster,cp3
p346,88bcdfd021d935a28f245e178792207881b14794,j112,Cambridge International Law Journal,Learning from Imbalanced Data Sets,Abstract content,fullPaper,jv112
p347,993c9eb9bba80e2d8993e8c99acca1825cd0302f,j97,Science,Next Steps for Citizen Science,"Strategic investments and coordination are needed for citizen science to reach its full potential. Around the globe, thousands of research projects are engaging millions of individuals—many of whom are not trained as scientists—in collecting, categorizing, transcribing, or analyzing scientific data. These projects, known as citizen science, cover a breadth of topics from microbiomes to native bees to water quality to galaxies. Most projects obtain or manage scientific information at scales or resolutions unattainable by individual researchers or research teams, whether enrolling thousands of individuals collecting data across several continents, enlisting small armies of participants in categorizing vast quantities of online data, or organizing small groups of volunteers to tackle local problems.",fullPaper,jv97
p348,11c6d8e4b34596a4e3ab77357b8fa505f83427cb,j104,Scientometrics,"Google Scholar, Scopus and the Web of Science: a longitudinal and cross-disciplinary comparison",Abstract content,fullPaper,jv104
p349,c1e49d830e67269d4d2053a5f124ea773c79b740,j97,Science,Computational social science: Obstacles and opportunities,"Data sharing, research ethics, and incentives must improve The field of computational social science (CSS) has exploded in prominence over the past decade, with thousands of papers published using observational data, experimental designs, and large-scale simulations that were once unfeasible or unavailable to researchers. These studies have greatly improved our understanding of important phenomena, ranging from social inequality to the spread of infectious diseases. The institutions supporting CSS in the academy have also grown substantially, as evidenced by the proliferation of conferences, workshops, and summer schools across the globe, across disciplines, and across sources of data. But the field has also fallen short in important ways. Many institutional structures around the field—including research ethics, pedagogy, and data infrastructure—are still nascent. We suggest opportunities to address these issues, especially in improving the alignment between the organization of the 20th-century university and the intellectual requirements of the field.",fullPaper,jv97
p350,ac8db14cbc7ad0119d0130e88f98ccb3ec61780f,c88,Symposium on the Theory of Computing,"Big Data, Digital Media, and Computational Social Science",forecasts and misrepresent,poster,cp88
p351,ff1068a7e2acaa41fae2a8e1b180264434f06ce8,j97,Science,Liberating field science samples and data,"Promote reproducibility by moving beyond “available upon request” Transparency and reproducibility enhance the integrity of research results for scientific and public uses and empower novel research applications. Access to data, samples, methods, and reagents used to conduct research and analysis, as well as to the code used to analyze and process data and samples, is a fundamental requirement for transparency and reproducibility. The field sciences (e.g., geology, ecology, and archaeology), where each study is temporally (and often spatially) unique, provide exemplars for the importance of preserving data and samples for further analysis. Yet field sciences, if they even address such access, commonly do so by simply noting “data and samples available upon request.” They lag behind some laboratory sciences in making data and samples available to the broader research community. It is time for this to change. We discuss cultural, financial, and technical barriers to change and ways in which funders, publishers, scientific societies, and others are responding.",fullPaper,jv97
p352,b7d5dda24d0c540929cd58b0226eac8a85e9769b,j113,Review of Economics and Statistics,Consistent Covariance Matrix Estimation with Spatially Dependent Panel Data,"Many panel data sets encountered in macroeconomics, international economics, regional science, and finance are characterized by cross-sectional or spatial dependence. Standard techniques that fail to account for this dependence will result in inconsistently estimated standard errors. In this paper we present conditions under which a simple extension of common nonparametric covariance matrix estimation techniques yields standard error estimates that are robust to very general forms of spatial and temporal dependence as the time dimension becomes large. We illustrate the relevance of this approach using Monte Carlo simulations and a number of empirical examples.",fullPaper,jv113
p353,f66a006bb6535784618c86e9c26b235df1e4d6d3,j114,EMBO Reports,Could Big Data be the end of theory in science?,"Afew years ago, Chris Anderson, former editor in chief of Wired magazine, published a provocative and thought‐provoking article: “The end of theory: the data deluge makes the scientific method obsolete” (http://archive.wired.com/science/discoveries/magazine/16-07/pb_theory/). As the title indicates, Anderson asserted that in the era of petabyte information and supercomputing, the traditional, hypothesis‐driven scientific method would become obsolete. No more theories or hypotheses, no more discussions whether the experimental results refute or support the original hypotheses. In this new era, what counts are sophisticated algorithms and statistical tools to sift through a massive amount of data to find information that could be turned into knowledge.

> … [an] imagined future in which the long‐established way of doing scientific research is replaced by computers that divulge knowledge from data at the press of a button…

Anderson's essay started an intense discussion about the relative merits of data‐driven research versus hypothesis‐driven research that has much relevance for many areas of research, including bioinformatics, systems biology, epidemiology and ecology. Yet, his imagined future in which the long‐established way of doing scientific research is replaced by computers that divulge knowledge from data at the press of a button deserves some inquiry from an epistemological point of view. Is data‐driven research a genuine mode of knowledge production, or is it above all a tool to identify potentially useful information? Given the amount of scientific data available, is it now possible to dismiss the role of theoretical assumptions and hypotheses? Should this new mode of gathering information supersede the old way of doing research?

The scientific method encompasses an ongoing process of formulate a hypothesis‐test with an experiment–analyze the results‐reformulate the hypothesis. Such a way of proceeding has been in use for centuries and is basically accepted in our Western society as the most reliable way to produce robust knowledge.

However, Anderson is not the …",fullPaper,jv114
p354,b016670e4e0986a85cc70a625e8627a80217a0a7,j1,IEEE Transactions on Knowledge and Data Engineering,Data mining with big data,"Big Data concern large-volume, complex, growing data sets with multiple, autonomous sources. With the fast development of networking, data storage, and the data collection capacity, Big Data are now rapidly expanding in all science and engineering domains, including physical, biological and biomedical sciences. This paper presents a HACE theorem that characterizes the features of the Big Data revolution, and proposes a Big Data processing model, from the data mining perspective. This data-driven model involves demand-driven aggregation of information sources, mining and analysis, user interest modeling, and security and privacy considerations. We analyze the challenging issues in the data-driven model and also in the Big Data revolution.",fullPaper,jv1
p355,fe5bb5d8d6b7ac251d87bc16e75ea5889cc92425,j115,Political Science Research and Methods,Explaining Fixed Effects: Random Effects Modeling of Time-Series Cross-Sectional and Panel Data*,"This article challenges Fixed Effects (FE) modeling as the ‘default’ for time-series-cross-sectional and panel data. Understanding different within and between effects is crucial when choosing modeling strategies. The downside of Random Effects (RE) modeling—correlated lower-level covariates and higher-level residuals—is omitted-variable bias, solvable with Mundlak's (1978a) formulation. Consequently, RE can provide everything that FE promises and more, as confirmed by Monte-Carlo simulations, which additionally show problems with Plümper and Troeger's FE Vector Decomposition method when data are unbalanced. As well as incorporating time-invariant variables, RE models are readily extendable, with random coefficients, cross-level interactions and complex variance functions. We argue not simply for technical solutions to endogeneity, but for the substantive importance of context/heterogeneity, modeled using RE. The implications extend beyond political science to all multilevel datasets. However, omitted variables could still bias estimated higher-level variable effects; as with any model, care is required in interpretation.",fullPaper,jv115
p356,fb7538d8c38a197146a9bafbf4fecc1f94498f5c,c71,IEEE International Conference on Information Reuse and Integration,"Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython","Get complete instructions for manipulating, processing, cleaning, and crunching datasets in Python. Updated for Python 3.6, the second edition of this hands-on guide is packed with practical case studies that show you how to solve a broad set of data analysis problems effectively. Youll learn the latest versions of pandas, NumPy, IPython, and Jupyter in the process. Written by Wes McKinney, the creator of the Python pandas project, this book is a practical, modern introduction to data science tools in Python. Its ideal for analysts new to Python and for Python programmers new to data science and scientific computing. Data files and related material are available on GitHub. Use the IPython shell and Jupyter notebook for exploratory computing Learn basic and advanced features in NumPy (Numerical Python)Get started with data analysis tools in the pandas library Use flexible tools to load, clean, transform, merge, and reshape data Create informative visualizations with matplotlib Apply the pandas groupby facility to slice, dice, and summarize datasets Analyze and manipulate regular and irregular time series dataLearn how to solve real-world data analysis problems with thorough, detailed examples",poster,cp71
p357,b598b8dd79654dc865b02c2af0a0bdb565d24049,j116,Ambio,Taking a ‘Big Data’ approach to data quality in a citizen science project,Abstract content,fullPaper,jv116
p358,9e18c191022aa0f512f5be57fac6c9350a2acf08,j97,Science,Possible artifacts of data biases in the recent global surface warming hiatus,"Walking back talk of the end of warming Previous analyses of global temperature trends during the first decade of the 21st century seemed to indicate that warming had stalled. This allowed critics of the idea of global warming to claim that concern about climate change was misplaced. Karl et al. now show that temperatures did not plateau as thought and that the supposed warming “hiatus” is just an artifact of earlier analyses. Warming has continued at a pace similar to that of the last half of the 20th century, and the slowdown was just an illusion. Science, this issue p. 1469 Updated global surface temperature data do not support the notion of a global warming “hiatus.” Much study has been devoted to the possible causes of an apparent decrease in the upward trend of global surface temperatures since 1998, a phenomenon that has been dubbed the global warming “hiatus.” Here, we present an updated global surface temperature analysis that reveals that global trends are higher than those reported by the Intergovernmental Panel on Climate Change, especially in recent decades, and that the central estimate for the rate of warming during the first 15 years of the 21st century is at least as great as the last half of the 20th century. These results do not support the notion of a “slowdown” in the increase of global surface temperature.",fullPaper,jv97
p359,bf96377353bf9daa8dc0e98eee17335f54cbcc60,j66,Data Science Journal,Data science as an academic discipline,"I recall being a proud young academic about 1970; I had just received a research grant to build and study a scientific database, and I had joined CODATA. I was looking forward to the future in this new exciting discipline when the head of my department, an internationally known professor, advised me that data was “a low level activity” not suitable for an academic. I recall my dismay. What can we do to ensure that this does not happen again and that data science is universally recognized as a worthwhile academic activity? Incidentally, I did not take that advice, or I would not be writing this essay, but moved into computer science. I will use my experience to draw comparisons between the problems computer science had to become academically recognized and those faced by data science.",fullPaper,jv66
p360,8e600778160ff986b5460bc2584066148e55e5d4,j97,Science,Protein structure determination using metagenome sequence data,"Filling in the protein fold picture Fewer than a third of the 14,849 known protein families have at least one member with an experimentally determined structure. This leaves more than 5000 protein families with no structural information. Protein modeling using residue-residue contacts inferred from evolutionary data has been successful in modeling unknown structures, but it requires large numbers of aligned sequences. Ovchinnikov et al. augmented such sequence alignments with metagenome sequence data (see the Perspective by Söding). They determined the number of sequences required to allow modeling, developed criteria for model quality, and, where possible, improved modeling by matching predicted contacts to known structures. Their method predicted quality structural models for 614 protein families, of which about 140 represent newly discovered protein folds. Science, this issue p. 294; see also p. 248 Combining metagenome data with protein structure prediction generates models for 614 families with unknown structures. Despite decades of work by structural biologists, there are still ~5200 protein families with unknown structure outside the range of comparative modeling. We show that Rosetta structure prediction guided by residue-residue contacts inferred from evolutionary information can accurately model proteins that belong to large families and that metagenome sequence data more than triple the number of protein families with sufficient sequences for accurate modeling. We then integrate metagenome data, contact-based structure matching, and Rosetta structure calculations to generate models for 614 protein families with currently unknown structures; 206 are membrane proteins and 137 have folds not represented in the Protein Data Bank. This approach provides the representative models for large protein families originally envisioned as the goal of the Protein Structure Initiative at a fraction of the cost.",fullPaper,jv97
p361,cf9ecfbbd0095687c4cfbbbfa0546914e651b109,j117,Medicine & Science in Sports & Exercise,"Calibration of the Computer Science and Applications, Inc. accelerometer.","PURPOSE
We established accelerometer count ranges for the Computer Science and Applications, Inc. (CSA) activity monitor corresponding to commonly employed MET categories.


METHODS
Data were obtained from 50 adults (25 males, 25 females) during treadmill exercise at three different speeds (4.8, 6.4, and 9.7 km x h(-1)).


RESULTS
Activity counts and steady-state oxygen consumption were highly correlated (r = 0.88), and count ranges corresponding to light, moderate, hard, and very hard intensity levels were < or = 1951, 1952-5724, 5725-9498, > or = 9499 cnts x min(-1), respectively. A model to predict energy expenditure from activity counts and body mass was developed using data from a random sample of 35 subjects (r2 = 0.82, SEE = 1.40 kcal x min(-1)). Cross validation with data from the remaining 15 subjects revealed no significant differences between actual and predicted energy expenditure at any treadmill speed (SEE = 0.50-1.40 kcal x min(-1)).


CONCLUSIONS
These data provide a template on which patterns of activity can be classified into intensity levels using the CSA accelerometer.",fullPaper,jv117
p362,04e5f980428e1ec35429356b3e43ea611fc0e975,j118,Sociological Methods & Research,Using Twitter for Demographic and Social Science Research: Tools for Data Collection and Processing,"Despite recent and growing interest in using Twitter to examine human behavior and attitudes, there is still significant room for growth regarding the ability to leverage Twitter data for social science research. In particular, gleaning demographic information about Twitter users—a key component of much social science research—remains a challenge. This article develops an accurate and reliable data processing approach for social science researchers interested in using Twitter data to examine behaviors and attitudes, as well as the demographic characteristics of the populations expressing or engaging in them. Using information gathered from Twitter users who state an intention to not vote in the 2012 presidential election, we describe and evaluate a method for processing data to retrieve demographic information reported by users that is not encoded as text (e.g., details of images) and evaluate the reliability of these techniques. We end by assessing the challenges of this data collection strategy and discussing how large-scale social media data may benefit demographic researchers.",fullPaper,jv118
p363,e281464d9a558cc1d25084687efb75683e65d4f0,c76,International Conference on Artificial Neural Networks,Growth rates of modern science: A bibliometric analysis based on the number of publications and cited references,"Many studies (in information science) have looked at the growth of science. In this study, we reexamine the question of the growth of science. To do this we (a) use current data up to publication year 2012 and (b) analyze the data across all disciplines and also separately for the natural sciences and for the medical and health sciences. Furthermore, the data were analyzed with an advanced statistical technique—segmented regression analysis—which can identify specific segments with similar growth rates in the history of science. The study is based on two different sets of bibliometric data: (a) the number of publications held as source items in the Web of Science (WoS, Thomson Reuters) per publication year and (b) the number of cited references in the publications of the source items per cited reference year. We looked at the rate at which science has grown since the mid‐1600s. In our analysis of cited references we identified three essential growth phases in the development of science, which each led to growth rates tripling in comparison with the previous phase: from less than 1% up to the middle of the 18th century, to 2 to 3% up to the period between the two world wars, and 8 to 9% to 2010.",poster,cp76
p364,5b5332e79aefa3b913d42a434b8ddb09b31b5b2e,c57,IEEE International Conference on Engineering of Complex Computer Systems,Voronoi diagrams—a survey of a fundamental geometric data structure,"Computational geometry is concerned with the design and analysis of algorithms for geometrical problems. In addition, other more practically oriented, areas of computer science— such as computer graphics, computer-aided design, robotics, pattern recognition, and operations research—give rise to problems that inherently are geometrical. This is one reason computational geometry has attracted enormous research interest in the past decade and is a well-established area today. (For standard sources, we refer to the survey article by Lee and Preparata [19841 and to the textbooks by Preparata and Shames [1985] and Edelsbrunner [1987bl.) Readers familiar with the literature of computational geometry will have noticed, especially in the last few years, an increasing interest in a geometrical construct called the Voronoi diagram. This trend can also be observed in combinatorial geometry and in a considerable number of articles in natural science journals that address the Voronoi diagram under different names specific to the respective area. Given some number of points in the plane, their Voronoi diagram divides the plane according to the nearest-neighbor",poster,cp57
p365,8cd71d704f9d3eeb5eb697e412ba54b680f00636,j119,JMIR Medical Informatics,Big Data and Clinicians: A Review on the State of the Science,"Background In the past few decades, medically related data collection saw a huge increase, referred to as big data. These huge datasets bring challenges in storage, processing, and analysis. In clinical medicine, big data is expected to play an important role in identifying causality of patient symptoms, in predicting hazards of disease incidence or reoccurrence, and in improving primary-care quality. Objective The objective of this review was to provide an overview of the features of clinical big data, describe a few commonly employed computational algorithms, statistical methods, and software toolkits for data manipulation and analysis, and discuss the challenges and limitations in this realm. Methods We conducted a literature review to identify studies on big data in medicine, especially clinical medicine. We used different combinations of keywords to search PubMed, Science Direct, Web of Knowledge, and Google Scholar for literature of interest from the past 10 years. Results This paper reviewed studies that analyzed clinical big data and discussed issues related to storage and analysis of this type of data. Conclusions Big data is becoming a common feature of biological and clinical studies. Researchers who use clinical big data face multiple challenges, and the data itself has limitations. It is imperative that methodologies for data analysis keep pace with our ability to collect and store data.",fullPaper,jv119
p366,ff7a79011e4ddba98474efe776432ac2b4431473,c66,Annual Conference on Innovation and Technology in Computer Science Education,Citizen science and the United Nations Sustainable Development Goals,Abstract content,poster,cp66
p367,09ee0ba924ffd21fc7e14ad3147284133cf2f576,c11,Hawaii International Conference on System Sciences,"Color Science, Concepts and Methods. Quantitative Data and Formulas","G. Wyszecki and W. S. Stiles London: John Wiley. 1967. Pp. xiv + 628. Price £11. This remarkable and unusual book is by two outstanding authorities on the science of colour: Dr. Stiles, for many years a senior member of the Light Division at the National Physical Laboratory, and Dr. Wyszecki, currently in charge of the Radiation Optics Section of the Canadian National Research Council. The authors' aim has been to provide a comprehensive source book of data required by the practical and theoretical worker in the field of colour and they have achieved this aim so successfully that their book is likely to become the standard work on the subject and to remain so for a good many years.",poster,cp11
p368,a07a64ba110e0f9f7156f3bd1e376f0d2e1cddf1,j60,PLoS Biology,The Extent and Consequences of P-Hacking in Science,"A focus on novel, confirmatory, and statistically significant results leads to substantial bias in the scientific literature. One type of bias, known as “p-hacking,” occurs when researchers collect or select data or statistical analyses until nonsignificant results become significant. Here, we use text-mining to demonstrate that p-hacking is widespread throughout science. We then illustrate how one can test for p-hacking when performing a meta-analysis and show that, while p-hacking is probably common, its effect seems to be weak relative to the real effect sizes being measured. This result suggests that p-hacking probably does not drastically alter scientific consensuses drawn from meta-analyses.",fullPaper,jv60
p369,c8bad3f510224e5cb010ca422149bf6ebcaa1d7f,c48,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",Impact of data sources on citation counts and rankings of LIS faculty: Web of science versus scopus and google scholar,"The Institute for Scientific Information's (ISI, now Thomson Scientific, Philadelphia, PA) citation databases have been used for decades as a starting point and often as the only tools for locating citations andsor conducting citation analyses. The ISI databases (or Web of Science [WoS]), however, may no longer be sufficient because new databases and tools that allow citation searching are now available. Using citations to the work of 25 library and information science (LIS) faculty members as a case study, the authors examine the effects of using Scopus and Google Scholar (GS) on the citation counts and rankings of scholars as measured by WoS. Overall, more than 10,000 citing and purportedly citing documents were examined. Results show that Scopus significantly alters the relative ranking of those scholars that appear in the middle of the rankings and that GS stands out in its coverage of conference proceedings as well as international, non-English language journals. The use of Scopus and GS, in addition to WoS, helps reveal a more accurate and comprehensive picture of the scholarly impact of authors. The WoS data took about 100 hours of collecting and processing time, Scopus consumed 200 hours, and GS a grueling 3,000 hours. © 2007 Wiley Periodicals, Inc.",poster,cp48
p370,3dcb7cab70222f78d790da8c54bde2b9043c3e5b,j120,Brain Imaging and Behavior,Human neuroimaging as a “Big Data” science,Abstract content,fullPaper,jv120
p371,edacaedb1b2312023c4b0cf1d42bbdbed2793c65,c43,ACM Symposium on Applied Computing,The Electric and Magnetic Field Instrument Suite and Integrated Science (EMFISIS) on RBSP,Abstract content,poster,cp43
p372,023eb29b711014b1a3d2895e19a0fc2aed7a6ab4,c96,USENIX Symposium on Operating Systems Design and Implementation,Data Science and Classification,Abstract content,poster,cp96
p373,e27adaaa84b7ed71990098bcff9971ae32d44c09,c76,International Conference on Artificial Neural Networks,The inevitable application of big data to health care.,"THE AMOUNT OF DATA BEING DIGITALLY COLLECTED AND stored is vast and expanding rapidly. As a result, the science of data management and analysis is also advancing to enable organizations to convert this vast resource into information and knowledge that helps them achieve their objectives. Computer scientists have invented the term big data to describe this evolving technology. Big data has been successfully used in astronomy (eg, the Sloan Digital Sky Survey of telescopic information), retail sales (eg, Walmart’s expansive number of transactions), search engines (eg, Google’s customization of individual searches based on previous web data), and politics (eg, a campaign’s focus of political advertisements on people most likely to support their candidate based on web searches). In this Viewpoint, we discuss the application of big data to health care, using an economic framework to highlight the opportunities it will offer and the roadblocks to implementation. We suggest that leveraging the collection of patient and practitioner data could be an important way to improve quality and efficiency of health care delivery. Widespread uptake of electronic health records (EHRs) has generated massive data sets. A survey by the American Hospital Association showed that adoption of EHRs has doubled from 2009 to 2011, partly a result of funding provided by the Health Information Technology for Economic and Clinical Health Act of 2009. Most EHRs now contain quantitative data (eg, laboratory values), qualitative data (eg, text-based documents and demographics), and transactional data (eg, a record of medication delivery). However, much of this rich data set is currently perceived as a byproduct of health care delivery, rather than a central asset to improve its efficiency. The transition of data from refuse to riches has been key in the big data revolution of other industries. Advances in analytic techniques in the computer sciences, especially in machine learning, have been a major catalyst for dealing with these large information sets. These analytic techniques are in contrast to traditional statistical methods (derived from the social and physical sciences), which are largely not useful for analysis of unstructured data such as text-based documents that do not fit into relational tables. One estimate suggests that 80% of business-related data exist in an unstructured format. The same could probably be said for health care data, a large proportion of which is text-based. In contrast to most consumer service industries, medicine adopted a practice of generating evidence from experimental (randomized trials) and quasi-experimental studies to inform patients and clinicians. The evidence-based movement is founded on the belief that scientific inquiry is superior to expert opinion and testimonials. In this way, medicine was ahead of many other industries in terms of recognizing the value of data and information guiding rational decision making. However, health care has lagged in uptake of newer techniques to leverage the rich information contained in EHRs. There are 4 ways big data may advance the economic mission of health care delivery by improving quality and efficiency. First, big data may greatly expand the capacity to generate new knowledge. The cost of answering many clinical questions prospectively, and even retrospectively, by collecting structured data is prohibitive. Analyzing the unstructured data contained within EHRs using computational techniques (eg, natural language processing to extract medical concepts from free-text documents) permits finer data acquisition in an automated fashion. For instance, automated identification within EHRs using natural language processing was superior in detecting postoperative complications compared with patient safety indicators based on discharge coding. Big data offers the potential to create an observational evidence base for clinical questions that would otherwise not be possible and may be especially helpful with issues of generalizability. The latter issue limits the application of conclusions derived from randomized trials performed on a narrow spectrum of participants to patients who exhibit very different characteristics. Second, big data may help with knowledge dissemination. Most physicians struggle to stay current with the latest evidence guiding clinical practice. The digitization of medical literature has greatly improved access; however, the sheer",poster,cp76
p374,a96e508a94c37ef847b172e7d31b5bbe25629cbb,c49,International Symposium on Search Based Software Engineering,"Data Mining: Concepts and Techniques, 3rd edition","The book Knowledge Discovery in Databases, edited by Piatetsky-Shapiro and Frawley [PSF91], is an early collection of research papers on knowledge discovery from data. The book Advances in Knowledge Discovery and Data Mining, edited by Fayyad, Piatetsky-Shapiro, Smyth, and Uthurusamy [FPSSe96], is a collection of later research results on knowledge discovery and data mining. There have been many data mining books published in recent years, including Predictive Data Mining by Weiss and Indurkhya [WI98], Data Mining Solutions: Methods and Tools for Solving Real-World Problems by Westphal and Blaxton [WB98], Mastering Data Mining: The Art and Science of Customer Relationship Management by Berry and Linofi [BL99], Building Data Mining Applications for CRM by Berson, Smith, and Thearling [BST99], Data Mining: Practical Machine Learning Tools and Techniques by Witten and Frank [WF05], Principles of Data Mining (Adaptive Computation and Machine Learning) by Hand, Mannila, and Smyth [HMS01], The Elements of Statistical Learning by Hastie, Tibshirani, and Friedman [HTF01], Data Mining: Introductory and Advanced Topics by Dunham, and Data Mining: Multimedia, Soft Computing, and Bioinformatics by Mitra and Acharya [MA03]. There are also books containing collections of papers on particular aspects of knowledge discovery, such as Machine Learning and Data Mining: Methods and Applications edited by Michalski, Brakto, and Kubat [MBK98], and Relational Data Mining edited by Dzeroski and Lavrac [De01], as well as many tutorial notes on data mining in major database, data mining and machine learning conferences.",poster,cp49
p375,952241d28abed7d221fc059845043a6463a522bc,c104,IEEE International Conference on Multimedia and Expo,Qualitative Descriptive Methods in Health Science Research,"Objective: The purpose of this methodology paper is to describe an approach to qualitative design known as qualitative descriptive that is well suited to junior health sciences researchers because it can be used with a variety of theoretical approaches, sampling techniques, and data collection strategies. Background: It is often difficult for junior qualitative researchers to pull together the tools and resources they need to embark on a high-quality qualitative research study and to manage the volumes of data they collect during qualitative studies. This paper seeks to pull together much needed resources and provide an overview of methods. Methods: A step-by-step guide to planning a qualitative descriptive study and analyzing the data is provided, utilizing exemplars from the authors’ research. Results: This paper presents steps to conducting a qualitative descriptive study under the following headings: describing the qualitative descriptive approach, designing a qualitative descriptive study, steps to data analysis, and ensuring rigor of findings. Conclusions: The qualitative descriptive approach results in a summary in everyday, factual language that facilitates understanding of a selected phenomenon across disciplines of health science researchers.",poster,cp104
p376,e048f6fdb0a728638af5d8684a32b3dc2ee83259,j121,Big Data Research,Big Data and Science: Myths and Reality,Abstract content,fullPaper,jv121
p377,3caf430acf63f9610ccfe5165666077167542cd3,c35,EUROMICRO Conference on Software Engineering and Advanced Applications,The Science DMZ: A network design pattern for data-intensive science,"The ever-increasing scale of scientific data has become a significant challenge for researchers that rely on networks to interact with remote computing systems and transfer results to collaborators worldwide. Despite the availability of high-capacity connections, scientists struggle with inadequate cyberinfrastructure that cripples data transfer performance, and impedes scientific progress. The Science DMZ paradigm comprises a proven set of network design patterns that collectively address these problems for scientists. We explain the Science DMZ model, including network architecture, system configuration, cybersecurity, and performance tools, that creates an optimized network environment for science. We describe use cases from universities, supercomputing centers and research laboratories, highlighting the effectiveness of the Science DMZ model in diverse operational settings. In all, the Science DMZ model is a solid platform that supports any science workflow, and flexibly accommodates emerging network technologies. As a result, the Science DMZ vastly improves collaboration, accelerating scientific discovery.",poster,cp35
p378,c278f3e91bf11c72be6808972f00810f15d877a4,j122,Sustainability Science,Mapping citizen science contributions to the UN sustainable development goals,Abstract content,fullPaper,jv122
p379,8958efba7a02e3653f27c0e759882b2f3352e896,j14,Scientific Data,"Materials Cloud, a platform for open computational science",Abstract content,fullPaper,jv14
p380,14b417f0d88e0d26a1c26a4d616d1093b616c6d0,j123,EURASIP Journal on Advances in Signal Processing,A survey of machine learning for big data processing,Abstract content,fullPaper,jv123
p381,500b73ecdf8ff5590718edb03367e3836a368485,c46,Brazilian Symposium on Software Engineering,Secondary Data Analysis: A Method of which the Time Has Come,"Technological advances have led to vast amounts of data that has been collected, compiled, and archived, and that is now easily accessible for research. As a result, utilizing existing data for research is becoming more prevalent, and therefore secondary data analysis. While secondary analysis is flexible and can be utilized in several ways, it is also an empirical exercise and a systematic method with procedural and evaluative steps, just as in collecting and evaluating primary data. This paper asserts that secondary data analysis is a viable method to utilize in the process of inquiry when a systematic procedure is followed and presents an illustrative research application utilizing secondary data analysis in library and information science research.",poster,cp46
p382,d62126bfe0e1b299c9383bb30ee099c77aee5222,c90,Computer Vision and Pattern Recognition,"Interpreting Qualitative Data: Methods for Analysing Talk, Text and Interaction","This a much expanded and updated version of David Silvermans best-selling introductory textbook for the beginning qualitative researcher. 
 
Features of the New Edition: 
• Takes account of the flood of qualitative work since the 1990s 
• All chapters have been substantially rewritten with the aim of greater clarity 
• A new chapter on Visual Images and a considerably expanded treatment of discourse analysis are provided 
• The number of student exercises has been considerably increased and are now present at the end of every chapter 
• An even greater degree of student accessibility: Key Points and Recommended Readings appear at the end of each chapter and technical terms are highlighted and appear in a Glossary 
• A more inter-disciplinary social science text which takes account of the growing interest in qualitative research outside sociology and anthropology from psychology to geography, information systems, health promotion, management and many other disciplines 
• Expanded coverage 50% longer than the First Edition 
This book has a more recent edition (2006)",poster,cp90
p383,1842a5fb9739149dadba962c94dd7243a5f62242,c14,International Conference on Exploring Services Science,What is Data Science ? Fundamental Concepts and a Heuristic Example,Abstract content,poster,cp14
p384,cebed64039064dfe950587b919ddc01dee7d871f,c4,Annual Conference on Genetic and Evolutionary Computation,From Little Science to Big Science,"In Little Science, Big Science (1963), Derek J. de Solla Price undertook a sociology of science that dealt with the growth and changing shape of scientific publishing and the social organization of science. The focus of Price’s work was on the long-term, gradual shift from “little science,” with the solo scientist, small laboratory, and minimal research funds, to “big science,” with collaborative research teams, large-scale research hardware, extensive funding, and corporate-political suitors of scientists. We extend Price’s focus on scientific publications by moving beyond his analysis of practices in physics and chemistry to examine a social science; namely, sociology. Specifically, we analyze 3,000 articles in four long-standing sociology journals over the fifty-year period from 1960-2010 to determine the gender of authors, the prestige of authors’ departments, length of articles, number of references, sources of data for studies, and patterns of funding for research. We find that sociology is not immune from the shift from “little science” to “big science.”",poster,cp4
p385,43789305e5d2212da05f9c16b148e84aae5614b2,c111,International Society for Music Information Retrieval Conference,Citizen Science and Volunteered Geographic Information: Overview and Typology of Participation,Abstract content,poster,cp111
p386,06ba782753bad19254db5d28ad4155556f286ee0,c48,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",Data Management and Analysis Methods,"This chapter is about methods for managing and analyzing qualitative data. By qualitative data the authors mean text: newspapers, movies, sitcoms, e-mail traffic, folktales, life histories. They also mean narratives--narratives about getting divorced, about being sick, about surviving hand-to-hand combat, about selling sex, about trying to quit smoking. In fact, most of the archaeologically recoverable information about human thought and human behavior is text, the good stuff of social science.",poster,cp48
p387,43d75d3a22db904d052d4c435e2d1f22be3887e0,j1,IEEE Transactions on Knowledge and Data Engineering,Outlier Detection for Temporal Data: A Survey,"In the statistics community, outlier detection for time series data has been studied for decades. Recently, with advances in hardware and software technology, there has been a large body of work on temporal outlier detection from a computational perspective within the computer science community. In particular, advances in hardware technology have enabled the availability of various forms of temporal data collection mechanisms, and advances in software technology have enabled a variety of data management mechanisms. This has fueled the growth of different kinds of data sets such as data streams, spatio-temporal data, distributed streams, temporal networks, and time series data, generated by a multitude of applications. There arises a need for an organized and detailed study of the work done in the area of outlier detection with respect to such temporal datasets. In this survey, we provide a comprehensive and structured overview of a large set of interesting outlier definitions for various forms of temporal data, novel techniques, and application scenarios in which specific definitions and techniques have been widely used.",fullPaper,jv1
p388,3479f3d2f1116effd205940b398749c3e1a2a21c,c88,Symposium on the Theory of Computing,Data Streams - Models and Algorithms,Abstract content,poster,cp88
p389,ecb81c5d18e38b29316da77f69c8a36d5b98f196,j6,Nature Methods,scmap: projection of single-cell RNA-seq data across data sets,Abstract content,fullPaper,jv6
p390,3859aef8d52ef1bad6351ec25c4fe4009b184689,c57,IEEE International Conference on Engineering of Complex Computer Systems,Characterization of the LIGO detectors during their sixth science run,"In 2009–2010, the Laser Interferometer Gravitational-Wave Observatory (LIGO) operated together with international partners Virgo and GEO600 as a network to search for gravitational waves (GWs) of astrophysical origin. The sensitivity of these detectors was limited by a combination of noise sources inherent to the instrumental design and its environment, often localized in time or frequency, that couple into the GW readout. Here we review the performance of the LIGO instruments during this epoch, the work done to characterize the detectors and their data, and the effect that transient and continuous noise artefacts have on the sensitivity of LIGO to a variety of astrophysical sources.",poster,cp57
p391,fd40e458a67f9a3854834fd42b66b0d6ed43ab8d,c68,International Workshop on Software Engineering for Large-Scale Multi-Agent Systems,Educational Data Mining and Learning Analytics,Abstract content,poster,cp68
p392,34ad09cda075101dc4ce3c04006ff804aca3ebf8,c25,International Conference on Contemporary Computing,"Big data: Issues, challenges, tools and Good practices","Big data is defined as large amount of data which requires new technologies and architectures so that it becomes possible to extract value from it by capturing and analysis process. Due to such large size of data it becomes very difficult to perform effective analysis using the existing traditional techniques. Big data due to its various properties like volume, velocity, variety, variability, value and complexity put forward many challenges. Since Big data is a recent upcoming technology in the market which can bring huge benefits to the business organizations, it becomes necessary that various challenges and issues associated in bringing and adapting to this technology are brought into light. This paper introduces the Big data technology along with its importance in the modern world and existing projects which are effective and important in changing the concept of science into big science and society too. The various challenges and issues in adapting and accepting Big data technology, its tools (Hadoop) are also discussed in detail along with the problems Hadoop is facing. The paper concludes with the Good Big data practices to be followed.",fullPaper,cp25
p393,d65d64c3f6ea322d9e85138fe5c8e85acbf661e3,c63,IEEE International Software Metrics Symposium,A Bibliometric Analysis and Visualization of Medical Big Data Research,"With the rapid development of “Internet plus”, medical care has entered the era of big data. However, there is little research on medical big data (MBD) from the perspectives of bibliometrics and visualization. The substantive research on the basic aspects of MBD itself is also rare. This study aims to explore the current status of medical big data through visualization analysis on the journal papers related to MBD. We analyze a total of 988 references which were downloaded from the Science Citation Index Expanded and the Social Science Citation Index databases from Web of Science and the time span was defined as “all years”. The GraphPad Prism 5, VOSviewer and CiteSpace softwares are used for analysis. Many results concerning the annual trends, the top players in terms of journal and institute levels, the citations and H-index in terms of country level, the keywords distribution, the highly cited papers, the co-authorship status and the most influential journals and authors are presented in this paper. This study points out the development status and trends on MBD. It can help people in the medical profession to get comprehensive understanding on the state of the art of MBD. It also has reference values for the research and application of the MBD visualization methods.",poster,cp63
p394,835a5484292f32a3c02f507cbd8fb1f5d9f4aacf,j124,Royal Society Open Science,The natural selection of bad science,"Poor research design and data analysis encourage false-positive findings. Such poor methods persist despite perennial calls for improvement, suggesting that they result from something more than just misunderstanding. The persistence of poor methods results partly from incentives that favour them, leading to the natural selection of bad science. This dynamic requires no conscious strategizing—no deliberate cheating nor loafing—by scientists, only that publication is a principal factor for career advancement. Some normative methods of analysis have almost certainly been selected to further publication instead of discovery. In order to improve the culture of science, a shift must be made away from correcting misunderstandings and towards rewarding understanding. We support this argument with empirical evidence and computational modelling. We first present a 60-year meta-analysis of statistical power in the behavioural sciences and show that power has not improved despite repeated demonstrations of the necessity of increasing power. To demonstrate the logical consequences of structural incentives, we then present a dynamic model of scientific communities in which competing laboratories investigate novel or previously published hypotheses using culturally transmitted research methods. As in the real world, successful labs produce more ‘progeny,’ such that their methods are more often copied and their students are more likely to start labs of their own. Selection for high output leads to poorer methods and increasingly high false discovery rates. We additionally show that replication slows but does not stop the process of methodological deterioration. Improving the quality of research requires change at the institutional level.",fullPaper,jv124
p395,c3665722a7cc81caca8c90ac3c5b0572f7bba055,j125,Public Understanding of Science,Can citizen science enhance public understanding of science?,"Over the past 20 years, thousands of citizen science projects engaging millions of participants in collecting and/or processing data have sprung up around the world. Here we review documented outcomes from four categories of citizen science projects which are defined by the nature of the activities in which their participants engage – Data Collection, Data Processing, Curriculum-based, and Community Science. We find strong evidence that scientific outcomes of citizen science are well documented, particularly for Data Collection and Data Processing projects. We find limited but growing evidence that citizen science projects achieve participant gains in knowledge about science knowledge and process, increase public awareness of the diversity of scientific research, and provide deeper meaning to participants’ hobbies. We also find some evidence that citizen science can contribute positively to social well-being by influencing the questions that are being addressed and by giving people a voice in local environmental decision making. While not all citizen science projects are intended to achieve a greater degree of public understanding of science, social change, or improved science -society relationships, those projects that do require effort and resources in four main categories: (1) project design, (2) outcomes measurement, (3) engagement of new audiences, and (4) new directions for research.",fullPaper,jv125
p396,197b30ab1460fe200dba90dc3392ad49a92c2ca4,c24,Decision Support Systems,Between Data Science and Applied Data Analysis,Abstract content,poster,cp24
p397,de5acd80c5fd8db442a4a5e5ffbc3f3f51161237,c20,Research Conference of the South African Institute of Computer Scientists and Information Technologists,"Data Science, Classification and Related Methods",Abstract content,poster,cp20
p398,f03a847c6325d7d5973efd687d2ca86a9c06dd76,c20,Research Conference of the South African Institute of Computer Scientists and Information Technologists,Advances in data science and classification,Abstract content,poster,cp20
p399,026265a2c39913f0c6337ab28b36f332caa53d7a,j11,International Journal of Digital Earth,Big Data and cloud computing: innovation opportunities and challenges,"ABSTRACT Big Data has emerged in the past few years as a new paradigm providing abundant data and opportunities to improve and/or enable research and decision-support applications with unprecedented value for digital earth applications including business, sciences and engineering. At the same time, Big Data presents challenges for digital earth to store, transport, process, mine and serve the data. Cloud computing provides fundamental support to address the challenges with shared computing resources including computing, storage, networking and analytical software; the application of these resources has fostered impressive Big Data advancements. This paper surveys the two frontiers – Big Data and cloud computing – and reviews the advantages and consequences of utilizing cloud computing to tackling Big Data in the digital earth and relevant science domains. From the aspects of a general introduction, sources, challenges, technology status and research opportunities, the following observations are offered: (i) cloud computing and Big Data enable science discoveries and application developments; (ii) cloud computing provides major solutions for Big Data; (iii) Big Data, spatiotemporal thinking and various application domains drive the advancement of cloud computing and relevant technologies with new requirements; (iv) intrinsic spatiotemporal principles of Big Data and geospatial sciences provide the source for finding technical and theoretical solutions to optimize cloud computing and processing Big Data; (v) open availability of Big Data and processing capability pose social challenges of geospatial significance and (vi) a weave of innovations is transforming Big Data into geospatial research, engineering and business values. This review introduces future innovations and a research agenda for cloud computing supporting the transformation of the volume, velocity, variety and veracity into values of Big Data for local to global digital earth science and applications.",fullPaper,jv11
p400,d33d879ea94fd36363dc7f015896ac6c0236acac,c92,Advances in Soft Computing,Data Preprocessing in Data Mining,Abstract content,poster,cp92
p401,937287b0c249add46f61718c170542294b226ccf,c95,IEEE International Conference on Computer Vision,Astronomical Data Analysis Software and Systems XV,"Until 23 Nov 2003, no total solar eclipse (TSE) had ever been observed from the Antarctic. Yet, interest in securing observations of that event, visible only from the Antarctic, was extremely high and provided the impetus for breaking that paradigm of elusivity in the historical record of science and exploration. The execution of a lunar shadow intercept and the conduction of an observing program from a Boeing 747-400 ER aircraft over the Antarctic interior permitted the previously unobtainable to be accomplished. The unique computational and navigational requirements for this flight are discussed from the enabling perspective of control and data acquisition S/W specifically developed for this task.",poster,cp95
p402,5952a9f10ef65983042794369d376e23d2682d7e,c26,PS,Openness in Political Science: Data Access and Research Transparency,"In 2012, the American Political Science Association (APSA) Council adopted new policies guiding data access and research transparency in political science. The policies appear as a revision to APSA's Guide to Professional Ethics in Political Science. The revisions were the product of an extended and broad consultation with a variety of APSA committees and the association's membership.",fullPaper,cp26
p403,a3324c0dcb1efaf5d88003b3fe22a3351b4c16da,c19,ACM Conference on Economics and Computation,"""Big Data"" : big gaps of knowledge in the field of internet science","Research on so-called ‘Big Data’ has received a considerable momentum and is expected to grow in the future. One very interesting stream of research on Big Data analyzes online networks. Many online networks are known to have some typical macro-characteristics, such as ‘small world’ properties. Much less is known about underlying micro-processes leading to these properties. The models used by Big Data researchers usually are inspired by mathematical ease of exposition. We propose to follow in addition a different strategy that leads to knowledge about micro-processes that match with actual online behavior. This knowledge can then be used for the selection of mathematically-tractable models of online network formation and evolution. Insight from social and behavioral research is needed for pursuing this strategy of knowledge generation about micro-processes. Accordingly, our proposal points to a unique role that social scientists could play in Big Data research.",poster,cp19
p404,d43e2d9b90c0f509c9f569b9d4bd431ebd711f4f,j126,Psychology Science,Sharing Data and Materials in Psychological Science,Abstract content,fullPaper,jv126
p405,36d206bb822865f725a18cdc2cfae17fb8775da7,c60,IEEE International Conference on Software Engineering and Formal Methods,Next Generation Science Standards,"Science and Engineering Practices that connect to garden-based education (all 8): • Asking questions (for science) and defining problems (for engineering) • Developing and using models • Planning and carrying out investigations • Analyzing and interpreting data • Using mathematics and computational thinking • Constructing explanations (for science) and designing solutions (for engineering) • Engaging in argument from evidence • Obtaining, evaluating, and communicating information",poster,cp60
p406,1a46465ab69ec13d3c84d66166e979989afa596d,j97,Science,Comment on “Estimating the reproducibility of psychological science”,"A paper from the Open Science Collaboration (Research Articles, 28 August 2015, aac4716) attempting to replicate 100 published studies suggests that the reproducibility of psychological science is surprisingly low. We show that this article contains three statistical errors and provides no support for such a conclusion. Indeed, the data are consistent with the opposite conclusion, namely, that the reproducibility of psychological science is quite high.",fullPaper,jv97
p407,7a1b9cc42e6fc611970b451fbef795e72cbea46d,j127,Trends in Ecology & Evolution,Ecoinformatics: supporting ecology as a data-intensive science.,Abstract content,fullPaper,jv127
p408,57f6a3f6506ed06980966e1af0fe039d9f1c124f,j8,Journal of Big Data,Big healthcare data: preserving security and privacy,Abstract content,fullPaper,jv8
p409,817ef07fa5c0c6328a18d25e78660157fe4759d2,c22,International Conference on Data Technologies and Applications,Open Science - Practical Issues in Open Research Data,"The term âopen dataâ refers to information that has been made technically and legally available for reuse. In our research, we focus on the particular case of open research data. We conducted a literature review in order to determine what are the motivations to release open research data and what are the issues related to the development of open research data. Our research allowed to identify seven motivations for researchers to open research data and discuss seven issues. The paper highlights the lack of data infrastructure dedicated to open research data and the need for developing the researcherâs ability to animate online communities.",fullPaper,cp22
p410,86b05bc7e953e683fa839ad01d6100a8f99558df,c18,Conference on Innovative Data Systems Research,Concrete mathematics - a foundation for computer science,"From the Publisher: 
This book introduces the mathematics that supports advanced computer programming and the analysis of algorithms. The primary aim of its well-known authors is to provide a solid and relevant base of mathematical skills - the skills needed to solve complex problems, to evaluate horrendous sums, and to discover subtle patterns in data. It is an indispensable text and reference not only for computer scientists - the authors themselves rely heavily on it! - but for serious users of mathematics in virtually every discipline. 
 
Concrete Mathematics is a blending of CONtinuous and disCRETE mathematics. ""More concretely,"" the authors explain, ""it is the controlled manipulation of mathematical formulas, using a collection of techniques for solving problems."" The subject matter is primarily an expansion of the Mathematical Preliminaries section in Knuth's classic Art of Computer Programming, but the style of presentation is more leisurely, and individual topics are covered more deeply. Several new topics have been added, and the most significant ideas have been traced to their historical roots. The book includes more than 500 exercises, divided into six categories. Complete answers are provided for all exercises, except research problems, making the book particularly valuable for self-study. 
 
Major topics include: 
 
Sums 
Recurrences 
Integer functions 
Elementary number theory 
Binomial coefficients 
Generating functions 
Discrete probability 
Asymptotic methods 
 
 
This second edition includes important new material about mechanical summation. In response to the widespread use ofthe first edition as a reference book, the bibliography and index have also been expanded, and additional nontrivial improvements can be found on almost every page. Readers will appreciate the informal style of Concrete Mathematics. Particularly enjoyable are the marginal graffiti contributed by students who have taken courses based on this material. The authors want to convey not only the importance of the techniques presented, but some of the fun in learning and using them.",poster,cp18
p411,a4b603ca6aaaa18968e08ac1b0ee093db8a99a6b,c75,International Conference on Machine Learning,Topology and data,"An important feature of modern science and engineering is that data of various kinds is being produced at an unprecedented rate. This is so in part because of new experimental methods, and in part because of the increase in the availability of high powered computing technology. It is also clear that the nature of the data we are obtaining is significantly different. For example, it is now often the case that we are given data in the form of very long vectors, where all but a few of the coordinates turn out to be irrelevant to the questions of interest, and further that we don’t necessarily know which coordinates are the interesting ones. A related fact is that the data is often very high-dimensional, which severely restricts our ability to visualize it. The data obtained is also often much noisier than in the past and has more missing information (missing data). This is particularly so in the case of biological data, particularly high throughput data from microarray or other sources. Our ability to analyze this data, both in terms of quantity and the nature of the data, is clearly not keeping pace with the data being produced. In this paper, we will discuss how geometry and topology can be applied to make useful contributions to the analysis of various kinds of data. Geometry and topology are very natural tools to apply in this direction, since geometry can be regarded as the study of distance functions, and what one often works with are distance functions on large finite sets of data. The mathematical formalism which has been developed for incorporating geometric and topological techniques deals with point clouds, i.e. finite sets of points equipped with a distance function. It then adapts tools from the various branches of geometry to the study of point clouds. The point clouds are intended to be thought of as finite samples taken from a geometric object, perhaps with noise. Here are some of the key points which come up when applying these geometric methods to data analysis. • Qualitative information is needed: One important goal of data analysis is to allow the user to obtain knowledge about the data, i.e. to understand how it is organized on a large scale. For example, if we imagine that we are looking at a data set constructed somehow from diabetes patients, it would be important to develop the understanding that there are two types of the disease, namely the juvenile and adult onset forms. Once that is established, one of course wants to develop quantitative methods for distinguishing them, but the first insight about the distinct forms of the disease is key.",poster,cp75
p412,bfcf1ed94050a4c60d459cd02456dfd9f08fdb4c,c41,Software Product Lines Conference,"Statistics for experimenters : an introduction to design, data analysis, and model building","Science and Statistics. COMPARING TWO TREATMENTS. Use of External Reference Distribution to Compare Two Means. Random Sampling and the Declaration of Independence. Randomization and Blocking with Paired Comparisons. Significance Tests and Confidence Intervals for Means, Variances, Proportions and Frequences. COMPARING MORE THAN TWO TREATMENTS. Experiments to Compare k Treatment Means. Randomized Block and Two--Way Factorial Designs. Designs with More Than One Blocking Variable. MEASURING THE EFFECTS OF VARIABLES. Empirical Modeling. Factorial Designs at Two Levels. More Applications of Factorial Designs. Fractional Factorial Designs at Two Levels. More Applications of Fractional Factorial Designs. BUILDING MODELS AND USING THEM. Simple Modeling with Least Squares (Regression Analysis). Response Surface Methods. Mechanistic Model Building. Study of Variation. Modeling Dependence: Times Series. Appendix Tables. Index.",poster,cp41
p413,db8335198bd47c8865d0b3408b97e547abfd9ba2,c56,European Conference on Software Process Improvement,The Fourth Paradigm: Data-Intensive Scientific Discovery,Abstract content,poster,cp56
p414,1715fdc4df6774d95ed63f3feb58fa93a84dbed7,j127,Trends in Ecology & Evolution,Data-intensive science applied to broad-scale citizen science.,Abstract content,fullPaper,jv127
p415,8807a8327e27298fd601fc65e6a9ccfae1cca195,j108,PLoS ONE,What Is Citizen Science? – A Scientometric Meta-Analysis,"Context The concept of citizen science (CS) is currently referred to by many actors inside and outside science and research. Several descriptions of this purportedly new approach of science are often heard in connection with large datasets and the possibilities of mobilizing crowds outside science to assists with observations and classifications. However, other accounts refer to CS as a way of democratizing science, aiding concerned communities in creating data to influence policy and as a way of promoting political decision processes involving environment and health. Objective In this study we analyse two datasets (N = 1935, N = 633) retrieved from the Web of Science (WoS) with the aim of giving a scientometric description of what the concept of CS entails. We account for its development over time, and what strands of research that has adopted CS and give an assessment of what scientific output has been achieved in CS-related projects. To attain this, scientometric methods have been combined with qualitative approaches to render more precise search terms. Results Results indicate that there are three main focal points of CS. The largest is composed of research on biology, conservation and ecology, and utilizes CS mainly as a methodology of collecting and classifying data. A second strand of research has emerged through geographic information research, where citizens participate in the collection of geographic data. Thirdly, there is a line of research relating to the social sciences and epidemiology, which studies and facilitates public participation in relation to environmental issues and health. In terms of scientific output, the largest body of articles are to be found in biology and conservation research. In absolute numbers, the amount of publications generated by CS is low (N = 1935), but over the past decade a new and very productive line of CS based on digital platforms has emerged for the collection and classification of data.",fullPaper,jv108
p416,4a6377c0d1512ee5c780320828699ed19e158323,c28,International Conference on Collaboration Technologies and Systems,Big data and the future of ecology,"The need for sound ecological science has escalated alongside the rise of the information age and “big data” across all sectors of society. Big data generally refer to massive volumes of data not readily handled by the usual data tools and practices and present unprecedented opportunities for advancing science and inform- ing resource management through data-intensive approaches. The era of big data need not be propelled only by “big science” – the term used to describe large-scale efforts that have had mixed success in the individual-driven culture of ecology. Collectively, ecologists already have big data to bolster the scientific effort – a large volume of distributed, high-value information – but many simply fail to contribute. We encourage ecologists to join the larger scientific community in global initiatives to address major scientific and societal problems by bringing their distributed data to the table and harnessing its collective power. The scientists who contribute such information will be at the forefront of socially relevant science – but will they be ecologists?",poster,cp28
p417,e1ababf08c9ec103db854a2c1b4db611142cfdb7,c112,Very Large Data Bases Conference,Linear Mixed Models for Longitudinal Data,Abstract content,poster,cp112
p418,83b2bc2583862fa662cdfeb6cc7950bb2972347d,j128,Immunologic research,ImmPort: disseminating data to the public for the future of immunology,Abstract content,fullPaper,jv128
p419,6e78b1133713cb17aabbc3bf421a6e51bc538eca,c101,International Conference on Automatic Face and Gesture Recognition,Social Science in the Era of Big Data,"Digital technologies keep track of everything we do and say while we are online, and we spend online an increasing portion of our time. Databases hidden behind web services and applications are constantly fed with information of our movements and communication patterns, and a significant dimension of our lives, quantified to unprecedented levels, gets stored in those vast online repositories. This article considers some of the implications of this torrent of data for social science research, and for the types of questions we can ask of the world we inhabit. The goal of the article is twofold: to explain why, in spite of all the data, theory still matters to build credible stories of what the data reveal; and to show how this allows social scientists to revisit old questions at the intersection of new technologies and disciplinary approaches. The article also considers how Big Data research can transform policy making, with a focus on how it can help us improve communication and governance in policy-relevant domains.",poster,cp101
p420,299bab6b327e34c3e4f97cc8d0f9c64c9741fa99,c45,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",Where are human subjects in Big Data research? The emerging ethics divide,"There are growing discontinuities between the research practices of data science and established tools of research ethics regulation. Some of the core commitments of existing research ethics regulations, such as the distinction between research and practice, cannot be cleanly exported from biomedical research to data science research. Such discontinuities have led some data science practitioners and researchers to move toward rejecting ethics regulations outright. These shifts occur at the same time as a proposal for major revisions to the Common Rule—the primary regulation governing human-subjects research in the USA—is under consideration for the first time in decades. We contextualize these revisions in long-running complaints about regulation of social science research and argue data science should be understood as continuous with social sciences in this regard. The proposed regulations are more flexible and scalable to the methods of non-biomedical research, yet problematically largely exclude data science methods from human-subjects regulation, particularly uses of public datasets. The ethical frameworks for Big Data research are highly contested and in flux, and the potential harms of data science research are unpredictable. We examine several contentious cases of research harms in data science, including the 2014 Facebook emotional contagion study and the 2016 use of geographical data techniques to identify the pseudonymous artist Banksy. To address disputes about application of human-subjects research ethics in data science, critical data studies should offer a historically nuanced theory of “data subjectivity” responsive to the epistemic methods, harms and benefits of data science and commerce.",poster,cp45
p421,8801ce73bea0c97f2d35f5e3bd4f4fdb49698461,c114,IEEE International Conference on Robotics and Automation,Lessons from lady beetles: accuracy of monitoring data from US and UK citizen-science programs,"Citizen scientists have the potential to play a crucial role in the study of rapidly changing lady beetle (Coccinellidae) populations. We used data derived from three coccinellid-focused citizen-science programs to examine the costs and benefits of data collection from direct citizen-science (data used without verification) and verified citizen-science (observations verified by trained experts) programs. Data collated through direct citizen science overestimated species richness and diversity values in comparison to verified data, thereby influencing interpretation. The use of citizen scientists to collect data also influenced research costs; our analysis shows that verified citizen science was more cost effective than traditional science (in terms of data gathered per dollar). The ability to collect a greater number of samples through direct citizen science may compensate for reduced accuracy, depending on the type of data collected and the type(s) and extent of errors committed by volunteers.",poster,cp114
p422,760d38a08bff329ff67719935c18fa1631e3ded8,c14,International Conference on Exploring Services Science,The View from Above: Applications of Satellite Data in Economics,"The past decade or so has seen a dramatic change in the way that economists can learn by watching our planet from above. A revolution has taken place in remote sensing and allied fields such as computer science, engineering, and geography. Petabytes of satellite imagery have become publicly accessible at increasing resolution, many algorithms for extracting meaningful social science information from these images are now routine, and modern cloud-based processing power allows these algorithms to be run at global scale. This paper seeks to introduce economists to the science of remotely sensed data, and to give a flavor of how this new source of data has been used by economists so far and what might be done in the future.",poster,cp14
p423,0131258a516da6f9d86795fc6ed4968206dba005,j58,Journal of Data and Information Science,A Criteria-based Assessment of the Coverage of Scopus and Web of Science,"Abstract Purpose The purpose of this study is to assess the coverage of the scientific literature in Scopus and Web of Science from the perspective of research evaluation. Design/methodology/approach The academic communities of Norway have agreed on certain criteria for what should be included as original research publications in research evaluation and funding contexts. These criteria have been applied since 2004 in a comprehensive bibliographic database called the Norwegian Science Index (NSI). The relative coverages of Scopus and Web of Science are compared with regard to publication type, field of research and language. Findings Our results show that Scopus covers 72 percent of the total Norwegian scientific and scholarly publication output in 2015 and 2016, while the corresponding figure for Web of Science Core Collection is 69 percent. The coverages are most comprehensive in medicine and health (89 and 87 percent) and in the natural sciences and technology (85 and 84 percent). The social sciences (48 percent in Scopus and 40 percent in Web of Science Core Collection) and particularly the humanities (27 and 23 percent) are much less covered in the two international data sources. Research limitation Comparing with data from only one country is a limitation of the study, but the criteria used to define a country’s scientific output as well as the identification of patterns of field-dependent partial representations in Scopus and Web of Science should be recognizable and useful also for other countries. Originality/value The novelty of this study is the criteria-based approach to studying coverage problems in the two data sources.",fullPaper,jv58
p424,c1773a2d262e2001ef04b97b2023516a8bcac6f0,c10,Big Data,"Statistics for High-Dimensional Data: Methods, Theory and Applications","Modern statistics deals with large and complex data sets, and consequently with models containing a large number of parameters. This book presents a detailed account of recently developed approaches, including the Lasso and versions of it for various models, boosting methods, undirected graphical modeling, and procedures controlling false positive selections.A special characteristic of the book is that it contains comprehensive mathematical theory on high-dimensional statistics combined with methodology, algorithms and illustrations with real data examples. This in-depth approach highlights the methods great potential and practical applicability in a variety of settings. As such, it is a valuable resource for researchers, graduate students and experts in statistics, applied mathematics and computer science.",poster,cp10
p425,0c0b55fbb3fa524f3863eaaeba8606d403f65d45,j129,Concurrency and Computation,Assessing the quality and trustworthiness of citizen science data,"The Internet, Web 2.0 and Social Networking technologies are enabling citizens to actively participate in ‘citizen science’ projects by contributing data to scientific programmes via the Web. However, the limited training, knowledge and expertise of contributors can lead to poor quality, misleading or even malicious data being submitted. Subsequently, the scientific community often perceive citizen science data as not worthy of being used in serious scientific research—which in turn, leads to poor retention rates for volunteers. In this paper, we describe a technological framework that combines data quality improvements and trust metrics to enhance the reliability of citizen science data. We describe how online social trust models can provide a simple and effective mechanism for measuring the trustworthiness of community‐generated data. We also describe filtering services that remove unreliable or untrusted data and enable scientists to confidently reuse citizen science data. The resulting software services are evaluated in the context of the CoralWatch project—a citizen science project that uses volunteers to collect comprehensive data on coral reef health. Copyright © 2012 John Wiley & Sons, Ltd.",fullPaper,jv129
p426,b2e56a936b9dd333d6b81cab0fab8993589bb0c1,c71,IEEE International Conference on Information Reuse and Integration,Functional Data Analysis,Abstract content,poster,cp71
p427,a93d8676707b77db9476f7367d80e41f616f85a1,c4,Annual Conference on Genetic and Evolutionary Computation,Qualitative Data Analysis After Coding,"In 1991, Patti Lather (1991) called data analysis “the ‘black hole’ of qualitative research” (p. 149), and, as co-editors of this special issue on qualitative data analysis after coding, we suspect it still is. In fact, we think analysis—”thinking with theory” (Jackson & Mazzei, 2012)—is so difficult to describe and explain to the non-positivist—and to teach to our students—that we have resorted to equating qualitative data analysis with coding data. In other words, we teach analysis as coding because it is teachable. The critiques’ qualitative methodology endured during the debates about “scientifically based research” in the first decade of the 21st century surely intensified its already confused epistemological and ontological commitments. The incommensurability in this methodology is that a social science approach that claims to be interpretive supports a positivist, quasi-statistical analytic practice—coding data—that has, unfortunately, been proliferated and formalized in too many introductory textbooks and university research courses. A question we might ask at the outset is whether one would code data if one had not been taught to do so. We should state here at the beginning that we are not referring to the kind of analysis MacLure (2008) described as follows:",poster,cp4
p428,90478017154dd6e4dbcb71895c64c9ddddebfb8c,j130,Scientific Reports,Taxonomic bias in biodiversity data and societal preferences,Abstract content,fullPaper,jv130
p429,40f19bdaa4e869ab9784880fec5e9e229a2a61ab,j131,Astrophysical Journal Supplement Series,The Pan-STARRS1 Database and Data Products,"This paper describes the organization of the database and the catalog data products from the Pan-STARRS1 3π Steradian Survey. The catalog data products are available in the form of an SQL-based relational database from MAST, the Mikulski Archive for Space Telescopes at STScI. The database is described in detail, including the construction of the database, the provenance of the data, the schema, and how the database tables are related. Examples of queries for a range of science goals are included.",fullPaper,jv131
p430,62e0c6cf57bc345026d56fd654e80beaf9315c92,c96,USENIX Symposium on Operating Systems Design and Implementation,JENDL-4.0: A New Library for Nuclear Science and Engineering,"The fourth version of the Japanese Evaluated Nuclear Data Library has been produced in cooperation with the Japanese Nuclear Data Committee. In the new library, much emphasis is placed on the improvements of fission product and minor actinoid data. Two nuclear model codes were developed in order to evaluate the cross sections of fission products and minor actinoids. Coupled-channel optical model parameters, which can be applied to wide mass and energy regions, were obtained for nuclear model calculations. Thermal cross sections of actinoids were carefully examined by considering experimental data or by the systematics of neighboring nuclei. Most of the fission cross sections were derived from experimental data. A simultaneous evaluation was performed for the fission cross sections of important uranium and plutonium isotopes above 10 keV. New evaluations were performed for the thirty fissionproduct nuclides that had not been contained in the previous library JENDL-3.3. The data for light elements and structural materials were partly reevaluated. Moreover, covariances were estimated mainly for actinoids. The new library was released as JENDL-4.0, and the data can be retrieved from the Web site of the JAEA Nuclear Data Center.",poster,cp96
p431,2e888654c68524163fbf7a54396488249e73a702,c84,The Web Conference,Citizen Science: A Developing Tool for Expanding Science Knowledge and Scientific Literacy,"Citizen science enlists the public in collecting large quantities of data across an array of habitats and locations over long spans of time. Citizen science projects have been remarkably successful in advancing scientific knowledge, and contributions from citizen scientists now provide a vast quantity of data about species occurrence and distribution around the world. Most citizen science projects also strive to help participants learn about the organisms they are observing and to experience the process by which scientific investigations are conducted. Developing and implementing public data-collection projects that yield both scientific and educational outcomes requires significant effort. This article describes the model for building and operating citizen science projects that has evolved at the Cornell Lab of Ornithology over the past two decades. We hope that our model will inform the fields of biodiversity monitoring, biological research, and science education while providing a window into the culture of citizen science.",poster,cp84
p432,9a7dfcd3c35ebfbce9e359a1a97d6892b83a37ec,c56,European Conference on Software Process Improvement,Citizen Science as an Ecological Research Tool: Challenges and Benefits,"Citizen science, the involvement of volunteers in research, has increased the scale of ecological field studies with continent-wide, centralized monitoring efforts and, more rarely, tapping of volunteers to conduct large, coordinated, field experiments. The unique benefit for the field of ecology lies in understanding processes occurring at broad geographic scales and on private lands, which are impossible to sample extensively with traditional field research models. Citizen science produces large, longitudinal data sets, whose potential for error and bias is poorly understood. Because it does not usually aim to uncover mechanisms underlying ecological patterns, citizen science is best viewed as complementary to more localized, hypothesis-driven research. In the process of addressing the impacts of current, global “experiments” altering habitat and climate, large-scale citizen science has led to new, quantitative approaches to emerging questions about the distribution and abundance of organisms across spa...",poster,cp56
p433,029d03654fb0aff7f9e03083284dc04c741c9ab2,j85,BMC Bioinformatics,ImageJ2: ImageJ for the next generation of scientific image data,Abstract content,fullPaper,jv85
p434,04638c67b715b9d85ae5a44afd3730b83330fb66,j97,Science,Economics in the age of big data,"Background Economic science has evolved over several decades toward greater emphasis on empirical work. The data revolution of the past decade is likely to have a further and profound effect on economic research. Increasingly, economists make use of newly available large-scale administrative data or private sector data that often are obtained through collaborations with private firms, giving rise to new opportunities and challenges. The rising use of non–publicly available data in economic research. Here we show the percentage of papers published in the American Economic Review (AER) that obtained an exemption from the AER’s data availability policy, as a share of all papers published by the AER that relied on any form of data (excluding simulations and laboratory experiments). Notes and comments, as well as AER Papers and Proceedings issues, are not included in the analysis. We obtained a record of exemptions directly from the AER administrative staff and coded each exemption manually to reflect public sector versus private data. Our check of nonexempt papers suggests that the AER records may possibly understate the percentage of papers that actually obtained exemptions. The asterisk indicates that data run from when the AER started collecting these data (December 2005 issue) to the September 2014 issue. To make full use of the data, we define year 2006 to cover October 2005 through September 2006, year 2007 to cover October 2006 through September 2007, and so on. Advances These new data are affecting economic research along several dimensions. Many fields have shifted from a reliance on relatively small-sample government surveys to administrative data with universal or near-universal population coverage. This shift is transformative, as it allows researchers to rigorously examine variation in wages, health, productivity, education, and other measures across different subpopulations; construct consistent long-run statistical indices; generate new quasi-experimental research designs; and track diverse outcomes from natural and controlled experiments. Perhaps even more notable is the expansion of private sector data on economic activity. These data, sometimes available from public sources but other times obtained through data-sharing agreements with private firms, can help to create more granular and real-time measurement of aggregate economic statistics. The data also offer researchers a look inside the “black box” of firms and markets by providing meaningful statistics on economic behavior such as search and information gathering, communication, decision-making, and microlevel transactions. Collaborations with data-oriented firms also create new opportunities to conduct and evaluate randomized experiments. Economic theory plays an important role in the analysis of large data sets with complex structure. It can be difficult to organize and study this type of data (or even to decide which variables to construct) without a simplifying conceptual framework, which is where economic models become useful. Better data also allow for sharper tests of existing models and tests of theories that had previously been difficult to assess. Outlook The advent of big data is already allowing for better measurement of economic effects and outcomes and is enabling novel research designs across a range of topics. Over time, these data are likely to affect the types of questions economists pose, by allowing for more focus on population variation and the analysis of a broader range of economic activities and interactions. We also expect economists to increasingly adopt the large-data statistical methods that have been developed in neighboring fields and that often may complement traditional econometric techniques. These data opportunities also raise some important challenges. Perhaps the primary one is developing methods for researchers to access and explore data in ways that respect privacy and confidentiality concerns. This is a major issue in working with both government administrative data and private sector firms. Other challenges include developing the appropriate data management and programming capabilities, as well as designing creative and scalable approaches to summarize, describe, and analyze large-scale and relatively unstructured data sets. These challenges notwithstanding, the next few decades are likely to be a very exciting time for economic research. The quality and quantity of data on economic activity are expanding rapidly. Empirical research increasingly relies on newly available large-scale administrative data or private sector data that often is obtained through collaboration with private firms. Here we highlight some challenges in accessing and using these new data. We also discuss how new data sets may change the statistical methods used by economists and the types of questions posed in empirical research.",fullPaper,jv97
p435,411f9ebe18a885e687788841f4b3a60a0c3df3bc,j97,Science,Computational Social Science,Abstract content,fullPaper,jv97
p436,edf27bb5272ea6fe244deb3bbc8da0429bfe3ac5,j97,Science,The reusable holdout: Preserving validity in adaptive data analysis,"Testing hypotheses privately Large data sets offer a vast scope for testing already-formulated ideas and exploring new ones. Unfortunately, researchers who attempt to do both on the same data set run the risk of making false discoveries, even when testing and exploration are carried out on distinct subsets of data. Based on ideas drawn from differential privacy, Dwork et al. now provide a theoretical solution. Ideas are tested against aggregate information, whereas individual data set components remain confidential. Preserving that privacy also preserves statistical inference validity. Science, this issue p. 636 A statistical approach allows large data sets to be reanalyzed to test new hypotheses. Misapplication of statistical data analysis is a common cause of spurious discoveries in scientific research. Existing approaches to ensuring the validity of inferences drawn from data assume a fixed procedure to be performed, selected before the data are examined. In common practice, however, data analysis is an intrinsically adaptive process, with new analyses generated on the basis of data exploration, as well as the results of previous analyses on the same data. We demonstrate a new approach for addressing the challenges of adaptivity based on insights from privacy-preserving data analysis. As an application, we show how to safely reuse a holdout data set many times to validate the results of adaptively chosen analyses.",fullPaper,jv97
p437,06d2a3fde80c5644f14f743b29a57f6b02e850d9,j60,PLoS Biology,The iPlant Collaborative: Cyberinfrastructure for Enabling Data to Discovery for the Life Sciences,"The iPlant Collaborative provides life science research communities access to comprehensive, scalable, and cohesive computational infrastructure for data management; identity management; collaboration tools; and cloud, high-performance, high-throughput computing. iPlant provides training, learning material, and best practice resources to help all researchers make the best use of their data, expand their computational skill set, and effectively manage their data and computation when working as distributed teams. iPlant’s platform permits researchers to easily deposit and share their data and deploy new computational tools and analysis workflows, allowing the broader community to easily use and reuse those data and computational analyses.",fullPaper,jv60
p438,0bc97adfb3c77f27397d19395af2fdff9f04aaa0,c25,International Conference on Contemporary Computing,The TESS science processing operations center,"The Transiting Exoplanet Survey Satellite (TESS) will conduct a search for Earth's closest cousins starting in early 2018 and is expected to discover ∼1,000 small planets with Rp < 4 R⊕ and measure the masses of at least 50 of these small worlds. The Science Processing Operations Center (SPOC) is being developed at NASA Ames Research Center based on the Kepler science pipeline and will generate calibrated pixels and light curves on the NASA Advanced Supercomputing Division's Pleiades supercomputer. The SPOC will also search for periodic transit events and generate validation products for the transit-like features in the light curves. All TESS SPOC data products will be archived to the Mikulski Archive for Space Telescopes (MAST).",poster,cp25
p439,b7118fca8e7cd69d76090a5c145e89f303249eb8,c107,British Machine Vision Conference,The current state of citizen science as a tool for ecological research and public engagement,"Approaches to citizen science – an indispensable means of combining ecological research with environmental education and natural history observation – range from community-based monitoring to the use of the internet to “crowd-source” various scientific tasks, from data collection to discovery. With new tools and mechanisms for engaging learners, citizen science pushes the envelope of what ecologists can achieve, both in expanding the potential for spatial ecology research and in supplementing existing, but localized, research programs. The primary impacts of citizen science are seen in biological studies of global climate change, including analyses of phenology, landscape ecology, and macro-ecology, as well as in sub-disciplines focused on species (rare and invasive), disease, populations, communities, and ecosystems. Citizen science and the resulting ecological data can be viewed as a public good that is generated through increasingly collaborative tools and resources, while supporting public participation in science and Earth stewardship.",poster,cp107
p440,5ae073986408c9931bf6887fafb85e253866f7cc,c106,Chinese Conference on Biometric Recognition,Fuzzy-Set Social Science,"In this innovative approach to the practice of social science, Charles Ragin explores the use of fuzzy sets to bridge the divide between quantitative and qualitative methods. Paradoxically, the fuzzy set is a powerful tool because it replaces an unwieldy, ""fuzzy"" instrument—the variable, which establishes only the positions of cases relative to each other, with a precise one—degree of membership in a well-defined set. Ragin argues that fuzzy sets allow a far richer dialogue between ideas and evidence in social research than previously possible. They let quantitative researchers abandon ""homogenizing assumptions"" about cases and causes, they extend diversity-oriented research strategies, and they provide a powerful connection between theory and data analysis. Most important, fuzzy sets can be carefully tailored to fit evolving theoretical concepts, sharpening quantitative tools with in-depth knowledge gained through qualitative, case-oriented inquiry. This book will revolutionize research methods not only in sociology, political science, and anthropology but in any field of inquiry dealing with complex patterns of causation.",poster,cp106
p441,c8bc2d5edb9307b5c420adc4eee3cf641a781b14,c34,IEEE Working Conference on Mining Software Repositories,Online analysis enhances use of NASA Earth science data,"Giovanni, the Goddard Earth Sciences Data and Information Services Center (GES DISC) Interactive Online Visualization and Analysis Infrastructure, has provided researchers with advanced capabilities to perform data exploration and analysis with observational data from NASA Earth observation satellites. In the past 5–10 years, examining geophysical events and processes with remote-sensing data required a multistep process of data discovery, data acquisition, data management, and ultimately data analysis. Giovanni accelerates this process by enabling basic visualization and analysis directly on the World Wide Web. In the last two years, Giovanni has added new data acquisition functions and expanded analysis options to increase its usefulness to the Earth science research community.",poster,cp34
p442,9e7be12082f58cbf7ebdb84a8cbdc897a4e41683,c80,International Conference on Learning Representations,The Deluge of Spurious Correlations in Big Data,Abstract content,poster,cp80
p443,c32b03c3b5bbc97b0ec30663da1ff555f30acd95,j132,Prevention Science,Principled Missing Data Treatments,Abstract content,fullPaper,jv132
p444,6d962e9f04c653f732da82073a3446f75a371055,c73,Workshop on Algorithms in Bioinformatics,The KDD process for extracting useful knowledge from volumes of data,"AS WE MARCH INTO THE AGE of digital information, the problem of data overload looms ominously ahead. Our ability to analyze and understand massive datasets lags far behind our ability to gather and store the data. A new generation of computational techniques and tools is required to support the extraction of useful knowledge from the rapidly growing volumes of data. These techniques and tools are the subject of the emerging field of knowledge discovery in databases (KDD) and data mining. Large databases of digital information are ubiquitous. Data from the neighborhood store’s checkout register, your bank’s credit card authorization device, records in your doctor’s office, patterns in your telephone calls, and many more applications generate streams of digital records archived in huge databases, sometimes in so-called data warehouses. Current hardware and database technology allow efficient and inexpensive reliable data storage and access. However, whether the context is business, medicine, science, or government, the datasets themselves (in raw form) are of little direct value. What is of value is the knowledge that can be inferred from the data and put to use. For example, the marketing database of a consumer U s a m a F a y y a d ,",poster,cp73
p445,85cd1c3c6346d8fe3b245cc41e2757631301bc27,j125,Public Understanding of Science,The lure of rationality: Why does the deficit model persist in science communication?,"Science communication has been historically predicated on the knowledge deficit model. Yet, empirical research has shown that public communication of science is more complex than what the knowledge deficit model suggests. In this essay, we pose four lines of reasoning and present empirical data for why we believe the deficit model still persists in public communication of science. First, we posit that scientists’ training results in the belief that public audiences can and do process information in a rational manner. Second, the persistence of this model may be a product of current institutional structures. Many graduate education programs in science, technology, engineering, and math (STEM) fields generally lack formal training in public communication. We offer empirical evidence that demonstrates that scientists who have less positive attitudes toward the social sciences are more likely to adhere to the knowledge deficit model of science communication. Third, we present empirical evidence of how scientists conceptualize “the public” and link this to attitudes toward the deficit model. We find that perceiving a knowledge deficit in the public is closely tied to scientists’ perceptions of the individuals who comprise the public. Finally, we argue that the knowledge deficit model is perpetuated because it can easily influence public policy for science issues. We propose some ways to uproot the deficit model and move toward more effective science communication efforts, which include training scientists in communication methods grounded in social science research and using approaches that engage community members around scientific issues.",fullPaper,jv125
p446,5ef714f9fedfadd4a74ddee44c2cc811dc3025cf,c51,Conference of the Centre for Advanced Studies on Collaborative Research,Critical Reviews in Food Science and Nutrition,"Background: Health researchers may struggle to choose suitable validated dietary assessment tools (DATs) for their target population. The aim of this review was to identify and collate information on validated UK DATs and validation studies for inclusion on a website to support researchers to choose appropriate DATs. Design: A systematic review of reviews of DATs was undertaken. DATs validated in UK populations were extracted from the studies identified. A searchable website was designed to display these data. Additionally, mean differences and limits of agreement between test and comparison methods were summarized by a method, weighting by sample size. Results: Over 900 validation results covering 5 life stages, 18 nutrients, 6 dietary assessment methods, and 9 validation method types were extracted from 63 validated DATs which were identified from 68 reviews. These were incorporated into www.nutritools.org. Limits of agreement were determined for about half of validations. Thirty four DATs were FFQs. Only 17 DATs were validated against biomarkers, and only 19 DATs were validated in infant/children/adolescents. Conclusions: The interactive www.nutritools.org website holds extensive validation data identified from this review and can be used to guide researchers to critically compare and choose a suitable DAT for their research question, leading to improvement of nutritional epidemiology research.",poster,cp51
p447,69732dcf45024f28e5c43de68d1208f6e737eada,c60,IEEE International Conference on Software Engineering and Formal Methods,The BIG Data Center: from deposition to integration to translation,"Biological data are generated at unprecedentedly exponential rates, posing considerable challenges in big data deposition, integration and translation. The BIG Data Center, established at Beijing Institute of Genomics (BIG), Chinese Academy of Sciences, provides a suite of database resources, including (i) Genome Sequence Archive, a data repository specialized for archiving raw sequence reads, (ii) Gene Expression Nebulas, a data portal of gene expression profiles based entirely on RNA-Seq data, (iii) Genome Variation Map, a comprehensive collection of genome variations for featured species, (iv) Genome Warehouse, a centralized resource housing genome-scale data with particular focus on economically important animals and plants, (v) Methylation Bank, an integrated database of whole-genome single-base resolution methylomes and (vi) Science Wikis, a central access point for biological wikis developed for community annotations. The BIG Data Center is dedicated to constructing and maintaining biological databases through big data integration and value-added curation, conducting basic research to translate big data into big knowledge and providing freely open access to a variety of data resources in support of worldwide research activities in both academia and industry. All of these resources are publicly available and can be found at http://bigd.big.ac.cn.",poster,cp60
p448,b55fda1f58af7fd9ecde8f1dc193ddd6ab6e9d26,c11,Hawaii International Conference on System Sciences,Handbook of Theoretical Computer Science,"""Of all the books I have covered in the Forum to date, this set is the most unique and possibly the most useful to the SIGACT community, in support both of teaching and research.... The books can be used by anyone wanting simply to gain an understanding of one of these areas, or by someone desiring to be in research in a topic, or by instructors wishing to find timely information on a subject they are teaching outside their major areas of expertise."" -- Rocky Ross, ""SIGACT News"" ""This is a reference which has a place in every computer science library."" -- Raymond Lauzzana, ""Languages of Design"" The Handbook of Theoretical Computer Science provides professionals and students with a comprehensive overview of the main results and developments in this rapidly evolving field. Volume A covers models of computation, complexity theory, data structures, and efficient computation in many recognized subdisciplines of theoretical computer science. Volume B takes up the theory of automata and rewriting systems, the foundations of modern programming languages, and logics for program specification and verification, and presents several studies on the theoretic modeling of advanced information processing. The two volumes contain thirty-seven chapters, with extensive chapter references and individual tables of contents for each chapter. There are 5,387 entry subject indexes that include notational symbols, and a list of contributors and affiliations in each volume.",poster,cp11
p449,2c4203d34b4265e439c4b8c53331b8ee233b16bc,c23,International Conference on Open and Big Data,"Big Data, Little Data, No Data: Scholarship in the Networked World","""Big Data"" is on the covers of Science, Nature, the Economist, and Wired magazines, on the front pages of the Wall Street Journal and the New York Times. But despite the media hyperbole, as Christine Borgman points out in this examination of data and scholarly research, having the right data is usually better than having more data; little data can be just as valuable as big data. In many cases, there are no data -- because relevant data don't exist, cannot be found, or are not available. Moreover, data sharing is difficult, incentives to do so are minimal, and data practices vary widely across disciplines. Borgman, an often-cited authority on scholarly communication, argues that data have no value or meaning in isolation; they exist within a knowledge infrastructure -- an ecology of people, practices, technologies, institutions, material objects, and relationships. After laying out the premises of her investigation -- six ""provocations"" meant to inspire discussion about the uses of data in scholarship -- Borgman offers case studies of data practices in the sciences, the social sciences, and the humanities, and then considers the implications of her findings for scholarly practice and research policy. To manage and exploit data over the long term, Borgman argues, requires massive investment in knowledge infrastructures; at stake is the future of scholarship.",poster,cp23
p450,a6e594b11bd8195e96a1826f591fcec9a20fdcf3,c80,International Conference on Learning Representations,"Frascati manual 2015 : guidelines for collecting and reporting data in research and experimental development: the measurement of scientific, technological and innovation activities.","The Frascati Manual is firmly based on experience gained from collecting R&D 
statistics in both OECD and non-member countries. It is a result of the collective work 
of national experts in NESTI, the OECD Working Party of National Experts on Science 
and Technology Indicators. This group, with support from the OECD Secretariat, has 
worked over now more than 50 years as an effective community of practitioners to 
implement measurement approaches for the concepts of science, technology and 
innovation. This effort has resulted in a series of methodological manuals known as the 
“Frascati Family”, which in addition to this manual includes guidance documents on 
the measurement of innovation (the Oslo Manual), human resources devoted to science 
and technology, patents, and technological balance of payments, but most importantly, 
it has provided the basis for the main statistics and indicators on science and technology 
that are currently used.",poster,cp80
p451,052fcf10e96f282c0fb50f778150afeaf92bb65d,c21,Grid Computing Environments,Inquiry-based science instruction—what is it and does it matter? Results from a research synthesis years 1984 to 2002,"The goal of the Inquiry Synthesis Project was to synthesize findings from research conducted between 1984 and 2002 to address the research question, What is the impact of inquiry science instruction on K-12 student outcomes? The timeframe of 1984 to 2002 was selected to continue a line of synthesis work last completed in 1983 by Bredderman (Bredderman (1983) Review of Educational Research 53: 499-518) and Shymansky, Kyle, and Alport (Shymansky et al. (1983) Journal of Research in Science Teaching 20: 387-404), and to accommodate a practicable cut- off date given the research project timeline, which ran from 2001 to 2006. The research question for the project was addressed by developing a conceptual framework that clarifies and specifies what is meant by ''inquiry-based science instruction,'' and by using a mixed-methodology approach to analyze both numerical and text data describing the impact of instruction on K-12 student science conceptual learning. Various findings across 138 analyzed studies indicate a clear, positive trend favoring inquiry-based instructional practices, particularly instruction that emphasizes student active thinking and drawing conclusions from data. Teaching strategies that actively engage students in the learning process through scientific investigations are more likely to increase conceptual understanding than are strategies that rely on more passive techniques, which are often necessary in the current standardized-assessment laden educational environment.",poster,cp21
p452,a418d8fd1cc0abb34cf131d81723bc5da8817c93,c103,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,Politicization of Science in the Public Sphere,"This study explores time trends in public trust in science in the United States from 1974 to 2010. More precisely, I test Mooney’s (2005) claim that conservatives in the United States have become increasingly distrustful of science. Using data from the 1974 to 2010 General Social Survey, I examine group differences in trust in science and group-specific change in these attitudes over time. Results show that group differences in trust in science are largely stable over the period, except for respondents identifying as conservative. Conservatives began the period with the highest trust in science, relative to liberals and moderates, and ended the period with the lowest. The patterns for science are also unique when compared to public trust in other secular institutions. Results show enduring differences in trust in science by social class, ethnicity, gender, church attendance, and region. I explore the implications of these findings, specifically, the potential for political divisions to emerge over the cultural authority of science and the social role of experts in the formation of public policy.",poster,cp103
p453,954f2a7b1c6f28c4a845ccda5761eb09da032a64,j97,Science,Data sharing,"The Science family of journals is committed to sharing data relevant to public health emergencies, and therefore we are signatories to, and wholeheartedly endorse, the following statement by funders and journals.*",fullPaper,jv97
p454,951eab2b27c673e0ff1a20800f576d4792f60d5f,j97,Science,Crisis informatics—New data for extraordinary times,"Focus on behaviors, not on fetishizing social media tools Crisis informatics is a multidisciplinary field combining computing and social science knowledge of disasters; its central tenet is that people use personal information and communication technology to respond to disaster in creative ways to cope with uncertainty. We study and develop computational support for collection and sociobehavioral analysis of online participation (i.e., tweets and Facebook posts) to address challenges in disaster warning, response, and recovery. Because such data are rarely tidy, we offer lessons—learned the hard way, as we have made every mistake described below—with respect to the opportunities and limitations of social media research on crisis events.",fullPaper,jv97
p455,18a940ff6dce8bc140658da52d686291ca965979,c112,Very Large Data Bases Conference,The Analysis of Social Science Data with Missing Values,"Methods for handling missing data in social science data sets are reviewed. Limitations of common practical approaches, including complete-case analysis, available-case analysis and imputation, are illustrated on a simple missing-data problem with one complete and one incomplete variable. Two more principled approaches, namely maximum likelihood under a model for the data and missing-data mechanism and multiple imputation, are applied to the bivariate problem. General properties of these methods are outlined, and applications to more complex missing-data problems are discussed. The EM algorithm, a convenient method for computing maximum likelihood estimates in missing-data problems, is described and applied to two common models, the multivariate normal model for continuous data and the multinomial model for discrete data. Multiple imputation under explicit or implicit models is recommended as a method that retains the advantages of imputation and overcomes its limitations.",poster,cp112
p456,7e95c6f943b7c47af1b2ef1651b86022a001ce81,c52,Workshop on Learning from Authoritative Security Experiment Results,A Review of Microsoft Academic Services for Science of Science Studies,"Since the relaunch of Microsoft Academic Services (MAS) 4 years ago, scholarly communications have undergone dramatic changes: more ideas are being exchanged online, more authors are sharing their data, and more software tools used to make discoveries and reproduce the results are being distributed openly. The sheer amount of information available is overwhelming for individual humans to keep up and digest. In the meantime, artificial intelligence (AI) technologies have made great strides and the cost of computing has plummeted to the extent that it has become practical to employ intelligent agents to comprehensively collect and analyze scholarly communications. MAS is one such effort and this paper describes its recent progresses since the last disclosure. As there are plenty of independent studies affirming the effectiveness of MAS, this paper focuses on the use of three key AI technologies that underlies its prowess in capturing scholarly communications with adequate quality and broad coverage: (1) natural language understanding in extracting factoids from individual articles at the web scale, (2) knowledge assisted inference and reasoning in assembling the factoids into a knowledge graph, and (3) a reinforcement learning approach to assessing scholarly importance for entities participating in scholarly communications, called the saliency, that serves both as an analytic and a predictive metric in MAS. These elements enhance the capabilities of MAS in supporting the studies of science of science based on the GOTO principle, i.e., good and open data with transparent and objective methodologies. The current direction of development and how to access the regularly updated data and tools from MAS, including the knowledge graph, a REST API and a website, are also described.",poster,cp52
p457,64ad643e8084486ca7d3312ed491a814d3fe440c,c13,International Conference on Data Science and Advanced Analytics,The Synthetic Data Vault,"The goal of this paper is to build a system that automatically creates synthetic data to enable data science endeavors. To achieve this, we present the Synthetic Data Vault (SDV), a system that builds generative models of relational databases. We are able to sample from the model and create synthetic data, hence the name SDV. When implementing the SDV, we also developed an algorithm that computes statistics at the intersection of related database tables. We then used a state-of-the-art multivariate modeling approach to model this data. The SDV iterates through all possible relations, ultimately creating a model for the entire database. Once this model is computed, the same relational information allows the SDV to synthesize data by sampling from any part of the database. After building the SDV, we used it to generate synthetic data for five different publicly available datasets. We then published these datasets, and asked data scientists to develop predictive models for them as part of a crowdsourced experiment. By analyzing the outcomes, we show that synthetic data can successfully replace original data for data science. Our analysis indicates that there is no significant difference in the work produced by data scientists who used synthetic data as opposed to real data. We conclude that the SDV is a viable solution for synthetic data generation.",fullPaper,cp13
p458,0233eabf848e9bbc6b6debfaa9ec3cf5a195b23d,c27,ACM-SIAM Symposium on Discrete Algorithms,Data streams: algorithms and applications,"Data stream algorithms as an active research agenda emerged only over the past few years, even though the concept of making few passes over the data for performing computations has been around since the early days of Automata Theory. The data stream agenda now pervades many branches of Computer Science including databases, networking, knowledge discovery and data mining, and hardware systems. Industry is in synch too, with Data Stream Management Systems (DSMSs) and special hardware to deal with data speeds. Even beyond Computer Science, data stream concerns are emerging in physics, atmospheric science and statistics. Data Streams: Algorithms and Applications focuses on the algorithmic foundations of data streaming. In the data stream scenario, input arrives very rapidly and there is limited memory to store the input. Algorithms have to work with one or few passes over the data, space less than linear in the input size or time significantly less than the input size. In the past few years, a new theory has emerged for reasoning about algorithms that work within these constraints on space, time and number of passes. Some of the methods rely on metric embeddings, pseudo-random computations, sparse approximation theory and communication complexity. The applications for this scenario include IP network traffic analysis, mining text message streams and processing massive data sets in general. Data Streams: Algorithms and Applications surveys the emerging area of algorithms for processing data streams and associated applications. An extensive bibliography with over 200 entries points the reader to further resources for exploration.",fullPaper,cp27
p459,06fd9101e742ca920c23f3797c19022c13f5a9b5,j133,Social Studies of Science,"Science friction: Data, metadata, and collaboration","When scientists from two or more disciplines work together on related problems, they often face what we call ‘science friction’. As science becomes more data-driven, collaborative, and interdisciplinary, demand increases for interoperability among data, tools, and services. Metadata – usually viewed simply as ‘data about data’, describing objects such as books, journal articles, or datasets – serve key roles in interoperability. Yet we find that metadata may be a source of friction between scientific collaborators, impeding data sharing. We propose an alternative view of metadata, focusing on its role in an ephemeral process of scientific communication, rather than as an enduring outcome or product. We report examples of highly useful, yet ad hoc, incomplete, loosely structured, and mutable, descriptions of data found in our ethnographic studies of several large projects in the environmental sciences. Based on this evidence, we argue that while metadata products can be powerful resources, usually they must be supplemented with metadata processes. Metadata-as-process suggests the very large role of the ad hoc, the incomplete, and the unfinished in everyday scientific work.",fullPaper,jv133
p460,f41f3ad8c286a8dc509b2fa42b5febe81bcec9d8,j60,PLoS Biology,The Open Knowledge Foundation: Open Data Means Better Science,"Open data leads to better science, but overcoming the barriers to widespread publication and availability of open scientific data requires a community effort. The Open Knowledge Foundation Open Data in Science Working Group describes their role in this movement.",fullPaper,jv60
p461,f39f562f706e9f8771e6305d086fed159366b5a8,c57,IEEE International Conference on Engineering of Complex Computer Systems,Assessing citizen science data quality: an invasive species case study,"An increase in the number of citizen science programs has prompted an examination of their ability to provide data of sufficient quality. We tested the ability of volunteers relative to professionals in identifying invasive plant species, mapping their distributions, and estimating their abundance within plots. We generally found that volunteers perform almost as well as professionals in some areas, but that we should be cautious about data quality in both groups. We analyzed predictors of volunteer success (age, education, experience, science literacy, attitudes) in training‐related skills, but these proved to be poor predictors of performance and could not be used as effective eligibility criteria. However, volunteer success with species identification increased with their self‐identified comfort level. Based on our case study results, we offer lessons learned and their application to other programs and provide recommendations for future research in this area.",poster,cp57
p462,dee89541feff018c45e4a2701e03efcd1d3d3fc5,c33,International Conference on Agile Software Development,Google's PageRank and beyond - the science of search engine rankings,"Why doesn't your home page appear on the first page of search results, even when you query your own name? How do other web pages always appear at the top? What creates these powerful rankings? And how? The first book ever about the science of web page rankings, Google's PageRank and Beyond supplies the answers to these and other questions and more. The book serves two very different audiences: the curious science reader and the technical computational reader. The chapters build in mathematical sophistication, so that the first five are accessible to the general academic reader. While other chapters are much more mathematical in nature, each one contains something for both audiences. For example, the authors include entertaining asides such as how search engines make money and how the Great Firewall of China influences research. The book includes an extensive background chapter designed to help readers learn more about the mathematics of search engines, and it contains several MATLAB codes and links to sample web data sets. The philosophy throughout is to encourage readers to experiment with the ideas and algorithms in the text. Any business seriously interested in improving its rankings in the major search engines can benefit from the clear examples, sample code, and list of resources provided. Many illustrative examples and entertaining asides MATLAB code Accessible and informal style Complete and self-contained section for mathematics review",poster,cp33
p463,59b2796c176636a3222d7b129c6209fa6e979aa7,j38,Big Data & Society,Data infrastructure literacy,"A recent report from the UN makes the case for “global data literacy” in order to realise the opportunities afforded by the “data revolution”. Here and in many other contexts, data literacy is characterised in terms of a combination of numerical, statistical and technical capacities. In this article, we argue for an expansion of the concept to include not just competencies in reading and working with datasets but also the ability to account for, intervene around and participate in the wider socio-technical infrastructures through which data is created, stored and analysed – which we call “data infrastructure literacy”. We illustrate this notion with examples of “inventive data practice” from previous and ongoing research on open data, online platforms, data journalism and data activism. Drawing on these perspectives, we argue that data literacy initiatives might cultivate sensibilities not only for data science but also for data sociology, data politics as well as wider public engagement with digital data infrastructures. The proposed notion of data infrastructure literacy is intended to make space for collective inquiry, experimentation, imagination and intervention around data in educational programmes and beyond, including how data infrastructures can be challenged, contested, reshaped and repurposed to align with interests and publics other than those originally intended.",fullPaper,jv38
p464,832139bd87f51f0a173b5bd9255944748bc31a96,c109,International Conference on Mobile Data Management,Global multi-resolution terrain elevation data 2010 (GMTED2010),"For more information on the USGS—the Federal source for science about the Earth, its natural and living resources, natural hazards, and the environment, visit http://www.usgs.gov or call 1–888–ASK–USGS. For an overview of USGS information products, including maps, imagery, and publications, Any use of trade, product, or firm names is for descriptive purposes only and does not imply endorsement by the U.S. Government. Although this report is in the public domain, permission must be secured from the individual copyright owners to reproduce any copyrighted materials contained within this report. 10. Diagram showing the GMTED2010 layer extents (minimum and maximum latitude and longitude) are a result of the coordinate system inherited from the 1-arc-second SRTM",poster,cp109
p465,afefe35db68dabf33fa548ab818b46c47c860e08,c42,Joint Conference of International Workshop on Software Measurement and International Conference on Software Process and Product Measurement,Big Data and Internet of Things: A Roadmap for Smart Environments,Abstract content,poster,cp42
p466,917943472ec4a00443d78bb696ed4d8f8d8c7f0a,c23,International Conference on Open and Big Data,Understanding the Science Experiences of Successful Women of Color: Science Identity as an Analytic Lens.,"In this study, we develop a model of science identity to make sense of the science experiences of 15 successful women of color over the course of their undergraduate and graduate studies in science and into science-related careers. In our view, science identity accounts both for how women make meaning of science experiences and how society structures possible meanings. Primary data included ethnographic interviews during students' undergraduate careers, follow-up interviews 6 years later, and ongoing member-checking. Our results highlight the importance of recognition by others for women in the three science identity trajectories: research scientist; altruistic scientist; and disrupted scientist. The women with research scientist identities were passionate about science and recognized themselves and were recognized by science faculty as science people. The women with altruistic scientist identities regarded science as a vehicle for altruism and created innovative meanings of ''science,'' ''recognition by others,'' and ''woman of color in science.'' The women with disrupted scientist identities sought, but did not often receive, recognition by meaningful scientific others. Although they were ultimately successful, their trajectories were more difficult because, in part, their bids for recognition were disrupted by the interaction with gendered, ethnic, and racial factors. This study clarifies theoretical conceptions of science identity, promotes a rethinking of recruitment and retention efforts, and illuminates various ways women of color experience, make meaning of, and negotiate the culture of science. 2007 Wiley Periodicals, Inc. J Res Sci Teach 44: 1187-1218, 2007.",poster,cp23
p467,e5553a3754a20cfe3e8a7b50bee715348e201a50,j134,Frontiers in Earth Science,"Citizen science in hydrology and water resources: opportunities for knowledge generation, ecosystem service management, and sustainable development","The participation of the general public in the research design, data collection and interpretation process together with scientists is often referred to as citizen science. While citizen science itself has existed since the start of scientific practice, developments in sensing technology, data processing and visualisation, and communication of ideas and results, are creating a wide range of new opportunities for public participation in scientific research. This paper reviews the state of citizen science in a hydrological context and explores the potential of citizen science to complement more traditional ways of scientific data collection and knowledge generation for hydrological sciences and water resources management. Although hydrological data collection often involves advanced technology, the advent of robust, cheap and low-maintenance sensing equipment provides unprecedented opportunities for data collection in a citizen science context. These data have a significant potential to create new hydrological knowledge, especially in relation to the characterisation of process heterogeneity, remote regions, and human impacts on the water cycle. However, the nature and quality of data collected in citizen science experiments is potentially very different from those of traditional monitoring networks. This poses challenges in terms of their processing, interpretation, and use, especially with regard to assimilation of traditional knowledge, the quantification of uncertainties, and their role in decision support. It also requires care in designing citizen science projects such that the generated data complement optimally other available knowledge. Lastly, we reflect on the challenges and opportunities in the integration of hydrologically-oriented citizen science in water resources management, the role of scientific knowledge in the decision-making process, and the potential contestation to established community institutions posed by co-generation of new knowledge.",fullPaper,jv134
p468,7ee986de710617168d8adde0aa35a8c079011a09,c73,Workshop on Algorithms in Bioinformatics,Data Scientist,"The last installment of this column dealt with the specter of IT-caused unemployment. Here, the author considers a new IT-created employment opportunity--the data scientist. He looks at data, information, and knowledge and current IT job classifications to provide context, describes how big data has inspired the field of data science, and defines what data science is and what data scientists do.",poster,cp73
p469,f6ce14f91b4641942947882062682125369847f7,j40,Social Science Research Network,The V–Dem Measurement Model: Latent Variable Analysis for Cross-National and Cross-Temporal Expert-Coded Data,"This material is based upon work supported by the National Science Foundation (SES-1423944, PI: Daniel Pemstein), Riksbankens Jubileumsfond (Grant M13-0559:1, PI: Staffan I. Lindberg), the Swedish Research Council (2013.0166, PI: Staffan I. Lindberg and Jan Teorell), the Knut and Alice Wallenberg Foundation (PI: Staffan I. Lindberg), and the University of Gothenburg (E 2013/43); as well as internal grants from the Vice-Chancellor’s office, the Dean of the College of Social Sciences, and the Department of Political Science at University of Gothenburg. Marquardt acknowledges research support from the Russian Academic Excellence Project ‘5-100.’ We performed simulations and other computational tasks using resources provided by the Notre Dame Center for Research Computing (CRC) through the High Performance Computing section and the Swedish National Infrastructure for Computing (SNIC) at the National Supercomputer Centre in Sweden (SNIC 2016/1-382, SNIC 2017/1-406 and 2017/1-68). We specifically acknowledge the assistance of In-Saeng Suh at CRC and Johan Raber and Peter Mu nger at SNIC in facilitating our use of their respective systems.",fullPaper,jv40
p470,454a5afdeeee4b10b912ea9cca4d9dd13beb2aa4,c46,Brazilian Symposium on Software Engineering,Data validation in citizen science: a case study from Project FeederWatch,"To become more widely accepted as a valuable research tool, citizen-science projects must find ways to ensure that data gathered by large numbers of people with varying levels of expertise are of consistently high quality. Here, we describe a data validation protocol developed for Project FeederWatch, a continent-wide bird monitoring program, that is designed to increase researchers' and participants' confidence in the data being collected.",poster,cp46
p471,687e00a5fec7d747d18866f60b7a21973e80b04f,c97,Interspeech,The ethics of smart cities and urban science,"Software-enabled technologies and urban big data have become essential to the functioning of cities. Consequently, urban operational governance and city services are becoming highly responsive to a form of data-driven urbanism that is the key mode of production for smart cities. At the heart of data-driven urbanism is a computational understanding of city systems that reduces urban life to logic and calculative rules and procedures, which is underpinned by an instrumental rationality and realist epistemology. This rationality and epistemology are informed by and sustains urban science and urban informatics, which seek to make cities more knowable and controllable. This paper examines the forms, practices and ethics of smart cities and urban science, paying particular attention to: instrumental rationality and realist epistemology; privacy, datafication, dataveillance and geosurveillance; and data uses, such as social sorting and anticipatory governance. It argues that smart city initiatives and urban science need to be re-cast in three ways: a re-orientation in how cities are conceived; a reconfiguring of the underlying epistemology to openly recognize the contingent and relational nature of urban systems, processes and science; and the adoption of ethical principles designed to realize benefits of smart cities and urban science while reducing pernicious effects. This article is part of the themed issue ‘The ethical impact of data science’.",poster,cp97
p472,40e1ac2c42a7c9c1aeb80d3c0dab6de55b40eddd,c96,USENIX Symposium on Operating Systems Design and Implementation,The conundrum of sharing research data,"We must all accept that science is data and that data are science, and thus provide for, and justify the need for the support of, much-improved data curation. (Hanson, Sugden, & Alberts) 
 
Researchers are producing an unprecedented deluge of data by using new methods and instrumentation. Others may wish to mine these data for new discoveries and innovations. However, research data are not readily available as sharing is common in only a few fields such as astronomy and genomics. Data sharing practices in other fields vary widely. Moreover, research data take many forms, are handled in many ways, using many approaches, and often are difficult to interpret once removed from their initial context. Data sharing is thus a conundrum. Four rationales for sharing data are examined, drawing examples from the sciences, social sciences, and humanities: (1) to reproduce or to verify research, (2) to make results of publicly funded research available to the public, (3) to enable others to ask new questions of extant data, and (4) to advance the state of research and innovation. These rationales differ by the arguments for sharing, by beneficiaries, and by the motivations and incentives of the many stakeholders involved. The challenges are to understand which data might be shared, by whom, with whom, under what conditions, why, and to what effects. Answers will inform data policy and practice. © 2012 Wiley Periodicals, Inc.",poster,cp96
p473,e7f5ab8f486487dcefdf9b989d0eff2f0beff48c,c38,"IEEE International Conference on Software Analysis, Evolution, and Reengineering",A Qualitative Framework for Collecting and Analyzing Data in Focus Group Research,"Despite the abundance of published material on conducting focus groups, scant specific information exists on how to analyze focus group data in social science research. Thus, the authors provide a new qualitative framework for collecting and analyzing focus group data. First, they identify types of data that can be collected during focus groups. Second, they identify the qualitative data analysis techniques best suited for analyzing these data. Third, they introduce what they term as a micro-interlocutor analysis, wherein meticulous information about which participant responds to each question, the order in which each participant responds, response characteristics, the nonverbal communication used, and the like is collected, analyzed, and interpreted. They conceptualize how conversation analysis offers great potential for analyzing focus group data. They believe that their framework goes far beyond analyzing only the verbal communication of focus group participants, thereby increasing the rigor of focus group analyses in social science research.",poster,cp38
p474,ad4f067304551c20b50a3c0f7e02e3e9d6946003,c111,International Society for Music Information Retrieval Conference,Automated data reduction workflows for astronomy,"Data from complex modern astronomical instruments often consist of a large number of different science and calibration files, and their reduction requires a variety of software tools. The execution chain of the tools represents a complex workflow that needs to be tuned and supervised, often by individual researchers that are not necessarily experts for any specific instrument. The efficiency of data reduction can be improved by using automatic workflows to organise data and execute the sequence of data reduction steps. To realize such efficiency gains, we designed a system that allows intuitive representation, execution and modification of the data reduction workflow, and has facilities for inspection and interaction with the data. The European Southern Observatory (ESO) has developed Reflex, an environment to automate data reduction workflows. Reflex is implemented as a package of customized components for the Kepler workflow engine. Kepler provides the graphical user interface to create an executable flowchart-like representation of the data reduction process. Key features of Reflex are a rule-based data organiser, infrastructure to re-use results, thorough book-keeping, data progeny tracking, interactive user interfaces, and a novel concept to exploit information created during data organisation for the workflow execution. Reflex includes novel concepts to increase the efficiency of astronomical data processing. While Reflex is a specific implementation of astronomical scientific workflows within the Kepler workflow engine, the overall design choices and methods can also be applied to other environments for running automated science workflows.",poster,cp111
p475,af7bdb358b366260b657d0af92eb5e50e916c6b8,j96,Perspectives on Psychological Science,Advancing Science Through Collaborative Data Sharing and Synthesis,"The demand for researchers to share their data has increased dramatically in recent years. There is a need to replicate and confirm scientific findings to bolster confidence in many research areas. Data sharing also serves the critical function of allowing synthesis of findings across trials. As innovative statistical methods have helped resolve barriers to synthesis analyses, data sharing and synthesis can help answer research questions that cannot be answered by individual trials alone. However, the sharing of data among researchers remains challenging and infrequent. This article aims to (a) increase support for data sharing and synthesis collaborations among researchers to advance scientific knowledge and (b) provide a model for establishing these collaborations using the example of the ongoing National Institute of Mental Health’s Collaborative Data Synthesis on Adolescent Depression Trials. This study brings together datasets from existing prevention and treatment trials in adolescent depression, as well as researchers and stakeholders, to answer questions about “for whom interventions work” and “by what pathways interventions have their effects.” This is critical to improving interventions, including increasing knowledge about intervention efficacy among minority populations, or what we call “scientific equity.” The collaborative model described is relevant to fields with research questions that can only be addressed by synthesizing individual-level data.",fullPaper,jv96
p476,8a88e9710ad9b4cb8db6a3086ede9c531d994917,j135,"Marketing science (Providence, R.I.)",The Service Revolution and the Transformation of Marketing Science,"The nature of marketing science is changing in a systematic, predictable, and irrevocable way. As information technology enables ubiquitous customer communication and big customer data, the fundamental nature of the firm's connection to the customer changes: better, more personalized service can be offered, from which service relationships are deepened, and consequently, more profitable customers grow the influence of service within the goods sector and expand the service sector in the economy. Marketing is becoming more personalized, and marketing science techniques that exploit customer heterogeneity are becoming more important. Information technology improvements also guarantee the increasing importance and usage of computationally intensive data processing and “big data.” Most importantly, these trends have already lasted for more than a century, and they will become even more pronounced in the coming years as a result of the monotonic nature of technology improvement. These changes imply a transformation of marketing science in both the topics to be emphasized and the methods to be employed. Increasingly, and inevitably, all of marketing will come to resemble to a greater degree the formerly specialized area of service marketing, only with an increased emphasis on marketing analytics.",fullPaper,jv135
p477,361df482f4c74cb85d0f2fa897a4491fbf53ab6f,j136,Journal of the Royal Society Interface,The Matthew effect in empirical data,"The Matthew effect describes the phenomenon that in societies, the rich tend to get richer and the potent even more powerful. It is closely related to the concept of preferential attachment in network science, where the more connected nodes are destined to acquire many more links in the future than the auxiliary nodes. Cumulative advantage and success-breads-success also both describe the fact that advantage tends to beget further advantage. The concept is behind the many power laws and scaling behaviour in empirical data, and it is at the heart of self-organization across social and natural sciences. Here, we review the methodology for measuring preferential attachment in empirical data, as well as the observations of the Matthew effect in patterns of scientific collaboration, socio-technical and biological networks, the propagation of citations, the emergence of scientific progress and impact, career longevity, the evolution of common English words and phrases, as well as in education and brain development. We also discuss whether the Matthew effect is due to chance or optimization, for example related to homophily in social systems or efficacy in technological systems, and we outline possible directions for future research.",fullPaper,jv136
p478,16c30c6449182f6ad1235a58321d4bc4c297fbf6,j14,Scientific Data,An open science resource for establishing reliability and reproducibility in functional connectomics,Abstract content,fullPaper,jv14
p479,ab2b6f43b0a99dc513f46e7f1684f55ce12de5d9,c13,International Conference on Data Science and Advanced Analytics,The end of theory: The data deluge makes the scientific method obsolete,"Illustration: Marian Bantjes The Petabyte Age: Sensors everywhere. Infinite storage. Clouds of processors. Our ability to capture, warehouse, and understand massive amounts of data is changing science, medicine, business, and technology. As our collection of facts and figures grows, so will the opportunity to find answers to fundamental questions. Because in the era of big data, more isn't just more. More is different. The End of Theory: Essay: The Data Deluge Makes the Scientific Method Obsolete",poster,cp13
p480,8ee4eda834e95124aca1e5ff05a1b8ce7d1487ec,j137,SIAM Journal on Mathematics of Data Science,Why Are Big Data Matrices Approximately Low Rank?,"Matrices of (approximate) low rank are pervasive in data science, appearing in movie preferences, text documents, survey data, medical records, and genomics. While there is a vast literature on how...",fullPaper,jv137
p481,eb286565b6a18e21b9daf5375c75d56513cd2853,c73,Workshop on Algorithms in Bioinformatics,Big Earth data: A new frontier in Earth and information sciences,"Abstract Big data is a revolutionary innovation that has allowed the development of many new methods in scientific research. This new way of thinking has encouraged the pursuit of new discoveries. Big data occupies the strategic high ground in the era of knowledge economies and also constitutes a new national and global strategic resource. “Big Earth data”, derived from, but not limited to, Earth observation has macro-level capabilities that enable rapid and accurate monitoring of the Earth, and is becoming a new frontier contributing to the advancement of Earth science and significant scientific discoveries. Within the context of the development of big data, this paper analyzes the characteristics of scientific big data and recognizes its great potential for development, particularly with regard to the role that big Earth data can play in promoting the development of Earth science. On this basis, the paper outlines the Big Earth Data Science Engineering Project (CASEarth) of the Chinese Academy of Sciences Strategic Priority Research Program. Big data is at the forefront of the integration of geoscience, information science, and space science and technology, and it is expected that big Earth data will provide new prospects for the development of Earth science.",poster,cp73
p482,1a67f9a4624b4ea989e4ea9b14ea178a010017c0,j96,Perspectives on Psychological Science,Editors’ Introduction to the Special Section on Replicability in Psychological Science,"Is there currently a crisis of confidence in psychological science reflecting an unprecedented level of doubt among practitioners about the reliability of research findings in the field? It would certainly appear that there is. These doubts emerged and grew as a series of unhappy events unfolded in 2011: the Diederik Stapel fraud case (see Stroebe, Postmes, & Spears, 2012, this issue), the publication in a major social psychology journal of an article purporting to show evidence of extrasensory perception (Bem, 2011) followed by widespread public mockery (see Galak, LeBoeuf, Nelson, & Simmons, in press; Wagenmakers, Wetzels, Borsboom, & van der Maas, 2011), reports by Wicherts and colleagues that psychologists are often unwilling or unable to share their published data for reanalysis (Wicherts, Bakker, & Molenaar, 2011; see also Wicherts, Borsboom, Kats, & Molenaar, 2006), and the publication of an important article in Psychological Science showing how easily researchers can, in the absence of any real effects, nonetheless obtain statistically significant differences through various questionable research practices (QRPs) such as exploring multiple dependent variables or covariates and only reporting these when they yield significant results (Simmons, Nelson, & Simonsohn, 2011). For those psychologists who expected that the embarrassments of 2011 would soon recede into memory, 2012 offered instead a quick plunge from bad to worse, with new indications of outright fraud in the field of social cognition (Simonsohn, 2012), an article in Psychological Science showing that many psychologists admit to engaging in at least some of the QRPs examined by Simmons and colleagues (John, Loewenstein, & Prelec, 2012), troubling new meta-analytic evidence suggesting that the QRPs described by Simmons and colleagues may even be leaving telltale signs visible in the distribution of p values in the psychological literature (Masicampo & Lalande, in press; Simonsohn, 2012), and an acrimonious dust-up in science magazines and blogs centered around the problems some investigators were having in replicating well-known results from the field of social cognition (Bower, 2012; Yong, 2012). Although the very public problems experienced by psychology over this 2-year period are embarrassing to those of us working in the field, some have found comfort in the fact that, over the same period, similar concerns have been arising across the scientific landscape (triggered by revelations that will be described shortly). Some of the suspected causes of unreplicability, such as publication bias (the tendency to publish only positive findings) have been discussed for years; in fact, the phrase file-drawer problem was first coined by a distinguished psychologist several decades ago (Rosenthal, 1979). However, many have speculated that these problems have been exacerbated in recent years as academia reaps the harvest of a hypercompetitive academic climate and an incentive scheme that provides rich rewards for overselling one’s work and few rewards at all for caution and circumspection (see Giner-Sorolla, 2012, this issue). Equally disturbing, investigators seem to be replicating each others’ work even less often than they did in the past, again presumably reflecting an incentive scheme gone askew (a point discussed in several articles in this issue, e.g., Makel, Plucker, & Hegarty, 2012). The frequency with which errors appear in the psychological literature is not presently known, but a number of facts suggest it might be disturbingly high. Ioannidis (2005) has shown through simple mathematical modeling that any scientific field that ignores replication can easily come to the miserable state wherein (as the title of his most famous article puts it) “most published research findings are false” (see also Ioannidis, 2012, this issue, and Pashler & Harris, 2012, this issue). Meanwhile, reports emerging from cancer research have made such grim scenarios seem more plausible: In 2012, several large pharmaceutical companies revealed that their efforts to replicate exciting preclinical findings from published academic studies in cancer biology were only rarely verifying the original results (Begley & Ellis, 2012; see also Osherovich, 2011; Prinz, Schlange, & Asadullah, 2011).",fullPaper,jv96
p483,3954e2d220d9a7b7a46f9561cafb6251524d8ee5,c18,Conference on Innovative Data Systems Research,Mars Reconnaissance Orbiter's High Resolution Imaging Science Experiment (HiRISE),"[1] The HiRISE camera features a 0.5 m diameter primary mirror, 12 m effective focal length, and a focal plane system that can acquire images containing up to 28 Gb (gigabits) of data in as little as 6 seconds. HiRISE will provide detailed images (0.25 to 1.3 m/pixel) covering ∼1% of the Martian surface during the 2-year Primary Science Phase (PSP) beginning November 2006. Most images will include color data covering 20% of the potential field of view. A top priority is to acquire ∼1000 stereo pairs and apply precision geometric corrections to enable topographic measurements to better than 25 cm vertical precision. We expect to return more than 12 Tb of HiRISE data during the 2-year PSP, and use pixel binning, conversion from 14 to 8 bit values, and a lossless compression system to increase coverage. HiRISE images are acquired via 14 CCD detectors, each with 2 output channels, and with multiple choices for pixel binning and number of Time Delay and Integration lines. HiRISE will support Mars exploration by locating and characterizing past, present, and future landing sites, unsuccessful landing sites, and past and potentially future rover traverses. We will investigate cratering, volcanism, tectonism, hydrology, sedimentary processes, stratigraphy, aeolian processes, mass wasting, landscape evolution, seasonal processes, climate change, spectrophotometry, glacial and periglacial processes, polar geology, and regolith properties. An Internet Web site (HiWeb) will enable anyone in the world to suggest HiRISE targets on Mars and to easily locate, view, and download HiRISE data products.",poster,cp18
p484,e981f16fde9185373634b53d94baa1f9185ff890,c70,International Conference on Intelligent Robotics and Applications,A correlated topic model of Science,"Topic models, such as latent Dirichlet allocation (LDA), can be useful tools for the statistical analysis of document collections and other discrete data. The LDA model assumes that the words of each document arise from a mixture of topics, each of which is a distribution over the vocabulary. A limitation of LDA is the inability to model topic correlation even though, for example, a document about genetics is more likely to also be about disease than X-ray astronomy. This limitation stems from the use of the Dirichlet distribution to model the variability among the topic proportions. In this paper we develop the correlated topic model (CTM), where the topic proportions exhibit correlation via the logistic normal distribution [J. Roy. Statist. Soc. Ser. B 44 (1982) 139--177]. We derive a fast variational inference algorithm for approximate posterior inference in this model, which is complicated by the fact that the logistic normal is not conjugate to the multinomial. We apply the CTM to the articles from Science published from 1990--1999, a data set that comprises 57M words. The CTM gives a better fit of the data than LDA, and we demonstrate its use as an exploratory tool of large document collections.",poster,cp70
p485,bbe72eaaef4f03ea9bf1028379ee81ee88416dfe,j97,Science,U.S. science policy. Agencies rally to tackle big data.,"A federal effort is under way to improve the nation9s ability to manage, understand, and act upon the 1.2 zettabytes (1021) of electronic data generated each year. Its goal is to increase fundamental understanding of the technologies needed to manipulate and mine massive amounts of information; apply that knowledge to other scientific fields; address national goals in health, energy, defense, and education; and train more researchers to work with those technologies. The impetus for the initiative, to be managed by the Office of Science and Technology Policy, comes from a December 2010 report by a presidential task force that concluded the nation was ""underinvesting"" in the field. Computer scientists welcome the spotlight that the White House is shining on big-data research.",fullPaper,jv97
p486,7657d17d796659427e067f1cac93e4038d01725f,j138,IEEE Transactions on Big Data,Big Scholarly Data: A Survey,"With the rapid growth of digital publishing, harvesting, managing, and analyzing scholarly information have become increasingly challenging. The term Big Scholarly Data is coined for the rapidly growing scholarly data, which contains information including millions of authors, papers, citations, figures, tables, as well as scholarly networks and digital libraries. Nowadays, various scholarly data can be easily accessed and powerful data analysis technologies are being developed, which enable us to look into science itself with a new perspective. In this paper, we examine the background and state of the art of big scholarly data. We first introduce the background of scholarly data management and relevant technologies. Second, we review data analysis methods, such as statistical analysis, social network analysis, and content analysis for dealing with big scholarly data. Finally, we look into representative research issues in this area, including scientific impact evaluation, academic recommendation, and expert finding. For each issue, the background, main challenges, and latest research are covered. These discussions aim to provide a comprehensive review of this emerging area. This survey paper concludes with a discussion of open issues and promising future directions.",fullPaper,jv138
p487,9355e60deaad86d1efea4b7767dd77103d647f37,c16,Knowledge Discovery and Data Mining,Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,"It is our great pleasure to welcome you to the 2016 ACM Conference on Knowledge Discovery and Data Mining -- KDD'16. We hope that the content and the professional network at KDD'16 will help you succeed professionally by enabling you to: identify technology trends early; make new/creative contributions; increase your productivity by using newer/better tools, processes or ways of organizing teams; identify new job opportunities; and hire new team members. 
 
We are living in an exciting time for our profession. On the one hand, we are witnessing the industrialization of data science, and the emergence of the industrial assembly line processes characterized by the division of labor, integrated processes/pipelines of work, standards, automation, and repeatability. Data science practitioners are organizing themselves in more sophisticated ways, embedding themselves in larger teams in many industry verticals, improving their productivity substantially, and achieving a much larger scale of social impact. On the other hand we are also witnessing astonishing progress from research in algorithms and systems -- for example the field of deep neural networks has revolutionized speech recognition, NLP, computer vision, image recognition, etc. By facilitating interaction between practitioners at large companies & startups on the one hand, and the algorithm development researchers including leading academics on the other, KDD'16 fosters technological and entrepreneurial innovation in the area of data science. 
 
This year's conference continues its tradition of being the premier forum for presentation of results in the field of data mining, both in the form of cutting edge research, and in the form of insights from the development and deployment of real world applications. Further, the conference continues with its tradition of a strong tutorial and workshop program on leading edge issues of data mining. The mission of this conference has broadened in recent years even as we placed a significant amount of focus on both the research and applied aspects of data mining. As an example of this broadened focus, this year we have introduced a strong hands-on tutorial program nduring the conference in which participants will learn how to use practical tools for data mining. KDD'16 also gives researchers and practitioners a unique opportunity to form professional networks, and to share their perspectives with others interested in the various aspects of data mining. For example, we have introduced office hours for budding entrepreneurs from our community to meet leading Venture Capitalists investing in this area. We hope that KDD 2016 conference will serve as a meeting ground for researchers, practitioners, funding agencies, and investors to help create new algorithms and commercial products. 
 
The call for papers attracted a significant number of submissions from countries all over the world. In particular, the research track attracted 784 submissions and the applied data science track attracted 331 submissions. Papers were accepted either as full papers or as posters. The overall acceptance rate either as full papers or posters was less than 20%. For full papers in the research track, the acceptance rate was lower than 10%. This is consistent with the fact that the KDD Conference is a premier conference in data mining and the acceptance rates historically tend to be low. It is noteworthy that the applied data science track received a larger number of submissions compared to previous years. We view this as an encouraging sign that research in data mining is increasingly becoming relevant to industrial applications. All papers were reviewed by at least three program committee members and then discussed by the PC members in a discussion moderated by a meta-reviewer. Borderline papers were thoroughly reviewed by the program chairs before final decisions were made.",fullPaper,cp16
p488,5724121504a3cc1ed648dd61bc980ffa7c4e1fb5,c28,International Conference on Collaboration Technologies and Systems,Addressing big data issues in Scientific Data Infrastructure,"Big Data are becoming a new technology focus both in science and in industry. This paper discusses the challenges that are imposed by Big Data on the modern and future Scientific Data Infrastructure (SDI). The paper discusses a nature and definition of Big Data that include such features as Volume, Velocity, Variety, Value and Veracity. The paper refers to different scientific communities to define requirements on data management, access control and security. The paper introduces the Scientific Data Lifecycle Management (SDLM) model that includes all the major stages and reflects specifics in data management in modern e-Science. The paper proposes the SDI generic architecture model that provides a basis for building interoperable data or project centric SDI using modern technologies and best practices. The paper explains how the proposed models SDLM and SDI can be naturally implemented using modern cloud based infrastructure services provisioning model and suggests the major infrastructure components for Big Data.",fullPaper,cp28
p489,a2b5e0c1d0b23d11fd8497f1b16fc9564246b482,c54,International Workshop on Agent-Oriented Software Engineering,The National Institutes of Health's Big Data to Knowledge (BD2K) initiative: capitalizing on biomedical big data,"Biomedical research has and will continue to generate large amounts of data (termed ‘big data’) in many formats and at all levels. Consequently, there is an increasing need to better understand and mine the data to further knowledge and foster new discovery. The National Institutes of Health (NIH) has initiated a Big Data to Knowledge (BD2K) initiative to maximize the use of biomedical big data. BD2K seeks to better define how to extract value from the data, both for the individual investigator and the overall research community, create the analytic tools needed to enhance utility of the data, provide the next generation of trained personnel, and develop data science concepts and tools that can be made available to all stakeholders.",poster,cp54
p490,306d35707288caafb27e4c81d33eb83c0384f204,c37,Asia-Pacific Software Engineering Conference,"Statistics : methods and applications : a comprehensive reference for science, industry, and data mining",Elementary concepts in statistics -- Basic statistics and tables -- ANOVA/MANOVA -- Association rules -- Boosting trees -- Canonical analysis -- CHAID analysis -- Classification and regression trees (CART) -- Classification trees -- Cluster analysis -- Correspondence analysis -- Data mining techniques -- Discriminant function analysis -- Distribution fitting -- Experimental design (Industrial DOE) -- Factor analysis and principal components -- General discrimination analysis (GDA) -- General linear models (GLM) -- General regression models (GRM) -- Generalized additive models (GAM) -- Generalized linear/nonlinear models (GLZ) -- Log linear analysis of frequency tables -- Machine learning -- Multivariate adaptive regression splines (MARSplines) -- Multidimensional scaling (MDS) -- Multiple linear regression -- Neural networks -- Nonlinear estimation -- Nonparametric statistics -- Partial least squares (PLS) -- Power analysis -- Process analysis -- Quality control charts -- Reliabilty/item analysis -- Structural equation modeling -- Survival/failure time analysis -- Text mining -- Time series/forecasting -- Variance components and mixed model ANOVA/ANCOVA.,poster,cp37
p491,cfa6fd6a3979fcd3cb3a5436405cc46ef8250cd4,c39,International Conference on Global Software Engineering,Data Products,Abstract content,poster,cp39
p492,abd6905fec5e1323dedb1311dbb7885e836c1877,j97,Science,Response to Comment on “Estimating the reproducibility of psychological science”,"Gilbert et al. conclude that evidence from the Open Science Collaboration’s Reproducibility Project: Psychology indicates high reproducibility, given the study methodology. Their very optimistic assessment is limited by statistical misconceptions and by causal inferences from selectively interpreted, correlational data. Using the Reproducibility Project: Psychology data, both optimistic and pessimistic conclusions about reproducibility are possible, and neither are yet warranted.",fullPaper,jv97
p493,579c2aad3834e525a90740913fb58a8c8e9ef218,c95,IEEE International Conference on Computer Vision,Big Data Methods,"Advances in data science, such as data mining, data visualization, and machine learning, are extremely well-suited to address numerous questions in the organizational sciences given the explosion of available data. Despite these opportunities, few scholars in our field have discussed the specific ways in which the lens of our science should be brought to bear on the topic of big data and big data's reciprocal impact on our science. The purpose of this paper is to provide an overview of the big data phenomenon and its potential for impacting organizational science in both positive and negative ways. We identifying the biggest opportunities afforded by big data along with the biggest obstacles, and we discuss specifically how we think our methods will be most impacted by the data analytics movement. We also provide a list of resources to help interested readers incorporate big data methods into their existing research. Our hope is that we stimulate interest in big data, motivate future research using big data sources, and encourage the application of associated data science techniques more broadly in the organizational sciences.",poster,cp95
p494,98dc9453cbe45d4a6bfb46bcb340307a4ec467ec,j103,Conservation Biology,Eliciting Expert Knowledge in Conservation Science,"Abstract:  Expert knowledge is used widely in the science and practice of conservation because of the complexity of problems, relative lack of data, and the imminent nature of many conservation decisions. Expert knowledge is substantive information on a particular topic that is not widely known by others. An expert is someone who holds this knowledge and who is often deferred to in its interpretation. We refer to predictions by experts of what may happen in a particular context as expert judgments. In general, an expert‐elicitation approach consists of five steps: deciding how information will be used, determining what to elicit, designing the elicitation process, performing the elicitation, and translating the elicited information into quantitative statements that can be used in a model or directly to make decisions. This last step is known as encoding. Some of the considerations in eliciting expert knowledge include determining how to work with multiple experts and how to combine multiple judgments, minimizing bias in the elicited information, and verifying the accuracy of expert information. We highlight structured elicitation techniques that, if adopted, will improve the accuracy and information content of expert judgment and ensure uncertainty is captured accurately. We suggest four aspects of an expert elicitation exercise be examined to determine its comprehensiveness and effectiveness: study design and context, elicitation design, elicitation method, and elicitation output. Just as the reliability of empirical data depends on the rigor with which it was acquired so too does that of expert knowledge.",fullPaper,jv103
p495,825725943fe1774b1b490d69094ba6269cc9c6b2,c75,International Conference on Machine Learning,SciMAT: A new science mapping analysis software tool,"This article presents a new open-source software tool, SciMAT, which performs science mapping analysis within a longitudinal framework. It provides different modules that help the analyst to carry out all the steps of the science mapping workflow. In addition, SciMAT presents three key features that are remarkable in respect to other science mapping software tools: (a) a powerful preprocessing module to clean the raw bibliographical data, (b) the use of bibliometric measures to study the impact of each studied element, and (c) a wizard to configure the analysis. © 2012 Wiley Periodicals, Inc.",poster,cp75
p496,03c8beceda4a4a94fe71fe9e09bf53fee024c5e2,j139,Light: Science & Applications,Optical storage arrays: a perspective for future big data storage,Abstract content,fullPaper,jv139
p497,1fae0c586f66a9f08fdc76b8bf38ff27a7a08dc4,c10,Big Data,Enhancing the quality of argumentation in school science,"The research reported in this paper focussed on the design of learning environments that support the teaching and learning of argumentation in a scientific context. The research took place over two years between 1999 and 2001 in junior high schools in the greater London area. The research was conducted in two phases. In the first developmental phase, working with a group of 12 science teachers, the main emphasis was to develop sets of materials and strategies to support argumentation in the classroom and to assess teachers‘ development with teaching argumentation. Data were collected by videoing and audio recording the teachers attempts to implement these lessons at the beginning and end of the year. During this phase, analytical tools for evaluating the quality of argumentation were developed based on Toulmin‘s argument pattern. Analysis of the data shows that there was significant development in the majority of teachers use of argumentation across the year. Results indicate that the pattern of use of argumentation is teacher specific, as is the nature of the change. In the second phase of the project, teachers taught the experimental groups a minimum of nine lessons which involved socioscientific or scientific argumentation. In addition, these teachers taught similar lessons to a control group at the beginning and end of the year. Here the emphasis lay on assessing the progression in student capabilities with argumentation. Hence data were collected from several lessons of two groups of students engaging in argumentation. Using a framework for evaluating the nature of the discourse and its quality, the findings show that there was an improvement in the quality of students‘ argumentation. In addition, the research offers methodological developments for work in this field.",poster,cp10
p498,a5df985921edc015bfd14ed827a0c1d3383ce90d,j140,Community Dentistry and Oral Epidemiology,Causal inference from observational data.,"Randomized controlled trials have long been considered the 'gold standard' for causal inference in clinical research. In the absence of randomized experiments, identification of reliable intervention points to improve oral health is often perceived as a challenge. But other fields of science, such as social science, have always been challenged by ethical constraints to conducting randomized controlled trials. Methods have been established to make causal inference using observational data, and these methods are becoming increasingly relevant in clinical medicine, health policy and public health research. This study provides an overview of state-of-the-art methods specifically designed for causal inference in observational data, including difference-in-differences (DiD) analyses, instrumental variables (IV), regression discontinuity designs (RDD) and fixed-effects panel data analysis. The described methods may be particularly useful in dental research, not least because of the increasing availability of routinely collected administrative data and electronic health records ('big data').",fullPaper,jv140
p499,7e2ca851bffcc47713f35b4b0d4b91dcfc0773fa,c45,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies","A Vast Machine: Computer Models, Climate Data, and the Politics of Global Warming","Global warming skeptics often fall back on the argument that the scientific case for global warming is all model predictions, nothing but simulation; they warn us that we need to wait for real data, ""sound science."" In A Vast Machine Paul Edwards has news for these skeptics: without models, there are no data. Today, no collection of signals or observationseven from satellites, which can ""see"" the whole planet with a single instrumentbecomes global in time and space without passing through a series of data models. Everything we know about the world's climate we know through models. Edwards offers an engaging and innovative history of how scientists learned to understand the atmosphereto measure it, trace its past, and model its future. Edwards argues that all our knowledge about climate change comes from three kinds of computer models: simulation models of weather and climate; reanalysis models, which recreate climate history from historical weather data; and data models, used to combine and adjust measurements from many different sources. Meteorology creates knowledge through an infrastructure (weather stations and other data platforms) that covers the whole world, making global data. This infrastructure generates information so vast in quantity and so diverse in quality and form that it can be understood only by computer analysismaking data global. Edwards describes the science behind the scientific consensus on climate change, arguing that over the years data and models have converged to create a stable, reliable, and trustworthy basis for establishing the reality of global warming.",poster,cp45
p500,55bdaa9d27ed595e2ccf34b3a7847020cc9c946c,c29,International Conference on Software Engineering,Performing systematic literature reviews in software engineering,"Context: Making best use of the growing number of empirical studies in Software Engineering, for making decisions and formulating research questions, requires the ability to construct an objective summary of available research evidence. Adopting a systematic approach to assessing and aggregating the outcomes from a set of empirical studies is also particularly important in Software Engineering, given that such studies may employ very different experimental forms and be undertaken in very different experimental contexts.Objectives: To provide an introduction to the role, form and processes involved in performing Systematic Literature Reviews. After the tutorial, participants should be able to read and use such reviews, and have gained the knowledge needed to conduct systematic reviews of their own.Method: We will use a blend of information presentation (including some experiences of the problems that can arise in the Software Engineering domain), and also of interactive working, using review material prepared in advance.",fullPaper,cp29
p501,27e57cc2f22c1921d2a1c3954d5062e3fe391553,j141,Empirical Software Engineering,Guidelines for conducting and reporting case study research in software engineering,Abstract content,fullPaper,jv141
p502,72910077a29caf411dbb03148997c72b47e65ab0,j142,IEEE Transactions on Software Engineering,Software Engineering Economics,"This paper summarizes the current state of the art and recent trends in software engineering economics. It provides an overview of economic analysis techniques and their applicability to software engineering and management. It surveys the field of software cost estimation, including the major estimation techniques available, the state of the art in algorithmic cost models, and the outstanding research issues in software cost estimation.",fullPaper,jv142
p503,d0bc1501ae6f54dd16534e651d90d2aeeeb1cfc1,c30,IEEE Aerospace Conference,Software engineering: What is it?,"In spite of many years of work by a multitude of organizations, a clear and simple standard for software engineering and management requirements and a method to assess their applicability to projects of various types and sizes remains elusive. From IEEE to CMMI to NASA's NPR 7150.2, there is no shortage of sources of information providing various types of requirements and standards for software engineering. Even a book on software project management for “dummies” approaches 400 pages. Wading through this information can dizzy the mind of even the most experienced software engineer; the newbie just trying to “do the right thing” will probably give up, open a text editor and start coding. This lack of clarity and simplicity perhaps goes a long way towards explaining why, in spite of this large body of work, there remains such an incredible variability in the knowledge and application of software engineering discipline not only from one organization to the next, but between groups within the same organization, or even between individual developers in the same group! Surely at least the basics of what should be done and why those things should be done can be conveyed in less than a novel-sized volume. There must be some timeless principles that cut across structured and object-oriented techniques, waterfall and agile methods, and CMMI and NASA standards. To properly interpret software engineering requirements and approaches and successfully (and selectively) apply them, one must first understand them at a fundamental level and how they can benefit the project. This paper will make an admittedly bold and brash attempt to boil it all down into something anyone can understand, hopefully resulting in a brief reference — a type of lens through which existing standards can be more practically viewed.",fullPaper,cp30
p504,81dbfc1bc890368979399874e47e0529ddceaece,c82,Workshop on Interdisciplinary Software Engineering Research,Software Engineering: A Practitioner's Approach,Abstract content,poster,cp82
p505,f70b2f20be241f445a61f33c4b8e76e554760340,c25,International Conference on Contemporary Computing,Software Engineering for Machine Learning: A Case Study,"Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workflow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workflow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components - models may be ""entangled"" in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations.",poster,cp25
p506,0961e2650b3a62a1d198a046bef5f0700ab8c08f,c31,International Conference on Evaluation & Assessment in Software Engineering,Guidelines for snowballing in systematic literature studies and a replication in software engineering,"Background: Systematic literature studies have become common in software engineering, and hence it is important to understand how to conduct them efficiently and reliably.
 Objective: This paper presents guidelines for conducting literature reviews using a snowballing approach, and they are illustrated and evaluated by replicating a published systematic literature review.
 Method: The guidelines are based on the experience from conducting several systematic literature reviews and experimenting with different approaches.
 Results: The guidelines for using snowballing as a way to search for relevant literature was successfully applied to a systematic literature review.
 Conclusions: It is concluded that using snowballing, as a first search strategy, may very well be a good alternative to the use of database searches.",fullPaper,cp31
p507,0d091c5aaf3d2df349692b13758045368d1b9a37,j143,Informatik-Spektrum,Software Engineering,Abstract content,fullPaper,jv143
p508,849a6be5adf1b9a2b6e59ba0290bca06692c0efd,j141,Empirical Software Engineering,Sampling in software engineering research: a critical review and guidelines,Abstract content,fullPaper,jv141
p509,75f3726a39b563c9890fb8ed7dd393da15ad6594,c32,International Conference on Software Technology: Methods and Tools,Object-oriented software engineering - a use case driven approach,Part 1. Introduction 1. System development as an industrial process Introduction A useful analogy System development characteristics Summary 2. The system life cycle Introduction System development as a process of change System development and reuse System development and methodology Objectory Summary 3. What is object-orientation? Introduction Object Class andinstance Polymorphism Inheritance Summary 4. Object-oriented system development Introduction Function/data methods Object-oriented analysis Object-oriented construction Object-oriented testing Summary 5. Object-oriented programming Introduction Objects Classes and instances Inheritance Polymorphism An example Summary Part II. Concepts 6. Architecture Introduction System development is model building Model architecture Requirements model Analysis model The design model The implementation model Test model Summary 7. Analysis Introduction The requirements model The analysis model Summary 8. Construction Introduction The design model Block design Working with construction Summary 9. Real-time specialization Introduction Classification of real-time systems Fundamental issues Analysis Construction Testing and verification Summary 10. Database Specialization Introduction Relational DBMS Object DBMS Discussion Summary 11. Components Introduction What is a component? Use of components Component management Summary 12. Testing Introduction On testing Unit testing Integration testing System testing The testing process Summary Part III. Applications 13. Case study: warehouse management system Introduction to the examples ACME Warehouse Management Inc. The requirements model The analysis model Construction 14. Case study: telecom Introduction Telecommunication switching systems The requirements model The analysis model The design model The implementation model 15. Managing object-oriented software engineering Introduction Project selection and preparation Project development organization Project organization and management Project staffing Software quality assurance Software metrics Summary 16. Other object-oriented methods Introduction A summary of object-oriented methods Object-Oriented Analysis (OOAD/Coad-Yourdon) Object-Oriented Design (OOD/Booch) Hierarchical Object-Oriented Design (HOOD) Object Modeling Technique (OMT) Responsibility-Driven Design Summary Appendix A On the development of Objectory Introduction Objectory as an activity From idea to reality References Index,fullPaper,cp32
p510,130862d54894966552cb85d3ee6f739f885d4989,c22,International Conference on Data Technologies and Applications,Model-Driven Software Engineering in Practice,"This book discusses how model-based approaches can improve the daily practice of software professionals. This is known as Model-Driven Software Engineering (MDSE) or, simply, Model-Driven Engineering (MDE). MDSE practices have proved to increase efficiency and effectiveness in software development, as demonstrated by various quantitative and qualitative studies. MDSE adoption in the software industry is foreseen to grow exponentially in the near future, e.g., due to the convergence of software development and business analysis. The aim of this book is to provide you with an agile and flexible tool to introduce you to the MDSE world, thus allowing you to quickly understand its basic principles and techniques and to choose the right set of MDSE instruments for your needs so that you can start to benefit from MDSE right away. The book is organized into two main parts. The first part discusses the foundations of MDSE in terms of basic concepts (i.e., models and transformations), driving principles, application scenarios and current standards, like the well-known MDA initiative proposed by OMG (Object Management Group) as well as the practices on how to integrate MDSE in existing development processes. The second part deals with the technical aspects of MDSE, spanning from the basics on when and how to build a domain-specific modeling language, to the description of Model-to-Text and Model-to-Model transformations, and the tools that support the management of MDSE projects. The book is targeted to a diverse set of readers, spanning: professionals, CTOs, CIOs, and team managers that need to have a bird's eye vision on the matter, so as to take the appropriate decisions when it comes to choosing the best development techniques for their company or team; software analysts, developers, or designers that expect to use MDSE for improving everyday work productivity, either by applying the basic modeling techniques and notations or by defining new domain-specific modeling languages and applying end-to-end MDSE practices in the software factory; and academic teachers and students to address undergrad and postgrad courses on MDSE. In addition to the contents of the book, more resources are provided on the book's website http://www.mdse-book.com/, including the examples presented in the book. Table of Contents: Introduction / MDSE Principles / MDSE Use Cases / Model-Driven Architecture (MDA) / Integration of MDSE in your Development Process / Modeling Languages at a Glance / Developing your Own Modeling Language / Model-to-Model Transformations / Model-to-Text Transformations / Managing Models / Summary",poster,cp22
p511,2bd576ce574df33c834b6032962cd5ae0be5299f,j144,Information and Software Technology,Guidelines for conducting systematic mapping studies in software engineering: An update,Abstract content,fullPaper,jv144
p512,947b29eb3cde8ccb8df9342bb2384ec480ea3964,c61,Jahrestagung der Gesellschaft für Informatik,Experimentation in software engineering: an introduction,Abstract content,poster,cp61
p513,e28bdc373de80d7ec0e64631a89e64fbdcdae230,c31,International Conference on Evaluation & Assessment in Software Engineering,Systematic Mapping Studies in Software Engineering,"BACKGROUND: A software engineering systematic map is a defined method to build a classification scheme and structure a software engineering field of interest. The analysis of results focuses on frequencies of publications for categories within the scheme. Thereby, the coverage of the research field can be determined. Different facets of the scheme can also be combined to answer more specific research questions. 
 
OBJECTIVE: We describe how to conduct a systematic mapping study in software engineering and provide guidelines. We also compare systematic maps and systematic reviews to clarify how to chose between them. This comparison leads to a set of guidelines for systematic maps. 
 
METHOD: We have defined a systematic mapping process and applied it to complete a systematic mapping study. Furthermore, we compare systematic maps with systematic reviews by systematically analyzing existing systematic reviews. 
 
RESULTS: We describe a process for software engineering systematic mapping studies and compare it to systematic reviews. Based on this, guidelines for conducting systematic maps are defined. 
 
CONCLUSIONS: Systematic maps and reviews are different in terms of goals, breadth, validity issues and implications. Thus, they should be used complementarily and require different methods (e.g., for analysis).",fullPaper,cp31
p514,a963d05b9d4acd347ad528e7d098eb53d8f555a2,j144,Information and Software Technology,Systematic literature reviews in software engineering - A systematic literature review,Abstract content,fullPaper,jv144
p515,21c6beb2a6df81f424e3d1283fbb9cc3157a3115,c33,International Conference on Agile Software Development,A Taxonomy of Software Engineering Challenges for Machine Learning Systems: An Empirical Investigation,Abstract content,fullPaper,cp33
p516,967f4eb786aa143b7eb09f00d9ba8ddfe44e039f,c29,International Conference on Software Engineering,Sentiment Analysis for Software Engineering: How Far Can We Go?,"Sentiment analysis has been applied to various software engineering (SE) tasks, such as evaluating app reviews or analyzing developers' emotions in commit messages. Studies indicate that sentiment analysis tools provide unreliable results when used out-of-the-box, since they are not designed to process SE datasets. The silver bullet for a successful application of sentiment analysis tools to SE datasets might be their customization to the specific usage context. We describe our experience in building a software library recommender exploiting crowdsourced opinions mined from Stack Overflow (e.g., what is the sentiment of developers about the usability of a library). To reach our goal, we retrained—on a set of 40k manually labeled sentences/words extracted from Stack Overflow—a state-of-the-art sentiment analysis tool exploiting deep learning. Despite such an effort- and time-consuming training process, the results were negative. We changed our focus and performed a thorough investigation of the accuracy of these tools on a variety of SE datasets. Our results should warn the research community about the strong limitations of current sentiment analysis tools.",fullPaper,cp29
p517,247e2ee1a84e25cdc2d0d68811e2ee05ca0bc6a9,j141,Empirical Software Engineering,Software engineering in start-up companies: An analysis of 88 experience reports,Abstract content,fullPaper,jv141
p518,f463018624b6f4b8dd576732b6cce36e31bac978,c6,Americas Conference on Information Systems,Software Engineering of Self-adaptive Systems,Abstract content,poster,cp6
p519,bca7c0902f600fa77b1e16d0e093e23f7d75f649,j141,Empirical Software Engineering,Empirical software engineering experts on the use of students and professionals in experiments,Abstract content,fullPaper,jv141
p520,fb2bb5777f1b1bd745070c006265edf8feb5f29f,c56,European Conference on Software Process Improvement,Smart contracts vulnerabilities: a call for blockchain software engineering?,"Smart Contracts have gained tremendous popularity in the past few years, to the point that billions of US Dollars are currently exchanged every day through such technology. However, since the release of the Frontier network of Ethereum in 2015, there have been many cases in which the execution of Smart Contracts managing Ether coins has led to problems or conflicts. Compared to traditional Software Engineering, a discipline of Smart Contract and Blockchain programming, with standardized best practices that can help solve the mentioned problems and conflicts, is not yet sufficiently developed. Furthermore, Smart Contracts rely on a non-standard software life-cycle, according to which, for instance, delivered applications can hardly be updated or bugs resolved by releasing a new version of the software. In this paper we advocate the need for a discipline of Blockchain Software Engineering, addressing the issues posed by smart contract programming and other applications running on blockchains.We analyse a case of study where a bug discovered in a Smart Contract library, and perhaps ""unsafe"" programming, allowed an attack on Parity, a wallet application, causing the freezing of about 500K Ethers (about 150M USD, in November 2017). In this study we analyze the source code of Parity and the library, and discuss how recognised best practices could mitigate, if adopted and adapted, such detrimental software misbehavior. We also reflect on the specificity of Smart Contract software development, which makes some of the existing approaches insufficient, and call for the definition of a specific Blockchain Software Engineering.",poster,cp56
p521,ed0759b7001f8be53bb4282750e98198b359307d,j79,Computer,No Silver Bullet Essence and Accidents of Software Engineering,"But, as we look to the horizon of a decade hence, we see no silver bullet. There is no single development, in either technology or in management technique, that by itself promises even one order-of-magnitude improvement in productivity, in reliability, in simplicity. In this article, I shall try to show why, by examining both the nature of the software problem and the properties of the bullets proposed.",fullPaper,jv79
p522,30b71975e26dd2709f58372419b712d97536402f,j145,Journal of Systems and Software,Continuous software engineering: A roadmap and agenda,Abstract content,fullPaper,jv145
p523,51b502b9ce774a615474ed8629e74d0dfaa33ee3,j146,ACM Transactions on Software Engineering and Methodology,The ABC of Software Engineering Research,"A variety of research methods and techniques are available to SE researchers, and while several overviews exist, there is consistency neither in the research methods covered nor in the terminology used. Furthermore, research is sometimes critically reviewed for characteristics inherent to the methods. We adopt a taxonomy from the social sciences, termed here the ABC framework for SE research, which offers a holistic view of eight archetypal research strategies. ABC refers to the research goal that strives for generalizability over Actors (A) and precise measurement of their Behavior (B), in a realistic Context (C). The ABC framework uses two dimensions widely considered to be key in research design: the level of obtrusiveness of the research and the generalizability of research findings. We discuss metaphors for each strategy and their inherent limitations and potential strengths. We illustrate these research strategies in two key SE domains, global software engineering and requirements engineering, and apply the framework on a sample of 75 articles. Finally, we discuss six ways in which the framework can advance SE research.",fullPaper,jv146
p524,6f58d8b98c652897842afdd023c535f9724b4eb2,c34,IEEE Working Conference on Mining Software Repositories,A Benchmark Study on Sentiment Analysis for Software Engineering Research,"A recent research trend has emerged to identify developers' emotions, by applying sentiment analysis to the content of communication traces left in collaborative development environments. Trying to overcome the limitations posed by using off-the-shelf sentiment analysis tools, researchers recently started to develop their own tools for the software engineering domain. In this paper, we report a benchmark study to assess the performance and reliability of three sentiment analysis tools specifically customized for software engineering. Furthermore, we offer a reflection on the open challenges, as they emerge from a qualitative analysis of misclassified texts.",fullPaper,cp34
p525,6e1cafd50333b3812bf002a51bcb1f720e35b7ed,c34,IEEE Working Conference on Mining Software Repositories,Word Embeddings for the Software Engineering Domain,"The software development process produces vast amounts of textual data expressed in natural language. Outcomes from the natural language processing community have been adapted in software engineering research for leveraging this rich textual information; these include methods and readily available tools, often furnished with pretrained models. State of the art pretrained models however, capture general, common sense knowledge, with limited value when it comes to handling data specific to a specialized domain. There is currently a lack of domain-specific pretrained models that would further enhance the processing of natural language artefacts related to software engineering. To this end, we release a word2vec model trained over 15GB of textual data from Stack Overflow posts. We illustrate how the model disambiguates polysemous words by interpreting them within their software engineering context. In addition, we present examples of fine-grained semantics captured by the model, that imply transferability of these results to diverse, targeted information retrieval tasks in software engineering and motivate for further reuse of the model.",fullPaper,cp34
p526,843964eecc10b28eff53f2d4a9db4e112a38e94e,c65,Formal Concept Analysis,Introduction to Software Engineering,"INTRODUCTION The Need for Software Engineering Are Software Teams Really Necessary? Software Engineering Software Lifecycles Different Views of Software Engineering Activities Software Engineering as an Emerging Discipline Some Techniques of Software Engineering Standards Commonly Used for Software Development Processes The Year 2000 Problem and Similar Problems Organization of the Book PROJECT MANAGEMENT Sub-Teams Needed in Software Engineering Projects The Nature of Project Teams Project Management Software Project Estimation Project Scheduling Project Measurement Project Management Tools The Role of Networks in Project Management Groupware An Example: Project Management for a Year 2000 Conversion Project REQUIREMENTS Some Problems with Requirements Determination Requirements Elicitation Requirements Traceability Software Architectures and Requirements Reengineering System Requirements Assessment of Feasibility of System Requirements Usability Requirements Specifying Requirements Using State Diagrams and Decision Tables Specifying Requirements Using Petri Nets Ethical Issues Some Metrics for Requirements The Requirements Review The Major Project - Problem Statement The Major Project - Requirements Elicitation The Major Software Project - Requirements Analysis SOFTWARE DESIGN Introduction Software Design Patterns Introduction to Software Design Representations Procedurally-Oriented Design Representations Software Architectures Software Design Principles for Procedurally-Oriented Programs What is an Object? Object-Oriented Design Representations Software Design Principles for Object-Oriented Programs Class Design Issues An Example of Class Development - The String Class User Interfaces Software Interfaces Some Metrics for Design Design Reviews A Manager's Viewpoint of Design Architecture of the Major Software Engineering Project Preliminary Design of the Major Software Project Subsystem Design of the Major Software Project Detailed Design for the Major Software Project CODING The Choice of Programming Language Coding Styles Coding Standards Coding, Design, Requirements, and Change Some Coding Metrics Coding Reviews and Inspections Configuration Management A Management Perspective on Coding Coding of the Major Software Project TESTING AND INTEGRATION Types of Software Testing Black-Box Module Testing White-Box Module Testing Reducing the Number of Test Cases by Effective Test Strategies Testing Objects for Encapsulation and Completeness Testing Objects with Inheritance General Testing Issues for Object-Oriented Software Test Plans Software Integration Managing Change in the Integration Process Performance and Stress Testing Quality Assurance Software Reliability A Manager's Viewpoint on Testing and Integration Testing the Major Software Project Integrating the Major Software Project DELIVERY, INSTALLATION, AND DOCUMENTATION Delivery Installation Internal Documentation External Documentation Design Rationales Installation, User, Training, and Operations Manuals On-Line Documentation Reading Levels A Manager's View of Delivery, Installation, and Documentation Delivery, Installation, and Documentation of the Major Software Project MAINTENANCE Introduction Corrective Software Maintenance Adaptive Software Maintenance Preventative Software Maintenance and the Year 2000 Problem How to Read Requirements, Designs, and Source Code A Manager's Perspective on Software Maintenance Maintenance of the Major Software Project RESEARCH ISSUES IN SOFTWARE ENGINEERING Some Important Research Problems in Software Engineering How to Read the Software Engineering Research Literature APPENDIX: COMMAND-LINE ARGUMENTS REFERENCES",poster,cp65
p527,cc3d1830b5220b68d8e785130e4b0d5ab7563d67,j147,IEEE Software,Software Engineering for Machine-Learning Applications: The Road Ahead,"The First Symposium on Software Engineering for Machine Learning Applications (SEMLA) aimed to create a space in which machine learning (ML) and software engineering (SE) experts could come together to discuss challenges, new insights, and practical ideas regarding the engineering of ML and AI-based systems. Key challenges discussed included the accuracy of systems built using ML and AI models, the testing of those systems, industrial applications of AI, and the rift between the ML and SE communities. This article is part of a theme issue on software engineering’s 50th anniversary.",fullPaper,jv147
p528,a75965753906c24a6a5715a695757a0de8447f26,j142,IEEE Transactions on Software Engineering,A Survey of App Store Analysis for Software Engineering,"App Store Analysis studies information about applications obtained from app stores. App stores provide a wealth of information derived from users that would not exist had the applications been distributed via previous software deployment methods. App Store Analysis combines this non-technical information with technical information to learn trends and behaviours within these forms of software repositories. Findings from App Store Analysis have a direct and actionable impact on the software teams that develop software for app stores, and have led to techniques for requirements engineering, release planning, software design, security and testing. This survey describes and compares the areas of research that have been explored thus far, drawing out common aspects, trends and directions future research should take to address open problems and challenges.",fullPaper,jv142
p529,18fdedc5d614ba0429314ebe824edbfc63ddb05a,j144,Information and Software Technology,Guidelines for including the grey literature and conducting multivocal literature reviews in software engineering,Abstract content,fullPaper,jv144
p530,11de94b684fa2eb4dffe4b99d959888382c6abfb,c35,EUROMICRO Conference on Software Engineering and Advanced Applications,Software Engineering Challenges of Deep Learning,"Surprisingly promising results have been achieved by deep learning (DL) systems in recent years. Many of these achievements have been reached in academic settings, or by large technology companies with highly skilled research groups and advanced supporting infrastructure. For companies without large research groups or advanced infrastructure, building high-quality production-ready systems with DL components has proven challenging. There is a clear lack of well-functioning tools and best practices for building DL systems. It is the goal of this research to identify what the main challenges are, by applying an interpretive research approach in close collaboration with companies of varying size and type. A set of seven projects have been selected to describe the potential with this new technology and to identify associated main challenges. A set of 12 main challenges has been identified and categorized into the three areas of development, production, and organizational challenges. Furthermore, a mapping between the challenges and the projects is defined, together with selected motivating descriptions of how and why the challenges apply to specific projects. Compared to other areas such as software engineering or database technologies, it is clear that DL is still rather immature and in need of further work to facilitate development of high-quality systems. The challenges identified in this paper can be used to guide future research by the software engineering and DL communities. Together, we could enable a large number of companies to start taking advantage of the high potential of the DL technology.",fullPaper,cp35
p531,11ebce51f6b0a901948953a024abf1cdb9111f3f,c74,IEEE International Conference on Tools with Artificial Intelligence,Design Science Methodology for Information Systems and Software Engineering,Abstract content,poster,cp74
p532,bccd2ffa530097e7617c35178de641f1295d2748,j147,IEEE Software,The History of Software Engineering,"Grady Booch, one of UML’s original authors, offers his perspective on the history of software engineering. This article is part of a theme issue on software engineering’s 50th anniversary. The Web Extra, a version of the article with an expanded bibliography, is at https://extras.computer.org/extra/mso2018050108s1.pdf.",fullPaper,jv147
p533,2b19768afa6fbf5abb19790cd1ee991574129933,j145,Journal of Systems and Software,A survey of the use of crowdsourcing in software engineering,Abstract content,fullPaper,jv145
p534,3f0b37f26829684b8417309577d77f3af6534707,j75,Lecture Notes in Computer Science,Fundamentals of Software Engineering,Abstract content,fullPaper,jv75
p535,bcc7041e0fb7717a7d67a9c00e08b7fb81384cbf,j141,Empirical Software Engineering,Robust Statistical Methods for Empirical Software Engineering,Abstract content,fullPaper,jv141
p536,8fe2f59ff3733f9ee50ffa295beda502f4e268e2,c29,International Conference on Software Engineering,Grounded Theory in Software Engineering Research: A Critical Review and Guidelines,"Grounded Theory (GT) has proved an extremely useful research approach in several fields including medical sociology, nursing, education and management theory. However, GT is a complex method based on an inductive paradigm that is fundamentally different from the traditional hypothetico-deductive research model. As there are at least three variants of GT, some ostensibly GT research suffers from method slurring, where researchers adopt an arbitrary subset of GT practices that are not recognizable as GT. In this paper, we describe the variants of GT and identify the core set of GT practices. We then analyze the use of grounded theory in software engineering. We carefully and systematically selected 98 articles that mention GT, of which 52 explicitly claim to use GT, with the other 46 using GT techniques only. Only 16 articles provide detailed accounts of their research procedures. We offer guidelines to improve the quality of both conducting and reporting GT studies. The latter is an important extension since current GT guidelines in software engineering do not cover the reporting process, despite good reporting being necessary for evaluating a study and informing subsequent research.",fullPaper,cp29
p537,63b8ebfe57af400c8bbadc5c111cb5fe71f331bd,j141,Empirical Software Engineering,On negative results when using sentiment analysis tools for software engineering research,Abstract content,fullPaper,jv141
p538,261043d80a2a66044e1ee0eaee46c7331687afa4,c34,IEEE Working Conference on Mining Software Repositories,Leveraging Automated Sentiment Analysis in Software Engineering,"Automated sentiment analysis in software engineering textual artifacts has long been suffering from inaccuracies in those few tools available for the purpose. We conduct an in-depth qualitative study to identify the difficulties responsible for such low accuracy. Majority of the exposed difficulties are then carefully addressed in developing SentiStrength-SE, a tool for improved sentiment analysis especially designed for application in the software engineering domain. Using a benchmark dataset consisting of 5,600 manually annotated JIRA issue comments, we carry out both quantitative and qualitative evaluations of our tool. SentiStrength-SE achieves 73.85% precision and 85% recall, which are significantly higher than a state-of-the-art sentiment analysis tool we compare with.",fullPaper,cp34
p539,3041a9265afb2ebdb4915aa9572668bb7f32b0ef,c29,International Conference on Software Engineering,From Word Embeddings to Document Similarities for Improved Information Retrieval in Software Engineering,"The application of information retrieval techniques to search tasks in software engineering is made difficult by the lexical gap between search queries, usually expressed in natural language (e.g. English), and retrieved documents, usually expressed in code (e.g. programming languages). This is often the case in bug and feature location, community question answering, or more generally the communication between technical personnel and non-technical stake holders in a software project. In this paper, we propose bridging the lexical gap by projecting natural language statements and code snippets as meaning vectors in a shared representation space. In the proposed architecture, word embeddings are rst trained on API documents, tutorials, and reference documents, and then aggregated in order to estimate semantic similarities between documents. Empirical evaluations show that the learned vector space embeddings lead to improvements in a previously explored bug localization task and a newly de ned task of linking API documents to computer programming questions.",fullPaper,cp29
p540,24359b0a34715df2179e084285c16a10f997a145,c36,Conference on Software Engineering Education and Training,Software Engineering Education: Converging with the Startup Industry,"Startups are agents of change that bring in innovations and find solutions to problems at various scales. An all-rounded engineering team is a key driver for the ability to execute the entrepreneurial ambition, from building a minimum viable product to later stages of product vision. Software engineering education provides students with the knowledge to transition to mature companies with defined structure in place successfully. However, the fluidity, risk, time-sensitivity, and uncertainty of startups demand a dynamic and agile set of skills to rapidly identify, conceptualize and deliver features as per market needs. This requires the adoption of latest development trends in software processes, engineering and DevOps practices with automation to iterate fast with low governance and the ability to take on multiple roles. This paper presents a study of the dynamics and engineering at startups and compares it with the current curriculum of software engineering.",fullPaper,cp36
p541,f9a9f3f016fa3123c3059cca66314d26f2357155,c93,Human Language Technology - The Baltic Perspectiv,"Model-Driven Software Engineering in Practice, Second Edition","This book discusses how model-based approaches can improve the daily practice of software professionals. This is known as Model-Driven Software Engineering (MDSE) or, simply, Model-Driven Engineering (MDE). MDSE practices have proved to increase efficiency and effectiveness in software development, as demonstrated by various quantitative and qualitative studies. MDSE adoption in the software industry is foreseen to grow exponentially in the near future, e.g., due to the convergence of software development and business analysis. The aim of this book is to provide you with an agile and flexible tool to introduce you to the MDSE world, thus allowing you to quickly understand its basic principles and techniques and to choose the right set of MDSE instruments for your needs so that you can start to benefit from MDSE right away. The book is organized into two main parts. The first part discusses the foundations of MDSE in terms of basic concepts (i.e., models and transformations), driving principles, application scenarios, and current standards, like the well-known MDA initiative proposed by OMG (Object Management Group) as well as the practices on how to integrate MDSE in existing development processes. The second part deals with the technical aspects of MDSE, spanning from the basics on when and how to build a domain-specific modeling language, to the description of Model-to-Text and Model-to-Model transformations, and the tools that support the management of MDSE projects. The second edition of the book features: a set of completely new topics, including: full example of the creation of a new modeling language (IFML), discussion of modeling issues and approaches in specific domains, like business process modeling, user interaction modeling, and enterprise architecture complete revision of examples, figures, and text, for improving readability, understandability, and coherence better formulation of definitions, dependencies between concepts and ideas addition of a complete index of book content In addition to the contents of the book, more resources are provided on the book's website http://www.mdse-book.com, including the examples presented in the book.",poster,cp93
p542,0a5d358e643f46f5a5d20892417260800cccc345,c5,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,Experimentation in Software Engineering,Abstract content,poster,cp5
p543,9a9643601989088ace41382b3c1cc61e1b4d5633,c65,Formal Concept Analysis,Blockchain-Oriented Software Engineering: Challenges and New Directions,"In this work, we acknowledge the need for software engineers to devise specialized tools and techniques for blockchain-oriented software development. Ensuring effective testing activities, enhancing collaboration in large teams, and facilitating the development of smart contracts all appear as key factors in the future of blockchain-oriented software development.",poster,cp65
p544,c176cf31862a7c5b324556e8dc3fdbef2a108391,j147,IEEE Software,Model-Based Software Engineering to Tame the IoT Jungle,"The Internet of Things (IoT) is a challenging combination of distribution and heterogeneity. A number of software engineering solutions address those challenges in isolation, but few solutions tackle them in combination, which poses a set of concrete challenges. The ThingML (Internet of Things Modeling Language) approach attempts to address those challenges. This model-driven, generative approach, which was inspired by UML, integrates concepts targeted at the IoT. Over the past six years, it has been continuously evolved and applied to cases in different domains, including a commercial e-health solution.",fullPaper,jv147
p545,9579ed0d182ba134ab3ed14ba0defbb324147399,j147,IEEE Software,Key Abstractions for IoT-Oriented Software Engineering,"Despite the progress in Internet of Things (IoT) research, a general software engineering approach for systematic development of IoT systems and applications is still missing. A synthesis of the state of the art in the area can help frame the key abstractions related to such development. Such a framework could be the basis for guidelines for IoT-oriented software engineering.",fullPaper,jv147
p546,819f36c0ddae12132d60ebc5cc0a19f7db8668b1,j142,IEEE Transactions on Software Engineering,Cognitive Biases in Software Engineering: A Systematic Mapping Study,"One source of software project challenges and failures is the systematic errors introduced by human cognitive biases. Although extensively explored in cognitive psychology, investigations concerning cognitive biases have only recently gained popularity in software engineering research. This paper therefore systematically maps, aggregates and synthesizes the literature on cognitive biases in software engineering to generate a comprehensive body of knowledge, understand state-of-the-art research and provide guidelines for future research and practise. Focusing on bias antecedents, effects and mitigation techniques, we identified 65 articles (published between 1990 and 2016), which investigate 37 cognitive biases. Despite strong and increasing interest, the results reveal a scarcity of research on mitigation techniques and poor theoretical foundations in understanding and interpreting cognitive biases. Although bias-related research has generated many new insights in the software engineering community, specific bias mitigation techniques are still needed for software professionals to overcome the deleterious effects of cognitive biases on their work.",fullPaper,jv142
p547,d1cc35e2a547ba79f1b07fdd81ee0da264c0d6b6,c29,International Conference on Software Engineering,Belief & Evidence in Empirical Software Engineering,"Empirical software engineering has produced a steady stream of evidence-based results concerning the factors that affect important outcomes such as cost, quality, and interval. However, programmers often also have strongly-held a priori opinions about these issues. These opinions are important, since developers are highlytrained professionals whose beliefs would doubtless affect their practice. As in evidence-based medicine, disseminating empirical findings to developers is a key step in ensuring that the findings impact practice. In this paper, we describe a case study, on the prior beliefs of developers at Microsoft, and the relationship of these beliefs to actual empirical data on the projects in which these developers work. Our findings are that a) programmers do indeed have very strong beliefs on certain topics b) their beliefs are primarily formed based on personal experience, rather than on findings in empirical research and c) beliefs can vary with each project, but do not necessarily correspond with actual evidence in that project. Our findings suggest that more effort should be taken to disseminate empirical findings to developers and that more in-depth study the interplay of belief and evidence in software practice is needed.",fullPaper,cp29
p548,7e991438547d69c1001b7664cd60a68d4dbc4023,c37,Asia-Pacific Software Engineering Conference,A Map of Threats to Validity of Systematic Literature Reviews in Software Engineering,"Context: The assessment of Threats to Validity (TTVs) is critical to secure the quality of empirical studies in Software Engineering (SE). In the recent decade, Systematic Literature Review (SLR) was becoming an increasingly important empirical research method in SE. One of the mechanisms of insuring the level of scientific value in the findings of an SLR is to rigorously assess its validity. Hence, it is necessary to realize the status quo and issues of TTVs of SLRs in SE. Objective: This study aims to investigate thestate-of-the-practice of TTVs of the SLRs published in SE, and further support SE researchers to improve the assessment and strategies against TTVs in order to increase the quality of SLRs in SE. Method: We conducted a tertiary study by reviewing the SLRs in SE that report the assessment of TTVs. Results: We identified 316 SLRs published from 2004 to the first half of 2015, in which TTVs are discussed. The issues associated to TTVs were also summarized and categorized. Conclusion: The common TTVs related to SLR research, such as internal validity and reliability, were thoroughly discussed in most SLRs. The threats to construct validity and external validity drew less attention. Moreover, there are few strategies and tactics being reported to cope with the various TTVs.",fullPaper,cp37
p549,64cc4ef5def3919049bdd3a645af198922d626c2,c29,International Conference on Software Engineering,An Empirical Study of Practitioners' Perspectives on Green Software Engineering,"The energy consumption of software is an increasing concern as the use of mobile applications, embedded systems, and data center-based services expands. While research in green software engineering is correspondingly increasing, little is known about the current practices and perspectives of software engineers in the field. This paper describes the first empirical study of how practitioners think about energy when they write requirements, design, construct, test, and maintain their software. We report findings from a quantitative,targeted survey of 464 practitioners from ABB, Google, IBM, and Microsoft, which was motivated by and supported with qualitative data from 18 in-depth interviews with Microsoft employees. The major findings and implications from the collected data contextualize existing green software engineering research and suggest directions for researchers aiming to develop strategies and tools to help practitioners improve the energy usage of their applications.",fullPaper,cp29
p550,4099f2b7a9282e3be30076f2532008e6caeb1c13,j141,Empirical Software Engineering,On the pragmatic design of literature studies in software engineering: an experience-based guideline,Abstract content,fullPaper,jv141
p551,cdc5bf80451d1f447cf82e5c37fec8089b1e6878,j145,Journal of Systems and Software,A framework for gamification in software engineering,Abstract content,fullPaper,jv145
p552,0e50bf23cb16ba66e738d88f0fffab75c338e02a,j147,IEEE Software,"Crowdsourcing in Software Engineering: Models, Motivations, and Challenges","Almost surreptitiously, crowdsourcing has entered software engineering practice. In-house development, contracting, and outsourcing still dominate, but many development projects use crowdsourcing-for example, to squash bugs, test software, or gather alternative UI designs. Although the overall impact has been mundane so far, crowdsourcing could lead to fundamental, disruptive changes in how software is developed. Various crowdsourcing models have been applied to software development. Such changes offer exciting opportunities, but several challenges must be met for crowdsourcing software development to reach its potential.",fullPaper,jv147
p553,266aa9741c6559af0c6dcee2e1947ced0385b4bd,c67,Enterprise Application Integration,Evidence-Based Software Engineering and Systematic Reviews,"In the decade since the idea of adapting the evidence-based paradigm for software engineering was first proposed, it has become a major tool of empirical software engineering. Evidence-Based Software Engineering and Systematic Reviews provides a clear introduction to the use of an evidence-based model for software engineering research and practice. The book explains the roles of primary studies (experiments, surveys, case studies) as elements of an over-arching evidence model, rather than as disjointed elements in the empirical spectrum. Supplying readers with a clear understanding of empirical software engineering best practices, it provides up-to-date guidance on how to conduct secondary studies in software engineeringreplacing the existing 2004 and 2007 technical reports. The book is divided into three parts. The first part discusses the nature of evidence and the evidence-based practices centered on a systematic review, both in general and as applying to software engineering. The second part examines the different elements that provide inputs to a systematic review (usually considered as forming a secondary study), especially the main forms of primary empirical study currently used in software engineering. The final part provides practical guidance on how to conduct systematic reviews (the guidelines), drawing together accumulated experiences to guide researchers and students in planning and conducting their own studies. The book includes an extensive glossary and an appendix that provides a catalogue of reviews that may be useful for practice and teaching.",poster,cp67
p554,ec59569fdee17844ae071be1536a08f937f08c57,j147,IEEE Software,"Speed, Data, and Ecosystems: The Future of Software Engineering","An evaluation of recent industrial and societal trends revealed three key factors driving software engineering's future: speed, data, and ecosystems. These factors' implications have led to guidelines for companies to evolve their software engineering practices. This article is part of a special issue on the Future of Software Engineering.",fullPaper,jv147
p555,12248c8cd5ba1f2594e2bcf4c3b4e8ddd10d13c7,j141,Empirical Software Engineering,Open innovation in software engineering: a systematic mapping study,Abstract content,fullPaper,jv141
p556,564614da76b5d9020c700b78e1fe154bd590c47d,j142,IEEE Transactions on Software Engineering,The Role of Ethnographic Studies in Empirical Software Engineering,"Ethnography is a qualitative research method used to study people and cultures. It is largely adopted in disciplines outside software engineering, including different areas of computer science. Ethnography can provide an in-depth understanding of the socio-technological realities surrounding everyday software development practice, i.e., it can help to uncover not only what practitioners do, but also why they do it. Despite its potential, ethnography has not been widely adopted by empirical software engineering researchers, and receives little attention in the related literature. The main goal of this paper is to explain how empirical software engineering researchers would benefit from adopting ethnography. This is achieved by explicating four roles that ethnography can play in furthering the goals of empirical software engineering: to strengthen investigations into the social and human aspects of software engineering; to inform the design of software engineering tools; to improve method and process development; and to inform research programmes. This article introduces ethnography, explains its origin, context, strengths and weaknesses, and presents a set of dimensions that position ethnography as a useful and usable approach to empirical software engineering research. Throughout the paper, relevant examples of ethnographic studies of software practice are used to illustrate the points being made.",fullPaper,jv142
p557,27c9101fa2f41ce15100f0f07802bb656eda50ad,j141,Empirical Software Engineering,On the use of many quality attributes for software refactoring: a many-objective search-based software engineering approach,Abstract content,fullPaper,jv141
p558,5602cbe797d20b17edfd08b3dc94622c842fbe80,j142,IEEE Transactions on Software Engineering,Crossover Designs in Software Engineering Experiments: Benefits and Perils,"In experiments with crossover design subjects apply more than one treatment. Crossover designs are widespread in software engineering experimentation: they require fewer subjects and control the variability among subjects. However, some researchers disapprove of crossover designs. The main criticisms are: the carryover threat and its troublesome analysis. Carryover is the persistence of the effect of one treatment when another treatment is applied later. It may invalidate the results of an experiment. Additionally, crossover designs are often not properly designed and/or analysed, limiting the validity of the results. In this paper, we aim to make SE researchers aware of the perils of crossover experiments and provide risk avoidance good practices. We study how another discipline (medicine) runs crossover experiments. We review the SE literature and discuss which good practices tend not to be adhered to, giving advice on how they should be applied in SE experiments. We illustrate the concepts discussed analysing a crossover experiment that we have run. We conclude that crossover experiments can yield valid results, provided they are properly designed and analysed, and that, if correctly addressed, carryover is no worse than other validity threats.",fullPaper,jv142
p559,b681cd520e9b2c4e2c96dda31d09b51edcf28e32,c19,ACM Conference on Economics and Computation,Systems and Software Engineering,Abstract content,poster,cp19
p560,a717f569c788ae23a3c0d2d6a18b109590a119c8,c105,Biometrics and Identity Management,A discipline for software engineering,Abstract content,poster,cp105
p561,b73694c24ec259da39e125c2c7d9496b2f222ba0,c38,"IEEE International Conference on Software Analysis, Evolution, and Reengineering",Future Trends in Software Engineering Research for Mobile Apps,"There has been tremendous growth in the use of mobile devices over the last few years. This growth has fueled the development of millions of software applications for these mobile devices often called as 'apps'. Current estimates indicate that there are hundreds of thousands of mobile app developers. As a result, in recent years, there has been an increasing amount of software engineering research conducted on mobile apps to help such mobile app developers. In this paper, we discuss current and future research trends within the framework of the various stages in the software development life-cycle: requirements (including non-functional), design and development, testing, and maintenance. While there are several non-functional requirements, we focus on the topics of energy and security in our paper, since mobile apps are not necessarily built by large companies that can afford to get experts for solving these two topics. For the same reason we also discuss the monetizing aspects of a mobile app at the end of the paper. For each topic of interest, we first present the recent advances done in these stages and then we present the challenges present in current work, followed by the future opportunities and the risks present in pursuing such research.",fullPaper,cp38
p562,9445d843e8416a4cffb6da681250162ac38e1e2f,c104,IEEE International Conference on Multimedia and Expo,Student Experiences Using GitHub in Software Engineering Courses: A Case Study,"GitHub has been embraced by the software development community as an important social platform for managing software projects and to support collaborative development. More recently, educators have begun to adopt it for hosting course content and student assignments. From our previous research, we found that educators leverage GitHub’s collaboration and transparency features to create, reuse and remix course materials, and to encourage student contributions and monitor student activity on assignments and projects. However, our previous research did not consider the student perspective. In this paper, we present a case study where GitHub is used as a learning platform for two software engineering courses. We gathered student perspectives on how the use of GitHub in their courses might benefit them and to identify the challenges they may face. The findings from our case study indicate that software engineering students do benefit from GitHub’s transparent and open workflow. However, students were concerned that since GitHub is not inherently an educational tool, it lacks key features important for education and poses learning and privacy concerns. Our findings provide recommendations for designers on how tools such as GitHub can be used to improve software engineering education, and also point to recommendations for instructors on how to use it more effectively in their courses.",poster,cp104
p563,1fe3f8c49567b71ef5537f4ff8686bbfec9b2cef,j147,IEEE Software,Software-Engineering the Internet of Things,New wiring transformed ENIAC into a versatile stored-program computer. Rewiring Internet of Things infrastructures into a general-purpose computing fabric can similarly change how modern computation interfaces with our environment.,fullPaper,jv147
p564,898f657c30f37d8b674cc7383bcccad72334b933,c91,Workshop on Algorithms and Models for the Web-Graph,Software Engineering,SE 3162 Professional Responsibility in Computer Science and Software Engineering (1 semester credit hour) Professional and ethical responsibilities of computer scientists and software engineers as influenced by growth in computer use and networks. Costs and benefits of computer technology. Risks and liabilities of safety-critical systems. Social implications of the Internet. Interaction between human values and technical decisions involving computing. Intellectual Property. Global impact of computing. Prerequisites or Corequisites: CS 3345 and CS 3354 and ECS 3361. (Same as CS 3162) (1-0) S,poster,cp91
p565,7aaa884f017f3f71a893b1755f75d801e17339ac,c84,The Web Conference,Software Engineering Component Based Software Engineering,"Component-based software engineering (CBSE) (also known as component-based development (CBD)) is a branch of software engineering that emphasizes the separation of concerns in respect of the wide-ranging functionality available throughout a given software system. It is a reuse-based approach to defining, implementing and composing loosely coupled independent components into systems. This practice aims to bring about an equally wide-ranging degree of benefits in both the short-term and the long-term for the software itself and for organizations that sponsor such software. This approach promises to alleviate the software crisis at great extents. The objective of this paper is to gain attention towards this new component based software development paradigm and to highlight the benefits of the approach for making it a successful software development approach to the concerned community",poster,cp84
p566,994b8dbfa12027ed24285a1e34c20cae8831173c,c38,"IEEE International Conference on Software Analysis, Evolution, and Reengineering",Software-Specific Named Entity Recognition in Software Engineering Social Content,"Software engineering social content, such as Q&A discussions on Stack Overflow, has become a wealth of information on software engineering. This textual content is centered around software-specific entities, and their usage patterns, issues-solutions, and alternatives. However, existing approaches to analyzing software engineering texts treat software-specific entities in the same way as other content, and thus cannot support the recent advance of entity-centric applications, such as direct answers and knowledge graph. The first step towards enabling these entity-centric applications for software engineering is to recognize and classify software-specific entities, which is referred to as Named Entity Recognition (NER) in the literature. Existing NER methods are designed for recognizing person, location and organization in formal and social texts, which are not applicable to NER in software engineering. Existing information extraction methods for software engineering are limited to API identification and linking of a particular programming language. In this paper, we formulate the research problem of NER in software engineering. We identify the challenges in designing a software-specific NER system and propose a machine learning based approach applied on software engineering social content. Our NER system, called S-NER, is general for software engineering in that it can recognize a broad category of software entities for a wide range of popular programming languages, platform, and library. We conduct systematic experiments to evaluate our machine learning based S-NER against a well-designed, and to study the effectiveness of widely-adopted NER techniques and features in the face of the unique characteristics of software engineering social content.",fullPaper,cp38
p567,768b444c84340d2210bd2782ce3aa39b723bd0b9,j148,Journal of Software Engineering Research and Development,Game development software engineering process life cycle: a systematic review,Abstract content,fullPaper,jv148
p568,7da816d0f1d2a2b33d6512a1e694c04cbe4d4963,c91,Workshop on Algorithms and Models for the Web-Graph,Experimentation in Software Engineering,Abstract content,poster,cp91
p569,794c598e037ffac8b8dec326ba29a5dd9044ece6,c105,Biometrics and Identity Management,Modeling in Event-B - System and Software Engineering,"A practical text suitable for an introductory or advanced course in formal methods, this book presents a mathematical approach to modelling and designing systems using an extension of the B formal method: Event-B. Based on the idea of refinement, the author's systematic approach allows the user to construct models gradually and to facilitate a systematic reasoning method by means of proofs. Readers will learn how to build models of programs and, more generally, discrete systems, but this is all done with practice in mind. The numerous examples provided arise from various sources of computer system developments, including sequential programs, concurrent programs and electronic circuits. The book also contains a large number of exercises and projects ranging in difficulty. Each of the examples included in the book has been proved using the Rodin Platform tool set, which is available free for download at www.event-b.org.",poster,cp105
p570,ece51631d79e2c017471f31767d7c3b62dd45769,c45,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",Guide to the Software Engineering Body of Knowledge (SWEBOK(R)): Version 3.0,"In the Guide to the Software Engineering Body of Knowledge (SWEBOK Guide), the IEEE Computer Society establishes a baseline for the body of knowledge for the field of software engineering, and the work supports the Societys responsibility to promote the advancement of both theory and practice in this field. It should be noted that the Guide does not purport to define the body of knowledge but rather to serve as a compendium and guide to the knowledge that has been developing and evolving over the past four decades. Now in Version 3.0, the Guides 15 knowledge areas summarize generally accepted topics and list references for detailed information. The editors for Version 3.0 of the SWEBOK Guide are Pierre Bourque (cole de technologie suprieure (TS), Universit du Qubec) and Richard E. (Dick) Fairley (Software and Systems Engineering Associates (S2EA)).",poster,cp45
p571,b69a938050093e592e19a3b6321b3382d4f8bc7a,c39,International Conference on Global Software Engineering,Global Software Engineering: Evolution and Trends,"Professional software products and IT systems and services today are developed mostly by globally distributed teams, projects, and companies. Successfully orchestrating Global Software Engineering (GSE) has become the major success factor both for organizations and practitioners. Yet, more than a half of all distributed projects does not achieve the intended objectives and is canceled. This paper summarizes experiences from academia and industry in a way to facilitate knowledge and technology transfer. It is based on an evaluation of 10 years of research, and industry collaboration and experience reported at the IEEE International Conference on Software Engineering (ICGSE) series. The outcomes of our analysis show GSE as a field highly attached to industry and, thus, a considerable share of ICGSE papers address the transfer of Software Engineering concepts and solutions to the global stage. We found collaboration and teams, processes and organization, sourcing and supplier management, and success factors to be the topics gaining the most interest of researchers and practitioners. Beyond the analysis of the past conferences, we also look at current trends in GSE to motivate further research and industrial collaboration.",fullPaper,cp39
p572,fac3a6f272428e9d6879227ef76c1fa9397b317f,j149,"Software testing, verification & reliability",A Hitchhiker's guide to statistical tests for assessing randomized algorithms in software engineering,"Randomized algorithms are widely used to address many types of software engineering problems, especially in the area of software verification and validation with a strong emphasis on test automation. However, randomized algorithms are affected by chance and so require the use of appropriate statistical tests to be properly analysed in a sound manner. This paper features a systematic review regarding recent publications in 2009 and 2010 showing that, overall, empirical analyses involving randomized algorithms in software engineering tend to not properly account for the random nature of these algorithms. Many of the novel techniques presented clearly appear promising, but the lack of soundness in their empirical evaluations casts unfortunate doubts on their actual usefulness. In software engineering, although there are guidelines on how to carry out empirical analyses involving human subjects, those guidelines are not directly and fully applicable to randomized algorithms. Furthermore, many of the textbooks on statistical analysis are written from the viewpoints of social and natural sciences, which present different challenges from randomized algorithms. To address the questionable overall quality of the empirical analyses reported in the systematic review, this paper provides guidelines on how to carry out and properly analyse randomized algorithms applied to solve software engineering tasks, with a particular focus on software testing, which is by far the most frequent application area of randomized algorithms within software engineering. Copyright © 2012 John Wiley & Sons, Ltd.",fullPaper,jv149
p573,ad7102b6bd0842fe1b46ce6a9246b1f00f51948f,c37,Asia-Pacific Software Engineering Conference,ISO / IEC 25010 : 2011 Systems and software engineering — Systems and software Quality Requirements and Evaluation ( SQuaRE ) — System and software quality models,Abstract content,poster,cp37
p574,2d1e79e057a9111ea6863378ffeca526a4e41c5f,c100,ACM SIGMOD Conference,On Non-Functional Requirements in Software Engineering,Abstract content,poster,cp100
p575,fa2a0ecf236164d19c5da8dc45d72c2014bc1eff,j145,Journal of Systems and Software,Twenty-eight years of component-based software engineering,Abstract content,fullPaper,jv145
p576,a5a6eb69869d2a25a5915ebec8e0991ba78c4769,c112,Very Large Data Bases Conference,Are Students Representatives of Professionals in Software Engineering Experiments?,"Background: Most of the experiments in software engineering (SE) employ students as subjects. This raises concerns about the realism of the results acquired through students and adaptability of the results to software industry. Aim: We compare students and professionals to understand how well students represent professionals as experimental subjects in SE research. Method: The comparison was made in the context of two test-driven development experiments conducted with students in an academic setting and with professionals in a software organization. We measured the code quality of several tasks implemented by both subject groups and checked whether students and professionals perform similarly in terms of code quality metrics. Results: Except for minor differences, neither of the subject groups is better than the other. Professionals produce larger, yet less complex, methods when they use their traditional development approach, whereas both subject groups perform similarly when they apply a new approach for the first time. Conclusion: Given a carefully scoped experiment on a development approach that is new to both students and professionals, similar performances are observed. Further investigation is necessary to analyze the effects of subject demographics and level of experience on the results of SE experiments.",poster,cp112
p577,63b35f109c6962247cbbf9458d55082c653f1d9e,j141,Empirical Software Engineering,A practical guide to controlled experiments of software engineering tools with human participants,Abstract content,fullPaper,jv141
p578,7a28a601877a3722126c74149efc2a5f207b08e2,c3,Frontiers in Education Conference,Docker [Software engineering],"In episode 217 of Software Engineering Radio, host Charles Anderson talks with James Turnbull, a software developer and security specialist who's vice president of services at Docker. Lightweight Docker containers are rapidly becoming a tool for deploying microservice-based architectures.",poster,cp3
p579,641bb6adbbef003b3b74a1bbd5e11dbe251d6a66,j141,Empirical Software Engineering,Towards a decision-making structure for selecting a research design in empirical software engineering,Abstract content,fullPaper,jv141
p580,bcd2c5379a34068040750a751e4fd2710d90c15c,j142,IEEE Transactions on Software Engineering,The “Physics” of Notations: Toward a Scientific Basis for Constructing Visual Notations in Software Engineering,"Visual notations form an integral part of the language of software engineering (SE). Yet historically, SE researchers and notation designers have ignored or undervalued issues of visual representation. In evaluating and comparing notations, details of visual syntax are rarely discussed. In designing notations, the majority of effort is spent on semantics, with graphical conventions largely an afterthought. Typically, no design rationale, scientific or otherwise, is provided for visual representation choices. While SE has developed mature methods for evaluating and designing semantics, it lacks equivalent methods for visual syntax. This paper defines a set of principles for designing cognitively effective visual notations: ones that are optimized for human communication and problem solving. Together these form a design theory, called the Physics of Notations as it focuses on the physical (perceptual) properties of notations rather than their logical (semantic) properties. The principles were synthesized from theory and empirical evidence from a wide range of fields and rest on an explicit theory of how visual notations communicate. They can be used to evaluate, compare, and improve existing visual notations as well as to construct new ones. The paper identifies serious design flaws in some of the leading SE notations, together with practical suggestions for improving them. It also showcases some examples of visual notation design excellence from SE and other fields.",fullPaper,jv142
p581,c605abdfc50787818fce169198e0807c5b474c1c,j144,Information and Software Technology,Gamification in software engineering - A systematic mapping,Abstract content,fullPaper,jv144
p582,64fd3ee86c29633d439d02bbdc044b132e56ec7c,c40,IEEE International Conference on Software Maintenance and Evolution,Choosing your weapons: On sentiment analysis tools for software engineering research,"Recent years have seen an increasing attention to social aspects of software engineering, including studies of emotions and sentiments experienced and expressed by the software developers. Most of these studies reuse existing sentiment analysis tools such as SentiStrength and NLTK. However, these tools have been trained on product reviews and movie reviews and, therefore, their results might not be applicable in the software engineering domain. In this paper we study whether the sentiment analysis tools agree with the sentiment recognized by human evaluators (as reported in an earlier study) as well as with each other. Furthermore, we evaluate the impact of the choice of a sentiment analysis tool on software engineering studies by conducting a simple study of differences in issue resolution times for positive, negative and neutral texts. We repeat the study for seven datasets (issue trackers and Stack Overflow questions) and different sentiment analysis tools and observe that the disagreement between the tools can lead to contradictory conclusions.",fullPaper,cp40
p583,5a4674e987c2d7c130c5303cbad3f4e4531f3259,c50,International Conference on Automated Software Engineering,Case Study Research in Software Engineering - Guidelines and Examples,"Based on their own experiences of in-depth case studies of software projects in international corporations, in this bookthe authors present detailed practical guidelines on the preparation, conduct, design and reporting of case studies of software engineering. This is the first software engineering specific book on thecase study research method.",poster,cp50
p584,c96d0620f567249a38d5f18147d3eb2e70d67602,j144,Information and Software Technology,A theory of distances in software engineering,Abstract content,fullPaper,jv144
p585,2f9a1286e7af4ab7706ad8cfcc8c8742a1964939,c60,IEEE International Conference on Software Engineering and Formal Methods,Views on Internal and External Validity in Empirical Software Engineering,"Empirical methods have grown common in software engineering, but there is no consensus on how to apply them properly. Is practical relevance key? Do internally valid studies have any value? Should we replicate more to address the tradeoff between internal and external validity? We asked the community how empirical research should take place in software engineering, with a focus on the tradeoff between internal and external validity and replication, complemented with a literature review about the status of empirical research in software engineering. We found that the opinions differ considerably, and that there is no consensus in the community when to focus on internal or external validity and how to conduct and review replications.",poster,cp60
p586,3dcc789f0ecc07500a3539ec3805f80a03412103,j144,Information and Software Technology,A systematic review of systematic review process research in software engineering,Abstract content,fullPaper,jv144
p587,0d63d4bcf9fb27ee0930d71cd0108fe629671b7a,c22,International Conference on Data Technologies and Applications,Software Engineering: A Practitioners Approach,"Software engineering is the art of war. So if you don't know how to wage a war, then the weapons are useless. Software engineering has become very important because of the impact of large, expensive software systems and the role of software in safety-critical applications. This book supports a process to refound software engineering based on a solid theory, proven principles and best practices and fills a long-standing need in the software development communities to make the essential aspects of software development available in one comprehensive work. Written in an easy-to-understand tutorial format, SOFTWARE ENGINEERING: A Practitioners Approach provides professionals, researchers, and students at all levels with a clear coverage of: Analyzing, designing, programming and testing software projects. Set of objectives to which a prospective should be targeting to achieve. Two types of review questions-short answer type and descriptive type. List of key terms referring to abstract concepts, which may be used for better and crisp communication. Solution manual in electronic form available for qualified teachers on demand. Instructor's manual including power point slides, brief notes on teaching and list of projects with descriptions on demand. List of key references for the concepts in the chapter. Useful websites appended to each chapter for quick reference",poster,cp22
p588,edb89d1462e96b4bace463f1fae9307717ac787a,c81,IEEE Annual Symposium on Foundations of Computer Science,Agent-Oriented Software Engineering,Abstract content,poster,cp81
p589,a652692b51c786bfa3ceb43f3ae9f6acb796921f,c51,Conference of the Centre for Advanced Studies on Collaborative Research,The (R) Evolution of social media in software engineering,"Software developers rely on media to communicate, learn, collaborate, and coordinate with others. Recently, social media has dramatically changed the landscape of software engineering, challenging some old assumptions about how developers learn and work with one another. We see the rise of the social programmer who actively participates in online communities and openly contributes to the creation of a large body of crowdsourced socio-technical content. In this paper, we examine the past, present, and future roles of social media in software engineering. We provide a review of research that examines the use of different media channels in software engineering from 1968 to the present day. We also provide preliminary results from a large survey with developers that actively use social media to understand how they communicate and collaborate, and to gain insights into the challenges they face. We find that while this particular population values social media, traditional channels, such as face-to-face communication, are still considered crucial. We synthesize findings from our historical review and survey to propose a roadmap for future research on this topic. Finally, we discuss implications for research methods as we argue that social media is poised to bring about a paradigm shift in software engineering research.",poster,cp51
p590,9992f1d6978c1e9442bf519aa213f5ca4e2159f8,c41,Software Product Lines Conference,Search based software engineering for software product line engineering: a survey and directions for future work,"This paper presents a survey of work on Search Based Software Engineering (SBSE) for Software Product Lines (SPLs). We have attempted to be comprehensive, in the sense that we have sought to include all papers that apply computational search techniques to problems in software product line engineering. Having surveyed the recent explosion in SBSE for SPL research activity, we highlight some directions for future work. We focus on suggestions for the development of recent advances in genetic improvement, showing how these might be exploited by SPL researchers and practitioners: Genetic improvement may grow new products with new functional and non-functional features and graft these into SPLs. It may also merge and parameterise multiple branches to cope with SPL branchmania.",fullPaper,cp41
p591,969d1e52140dbf8f58a1b3f61a29b03255490c86,c35,EUROMICRO Conference on Software Engineering and Advanced Applications,Software Engineering Meets Control Theory,"The software engineering community has proposed numerous approaches for making software self-adaptive. These approaches take inspiration from machine learning and control theory, constructing software that monitors and modifies its own behavior to meet goals. Control theory, in particular, has received considerable attention as it represents a general methodology for creating adaptive systems. Control-theoretical software implementations, however, tend to be ad hoc. While such solutions often work in practice, it is difficult to understand and reason about the desired properties and behavior of the resulting adaptive software and its controller. This paper discusses a control design process for software systems which enables automatic analysis and synthesis of a controller that is guaranteed to have the desired properties and behavior. The paper documents the process and illustrates its use in an example that walks through all necessary steps for self-adaptive controller synthesis.",poster,cp35
p592,0e528eb8167c68930c2e1ab20ab5c14f98446927,j142,IEEE Transactions on Software Engineering,Investigating Country Differences in Mobile App User Behavior and Challenges for Software Engineering,"Mobile applications (apps) are software developed for use on mobile devices and made available through app stores. App stores are highly competitive markets where developers need to cater to a large number of users spanning multiple countries. This work hypothesizes that there exist country differences in mobile app user behavior and conducts one of the largest surveys to date of app users across the world, in order to identify the precise nature of those differences. The survey investigated user adoption of the app store concept, app needs, and rationale for selecting or abandoning an app. We collected data from more than 15 countries, including USA, China, Japan, Germany, France, Brazil, United Kingdom, Italy, Russia, India, Canada, Spain, Australia, Mexico, and South Korea. Analysis of data provided by 4,824 participants showed significant differences in app user behaviors across countries, for example users from USA are more likely to download medical apps, users from the United Kingdom and Canada are more likely to be influenced by price, users from Japan and Australia are less likely to rate apps. Analysis of the results revealed new challenges to market-driven software engineering related to packaging requirements, feature space, quality expectations, app store dependency, price sensitivity, and ecosystem effect.",fullPaper,jv142
p593,2aa4b8ab64847b5d040a4508a4029dea07e9cb41,j141,Empirical Software Engineering,An empirically based terminology and taxonomy for global software engineering,Abstract content,fullPaper,jv141
p594,a639919277b2d0683d21b637bec2192fd475fa80,j150,PeerJ,Happy software developers solve problems better: psychological measurements in empirical software engineering,"For more than thirty years, it has been claimed that a way to improve software developers’ productivity and software quality is to focus on people and to provide incentives to make developers satisfied and happy. This claim has rarely been verified in software engineering research, which faces an additional challenge in comparison to more traditional engineering fields: software development is an intellectual activity and is dominated by often-neglected human factors (called human aspects in software engineering research). Among the many skills required for software development, developers must possess high analytical problem-solving skills and creativity for the software construction process. According to psychology research, affective states—emotions and moods—deeply influence the cognitive processing abilities and performance of workers, including creativity and analytical problem solving. Nonetheless, little research has investigated the correlation between the affective states, creativity, and analytical problem-solving performance of programmers. This article echoes the call to employ psychological measurements in software engineering research. We report a study with 42 participants to investigate the relationship between the affective states, creativity, and analytical problem-solving skills of software developers. The results offer support for the claim that happy developers are indeed better problem solvers in terms of their analytical abilities. The following contributions are made by this study: (1) providing a better understanding of the impact of affective states on the creativity and analytical problem-solving capacities of developers, (2) introducing and validating psychological measurements, theories, and concepts of affective states, creativity, and analytical-problem-solving skills in empirical software engineering, and (3) raising the need for studying the human factors of software engineering by employing a multidisciplinary viewpoint.",fullPaper,jv150
p595,7eb0399ae7e79449178d7fdaa86059fa0c856edd,c46,Brazilian Symposium on Software Engineering,"Software Product Line Engineering - Foundations, Principles, and Techniques","Software product line engineering has proven to be the methodology for developing a diversity of software products and software intensive systems at lower costs, in shorter time, and with higher quality. In this book, Pohl and his co-authors present a framework for software product line engineering which they have developed based on their academic as well as industrial experience gained in projects over the last eight years. They do not only detail the technical aspect of the development, but also an integrated view of the business, organisation and process aspects are given. In addition, they explicitly point out the key differences of software product line engineering compared to traditional single software system development, as the need for two distinct development processes for domain and application engineering respectively, or the need to define and manage variability.",poster,cp46
p596,75fb25c1dae20546b13f233bcfbbdffebbb0c3f1,c29,International Conference on Software Engineering,Software engineering at the speed of light: how developers stay current using twitter,"The microblogging service Twitter has over 500 million users posting over 500 million tweets daily. Research has established that software developers use Twitter in their work, but this has not yet been examined in detail. Twitter is an important medium in some software engineering circles—understanding its use could lead to improved support, and learning more about the reasons for non-adoption could inform the design of improved tools. In a qualitative study, we surveyed 271 and interviewed 27 developers active on GitHub. We find that Twitter helps them keep up with the fast-paced development landscape. They use it to stay aware of industry changes, for learning, and for building relationships. We discover the challenges they experience and extract their coping strategies. Some developers do not want to or cannot embrace Twitter for their work—we show their reasons and alternative channels. We validate our findings in a follow-up survey with more than 1,200 respondents.",fullPaper,cp29
p597,389aa97e59372fbc65cab81fe18d9f86fbb70a82,j145,Journal of Systems and Software,Behavioral software engineering: A definition and systematic literature review,Abstract content,fullPaper,jv145
p598,372fcbfccd9b02c69f0e42f1c94a64a044e1a11a,j141,Empirical Software Engineering,Replication of empirical studies in software engineering research: a systematic mapping study,Abstract content,fullPaper,jv141
p599,402a9ff765bbf5a590a59e59790256c1be0ca330,c16,Knowledge Discovery and Data Mining,Guidelines for Conducting Surveys in Software Engineering,Abstract content,poster,cp16
p600,72c32a1672fad31ef32fadc4e119a5c5569e4cd0,c37,Asia-Pacific Software Engineering Conference,Eye-Tracking Metrics in Software Engineering,"Eye-tracking studies are getting more prevalent in software engineering. Researchers often use different metrics when publishing their results in eye-tracking studies. Even when the same metrics are used, they are given different names, causing difficulties in comparing studies. To encourage replications and facilitate advancing the state of the art, it is important that the metrics used by researchers be clearly and consistently defined in the literature. There is therefore a need for a survey of eye-tracking metrics to support the (future) goal of standardizing eye-tracking metrics. This paper seeks to bring awareness to the use of different metrics along with practical suggestions on using them. It compares and contrasts various eye-tracking metrics used in software engineering. It also provides definitions for common metrics and discusses some metrics that the software engineering community might borrow from other fields.",fullPaper,cp37
p601,6723f2324a2f48d11d219ea47450ba6860d8856e,c31,International Conference on Evaluation & Assessment in Software Engineering,Systematic mapping study on software engineering for sustainability (SE4S),"Background/Context: The objective of achieving higher sustainability in our lifestyles by information and communication technology has lead to a plethora of research activities in related fields. Consequently, Software Engineering for Sustainability (SE4S) has developed as an active area of research. Objective/Aim: Though SE4S gained much attention over the past few years and has resulted in a number of contributions, there is only one rigorous survey of the field. We follow up on this systematic mapping study from 2012 with a more in-depth overview of the status of research, as most work has been conducted in the last 4 years. Method: The applied method is a systematic mapping study through which we investigate which contributions were made, which knowledge areas are most explored, and which research type facets have been used, to distill a common understanding of the state-of-the-art in SE4S. Results: We contribute an overview of current research topics and trends, and their distribution according to the research type facet and the application domains. Furthermore, we aggregate the topics into clusters and list proposed and used methods, frameworks, and tools. Conclusion: The research map shows that impact currently is limited to few knowledge areas and there is need for a future roadmap to fill the gaps.",fullPaper,cp31
p602,1a517c7b56d1d2f2d3e8e209c438e32663a750c9,j75,Lecture Notes in Computer Science,Fundamentals of Software Engineering,Abstract content,fullPaper,jv75
p603,c24d6c1bed6af6b99e506f2ba83e9552481b5615,j112,Cambridge International Law Journal,Green in Software Engineering,Abstract content,fullPaper,jv112
p604,717a8985cc3fcbd285efe50dbcb8fe5bb12921b1,c37,Asia-Pacific Software Engineering Conference,Challenges for Software Engineering in Automation,"This paper gives an 
introduction to the essential challenges of software engineering and 
requirements that software has to fulfill in the domain of automation. Besides, 
the functional characteristics, specific constraints and circumstances are 
considered for deriving requirements concerning usability, the technical 
process, the automation functions, used platform and the well-established 
models, which are described in detail. On the other hand, challenges result 
from the circumstances at different points in the single phases of the life 
cycle of the automated system. The requirements for life-cycle-management, 
tools and the changeability during runtime are described in detail.",poster,cp37
p605,6cd816c7bbc5cb95a0301823e80d98115d137d43,j75,Lecture Notes in Computer Science,Search Based Software Engineering,Abstract content,fullPaper,jv75
p606,ae19e603d5cc98b0347663752e98a704608c1d86,j141,Empirical Software Engineering,Replications of software engineering experiments,Abstract content,fullPaper,jv141
p607,8d972098b0e0530b526a234e7be1a4743ccc7cfb,c7,European Conference on Modelling and Simulation,Continuous software engineering and beyond: trends and challenges,"Throughout its short history, software development has been characterized by harmful disconnects between important activities e.g., planning, development and implementation. The problem is further exacerbated by the episodic and infrequent performance of activities such as planning, testing, integration and releases. Several emerging phenomena reflect attempts to address these problems. For example, the Enterprise Agile concept has emerged as a recognition that the benefits of agile software development will be sub- optimal if not complemented by an agile approach in related organizational function such as finance and HR. Continuous integration is a practice which has emerged to eliminate discontinuities between development and deployment. In a similar vein, the recent emphasis on DevOps recognizes that the integration between software development and its operational deployment needs to be a continuous one. We argue a similar continuity is required between business strategy and development, BizDev being the term we coin for this. These disconnects are even more problematic given the need for reliability and resilience in the complex and data-intensive systems being developed today. Drawing on the lean concept of flow, we identify a number of continuous activities which are important for software development in today’s context. These activities include continuous planning, continuous integration, continuous deployment, continuous delivery, continuous verification, continuous testing, continuous compliance,continuous security, continuous use, continuous trust, continuous run-time monitoring, continuous improvement (both process and product), all underpinned by continuous innovation. We use the umbrella term, ``Continuous *'' (continuous star) to identify this family of continuous activities.",poster,cp7
p608,50a72c2384ab9cb6c4a32710e874730f5799eb60,j112,Cambridge International Law Journal,Continuous Software Engineering,Abstract content,fullPaper,jv112
p609,b9618c3e0428e365f7600476b89e3c2cbb3e54e5,c11,Hawaii International Conference on System Sciences,Introduction to Green in Software Engineering,Abstract content,poster,cp11
p610,1c32125cc9fb052b881a6dec812b62ed998915d7,j145,Journal of Systems and Software,Lessons from applying the systematic literature review process within the software engineering domain,Abstract content,fullPaper,jv145
p611,eb2640199a8bf20b85b54757a99e07942b5909ee,j142,IEEE Transactions on Software Engineering,A Systematic Literature Review on Fault Prediction Performance in Software Engineering,"Background: The accurate prediction of where faults are likely to occur in code can help direct test effort, reduce costs, and improve the quality of software. Objective: We investigate how the context of models, the independent variables used, and the modeling techniques applied influence the performance of fault prediction models. Method: We used a systematic literature review to identify 208 fault prediction studies published from January 2000 to December 2010. We synthesize the quantitative and qualitative results of 36 studies which report sufficient contextual and methodological information according to the criteria we develop and apply. Results: The models that perform well tend to be based on simple modeling techniques such as Naive Bayes or Logistic Regression. Combinations of independent variables have been used by models that perform well. Feature selection has been applied to these combinations when models are performing particularly well. Conclusion: The methodology used to build models seems to be influential to predictive performance. Although there are a set of fault prediction studies in which confidence is possible, more studies are needed that use a reliable methodology and which report their context, methodology, and performance comprehensively.",fullPaper,jv142
p612,dce27ed67f146a38fa79159df9891e73712f75ed,c96,USENIX Symposium on Operating Systems Design and Implementation,Continuous Software Engineering: An Introduction,Abstract content,poster,cp96
p613,c98aa7f681d00b0ab6b612e0d731732cebee6003,c29,International Conference on Software Engineering,How to effectively use topic models for software engineering tasks? An approach based on Genetic Algorithms,"Information Retrieval (IR) methods, and in particular topic models, have recently been used to support essential software engineering (SE) tasks, by enabling software textual retrieval and analysis. In all these approaches, topic models have been used on software artifacts in a similar manner as they were used on natural language documents (e.g., using the same settings and parameters) because the underlying assumption was that source code and natural language documents are similar. However, applying topic models on software data using the same settings as for natural language text did not always produce the expected results. Recent research investigated this assumption and showed that source code is much more repetitive and predictable as compared to the natural language text. Our paper builds on this new fundamental finding and proposes a novel solution to adapt, configure and effectively use a topic modeling technique, namely Latent Dirichlet Allocation (LDA), to achieve better (acceptable) performance across various SE tasks. Our paper introduces a novel solution called LDA-GA, which uses Genetic Algorithms (GA) to determine a near-optimal configuration for LDA in the context of three different SE tasks: (1) traceability link recovery, (2) feature location, and (3) software artifact labeling. The results of our empirical studies demonstrate that LDA-GA is able to identify robust LDA configurations, which lead to a higher accuracy on all the datasets for these SE tasks as compared to previously published results, heuristics, and the results of a combinatorial search.",fullPaper,cp29
p614,fa9f622a1182400067d911b0900733695ec1b358,j141,Empirical Software Engineering,Parameter tuning or default values? An empirical investigation in search-based software engineering,Abstract content,fullPaper,jv141
p615,a361700a350f928b9d3a5730acff25c2722e3a60,c103,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,Design patterns: elements of reuseable object-oriented software,"The book is an introduction to the idea of design patterns in software engineering, and a catalog of twenty-three common patterns. The nice thing is, most experienced OOP designers will find out they've known about patterns all along. It's just that they've never considered them as such, or tried to centralize the idea behind a given pattern so that it will be easily reusable.",poster,cp103
p616,7ab9367fbaff7f8a0dce861a6b90a7511029b369,j151,IEEE Transactions on Industrial Informatics,Software Engineering in Industrial Automation: State-of-the-Art Review,"This paper presents one perspective on recent developments related to software engineering in the industrial automation sector that spans from manufacturing factory automation to process control systems and energy automation systems. The survey's methodology is based on the classic SWEBOK reference document that comprehensively defines the taxonomy of software engineering domain. This is mixed with classic automation artefacts, such as the set of the most influential international standards and dominating industrial practices. The survey focuses mainly on research publications which are believed to be representative of advanced industrial practices as well.",fullPaper,jv151
p617,96ad61decc6f94a58251fda19cb9b80e631218af,j152,IEEE Annals of the History of Computing,The mythical man-month: Essays on software engineering,"Like Bahbage, he lobbied for mathematical reform, stumped for the centrality of science in cultural advancement, argued that government support was crucial, and proved a stubborn and crotchety opponent when crossed. And, as Colin Burke reminds us in this fine and fresh new look at Bush, Bush envisioned machines relevant to the history of computing that never lived up to their promise. I doubt that Burke would agree with my description of Bush as a latter-day Babbage; nevertheless, this detailed study makes the comparison almost inevitable. Burke helps us appreciate how Bush's fascination with the mechanization of calculation and comparison caused his inventive work to swirl around problems relevant to the emergence of the modern computer. Moreover, Burke suggests that two of Bush's less familiar engines-one, the Rapid Selector, a bibliographic machine and a close cousin of the Memex of faddish fame; and the other, the Comparator, a cryptanalytic device-provide the stuff to fill in the holes in the history of the computer [p. ix). It is never very clear just what these holes are; this reader, at least, was not convinced that the careers of these two machines were anything but eddies along the shore of the main currents of computer evolution. They were decisive failures, as Burke admits, rooted in a stubborn commitment to intractdbk and ultimately unfashion-able if not outdated technologies. The strengths of this book indeed lie elsewhere. These exotic devices are of interest in themselves and deserve their biographer's attention. Burke details the labors of Bush and friends to use microfilm, electronics, and photoelectricity to mechanize the library-hereby resolving a putative information overload (it turns out that there wasn't one)-and help the U.S. Navy's cryptographers break enemy codes during World War 11. Burke is best, however, when discussing not machines themselves but when individuals and bureaucracies are at loggerheads. Ego, ambition, and organizational and technological vision were at stake. On the military side, and against much intcrnal resistance , Bush allies such as Stanford C. Hooper and Joseph Wenger dreamed of building the next generation of rapid analytic machines and, in doing so, dreamed of upgrading the scientific navy by forging alliances with "" college professors "" like Bush; on the civilian side, Bush and his "" boys "" worked to maneuver the navy into a project that promised much in the way of personal and institutional prestige, income for research, and opportunities for graduate …",fullPaper,jv152
p618,fe1801c4647adc317ff09082dab8c655e64f3b36,c5,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,"Search-based software engineering: Trends, techniques and applications","In the past five years there has been a dramatic increase in work on Search-Based Software Engineering (SBSE), an approach to Software Engineering (SE) in which Search-Based Optimization (SBO) algorithms are used to address problems in SE. SBSE has been applied to problems throughout the SE lifecycle, from requirements and project planning to maintenance and reengineering. The approach is attractive because it offers a suite of adaptive automated and semiautomated solutions in situations typified by large complex problem spaces with multiple competing and conflicting objectives.
 This article1 provides a review and classification of literature on SBSE. The work identifies research trends and relationships between the techniques applied and the applications to which they have been applied and highlights gaps in the literature and avenues for further research.",poster,cp5
p619,e708fe09fcbcebba443db826c6f0d6c839137cb0,c29,International Conference on Software Engineering,On the value of user preferences in search-based software engineering: A case study in software product lines,"Software design is a process of trading off competing objectives. If the user objective space is rich, then we should use optimizers that can fully exploit that richness. For example, this study configures software product lines (expressed as feature maps) using various search-based software engineering methods. As we increase the number of optimization objectives, we find that methods in widespread use (e.g. NSGA-II, SPEA2) perform much worse than IBEA (Indicator-Based Evolutionary Algorithm). IBEA works best since it makes most use of user preference knowledge. Hence it does better on the standard measures (hypervolume and spread) but it also generates far more products with 0% violations of domain constraints. Our conclusion is that we need to change our methods for search-based software engineering, particularly when studying complex decision spaces.",fullPaper,cp29
p620,0ed846e87ce0961d162e9115b4e9837537138e3a,c29,International Conference on Software Engineering,Analyze this! 145 questions for data scientists in software engineering,"In this paper, we present the results from two surveys related to data science applied to software engineering. The first survey solicited questions that software engineers would like data scientists to investigate about software, about software processes and practices, and about software engineers. Our analyses resulted in a list of 145 questions grouped into 12 categories. The second survey asked a different pool of software engineers to rate these 145 questions and identify the most important ones to work on first. Respondents favored questions that focus on how customers typically use their applications. We also saw opposition to questions that assess the performance of individual employees or compare them with one another. Our categorization and catalog of 145 questions can help researchers, practitioners, and educators to more easily focus their efforts on topics that are important to the software industry.",fullPaper,cp29
p621,1e62a8afbe6018540c60d9dcce1ff6bd98f2e404,c29,International Conference on Software Engineering,Automatic query reformulations for text retrieval in software engineering,"There are more than twenty distinct software engineering tasks addressed with text retrieval (TR) techniques, such as, traceability link recovery, feature location, refactoring, reuse, etc. A common issue with all TR applications is that the results of the retrieval depend largely on the quality of the query. When a query performs poorly, it has to be reformulated and this is a difficult task for someone who had trouble writing a good query in the first place. We propose a recommender (called Refoqus) based on machine learning, which is trained with a sample of queries and relevant results. Then, for a given query, it automatically recommends a reformulation strategy that should improve its performance, based on the properties of the query. We evaluated Refoqus empirically against four baseline approaches that are used in natural language document retrieval. The data used for the evaluation corresponds to changes from five open source systems in Java and C++ and it is used in the context of TR-based concept location in source code. Refoqus outperformed the baselines and its recommendations lead to query performance improvement or preservation in 84% of the cases (in average).",fullPaper,cp29
p622,6be2d983bd8bf7b198c03b6ec33ab34db75ad3d8,j75,Lecture Notes in Computer Science,Software Engineering for Self-Adaptive Systems II,Abstract content,fullPaper,jv75
p623,0bfca74dd16aad83216742115231d400d60a9f0d,c81,IEEE Annual Symposium on Foundations of Computer Science,Diversity in software engineering research,"One of the goals of software engineering research is to achieve generality: Are the phenomena found in a few projects reflective of others? Will a technique perform as well on projects other than the projects it is evaluated on? While it is common sense to select a sample that is representative of a population, the importance of diversity is often overlooked, yet as important. In this paper, we combine ideas from representativeness and diversity and introduce a measure called sample coverage, defined as the percentage of projects in a population that are similar to the given sample. We introduce algorithms to compute the sample coverage for a given set of projects and to select the projects that increase the coverage the most. We demonstrate our technique on research presented over the span of two years at ICSE and FSE with respect to a population of 20,000 active open source projects monitored by Ohloh.net. Knowing the coverage of a sample enhances our ability to reason about the findings of a study. Furthermore, we propose reporting guidelines for research: in addition to coverage scores, papers should discuss the target population of the research (universe) and dimensions that potentially can influence the outcomes of a research (space).",poster,cp81
p624,433f2ad582b75d593d12f47d717d0fea0bd824de,c42,Joint Conference of International Workshop on Software Measurement and International Conference on Software Process and Product Measurement,"Worldviews, Research Methods, and their Relationship to Validity in Empirical Software Engineering Research","Background - Validity threats should be considered and consistently reported to judge the value of an empirical software engineering research study. The relevance of specific threats for a particular research study depends on the worldview or philosophical worldview of the researchers of the study. Problem/Gap - In software engineering, different categorizations exist, which leads to inconsistent reporting and consideration of threats. Contribution - In this paper, we relate different worldviews to software engineering research methods, identify generic categories for validity threats, and provide a categorization of validity threats with respect to their relevance for different world views. Thereafter, we provide a checklist aiding researchers in identifying relevant threats. Method - Different threat categorizations and threats have been identified in literature, and are reflected on in relation to software engineering research. Results - Software engineering is dominated by the pragmatist worldviews, and therefore use multiple methods in research. Maxwell's categorization of validity threats has been chosen as very suitable for reporting validity threats in software engineering research. Conclusion - We recommend to follow a checklist approach, and reporting first the philosophical worldview of the researcher when doing the research, the research methods and all threats relevant, including open, reduced, and mitigated threats.",fullPaper,cp42
p625,09b4abe4142cfe4aef372d75288f5cf94893fea5,c43,ACM Symposium on Applied Computing,Towards a definition of sustainability in and for software engineering,"Sustainability is not supported by traditional software engineering methods. This lack of support leads to inefficient efforts to address sustainability or complete omission of this important concept. Defining and developing adequate support requires a commonly accepted definition of what sustainability means in and for software engineering.
 We contribute a description of the aspects of sustainability in software engineering.",fullPaper,cp43
p626,09f16130185fac1a9766707cbfc4285c7a108f65,c78,Neural Information Processing Systems,A Green Model for Sustainable Software Engineering,"Information Communication Technology (ICT) has a strong impact on sustainable development due its rising demands for energy and resources needed when building hardware and software products. Most of the efforts spent on Green ICT/IT have been dedicated to addressing the effects of hardware on the environment but little have been considering the effects of building software products as well. Efficient software will indirectly consume less energy by using up less hardware equipment to run. Our contributions in this paper are devoted to building a two level green software model that covers the sustainable life cycle of a software product and the software tools promoting green and environmentally sustainable software. In the first level we propose a new green software engineering process that is a hybrid process between sequential, iterative, and agile development processes to produce an environmentally sustainable one. Each stage of the software process is then further studied to produce a green and sustainable stage. We propose either green guidelines or green processes for each software stage in the engineering process. We add to the software life cycle the requirements stage and the testing stage. We also include in the first level a complete list of metrics to measure the greenness of each stage in terms of the first order effects of ICT on the environment for a green software engineering process. No effort has been placed before in designing a green software engineering process. The second level explains how software itself can be used as a tool to aid in green computing by monitoring resources in an energy efficient manner. Finally, we show and explain relationships that can be found between the two levels in our proposed model to make the software engineering process and product green and sustainable.",poster,cp78
p627,7fc5d33b8891b51d9eecd7bf3eed766a94ff5789,c43,ACM Symposium on Applied Computing,Software Engineering Processes for Self-Adaptive Systems,Abstract content,poster,cp43
p628,e8f19d716d3d5391d1bb6254b11fa88e7f011f62,c21,Grid Computing Environments,Organizational social structures for software engineering,"Software engineering evolved from a rigid process to a dynamic interplay of people (e.g., stakeholders or developers). Organizational and social literature call this interplay an Organizational Social Structure (OSS). Software practitioners still lack a systematic way to select, analyze, and support OSSs best fitting their problems (e.g., software development). We provide the state-of-the-art in OSSs, and discuss mechanisms to support OSS-related decisions in software engineering (e.g., choosing the OSS best fitting development scenarios). Our data supports two conclusions. First, software engineering focused on building software using project teams alone, yet these are one of thirteen OSS flavors from literature. Second, an emerging OSS should be further explored for software development: social networks. This article represents a first glimpse at OSS-aware software engineering, that is, to engineer software using OSSs best fit for the problem.",poster,cp21
p629,92ca9f57ebb0cfdbfa07e5d5956e11509f902f0b,c29,International Conference on Software Engineering,A practical guide for using statistical tests to assess randomized algorithms in software engineering,"Randomized algorithms have been used to successfully address many different types of software engineering problems. This type of algorithms employ a degree of randomness as part of their logic. Randomized algorithms are useful for difficult problems where a precise solution cannot be derived in a deterministic way within reasonable time. However, randomized algorithms produce different results on every run when applied to the same problem instance. It is hence important to assess the effectiveness of randomized algorithms by collecting data from a large enough number of runs. The use of rigorous statistical tests is then essential to provide support to the conclusions derived by analyzing such data. In this paper, we provide a systematic review of the use of randomized algorithms in selected software engineering venues in 2009. Its goal is not to perform a complete survey but to get a representative snapshot of current practice in software engineering research. We show that randomized algorithms are used in a significant percentage of papers but that, in most cases, randomness is not properly accounted for. This casts doubts on the validity of most empirical results assessing randomized algorithms. There are numerous statistical tests, based on different assumptions, and it is not always clear when and how to use these tests. We hence provide practical guidelines to support empirical research on randomized algorithms in software engineering",fullPaper,cp29
p630,1cf41a21d103549f411181a297cd7e22aac417ea,c44,International Workshop on Green and Sustainable Software,Green software engineering with agile methods,"The energy consumption of information and communication technology (ICT) is still increasing. Since several concepts regarding hardware solutions for Green IT exist, the contribution of software to Green IT is still not well investigated. This comprises the production and the usage impact of software on energy consumption. In our paper, we discuss this contribution. Especially, we present a model that integrates Green IT aspects into software engineering processes with agile methods in order to produce “greener” software from scratch.",fullPaper,cp44
p631,7bcfe45fbdccbfc4667540b3c54b4aff398d140c,c45,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",What is social debt in software engineering?,"“Social debt” in software engineering informally refers to unforeseen project cost connected to a “suboptimal” development community. The causes of suboptimal development communities can be many, ranging from global distance to organisational barriers to wrong or uninformed socio-technical decisions (i.e., decisions that influence both social and technical aspects of software development). Much like technical debt, social debt impacts heavily on software development success. We argue that, to ensure quality software engineering, practitioners should be provided with mechanisms to detect and manage the social debt connected to their development communities. This paper defines and elaborates on social debt, pointing out relevant research paths. We illustrate social debt by comparison with technical debt and discuss common real-life scenarios that exhibit “sub-optimal” development communities.",fullPaper,cp45
p632,be779149798a217e81a4d9deb46a68db218a22c2,j144,Information and Software Technology,Systematic literature reviews in software engineering,Abstract content,fullPaper,jv144
p633,5c7e1b47e0864c8e9e075389d17c31352b0484ee,c92,Advances in Soft Computing,Software Engineering for Self-Adaptive Systems: A Research Roadmap,Abstract content,poster,cp92
p634,606b072c6ca0f03cbaa3757f5897748b3921d466,c81,IEEE Annual Symposium on Foundations of Computer Science,Agent-based software engineering,"The technology of intelligent agents and multi-agent systems is expected to alter radically the way in which complex, distributed, open systems are conceptualised and implemented. The paper considers the problem of building a multi-agent system as a software engineering enterprise. Three issues are focused on: how agents might be specified; how these specifications might be refined or otherwise transformed into efficient implementations: and how implemented agents and multi-agent systems might subsequently be verified, to show that they are correct with respect to their specifications. These issues are discussed with reference to a number of case studies. The paper concludes by setting out some issues and open problems for future research.",poster,cp81
p635,969601d87215d4fcf3b83246bcdd132c41ab716d,j47,Environmental Modelling & Software,A software engineering perspective on environmental modeling framework design: The Object Modeling System,Abstract content,fullPaper,jv47
p636,dce99209120ebed7f5d68e3644fdcd160d4c366c,c34,IEEE Working Conference on Mining Software Repositories,Standard Glossary of Software Engineering Terminology,"IEEE Std 610.12-1990, IEEE Standard Glossary of Software Engineering Terminology, identifies terms currently in use in the field of Software Engineering. Standard definitions for those terms are established.",poster,cp34
p637,7f714a6def35b98aa94d1f6a8e0026d4e023ec08,j145,Journal of Systems and Software,On the reliability of mapping studies in software engineering,Abstract content,fullPaper,jv145
p638,396eab2434bcfd369b472fa494b62cee8465a2f4,c99,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,"Educational software engineering: Where software engineering, education, and gaming meet","We define and advocate the subfield of educational software engineering (i.e., software engineering for education), which develops software engineering technologies (e.g., software testing and analysis, software analytics) for general educational tasks, going beyond educational tasks for software engineering. In this subfield, gaming technologies often play an important role together with software engineering technologies. We expect that researchers in educational software engineering would be among key players in the education domain and in the coming age of Massive Open Online Courses (MOOCs). Educational software engineering can and will contribute significant solutions to address various critical challenges in education especially MOOCs such as automatic grading, intelligent tutoring, problem generation, and plagiarism detection. In this position paper, we define educational software engineering and illustrate Pex for Fun (in short as Pex4Fun), one of our recent examples on leveraging software engineering and gaming technologies to address educational tasks on teaching and learning programming and software engineering skills.",poster,cp99
p639,539dbb218dabc9fbe86fcac304522e2c95c8b429,c42,Joint Conference of International Workshop on Software Measurement and International Conference on Software Process and Product Measurement,Preliminary Guidelines for Empirical Research in Software Engineering,"Empirical software engineering research needs research guidelines to improve the research and reporting processes. We propose a preliminary set of research guidelines aimed at stimulating discussion among software researchers. They are based on a review of research guidelines developed for medical researchers and on our own experience in doing and reviewing software engineering research. The guidelines are intended to assist researchers, reviewers, and meta-analysts in designing, conducting, and evaluating empirical studies. Editorial boards of software engineering journals may wish to use our recommendations as a basis for developing guidelines for reviewers and for framing policies for dealing with the design, data collection, and analysis and reporting of empirical studies.",poster,cp42
p640,b7e9e308d4a48bdf266e1907a77476dfbd67f260,c5,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,Software Product Line Engineering Foundations Principles And Techniques,"Thank you for reading software product line engineering foundations principles and techniques. Maybe you have knowledge that, people have look numerous times for their favorite books like this software product line engineering foundations principles and techniques, but end up in malicious downloads. Rather than reading a good book with a cup of coffee in the afternoon, instead they are facing with some infectious bugs inside their desktop computer.",poster,cp5
p641,845afdf05ac75fedb65532487aadd0538bc4c6da,c10,Big Data,Qualitative Methods in Empirical Studies of Software Engineering,"While empirical studies in software engineering are beginning to gain recognition in the research community, this subarea is also entering a new level of maturity by beginning to address the human aspects of software development. This added focus has added a new layer of complexity to an already challenging area of research. Along with new research questions, new research methods are needed to study nontechnical aspects of software engineering. In many other disciplines, qualitative research methods have been developed and are commonly used to handle the complexity of issues involving human behaviour. The paper presents several qualitative methods for data collection and analysis and describes them in terms of how they might be incorporated into empirical studies of software engineering, in particular how they might be combined with quantitative methods. To illustrate this use of qualitative methods, examples from real software engineering studies are used throughout.",poster,cp10
p642,0d70f756c410370d56f9e61f25f68ab606533bf2,j153,bioRxiv,BEAST 2.5: An advanced software platform for Bayesian evolutionary analysis,"Elaboration of Bayesian phylogenetic inference methods has continued at pace in recent years with major new advances in nearly all aspects of the joint modelling of evolutionary data. It is increasingly appreciated that some evolutionary questions can only be adequately answered by combining evidence from multiple independent sources of data, including genome sequences, sampling dates, phenotypic data, radiocarbon dates, fossil occurrences, and biogeographic range information among others. Including all relevant data into a single joint model is very challenging both conceptually and computationally. Advanced computational software packages that allow robust development of compatible (sub-)models which can be composed into a full model hierarchy have played a key role in these developments. Developing such software frameworks is increasingly a major scientific activity in its own right, and comes with specific challenges, from practical software design, development and engineering challenges to statistical and conceptual modelling challenges. BEAST 2 is one such computational software platform, and was first announced over 4 years ago. Here we describe a series of major new developments in the BEAST 2 core platform and model hierarchy that have occurred since the first release of the software, culminating in the recent 2.5 release. Author summary Bayesian phylogenetic inference methods have undergone considerable development in recent years, and joint modelling of rich evolutionary data, including genomes, phenotypes and fossil occurrences is increasingly common. Advanced computational software packages that allow robust development of compatible (sub-)models which can be composed into a full model hierarchy have played a key role in these developments. Developing scientific software is increasingly crucial to advancement in many fields of biology. The challenges range from practical software development and engineering, distributed team coordination, conceptual development and statistical modelling, to validation and testing. BEAST 2 is one such computational software platform for phylogenetics, population genetics and phylodynamics, and was first announced over 4 years ago. Here we describe the full range of new tools and models available on the BEAST 2.5 platform, which expand joint evolutionary inference in many new directions, especially for joint inference over multiple data types, non-tree models and complex phylodynamics.",fullPaper,jv153
p643,acdc89274e34d2f905a055aad41e7f815aef52eb,c53,International Conference on Software Engineering and Knowledge Engineering,Selecting Empirical Methods for Software Engineering Research,Abstract content,poster,cp53
p644,19c64da92a8c67c14ddd9cced0d1d0b9ff6b39d1,j154,"Software, Practice & Experience",An open graph visualization system and its applications to software engineering,"We describe a package of practical tools and libraries for manipulating graphs and their drawings. Our design, which is aimed at facilitating the combination of the package components with other tools, includes stream and event interfaces for graph operations, high‐quality static and dynamic layout algorithms, and the ability to handle sizeable graphs. We conclude with a description of the applications of this package to a variety of software engineering tools. Copyright © 2000 John Wiley & Sons, Ltd.",fullPaper,jv154
p645,6f32fbbafcb25e85a167ecb17308abdbaf64f1aa,c88,Symposium on the Theory of Computing,Software design,"From the Publisher: 
 
Based on a curriculum module originally written for the Software Engineering Institute at Carnegie Mellon University, this text provides students with an introduction to the role of design in software engineering. The book surveys a wide range of design methods and evaluates their strengths and weaknesses in various applications. The author adopts a neutral approach, concentrating on the role of design in software development creating a more effective tutorial text for students. 
Features 
Provides a balanced introduction to software design, reviewing the leading design methods, both formal and informal, from a neutral viewpoint. 
Describes and evaluates a wide range of different design methods, including JSP, SSA/SD, JSD, object-oriented and object- based design 
Focuses on design principles and strategies, which can be directly applied in practice.",poster,cp88
p646,cf20d6130160843e5cd8ef8f5553d66a920bc6f3,c46,Brazilian Symposium on Software Engineering,Global software engineering and agile practices: a systematic review,"Agile practices have received attention from industry as an alternative to plan‐driven software development approaches. Agile encourages, for example, small self‐organized collocated teams, whereas global software engineering (GSE) implies distribution across cultural, temporal, and geographical boundaries. Hence, combining them is a challenge. A systematic review was conducted to capture the status of combining agility with GSE. The results were limited to peer‐reviewed conference papers or journal articles, published between 1999 and 2009. The synthesis was made through classifying the papers into different categories (e.g. publication year, contribution type, research method). At the end, 81 papers were judged as primary for further analysis. The distribution of papers over the years indicated that GSE and Agile in combination has received more attention in the last 5 years. However, the majority of the existing research is industrial experience reports in which Agile practices were modified with respect to the context and situational requirements. The emergent need in this research area is suggested to be developing a framework that considers various factors from different perspectives when incorporating Agile in GSE. Practitioners may use it as a decision‐making basis in early phases of software development. Copyright © 2011 John Wiley & Sons, Ltd.",poster,cp46
p647,291001b0585ecda5aa9b4ad39d435463bf03dc7d,c100,ACM SIGMOD Conference,Revisions to Software Engineering 2004: Curriculum Guidelines for Undergraduate Degree Programs in Software Engineering,Software Engineering 2004: Curriculum Guidelines for Undergraduate Degree Programs in Software Engineering (SE 2004) is one volume in a set of computing curricula adopted and supported by the ACM and the IEEE Computer Society. In order to keep the software engineering guidelines up to date the two professional societies established a review project in early 2011. This paper describes that review effort and plans to revise the guidelines over the next year and a half.,poster,cp100
p648,1727443b1a5c14663f4b04e7c6f27c36ca1c1ba3,c73,Workshop on Algorithms in Bioinformatics,"What works for whom, where, when, and why? On the role of context in empirical software engineering","Context is a central concept in empirical software engineering. It is one of the distinctive features of the discipline and it is an indispensable part of software practice. It is likely responsible for one of the most challenging methodological and theoretical problems: study-to-study variation in research findings. Still, empirical software engineering research is mostly concerned with attempts to identify universal relationships that are independent of how work settings and other contexts interact with the processes important to software practice. The aim of this paper is to provide an overview of how context affects empirical research and how empirical software engineering research can be better `contextualized' in order to provide a better understanding of what works for whom, where, when, and why. We exemplify the importance of context with examples from recent systematic reviews and offer recommendations on the way forward.",poster,cp73
p649,45559b74f535a218746b2e5d837f39f33b31d492,j144,Information and Software Technology,How to design gamification? A method for engineering gamified software,Abstract content,fullPaper,jv144
p650,276da75fba54fef4bfe20b6b8841f78933ed3d0e,j141,Empirical Software Engineering,Special issue on repeatable results in software engineering prediction,Abstract content,fullPaper,jv141
p651,ebbafe78a53a2086420034f6b3ee230c3bfae815,c53,International Conference on Software Engineering and Knowledge Engineering,Dynamic adaptive Search Based Software Engineering,"Search Based Software Engineering (SBSE) has proved to be a very effective way of optimising software engineering problems. Nevertheless, its full potential as a means of dynamic adaptivity remains under explored. This paper sets out the agenda for Dynamic Adaptive SBSE, in which the optimisation is embedded into deployed software to create self-optimising adaptive systems. Dynamic Adaptive SBSE will move the research agenda forward to encompass both software development processes and the software products they produce, addressing the long-standing, and as yet largely unsolved, grand challenge of self-adaptive systems.",poster,cp53
p652,7093ea1420fca2a2cf679073ae14b77f07d5fbe5,j147,IEEE Software,Where's the Theory for Software Engineering?,"Darwin's theory of natural selection, Maxwell's equations, the theory of demand and supply; almost all established academic disciplines place great emphasis on what their core theory is. This is not, however, the case in software engineering. What is the reason behind the software engineering community's apparent indifference to a concept that is so important to so many others?",fullPaper,jv147
p653,03992dcfe1943355229cae90b7b206a480220bad,j141,Empirical Software Engineering,On the dataset shift problem in software engineering prediction models,Abstract content,fullPaper,jv141
p654,e8dbf08e81b7db05abfc0526af0e97f1e679c66f,c98,North American Chapter of the Association for Computational Linguistics,The role of Artificial Intelligence in Software Engineering,"There has been a recent surge in interest in the application of Artificial Intelligence (AI) techniques to Software Engineering (SE) problems. The work is typified by recent advances in Search Based Software Engineering, but also by long established work in Probabilistic reasoning and machine learning for Software Engineering. This paper explores some of the relationships between these strands of closely related work, arguing that they have much in common and sets out some future challenges in the area of AI for SE.",poster,cp98
p655,35e06978cd069786ff7ab05ac29d6e2e28f1bad6,j155,Advances in Software Engineering,Clustering Methodologies for Software Engineering,"The size and complexity of industrial strength software systems are constantly increasing. This means that the task of managing a large software project is becoming even more challenging, especially in light of high turnover of experienced personnel. Software clustering approaches can help with the task of understanding large, complex software systems by automatically decomposing them into smaller, easier-to-manage subsystems. The main objective of this paper is to identify important research directions in the area of software clustering that require further attention in order to develop more effective and efficient clustering methodologies for software engineering. To that end, we first present the state of the art in software clustering research. We discuss the clustering methods that have received the most attention from the research community and outline their strengths and weaknesses. Our paper describes each phase of a clustering algorithm separately. We also present the most important approaches for evaluating the effectiveness of software clustering.",fullPaper,jv155
p656,d48a7458182aeebf080d4bc2b0100e6d126c1b79,j142,IEEE Transactions on Software Engineering,Engineering Trustworthy Self-Adaptive Software with Dynamic Assurance Cases,"Building on concepts drawn from control theory, self-adaptive software handles environmental and internal uncertainties by dynamically adjusting its architecture and parameters in response to events such as workload changes and component failures. Self-adaptive software is increasingly expected to meet strict functional and non-functional requirements in applications from areas as diverse as manufacturing, healthcare and finance. To address this need, we introduce a methodology for the systematic ENgineering of TRUstworthy Self-adaptive sofTware (ENTRUST). ENTRUST uses a combination of (1) design-time and runtime modelling and verification, and (2) industry-adopted assurance processes to develop trustworthy self-adaptive software and assurance cases arguing the suitability of the software for its intended application. To evaluate the effectiveness of our methodology, we present a tool-supported instance of ENTRUST and its use to develop proof-of-concept self-adaptive software for embedded and service-based systems from the oceanic monitoring and e-finance domains, respectively. The experimental results show that ENTRUST can be used to engineer self-adaptive software systems in different application domains and to generate dynamic assurance cases for these systems.",fullPaper,jv142
p657,009ae8dd5ef8034aba907fb8397f3261b62f193e,c32,International Conference on Software Technology: Methods and Tools,Object-Oriented Software Engineering,"A text on industrial system development using object- oriented techniques, rather than a book on object-oriented programming. Will be useful to systems developers and those seeking a deeper understanding of object orientation as it relates to the development process.",fullPaper,cp32
p658,29d2b8dba00379edccbdd2bda90de9762d0c1003,j147,IEEE Software,Embracing the Engineering Side of Software Engineering,"The author provides, based on 20 years of research and industrial experience, his assessment of software engineering research. He then builds on such analysis to provide recommendations on how we need to change as a research community to increase our impact, gain credibility, and ultimately ensure the success and recognition of our young discipline. The gist of the author's message is that we need to become a true engineering discipline.",fullPaper,jv147
p659,f366beb5c0e0233b22f7f86d91b978035061b010,c46,Brazilian Symposium on Software Engineering,Software Ecosystems: Trends and Impacts on Software Engineering,"Economic and social issues are pointed out as Software Engineering (SE) challenges for the next years, since the field needs to treat issues beyond the technical side. These challenges require analyzing the field of SE from another perspective. In this sense, the study of software ecosystems (SECOs) is an emerging discipline that investigates the relationships among companies in the software industry. Companies work cooperatively and competitively in order to achieve their strategic objectives. They must engage in a new perspective, now also including third parties motivations and movements in the ecosystem, besides their own business viewpoint. Inspired on properties of natural and business ecosystems, SECO covers technical and business aspects of software development as well as partnership among companies. In this paper, we undertake a review on SECOs status as an emerging research topic in SE community. We map what is currently known about SECOs and also analyze them in a three-dimensional perspective in SE, i.e., technical, business and social. We observed that SECOs research is concentrated in eight main areas in which the most relevant ones are open source software, ecosystem modeling, and business issues. This paper also contributes to summarize the body of knowledge and presents a research agenda in SECOs.",fullPaper,cp46
p660,e5dc4cae90bfe566c757028e42d77b013fe6f331,c72,Intelligent Systems in Molecular Biology,Software Engineering Game: Software Engineering Game,"The goal of this paper is to explore and evaluate the utility of different strategies used in educational games. These strategies include creating immersion and promote learning through visual gratification, feedback, scoring, reasoning, and cognitively demanding environments. A game prototype, based on the tactics and strategies outlined in the literature study, has been implemented and is subject for testing on students. The tests aims to determine how well different strategies would do in an actual implementation of a game.",poster,cp72
p661,8f5d22051835e2f303efefd62329c100972c1fb5,c70,International Conference on Intelligent Robotics and Applications,Requirements Engineering - From System Goals to UML Models to Software Specifications,"Essential comprehensive coverage of the fundamentals of requirements engineering Requirements engineering (RE) deals with the variety of prerequisites that must be met by a software system within an organization in order for that system to produce stellar results. With that explanation in mind, this must-have book presents a disciplined approach to the engineering of high-quality requirements. Serving as a helpful introduction to the fundamental concepts and principles of requirements engineering, this guide offers a comprehensive review of the aim, scope, and role of requirements engineering as well as best practices and flaws to avoid. Shares state-of-the-art techniques for domain analysis, requirements elicitation, risk analysis, conflict management, and more Features in-depth treatment of system modeling in the specific context of engineering requirements Presents various forms of reasoning about models for requirements quality assurance Discusses the transitions from requirements to software specifications to software architecture In addition, case studies are included that complement the many examples provided in the book in order to show you how the described method and techniques are applied in practical situations.",poster,cp70
p662,c3910095b25a674e7154acd9c38d0af220026e31,j144,Information and Software Technology,Systematic literature reviews in software engineering - A tertiary study,Abstract content,fullPaper,jv144
p663,22201608e353530edff3ca503ccf7dad5df66efa,j144,Information and Software Technology,A Process Framework for Global Software Engineering Teams,Abstract content,fullPaper,jv144
p664,b2723f0da6f6219a823f573ecb92868da3b40b20,c88,Symposium on the Theory of Computing,On software engineering repositories and their open problems,"In the last decade, a large number of software repositories have been created for different purposes. In this paper we present a survey of the publicly available repositories and classify the most common ones as well as discussing the problems faced by researchers when applying machine learning or statistical techniques to them.",poster,cp88
p665,ab2ec1a1b6fdb710b2782554086a4782d50fc2c8,c47,International Symposium on Empirical Software Engineering and Measurement,Recommended Steps for Thematic Synthesis in Software Engineering,"Thematic analysis is an approach that is often used for identifying, analyzing, and reporting patterns (themes) within data in primary qualitative research. 'Thematic synthesis' draws on the principles of thematic analysis and identifies the recurring themes or issues from multiple studies, interprets and explains these themes, and draws conclusions in systematic reviews. This paper conceptualizes the thematic synthesis approach in software engineering as a scientific inquiry involving five steps that parallel those of primary research. The process and outcome associated with each step are described and illustrated with examples from systematic reviews in software engineering.",fullPaper,cp47
p666,61eff4ad67da3606c8f63c3d398f06aa3e01d9f3,c90,Computer Vision and Pattern Recognition,Software Engineering for Self-Adaptive Systems: A Second Research Roadmap,Abstract content,poster,cp90
p667,bb16132f91b92e1e6b81cf7fa44e31c6abcd4f19,j49,ACM Computing Surveys,The state of the art in end-user software engineering,"Most programs today are written not by professional software developers, but by people with expertise in other domains working towards goals for which they need computational support. For example, a teacher might write a grading spreadsheet to save time grading, or an interaction designer might use an interface builder to test some user interface design ideas. Although these end-user programmers may not have the same goals as professional developers, they do face many of the same software engineering challenges, including understanding their requirements, as well as making decisions about design, reuse, integration, testing, and debugging. This article summarizes and classifies research on these activities, defining the area of End-User Software Engineering (EUSE) and related terminology. The article then discusses empirical research about end-user software engineering activities and the technologies designed to support them. The article also addresses several crosscutting issues in the design of EUSE tools, including the roles of risk, reward, and domain complexity, and self-efficacy in the design of EUSE tools and the potential of educating users about software engineering principles.",fullPaper,jv49
p668,eb2036e0a40c00f450e9d983e0ce23e40cbf1f7e,j156,Springer US,Basics of Software Engineering Experimentation,Abstract content,fullPaper,jv156
p669,6028656fb04af3859ae8eeb349ff4b0367db41f7,c66,Annual Conference on Innovation and Technology in Computer Science Education,Software engineering metrics and models,Abstract content,poster,cp66
p670,301311f883cb3df1b1c00077ddf5f2fc0ed2f4f8,c113,International Conference on Image Analysis and Processing,Traffic engineering in software defined networks,"Software Defined Networking is a new networking paradigm that separates the network control plane from the packet forwarding plane and provides applications with an abstracted centralized view of the distributed network state. A logically centralized controller that has a global network view is responsible for all the control decisions and it communicates with the network-wide distributed forwarding elements via standardized interfaces. Google recently announced [5] that it is using a Software Defined Network (SDN) to interconnect its data centers due to the ease, efficiency and flexibility in performing traffic engineering functions. It expects the SDN architecture to result in better network capacity utilization and improved delay and loss performance. The contribution of this paper is on the effective use of SDNs for traffic engineering especially when SDNs are incrementally introduced into an existing network. In particular, we show how to leverage the centralized controller to get significant improvements in network utilization as well as to reduce packet losses and delays. We show that these improvements are possible even in cases where there is only a partial deployment of SDN capability in a network. We formulate the SDN controller's optimization problem for traffic engineering with partial deployment and develop fast Fully Polynomial Time Approximation Schemes (FPTAS) for solving these problems. We show, by both analysis and ns-2 simulations, the performance gains that are achievable using these algorithms even with an incrementally deployed SDN.",poster,cp113
p671,dca45bd363820bce269a176a1ecde7e1885e2ea6,c48,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",B4: experience with a globally-deployed software defined wan,"We present the design, implementation, and evaluation of B4, a private WAN connecting Google's data centers across the planet. B4 has a number of unique characteristics: i) massive bandwidth requirements deployed to a modest number of sites, ii) elastic traffic demand that seeks to maximize average bandwidth, and iii) full control over the edge servers and network, which enables rate limiting and demand measurement at the edge. These characteristics led to a Software Defined Networking architecture using OpenFlow to control relatively simple switches built from merchant silicon. B4's centralized traffic engineering service drives links to near 100% utilization, while splitting application flows among multiple paths to balance capacity against application priority/demands. We describe experience with three years of B4 production deployment, lessons learned, and areas for future work.",fullPaper,cp48
p672,9f9fb8a7bbf8e14ffcc82c4ccc0c58885b3dfbfa,c87,European Conference on Computer Vision,Software product lines - practices and patterns,"Foreword. Preface. Acknowledgements. Dedication. Reader's Guide. I. SOFTWARE PRODUCT LINE FUNDAMENTALS. 1. Basic Ideas and Terms. What Is a Software Product Line? What Software Product Lines Are Not. Fortuitous Small-Grained Reuse. Single-System Development with Reuse. Just Component-Based Development. Just a Reconfigurable Architecture. Releases and Versions of Single Products. Just a Set of Technical Standards. A Note on Terminology. For Further Reading. Discussion Questions. 2. Benefits. Organizational Benefits. Individual Benefits. Benefits versus Costs. For Further Reading. Discussion Questions. 3. The Three Essential Activities. What Are the Essential Activities? Core Asset Development. Product Development. Management. All Three Together. For Further Reading. Discussion Questions. II. SOFTWARE PRODUCT LINE PRACTICE AREAS. Describing the Practice Areas. Starting versus Running a Product Line. Organizing the Practice Areas. 4. Software Engineering Practice Areas. Architecture Definition. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Architecture Evaluation. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Component Development. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. COTS Utilization. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Mining Existing Assets. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. Discussion Questions. Requirements Engineering. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Software System Integration. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Testing. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Understanding Relevant Domains. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. 5. Technical Management Practice Areas. Configuration Management. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Data Collection, Metrics, and Tracking. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Make/Buy/Mine/Commission Analysis. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Process Definition. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Scoping. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Technical Planning. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. Discussion Questions. Technical Risk Management. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Tool Support. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. 6. Organizational Management Practice Areas. Building a Business Case. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Customer Interface Management. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. Discussion Questions. Developing an Acquisition Strategy. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Funding. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. Discussion Questions. Launching and Institutionalizing. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. Discussion Questions. Market Analysis. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Operations. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Organizational Planning. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. Discussion Questions. Organizational Risk Management. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Structuring the Organization. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. Discussion Questions. Technology Forecasting. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Training. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. III. PUTTING THE PRACTICE AREAS INTO ACTION. 7. Software Product Line Practice Patterns. The Value of Patterns. Software Product Line Practice Pattern Descriptions. The Curriculum Pattern. The Essentials Coverage Pattern. Each Asset Pattern. What to Build Pattern. Product Parts Pattern. Assembly Line Pattern. Monitor Pattern. Product Builder Pattern. Cold Start Pattern. In Motion Pattern. Process Pattern. Factory Pattern. Other Patterns. Practice Area Coverage. Discussion Questions. 8. Product Line Technical Probe. What Is the Product Line Technical Probe? Probe Interview Questions. Probe Participants. Probe Process. Using the Probe Results. Conducting a Mini Self-Probe. Discussion Questions. 9. Cummins Engine Company: Embracing the Future. Prologue. Company History. A Product Line of Engine Software. Getting off the Ground. An Organization Structured for Cooperation. Running the Product Line. Results. Lessons Learned. Epilogue. Practice Area Compendium. For Further Reading. Discussion Questions. 10. Control Channel Toolkit: A Software Product Line that Controls Satellites. Contextual Background. Organizational Profiles. Project History. Control Channels. Launching CCT. Developing a Business Case for CCT. Developing the Acquisition Strategy and Funding CCT. Structuring the CCT Organization. Organizational and Technical Planning. Operations. Engineering the CCT Core Assets. Domain Analysis. Architecture. Component Engineering. Testing: Application and Test Engineering. Sustainment Engineering: Product Line Evolution. Documentation. Managing the CCT Effort. Early Benefits from CCT. First CCT Product. Benefits beyond CCT Products. Lessons and Issues. Tool Support Is Inadequate. Domain Analysis Documentation Is Important. An Early Architecture Focus Is Best. Product Builders Need More Support. CCT Users Need Reuse Metrics. It Pays to Be Flexible, and Cross-Unit Teams Work. A Real Product Is a Benefit. Summary. For Further Reading. Discussion Questions. 11. Successful Software product Line Development in Small Organization. Introduction. The Early Years. The MERGER Software Product Line. Market Maker Software Product Line Practices. Architecture Definition. Component Development. Structuring (and Staffing) the Organization. Testing. Data Collection and Metrics. Launching and Institutionalizing the Product Line. Understanding the Market. Technology Forecasting. A Few Observations. Effects of Company Culture. Cost Issues. The Customer Paradox. Tool Support. Lessons Learned. Drawbacks. Conclusions: Software Product Lines in Small Organizations. For Further Reading. Discussion Questions. 12. Conclusions: Practices, Patterns and Payoffs. The Practices. The Patterns. The Success Factors. The Payoff. Finale. Glossary. Bibliography. Index.",poster,cp87
p673,e74f28e66a2aa715fff2cf60158b177c45130fdc,j144,Information and Software Technology,Identifying relevant studies in software engineering,Abstract content,fullPaper,jv144
p674,f60e0b81f1cf178175e0d726aa8824123c981500,c49,International Symposium on Search Based Software Engineering,On Parameter Tuning in Search Based Software Engineering,Abstract content,fullPaper,cp49
p675,ccd4e0cf89be4f0a1b8e979308695df7172985fe,c33,International Conference on Agile Software Development,Human-Centered Software Engineering - Integrating Usability in the Software Development Lifecycle,Abstract content,poster,cp33
p676,188e778ae2b5c1dee1dd76ce9c786f566f7e7079,c23,International Conference on Open and Big Data,Software Product Line Engineering,Abstract content,poster,cp23
p677,fbfbc0dd1d6501c8cf7aecfe38d3a0149b22d37a,c31,International Conference on Evaluation & Assessment in Software Engineering,Motivation in software engineering: A systematic review update,"Background/Aim - Given the relevance and importance that the understanding of motivation has gained in the field of software engineering, this work was carried out in order to update the results of a literature review carried out in 2006 on motivation in software engineering. Method - Based on guidelines for this specific type of study, we replicated the original study protocol. Results - The combination of manual and automatic searches retrieved 6,534 papers, of which 53 relevant papers were selected for data extraction and analysis. Conclusions - Studies address motivation using several viewpoints and approaches and, even though the number of researches increased in this area, the overall understanding of what actually motivates software engineers does not seem to have significantly advanced in the last five years.",fullPaper,cp31
p678,7e5933ff05db0f1edf6d98992d1a3bd31813b4c0,c35,EUROMICRO Conference on Software Engineering and Advanced Applications,A Mapping Study on Requirements Engineering in Agile Software Development,"Agile software development (ASD) methods have gained popularity in the industry and been the subject of an increasing amount of academic research. Although requirements engineering (RE) in ASD has been studied, the overall understanding of RE in ASD as a phenomenon is still weak. We conducted a mapping study of RE in ASD to review the scientific literature. 28 articles on the topic were identified and analyzed. The results indicate that the definition of agile RE is vague. The proposed benefits from agile RE included lower process overheads, a better requirements understanding, a reduced tendency to over allocate development resources, responsiveness to change, rapid delivery of value, and improved customer relationships. The problematic areas of agile RE were the use of customer representatives, the user story requirements format, the prioritization of requirements, growing technical debt, tacit requirements knowledge, and imprecise effort estimation. We also report proposed solutions to the identified problems.",fullPaper,cp35
p679,bdfb23a874d6222d4a800b3379348d784ff4f43d,c50,International Conference on Automated Software Engineering,Ecological inference in empirical software engineering,"Software systems are decomposed hierarchically, for example, into modules, packages and files. This hierarchical decomposition has a profound influence on evolvability, maintainability and work assignment. Hierarchical decomposition is thus clearly of central concern for empirical software engineering researchers; but it also poses a quandary. At what level do we study phenomena, such as quality, distribution, collaboration and productivity? At the level of files? packages? or modules? How does the level of study affect the truth, meaning, and relevance of the findings? In other fields it has been found that choosing the wrong level might lead to misleading or fallacious results. Choosing a proper level, for study, is thus vitally important for empirical software engineering research; but this issue hasn't thus far been explicitly investigated. We describe the related idea of ecological inference and ecological fallacy from sociology and epidemiology, and explore its relevance to empirical software engineering; we also present some case studies, using defect and process data from 18 open source projects to illustrate the risks of modeling at an aggregation level in the context of defect prediction, as well as in hypothesis testing.",fullPaper,cp50
p680,bbb9a237eb0cb75812d05c2d6428253bb1627a56,c112,Very Large Data Bases Conference,Software architecture in practice,"From the Book: 
 
Our goals for the first edition were threefold. First, we wanted to show through authentic case studies actual examples of software architectures solving real-world problems. Second, we wanted to establish and show the strong connection between an architecture and an organization's business goals. And third, we wanted to explain the importance of software architecture in achieving the quality goals for a system. 
 
Our goals for this second edition are the same, but the passage of time since the writing of the first edition has brought new developments in the field and new understanding of the important underpinnings of software architecture. We reflect the new developments with new case studies and the new understanding both through new chapters and through additions to and elaboration of the existing chapters. 
 
Architecture analysis, design, reconstruction, and documentation have all had major developments since the first edition. Architecture analysis has developed into a mature field with industrial-strength methods. This is reflected by a new chapter about the architecture tradeoff analysis method (ATAM). The ATAM has been adopted by industrial organizations as a technique for evaluating their software architectures. 
 
Architecture design has also had major developments since the first edition. The capturing of quality requirements, the achievement of those requirements through small-scale and large-scale architectural approaches (tactics and patterns, respectively), and a design method that reflects knowledge of how to achieve qualities are all captured in various chapters. Three new chapters treat understanding quality requirements, achieving qualities, and theattribute driven design (ADD) method, respectively. 
 
Architecture reconstruction or reverse engineering is an essential activity for capturing undocumented architectures. It can be used as a portion of a design project, an analysis project, or to provide input into a decision process to determine what to use as a basis for reconstructing an existing system. In the first edition, we briefly mentioned a tool set (Dali) and its uses in the re-engineering context; in in this edition the topic merits its own chapter. 
 
Documenting software architectures is another topic that has matured considerably in the recent past. When the first edition was published, the Unified Modeling Language (UML) was just arriving on the scene. Now it is firmly entrenched, a reality reflected by all-new diagrams. But more important, an understanding of what kind of information to capture about an architecture, beyond what notation to use, has emerged. A new chapter covers architecture documentation. 
 
The understanding of the application of software architecture to enable organizations to efficiently produce a variety of systems based on a single architecture is summarized in a totally rewritten chapter on software product lines. The chapter reinforces the link between architecture and an organization's business goals, as product lines, based around a software architecture, can enable order-of-magnitude improvements in cost, quality, and time to market. 
 
In addition to the architectural developments, the technology for constructing distributed and Web-based systems has become prominent in today's economy. We reflect this trend by updating the World Wide Web chapter, by using Web-based examples for the ATAM chapter and the chapter on building systems from components, by replacing the CORBA case study with one on Enterprise JavaBeans (EJB), and by introducing a case study on a wireless EJB system designed to support wearable computers for maintenance technicians. 
 
Finally, we have added a chapter that looks more closely at the financial aspects of architectures. There we introduce a method--the CBAM--for basing architectural decisions on economic criteria, in addition to the technical criteria that we had focused on previously. 
 
As in the first edition, we use the architecture business cycle as a unifying motif and all of the case studies are described in terms of the quality goals that motivated the system design and how the architecture for the system achieves those quality goals. 
 
In this edition, as in the first, we were very aware that our primary audience is practitioners, so we focus on presenting material that has been found useful in many industrial applications, as well as what we expect practice to be in the near future. 
 
We hope that you enjoy reading it at least as much as we enjoyed writing it. 
 
 
0321154959P12162002",poster,cp112
p681,7430c82d279f7cbc28556e8086e7cc60fef7f69f,j141,Empirical Software Engineering,The role of non-exact replications in software engineering experiments,Abstract content,fullPaper,jv141
p682,483b89068572e437251b3fb8c64a0ebe8eed1e1b,j147,IEEE Software,A Whisper of Evidence in Global Software Engineering,"A systematic review of global software engineering (GSE) literature from 2000 to 2007 shows the field to be immature. Studies report many challenges but little evidence regarding specific GSE practices directly related to project success or failure. There is evidence that distance matters and, furthermore, that GSE--although driven by cost-reduction goals--seldom brings immediate cost savings.",fullPaper,jv147
p683,013c70a1f66f7c251490d771678ee747535bbe3d,c29,International Conference on Software Engineering,Toward sustainable software engineering: NIER track,"Current software engineering practices have significant effects on the environment. Examples include e-waste from computers made obsolete due to software upgrades, and changes in the power demands of new versions of software. Sustainable software engineering aims to create reliable, long-lasting software that meets the needs of users while reducing environmental impacts. We conducted three related research efforts to explore this area. First, we investigated the extent to which users thought about the environmental impact of their software usage. Second, we created a tool called GreenTracker, which measures the energy consumption of software in order to raise awareness about the environmental impact of software usage. Finally, we explored the indirect environmental effects of software in order to understand how software affects sustainability beyond its own power consumption. The relationship between environmental sustainability and software engineering is complex; understanding both direct and indirect effects is critical to helping humans live more sustainably.",fullPaper,cp29
p684,df28611f5b9990d12844d5c85bd2994419a397bf,j144,Information and Software Technology,Research synthesis in software engineering: A tertiary study,Abstract content,fullPaper,jv144
p685,f2f72555b1bf716ccd8ab91a9441ea6d4864119d,c93,Human Language Technology - The Baltic Perspectiv,Software engineering issues for mobile application development,"This paper provides an overview of important software engineering research issues related to the development of applications that run on mobile devices. Among the topics are development processes, tools, user interface design, application portability, quality, and security.",poster,cp93
p686,e435d02025c6fe595528d64d235e41838035cda1,j144,Information and Software Technology,Six years of systematic literature reviews in software engineering: An updated tertiary study,Abstract content,fullPaper,jv144
p687,1fad54f5bf4c1c699ae51294658d5cf5bdd4df17,j75,Lecture Notes in Computer Science,Requirements Engineering: Foundation for Software Quality,Abstract content,fullPaper,jv75
p688,96b21ef42f94dc8f7ff7860c066a00d284f7362c,j145,Journal of Systems and Software,Bridging metamodels and ontologies in software engineering,Abstract content,fullPaper,jv145
p689,b13d31d1e1e65f8d5f7ea043dab6c9106e38382d,c114,IEEE International Conference on Robotics and Automation,Software engineering / Ian Sommerville.,Abstract content,poster,cp114
p690,6b8283005a83f24e6301605acbaad3bb6d277ca5,j142,IEEE Transactions on Software Engineering,A Methodology for Collecting Valid Software Engineering Data,"An effective data collection method for evaluating software development methodologies and for studying the software development process is described. The method uses goal-directed data collection to evaluate methodologies with respect to the claims made for them. Such claims are used as a basis for defining the goals of the data collection, establishing a list of questions of interest to be answered by data analysis, defining a set of data categorization schemes, and designing a data collection form. The data to be collected are based on the changes made to the software during development, and are obtained when the changes are made. To ensure accuracy of the data, validation is performed concurrently with software development and data collection. Validation is based on interviews with those people supplying the data. Results from using the methodology show that data validation is a necessary part of change data collection. Without it, as much as 50 percent of the data may be erroneous. Feasibility of the data collection methodology was demonstrated by applying it to five different projects in two different environments. The application showed that the methodology was both feasible and useful.",fullPaper,jv142
p691,8f79b5b359e5d49bdd2d53299c395e45815e4ffc,j141,Empirical Software Engineering,Qualitative research in software engineering,Abstract content,fullPaper,jv141
p692,0ae521671e4bd3178313f8ae517dc7861be1c449,j75,Lecture Notes in Computer Science,Component-Based Software Engineering,Abstract content,fullPaper,jv75
p693,4c880ad1186754be17e1907d0265b450ab309b48,c68,International Workshop on Software Engineering for Large-Scale Multi-Agent Systems,Software Metrics : A Rigorous and Practical Approach,"From the Publisher: 
The Second Edition of Software Metrics provides an up-to-date, coherent, and rigorous framework for controlling, managing, and predicting software development processes. With an emphasis on real-world applications, Fenton and Pfleeger apply basic ideas in measurement theory to quantify software development resources, processes, and products. The book offers an accessible and comprehensive introduction to software metrics, now an essential component of software engineering for both classroom and industry. Software Metrics features extensive case studies from Hewlett Packard, IBM, the U.S. Department of Defense, Motorola, and others, in addition to worked examples and exercises. The Second Edition includes up-to-date material on process maturity and measurement, goal-question-metric, planning a metrics program, measurement in practice, experimentation, empirical studies, ISO9216, and metric tools.",poster,cp68
p694,05882b3376dddab5066015b46704bb7bfe3946cc,j147,IEEE Software,Recommendation Systems for Software Engineering,"Software development can be challenging because of the large information spaces that developers must navigate. Without assistance, developers can become bogged down and spend a disproportionate amount of their time seeking information at the expense of other value-producing tasks. Recommendation systems for software engineering (RSSEs) are software tools that can assist developers with a wide range of activities, from reusing code to writing effective bug reports. The authors provide an overview of recommendation systems for software engineering: what they are, what they can do for developers, and what they might do in the future.",fullPaper,jv147
p695,3caedff0a82950046730bce6f8d85aec46cf2e8c,j141,Empirical Software Engineering,Empirical evidence in global software engineering: a systematic review,Abstract content,fullPaper,jv141
p696,ec5d75da55878c2edef2f62e908e40e99596a672,c17,International Conference on Enterprise Information Systems,Software Engineering: A Practitioner's Approach (McGraw-Hill Series in Computer Science),Abstract content,poster,cp17
p697,5a30fd3718835c500d1833492d6cd833c959d155,c94,Vision,Non-Functional Requirements in Software Engineering,Abstract content,poster,cp94
p698,87cc9fb5129781b74750d88f83472f2cd644ca60,c51,Conference of the Centre for Advanced Studies on Collaborative Research,An examination of software engineering work practices,"This paper presents work practice data of the daily activities of software engineers. Four separate studies are presented; one looking longitudinally at an individual SE; two looking at a software engineering group; and one looking at company-wide tool usage statistics. We also discuss the advantages in considering work practices in designing tools for software engineers, and include some requirements for a tool we have developed as a result of our studies.",fullPaper,cp51
p699,367a101c220e5af0f02d61068a50fbb43edf4a1d,c52,Workshop on Learning from Authoritative Security Experiment Results,"Search Based Software Engineering: Techniques, Taxonomy, Tutorial",Abstract content,fullPaper,cp52
p700,ec5d75da55878c2edef2f62e908e40e99596a672,c47,International Symposium on Empirical Software Engineering and Measurement,Software Engineering: A Practitioner's Approach (McGraw-Hill Series in Computer Science),Abstract content,poster,cp47
p701,5a30fd3718835c500d1833492d6cd833c959d155,c41,Software Product Lines Conference,Non-Functional Requirements in Software Engineering,Abstract content,poster,cp41
p702,87cc9fb5129781b74750d88f83472f2cd644ca60,c51,Conference of the Centre for Advanced Studies on Collaborative Research,An examination of software engineering work practices,"This paper presents work practice data of the daily activities of software engineers. Four separate studies are presented; one looking longitudinally at an individual SE; two looking at a software engineering group; and one looking at company-wide tool usage statistics. We also discuss the advantages in considering work practices in designing tools for software engineers, and include some requirements for a tool we have developed as a result of our studies.",fullPaper,cp51
p703,367a101c220e5af0f02d61068a50fbb43edf4a1d,c52,Workshop on Learning from Authoritative Security Experiment Results,"Search Based Software Engineering: Techniques, Taxonomy, Tutorial",Abstract content,fullPaper,cp52
p704,b2cc04740a617190dd179ad46f32cd432efee528,c29,International Conference on Software Engineering,"Software Engineering: A Practitioner's Approach, 7Th Edition","As recognized, adventure as skillfully as experience about lesson, amusement, as well as deal can be gotten by just checking out a ebook software engineering a practitioner39s approach 7th edition then it is not directly done, you could take even more around this life, concerning the world. We allow you this proper as capably as simple artifice to acquire those all. We find the money for software engineering a practitioner39s approach 7th edition and numerous books collections from fictions to scientific research in any way. in the midst of them is this software engineering a practitioner39s approach 7th edition that can be your partner. Page Url",poster,cp29
p705,767ad8d2ce36ae1d79281b520c4ec15dea7499fe,c76,International Conference on Artificial Neural Networks,Ontologies for Software Engineering and Software Technology,Abstract content,poster,cp76
p706,1c5b80259a8b8b4d4f286a86ab2d5ea983ee0db5,c70,International Conference on Intelligent Robotics and Applications,Agile Practices in Global Software Engineering - A Systematic Map,"This paper presents the results of systematically reviewing the current research literature on the use of agile practices and lean software development in global software engineering (GSE). The primary purpose is to highlight under which circumstances they have been applied efficiently. Some common terms related to agile practices (e.g. scrum, extreme programming) were considered in formulating the search strings, along with a number of alternatives for GSE such as offshoring, outsourcing, and virtual teams. The results were limited to peer-reviewed conference papers/journal articles, published between 1999 and 2009. The synthesis was made through classifying the papers into different categories (e.g. research type, distribution). The analysis revealed that in most cases agile practices were modified with respect to the context and situational requirements. This indicates the need for future research on how to integrate all experiences and practices in a way to assist practitioners when setting up non-collocated agile projects.",poster,cp70
p707,bdc141c6bd4b9454c06d57744e43ca9eaa5c657a,c53,International Conference on Software Engineering and Knowledge Engineering,Validity Threats in Empirical Software Engineering Research - An Initial Survey,"In judging the quality of a research study it is very important to consider threats to the validity of the study and the results. This is particularly important for empirical research where there is often a multitude of possible threats. With a growing focus on empirical research methods in software engineering it is important that there is a consensus in the community on this importance, that validity analysis is done by every researcher and that there is common terminology and support on how to do and report it. Even though there are previous relevant results they have primarily focused on quantitative research methods and in particular experiments. Here we look at the existing advice and guidelines and then perform a review of 43 papers published in the ESEM conference in 2009 and analyse the validity analysis they include and which threats and strategies for overcoming them that were given by the authors. Based on this analysis we then discuss what is working well and less well in validity analysis of empirical software engineering research and present recommendations on how to better support validity analysis in the future.",fullPaper,cp53
p708,f4aae8bc0362133ad862f2160394cabc83607470,c108,International Conference on Information Integration and Web-based Applications & Services,The impact of social media on software engineering practices and tools,"Today's generation of software developers frequently make use of social media, either as an adjunct or integrated into a wide range of tools ranging from code editors and issue trackers, to IDEs and web-based portals. The role of social media usage in software engineering is not well understood, and yet the use of these mechanisms influences software development practices. In this position paper, we advocate for research that strives to understand the benefits, risks and limitations of using social media in software development at the team, project and community levels. Guided by the implications of current tools and social media features, we propose a set of pertinent research questions around community involvement, project coordination and management, as well as individual software development activities. Answers to these questions will guide future software engineering tool innovations and software development team practices.",poster,cp108
p709,185b75fa4b4b0096cbb957a4d7a2aa4438618f10,j147,IEEE Software,Collaboration Tools for Global Software Engineering,"Software engineering involves people collaborating to develop better software. Collaboration is challenging, especially across time zones and without face-to-face meetings. We therefore use collaboration tools all along the product life cycle to let us work together, stay together, and achieve results together. This article summarizes experiences and trends chosen from recent IEEE International Conference on Global Software Engineering (IGSCE) conferences.",fullPaper,jv147
p710,99e946ef6e9cfe65469750c7c69883d7cbbd6eda,c35,EUROMICRO Conference on Software Engineering and Advanced Applications,Managing Software Engineering Knowledge,Abstract content,poster,cp35
p711,dd8af788d55e95b9b4132bde4c7a5b2a7bb4dfbc,c95,IEEE International Conference on Computer Vision,A Comparison Between Five Models Of Software Engineering,"This research deals with a vital and important issue in computer world. It is concerned with the software management processes that examine the area of software development through the development models, which are known as software development life cycle. It represents five of the development models namely, waterfall, Iteration, V-shaped, spiral and Extreme programming. These models have advantages and disadvantages as well. Therefore, the main objective of this research is to represent different models of software development and make a comparison between them to show the features and defects of each model.",poster,cp95
p712,1a1aa3a659e8d21318414e822fa0008410f487bf,j157,Requirements Engineering,Assessing traceability of software engineering artifacts,Abstract content,fullPaper,jv157
p713,a3c99ea03a9d27f7f37ae7f3962e817b24abd3e1,c47,International Symposium on Empirical Software Engineering and Measurement,Synthesizing evidence in software engineering research,"Synthesizing the evidence from a set of studies that spans many countries and years, and that incorporates a wide variety of research methods and theoretical perspectives, is probably the single most challenging task of performing a systematic review. In this paper, we perform a tertiary review to assess the types and methods of research synthesis in systematic reviews in software engineering. Almost half of the 31 studies included in our review did not contain any synthesis; of the ones that did, two thirds performed a narrative or a thematic synthesis. The results show that, despite the focus on systematic reviews, there is, currently, limited attention to research synthesis in software engineering. This needs to change and a repertoire of synthesis methods needs to be an integral part of systematic reviews to increase their significance and utility for research and practice.",fullPaper,cp47
p714,f1193334cb708da67c8564ddb6a0247095af1ea5,j147,IEEE Software,The Guide to the Software Engineering Body of Knowledge,"Reporting on the SWEBOK project, the authors-who represent the project's editorial team-discuss the three-phase plan to characterize a body of knowledge, a vital step toward developing software engineering as a profession.",fullPaper,jv147
p715,7f11d4761b2bc41c8d261855607a724c9377a157,c89,Conference on Uncertainty in Artificial Intelligence,Guide to the Software Engineering Body of Knowledge (SWEBOK) and the Software Engineering Education Knowledge (SEEK) - a preliminary mapping,"This paper is the result of a workshop held in Montreal in October 2002 during the Software Technology and Practice Conference (STEP 2002). The purpose of the paper is to present a preliminary mapping of two related but distinct software engineering body of knowledge initiatives, and also a list of proposals to improve them: the guide to the Software Engineering Body of Knowledge (SWEBOK) and the Software Engineering Education [body of] Knowledge (SEEK). The SWEBOK guide is aimed at identifying and describing the body of knowledge of a software engineering professional who has an undergraduate degree and four years of experience. The intended audiences of the SWEBOK Guide include industry, academia and policy-making organizations. The SEEK is aimed at delimiting the knowledge that professionals teaching software engineering agree is necessary for anyone to obtain an undergraduate degree in this field. The mapping shows that, though there are no major ""school of thought"" divergences between the two bodies of knowledge, there are a number of differences in the details of each breakdown in terms of vocabulary, level of detail, decomposition approach and topics encompassed.",poster,cp89
p716,ad75e1e2d5c6d34a67be4173d860e43e8dafed4f,c107,British Machine Vision Conference,Validity concerns in software engineering research,"Empirical studies that use software repository artifacts have become popular in the last decade due to the ready availability of open source project archives. In this paper, we survey empirical studies in the last three years of ICSE and FSE proceedings, and categorize these studies in terms of open source projects vs. proprietary source projects and the diversity of subject programs used in these studies. Our survey has shown that almost half (49%) of recent empirical studies used solely open source projects. Existing studies either draw general conclusions from these results or explicitly disclaim any conclusions that can extend beyond specific subject software.
 We conclude that researchers in empirical software engineering must consider the external validity concerns that arise from using only several well-known open source software projects, and that discussion of data source selection is an important discussion topic in software engineering research. Furthermore, we propose a community research infrastructure for software repository benchmarks and sharing the empirical analysis results, in order to address external validity concerns and to raise the bar for empirical software engineering research that analyzes software artifacts.",poster,cp107
p717,108f91acc309d1b31336124bec48657a7736737b,j142,IEEE Transactions on Software Engineering,How Reliable Are Systematic Reviews in Empirical Software Engineering?,"BACKGROUND-The systematic review is becoming a more commonly employed research instrument in empirical software engineering. Before undue reliance is placed on the outcomes of such reviews it would seem useful to consider the robustness of the approach in this particular research context. OBJECTIVE-The aim of this study is to assess the reliability of systematic reviews as a research instrument. In particular, we wish to investigate the consistency of process and the stability of outcomes. METHOD-We compare the results of two independent reviews undertaken with a common research question. RESULTS-The two reviews find similar answers to the research question, although the means of arriving at those answers vary. CONCLUSIONS-In addressing a well-bounded research question, groups of researchers with similar domain experience can arrive at the same review outcomes, even though they may do so in different ways. This provides evidence that, in this context at least, the systematic review is a robust research method.",fullPaper,jv142
p718,e23e287baf50b5b9a19774de6d6fb356b6bac212,c61,Jahrestagung der Gesellschaft für Informatik,Software intelligence: the future of mining software engineering data,"Mining software engineering data has emerged as a successful research direction over the past decade. In this position paper, we advocate Software Intelligence (SI) as the future of mining software engineering data, within modern software engineering research, practice, and education. We coin the name SI as an inspiration from the Business Intelligence (BI) field, which offers concepts and techniques to improve business decision making by using fact-based support systems. Similarly, SI offers software practitioners (not just developers) up-to-date and pertinent information to support their daily decision-making processes. SI should support decision-making processes throughout the lifetime of a software system not just during its development phase.
 The vision of SI has yet to become a reality that would enable software engineering research to have a strong impact on modern software practice. Nevertheless, recent advances in the Mining Software Repositories (MSR) field show great promise and provide strong support for realizing SI in the near future. This position paper summarizes the state of practice and research of SI, and lays out future research directions for mining software engineering data to enable SI.",poster,cp61
p719,907e5aabc168ca7c4887b154a5f35025b3d4a491,j141,Empirical Software Engineering,Applying empirical software engineering to software architecture: challenges and lessons learned,Abstract content,fullPaper,jv141
p720,219f3c0c931113cd83f92b18e0868cd69c47544a,c12,International Conference on Statistical and Scientific Database Management,Global Software Engineering: A Software Process Approach,Abstract content,poster,cp12
p721,6ef662fc9b318531ef3add75f8f8b6c0d0f5b11e,c96,USENIX Symposium on Operating Systems Design and Implementation,Collaborative Software Engineering: Challenges and Prospects,Abstract content,poster,cp96
p722,cd3986727ae40fd6d140956f60fd34a614a219f6,c97,Interspeech,"Softwares Product Lines, Global Development and Ecosystems: Collaboration in Software Engineering",Abstract content,poster,cp97
p723,c4518592ff763d6746a197c2c5f3df2c4044d13d,c81,IEEE Annual Symposium on Foundations of Computer Science,Guide to the Software Engineering Body of Knowledge,data types Sorting and searching parallel and distributed algorithms 3. [AR] Computer Architecture,poster,cp81
p724,282b27fcdc35737d4edcc0bc54fe9560c68e1011,c54,International Workshop on Agent-Oriented Software Engineering,Agent-Oriented Software Engineering: The State of the Art,Abstract content,fullPaper,cp54
p725,15daae4c253eaeb6b1194e1fe6230ac3e394ae5a,c78,Neural Information Processing Systems,"Object-Oriented Software Engineering Using UML, Patterns, and Java","This widely used book teaches practical object-oriented software engineering with the key real world tools UML, design patterns and Java. This step-by-step approach allows the reader to address complex and changing problems with practical and state-of-the-art solutions. This book uses examples from real systems and examines the interaction between such techniques as UML, Java-based technologies, design patterns, rationale, configuration management, and quality control. It also discusses project management related issues and their impacts. A valuable book for development engineers, software engineers, consulting engineers, software architects, product managers, project leaders, and knowledge managers.",poster,cp78
p726,1f10df986e3235f1deda73e1200155f4f4b88713,j141,Empirical Software Engineering,Curating GitHub for engineered software projects,Abstract content,fullPaper,jv141
p727,8e18e83a759a8076a2f1e48eb751f9ae05542f15,c22,International Conference on Data Technologies and Applications,Component-Based Software Engineering: Putting the Pieces Together,Abstract content,poster,cp22
p728,d28585d2ec27b611f46125a56bc52a1d93fada98,c44,International Workshop on Green and Sustainable Software,Software engineering in an uncertain world,"In this paper, we argue that the reality of today's software systems requires us to consider uncertainty as a first-class concern in the design, implementation, and deployment of those systems. We further argue that this induces a paradigm shift, and a number of research challenges that must be addressed.",poster,cp44
p729,c9382574e6a868fe45aa9cc09d19c0f5fadbd652,c76,International Conference on Artificial Neural Networks,Evidence-based software engineering,"Our objective is to describe how software engineering might benefit from an evidence-based approach and to identify the potential difficulties associated with the approach. We compared the organisation and technical infrastructure supporting evidence-based medicine (EBM) with the situation in software engineering. We considered the impact that factors peculiar to software engineering (i.e. the skill factor and the lifecycle factor) would have on our ability to practice evidence-based software engineering (EBSE). EBSE promises a number of benefits by encouraging integration of research results with a view to supporting the needs of many different stakeholder groups. However, we do not currently have the infrastructure needed for widespread adoption of EBSE. The skill factor means software engineering experiments are vulnerable to subject and experimenter bias. The lifecycle factor means it is difficult to determine how technologies will behave once deployed. Software engineering would benefit from adopting what it can of the evidence approach provided that it deals with the specific problems that arise from the nature of software engineering.",poster,cp76
p730,3713667bb3ce322c3bb078f4499017697a07282e,c92,Advances in Soft Computing,Social media for software engineering,"Social media has changed the way that people collaborate and share information. In this paper, we highlight its impact for enabling new ways for software teams to form and work together. Individuals will self-organize within and across organizational boundaries. Grassroots software development communities will emerge centered around new technologies, common processes and attractive target markets. Companies consisting of lone individuals will able to leverage social media to conceive of, design, develop, and deploy successful and profitable product lines. A challenge for researchers who are interested in studying, influencing, and supporting this shift in software teaming is to make sure that their research methods protect the privacy and reputation of their stakeholders.",poster,cp92
p731,7f62cfa3b879ca9b8d3f736f2340f7a6c2c207da,j158,Artificial Intelligence,On agent-based software engineering,Abstract content,fullPaper,jv158
p732,4d4c071d51252c8007f3dc64feeabe4434a6c83f,j144,Information and Software Technology,Software engineering research for computer games: A systematic review,Abstract content,fullPaper,jv144
p733,8de121442c5df6ebd1d93c132086c80ae7613b06,c89,Conference on Uncertainty in Artificial Intelligence,Handbook of software reliability engineering,Technical foundations introduction software reliability and system reliability the operational profile software reliability modelling survey model evaluation and recalibration techniques practices and experiences best current practice of SRE software reliability measurement experience measurement-based analysis of software reliability software fault and failure classification techniques trend analysis in validation and maintenance software reliability and field data analysis software reliability process assessment emerging techniques software reliability prediction metrics software reliability and testing fault-tolerant SRE software reliability using fault trees software reliability process simulation neural networks and software reliability. Appendices: software reliability tools software failure data set repository.,poster,cp89
p734,6763a821d8f6f204256b57767f6d8350075c175b,c73,Workshop on Algorithms in Bioinformatics,Search Based Software Engineering: A Comprehensive Analysis and Review of Trends Techniques and Applications,"In the past five years there has been a dramatic increase in work on Search Based Software Engineering (SBSE), an approach to software engineering in which search based optimisation algorithms are used to address problems in Software Engineering. SBSE has been applied to problems throughout the Software Engineering lifecycle, from requirements and project planning to maintenance and re-engineering. The approach is attractive because it offers a suite of adaptive automated and semi-automated solutions in situations typified by large complex problem spaces with multiple competing and conflicting objectives. This paper provides a review and classification of literature on SBSE. The paper identifies research trends and relationships between the techniques applied and the applications to which they have been applied and highlights gaps in the literature and avenues for further research.",poster,cp73
p735,24b11e369c41137e30ed026061bcfc66855e6832,c36,Conference on Software Engineering Education and Training,Guide to Advanced Empirical Software Engineering,Abstract content,poster,cp36
p736,8537cec10f0405142ed3499cb60c627ec66c25f1,c62,International Conference on Software Reuse,Software Engineering for Self-Adaptive Systems [outcome of a Dagstuhl Seminar],Abstract content,poster,cp62
p737,3157b28976e723056952d920000cc24a93dbe05b,c55,Annual Workshop of the Psychology of Programming Interest Group,The mythical man-month - essays on software engineering (2. ed.),"1. The Tar Pit. 2. The Mythical Man-Month. 3. The Surgical Team. 4. Aristocracy, Democracy, and System Design. 5. The Second-System Effect. 6. Passing the Word. 7. Why Did the Tower of Babel Fail? 8. Calling the Shot. 9. Ten Pounds in a Five-Pound Sack. 10. The Documentary Hypothesis. 11. Plan to Throw One Away. 12. Sharp Tools. 13. The Whole and the Parts. 14. Hatching a Castrophe. 15. The Other Face. 16. No Silver Bullet -- Essence and Accident. 17. ""No Silver Bullet"" ReFired. 18. Propositions of The Mythical Man-Month: True or False? 19. The Mythical Man-Month After 20 Years. Epilogue. Notes and references. Index. 0201835959T04062001",poster,cp55
p738,90b8cfa993357cccd94d05a4317342892516731b,j79,Computer,Data Mining for Software Engineering,"To improve software productivity and quality, software engineers are increasingly applying data mining algorithms to various software engineering tasks. However, mining SE data poses several challenges. The authors present various algorithms to effectively mine sequences, graphs, and text from such data.",fullPaper,jv79
p739,5248768746e145728bd44067a359e4801c8fb0e6,j159,Information Systems,A software engineering approach to ontology building,Abstract content,fullPaper,jv159
p740,255149e790634fad3ecbc9d37aacfe05a7383c1e,c35,EUROMICRO Conference on Software Engineering and Advanced Applications,"Model-driven software development - technology, engineering, management","Part I: Introduction. 1. Introduction. 2. MDSD - Basic Ideas and Terminology. 3. Case Study: A Typical Web Application. 4. Concept Formation. 5. Classification. Part II: Domain Architectures. 6. Metamodeling. 7. MDSD-Capable Target Architectures. 8. Building Domain Architectures. 9. Code Generation Techniques. 10. Model Transformation Techniques. 11. MDSD Tools: Roles, Architecture, Selection Criteria, and Pointers. 12. The MDA Standard. Part III: Processes and Engineering. 13. MDSD Process Building Blocks and Best Practices. 14. Testing. 15. Versioning. 16. Case Study: Embedded Component Infrastructures. 17. Case Study: An Enterprise System. Part IV: Management. 18. Decision Support. 1.9 Organizational Aspects. 20. Adoption Strategies for MDSD. References. Index.",poster,cp35
p741,3b1228cee8e74fa231ad730c405bc1b1e1cc174f,c2,International Symposium on Intelligent Data Analysis,Software Engineering Challenges in Game Development,"In Software Engineering (SE), video game development is unique yet similar to other software endeavors. It is unique in that it combines the work of teams covering multiple disciplines (art, music, acting, programming, etc.), and that engaging game play is sought after through the use of prototypes and iterations. With that, game development is faced with challenges that can be addressed using traditional SE practices. The industry needs to adopt sound SE practices for their distinct needs such as managing multimedia assets and finding the “fun” in game play. The industry must take on the challenges by evolving SE methods to meet their needs. This work investigates these challenges and highlights engineering practices to mitigate these challenges.",poster,cp2
p742,831bacac789fad53065575efbe87cc25a06913e7,c49,International Symposium on Search Based Software Engineering,Software Engineering Best Practices,"Proven techniques for software development success 
 
In this practical guide, software-quality guru Capers Jones reveals best practices for ensuring software development success by illustrating the engineering methods used by the most successful large software projects at leading companies such as IBM, Microsoft, Sony, and EDS. 
 
Software Engineering Best Practices covers estimating and planning; requirements analysis; change control; quality control; progress and cost tracking; and maintenance and support after delivery. Agile development, extreme programming, joint application design (JAD), six-sigma for software, and other methods are discussed. 
 
Table of contents 
Chapter 1. Introduction and Definitions of Software Best Practices;Chapter 2. Overview of 50 Software Best Practices;Chapter 3. A Preview of Software Development and Maintenance in 2049;Chapter 4. How Software Personnel Learn New Skills;Chapter 5. Software Team Organization and Specialization;Chapter 6. Project Management and Software Engineering;Chapter 7. Requirements, Business Analysis, Architecture, Enterprise Architecture, and Design;Chapter 8. Programming and Code Development;Chapter 9. Software Quality: The Key to Successful Software Engineering;Index",poster,cp49
p743,761bf03e086ce887a44dc5e44aefb0de05b4a47f,c47,International Symposium on Empirical Software Engineering and Measurement,Systematic literature reviews in software engineering: Preliminary results from interviews with researchers,"Systematic Literature Reviews (SLRs) have been gaining significant attention from software engineering researchers since 2004. Several researchers have reported their experiences of and lessons learned from applying systematic reviews to different subject matters in software engineering. However, there has been no attempt at independently exploring experiences and perceptions of the practitioners of systematic reviews in order to gain an in-depth understanding of various aspects of systemic reviews as a new research methodology in software engineering. We assert that there is a need of evidence-based body of knowledge about the application of systematic reviews in software engineering. To address this need, we have started an empirical research program that aims to contribute to the growing body of knowledge about systematic reviews in software engineering. This paper reports the design, logistics, and results of the first phase empirical study carried out in this program. The results provide interesting insights into different aspects of systematic reviews based on the analysis of the data gathered from 17 interviewees with varying levels of knowledge of and experiences in systematic reviews. The findings from this study are expected to contribute to the existing knowledge about using systematic reviews and help further improve the state-of-the-practice of this research methodology in software engineering.",fullPaper,cp47
p744,9acba85c78ed1767e47e1c3346f22557eb82eefb,c36,Conference on Software Engineering Education and Training,"Software Engineering: A Practitionerʼs Approach, 7/e","ion—data, procedure, control Architecture—the overall structure of the software Patterns—”conveys the essence” of a proven design solution Separation of concerns—any complex problem can be more easily handled if it is subdivided into pieces Modularity—compartmentalization of data and function Hiding—controlled interfaces Functional independence—single-minded function and low coupling Refinement—elaboration of detail for all abstractions Aspects—a mechanism for understanding how global requirements affect design Refactoring—a reorganization technique that simplifies the design OO design concepts—Appendix II Design Classes—provide design detail that will enable analysis classes to be implemented",poster,cp36
p745,4bddb173c165b2cd33e255e427e29422b87d54de,j1,IEEE Transactions on Knowledge and Data Engineering,Development of a Software Engineering Ontology for Multisite Software Development,"This paper aims to present an ontology model of software engineering to represent its knowledge. The fundamental knowledge relating to software engineering is well described in the textbook entitled Software Engineering by Sommerville that is now in its eighth edition (2004) and the white paper, Software Engineering Body of Knowledge (SWEBOK), by the IEEE (203) upon which software engineering ontology is based. This paper gives an analysis of what software engineering ontology is, what it consists of, and what it is used for in the form of usage example scenarios. The usage scenarios presented in this paper highlight the characteristics of the software engineering ontology. The software engineering ontology assists in defining information for the exchange of semantic project information and is used as a communication framework. Its users are software engineers sharing domain knowledge as well as instance knowledge of software engineering.",fullPaper,jv1
p746,4e55f8aede401b870b01fa8e8fb5560b651fb66b,c47,International Symposium on Empirical Software Engineering and Measurement,Action research use in software engineering: An initial survey,"This paper presents a literature survey of action research (AR) studies published in nine major Software Engineering (SE) journals and three conference proceedings in the period 1993 to June 2009. A strict selection based on distinguishing SE from Information Systems research has identified 16 papers. Although they represent a very small fraction of the studies being conducted in SE, such papers concern with different SE contexts allowing to get information about the increasing tendency in the AR use in software engineering. However, as shown by the initial results, SE researchers should invest more on rigor when defining, applying and reporting AR studies inSE.",fullPaper,cp47
p747,60fcb28a2f9502ea3c13e39bd8390802272babd0,j147,IEEE Software,What Do We Know about Knowledge Management? Practical Implications for Software Engineering,"There have been many claims about knowledge management's benefits in software engineering, such as decreased time and cost for development, increased quality, and better decision-making abilities. Although we can find some success stories illustrating these claims, particularly on aspects related to the systems and engineering schools, more research is necessary to explore the intersection between each school and the software engineering field. Researchers should continue to emphasize the need for a broad focus across multiple KM schools to suceed in improving KM's practical application in software engineering.",fullPaper,jv147
p748,75917fc7a6959793abe36da6ec678bd9518207cd,j69,IEEE Transactions on Services Computing,Ontology Classification for Semantic-Web-Based Software Engineering,"The semantic Web is the second generation of the Web, which helps sharing and reusing data across application, enterprise, and community boundaries. Ontology defines a set of representational primitives with which a domain of knowledge is modeled. The main purpose of the semantic Web and ontology is to integrate heterogeneous data and enable interoperability among disparate systems. Ontology has been used to model software engineering knowledge by denoting the artifacts that are designed or produced during the engineering process. The semantic Web allows publishing reusable software engineering knowledge resources and providing services for searching and querying. This paper classifies the ontologies developed for software engineering, reviews the current efforts on applying the semantic Web techniques on different software engineering aspects, and presents the benefits of their applications. We also foresee the possible future research directions.",fullPaper,jv69
p749,eece42f4d4bb7143643baa66cf6132b1c9e4915d,c47,International Symposium on Empirical Software Engineering and Measurement,Context in industrial software engineering research,"In order to draw valid conclusions when aggregating evidence it is important to describe the context in which industrial studies were conducted. This paper structures the context for empirical industrial studies and provides a checklist. The aim is to aid researchers in making informed decisions concerning which parts of the context to include in the descriptions. Furthermore, descriptions of industrial studies were surveyed.",fullPaper,cp47
p750,84295abfb7ce04fc93ece825d0733ec6c6e44969,j147,IEEE Software,Software Engineering for Spreadsheets,The idiosyncratic structure of spreadsheets allows the adaptation of proven software engineering principles to an end-user domain and thus makes software engineering accessible to many users.,fullPaper,jv147
p751,3621eac63003c54f20ab387f6d4b5fbfd0624cfd,c103,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,Global Software Engineering: The Future of Socio-technical Coordination,"Globally-distributed projects are rapidly becoming the norm for large software systems, even as it becomes clear that global distribution of a project seriously impairs critical coordination mechanisms. In this paper, I describe a desired future for global development and the problems that stand in the way of achieving that vision. I review research and lay out research challenges in four critical areas: software architecture, eliciting and communicating requirements, environments and tools, and orchestrating global development. I conclude by noting the need for a systematic understanding of what drives the need to coordinate and effective mechanisms for bringing it about.",poster,cp103
p752,69c24b9aa48b5ba9b2a06e7a0572e0c1308cb4f4,c107,British Machine Vision Conference,"On ""Software engineering""","Software engineers work on multidisciplinary teams to identify and develop software solutions and to maintain software intensive systems of all sizes. The focus of this program is on the rigorous engineering practices necessary to build, maintain, and protect modern software intensive systems. Consistent with this focus, the software engineering baccalaureate program consists of a rigorous curriculum of science, math, computer science, and software engineering courses.",poster,cp107
p753,e2ee8df90829f8fe41bf1a1ebc233113885c124a,c86,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,The Current State and Future of Search Based Software Engineering,"This paper describes work on the application of optimization techniques in software engineering. These optimization techniques come from the operations research and metaheuristic computation research communities. The paper briefly reviews widely used optimization techniques and the key ingredients required for their successful application to software engineering, providing an overview of existing results in eight software engineering application domains. The paper also describes the benefits that are likely to accrue from the growing body of work in this area and provides a set of open problems, challenges and areas for future work.",poster,cp86
p754,fbcffae04423ea97cb5463cd9d0794d780708746,j147,IEEE Software,Trends in Embedded Software Engineering,"Software's importance in the development of embedded systems has been growing rapidly over the last 20 years. Because of current embedded systems' complexity, they require sophisticated engineering methods for systematically developing high-quality software. Embedded software development differs from IT system development in several ways. For example, IT systems developers can use standard hardware and software platforms and don't face the resource requirements that embedded systems developers must take into account. To meet embedded software's extrafunctional requirements, embedded systems development is shifting from programming to model-driven development. Another important trend is the emphasis on the quality assurance of safety-related systems.",fullPaper,jv147
p755,37893d23b27d7d38aa2ed1562b8f158e2629d3f3,j144,Information and Software Technology,Motivation in Software Engineering: A systematic literature review,Abstract content,fullPaper,jv144
p756,79651cbd4dc289d7e052885c4f904125da2a6d8a,j144,Information and Software Technology,Models of motivation in software engineering,Abstract content,fullPaper,jv144
p757,be501c83ecbc5f820a2b1125a985820040e8662c,c108,International Conference on Information Integration and Web-based Applications & Services,Experience and Knowledge Management in Software Engineering,Abstract content,poster,cp108
p758,c4354e1cda7cf6d45c61065d9778fe94e86172c5,j147,IEEE Software,ERP Customization as Software Engineering: Knowledge Sharing and Cooperation,"Enterprise resource planning (ERP) vendors provide multiple configuration possibilities ranging from module selection to master data provision to steer access rights for different users. These configuration possibilities cover anticipated variability. If the customer requires adaptation beyond what's anticipated, the source code of the product must be adapted. Customizations in this article's context are source code based adaptations of software products. The size and complexity of customizations range from simple report generation to developing independent add-ons that support specific businesses, for example, solutions for flight carriers. The size and lead time of such projects can compare to a full-size software development project. Enterprise resource planning (ERP) systems must be configured and customized to fit a specific company. The authors discuss cooperation with regard to ERP systems customization.",fullPaper,jv147
p759,2ae398028f82555931c8c504886ec5857640c59e,j160,Journal of Software,Context and Adaptivity in Pervasive Computing Environments: Links with Software Engineering and Ontological Engineering,"In this article we present a review of selected literature of context-aware pervasive computing while integrating theory and practice from various disciplines in order to construct a theoretical grounding and a technical follow-up path for our future research. This paper is not meant to provide an extensive review of the literature, but rather to integrate and extend fundamental and promising theoretical and technical aspects found in the literature. Our purpose is to use the constructed theory and practice in order to enable anywhere and anytime adaptive e-learning environments. We particularly elaborate on context, adaptivity, context-aware systems, ontologies and software development issues. Furthermore, we represent our view point for context-aware pervasive application development particularly based on higher abstraction where ontologies and semantic web activities, also web itself, are of crucial.",fullPaper,jv160
p760,a01c4fc02389cd3b8b442e124018fc2cabfedad9,c85,International Conference on Graph Transformation,Using the inverted classroom to teach software engineering,"An inverted classroom is a teaching environment that mixes the use of technology with hands-on activities. In an inverted classroom, typical in-class lecture time is replaced with laboratory and in-class activities. Outside class time, lectures are delivered over some other medium such as video on-demand. In a three credit hour course for instance, contact hours are spent having students actively engaged in learning activities. Outside of class, students are focused on viewing 3-6 hours of lectures per week. Additional time outside of class is spent completing learning activities. In this paper we present the inverted classroom model in the context of a software engineering curriculum. The paper motivates the use of the inverted classroom and suggests how different courses from the Software Engineering 2004 Model Curriculum Volume can incorporate the use of the inverted classroom. In addition, we present the results of a pilot course that utilized the inverted classroom model at Miami University and describe courses that are currently in process of piloting its use.",poster,cp85
p761,7deeb94777af3649a24402de1b0573294560bd56,j141,Empirical Software Engineering,Sentiment Polarity Detection for Software Development,Abstract content,fullPaper,jv141
p762,3da0f2dedae35fdacfe6d5c828c29e3ff7d315df,j161,IET Software,Progress on approaches to software defect prediction,"Software defect prediction is one of the most popular research topics in software engineering. It aims to predict defect-prone software modules before defects are discovered, therefore it can be used to better prioritise software quality assurance effort. In recent years, especially for recent 3 years, many new defect prediction studies have been proposed. The goal of this study is to comprehensively review, analyse and discuss the state-of-the-art of defect prediction. The authors survey almost 70 representative defect prediction papers in recent years (January 2014-April 2017), most of which are published in the prominent software engineering journals and top conferences. The selected defect prediction papers are summarised to four aspects: machine learning-based prediction algorithms, manipulating the data, effort-aware prediction and empirical studies. The research community is still facing a number of challenges for building methods and many research opportunities exist. The identified challenges can give some practical guidelines for both software engineering researchers and practitioners in future software defect prediction.",fullPaper,jv161
p763,f74d664f0c3451bd4c31cf973a549a6dc00897c6,c105,Biometrics and Identity Management,\{PROMISE\} Repository of empirical software engineering data,Abstract content,poster,cp105
p764,ffb87ecc6bb5ac8423ab469167708793c479e5fa,c84,The Web Conference,Usability engineering,Abstract content,poster,cp84
p765,a1b51ff5cfb974fdef34386bed5c5844ba7a8dcf,c51,Conference of the Centre for Advanced Studies on Collaborative Research,Managing the software process,Foreword. Preface. I. SOFTWARE PROCESS MATURITY. A Software Maturity Framework. The Principles of Software Process Change. Software Process Assessment. The Initial Process. II. THE REPEATABLE PROCESS. Managing Software Organizations. The Project Plan. Software Configuration Management-Part 1: Software Quality Assurance. III. THE DEFINED PROCESS. Software Standards. Software Inspections. Software Testing. Software Configuration Management (Continued). Defining the Software Process. The Software Engineering Process Group IV. THE MANAGED PROCESS. Data Gathering and Analysis. Managing Software Quality. V. THE OPTIMIZING PROCESS. Defect Prevention. Automating The Software Process. Contracting for Software. Conclusion. Appendices. Index. 0201180952T04062001,poster,cp51
p766,0aaf533cac246fcd83490cd0b8508ed0d886ab7d,c100,ACM SIGMOD Conference,Software agents,"The software world is one of great richness and diversity. Many thousands of software products are available to users today, providing a wide variety of information and services in a wide variety of domains. While most of these programs provide their users with significant value when used in isolation, there is increasing demand for programs that can interoperate – to exchange information and services with other programs and thereby solve problems that cannot be solved alone. Part of what makes interoperation difficult is heterogeneity. Programs are written by different people, at different times, in different languages; and, as a result, they often provide different interfaces. The difficulties created by heterogeneity are exacerbated by dynamics in the software environment. Programs are frequently rewritten; new programs are added; old programs removed. Agent-based software engineering was invented to facilitate the creation of software able to interoperate in such settings. In this approach to software development, application programs are written as software agents, i.e. software “components” that communicate with their peers by exchanging messages in an expressive agent communication language. Agents can be as simple as subroutines; but typically they are larger entities with some sort of persistent control (e.g. distinct control threads within a single address space, distinct processes on a single machine, or separate processes on different machines). The salient feature of the language used by agents is its expressiveness. It allows for the exchange of data and logical information, individual commands and scripts (i.e. programs). Using this language, agents can communicate complex information and goals, directly or indirectly “programming” each other in useful ways. Agent-based software engineering is often compared to object-oriented programming. Like an “object”, an agent provides a message-based interface independent of its internal data structures and algorithms. The primary difference between the two approaches lies in the language of the interface. In general object-oriented programming, the meaning of a message can vary from one object to another. In agent-based software engineering, agents use a common language with an agent-independent semantics. The concept of agent-based software engineering raises a number of important questions.",poster,cp100
p767,c0d9a396c6d3881dfcb0d1cd28b2433f83faf9ca,c73,Workshop on Algorithms in Bioinformatics,Property-Based Software Engineering Measurement,"Little theory exists in the field of software system measurement. Concepts such as complexity, coupling, cohesion or even size are very often subject to interpretation and appear to have inconsistent definitions in the literature. As a consequence, there is little guidance provided to the analyst attempting to define proper measures for specific problems. Many controversies in the literature are simply misunderstandings and stem from the fact that some people talk about different measurement concepts under the same label (complexity is the most common case). There is a need to define unambiguously the most important measurement concepts used in the measurement of software products. One way of doing so is to define precisely what mathematical properties characterize these concepts, regardless of the specific software artifacts to which these concepts are applied. Such a mathematical framework could generate a consensus in the software engineering community and provide a means for better communication among researchers, better guidelines for analysts, and better evaluation methods for commercial static analyzers for practitioners. We propose a mathematical framework which is generic, because it is not specific to any particular software artifact, and rigorous, because it is based on precise mathematical concepts. We use this framework to propose definitions of several important measurement concepts (size, length, complexity, cohesion, coupling). It does not intend to be complete or fully objective; other frameworks could have been proposed and different choices could have been made. However, we believe that the formalisms and properties we introduce are convenient and intuitive. This framework contributes constructively to a firmer theoretical ground of software measurement.",poster,cp73
p768,dfbc169d20897be26a005a08c29da7a1df5cd163,c94,Vision,Reporting Experiments in Software Engineering,Abstract content,poster,cp94
p769,f7644baad4c2c9bf0a7a2b1107209d2b9495aaff,c55,Annual Workshop of the Psychology of Programming Interest Group,Using Mapping Studies in Software Engineering,"Background: A mapping study provides a systematic and objective procedure for identifying the nature and extent of the empirical study data that is available to answer a particular research question. Such studies can also form a useful preliminary step for PhD study. Aim: We set out to assess how effective such studies have been when used for software engineering topics, and to identify the specific challenges that they present. Method: We have conducted an informal review of a number of mapping studies in software engineering, describing their main characteristics and the forms of analysis employed. Results: We examine the experiences and outcomes from six mapping studies, of which four are published. From these we note a recurring theme about the problems of classification and a preponderance of ‘gaps’ in the set of empirical studies. Conclusions: We identify our challenges as improving classification guidelines, encouraging better reporting of primary studies, and argue for identifying some ’empirical grand challenges’ for software engineering as a focus for the community.",fullPaper,cp55
p770,1edc7f14653fa2d89343b62bb7254297b107bff3,j141,Empirical Software Engineering,The role of replications in Empirical Software Engineering,Abstract content,fullPaper,jv141
p771,8c4702b08e7bd5b2e7582812ae754cc605bb6c3d,c111,International Society for Music Information Retrieval Conference,Building Theories in Software Engineering,Abstract content,poster,cp111
p772,4abd729f079a650b0ed14aa06f25b2f1f3611e32,c47,International Symposium on Empirical Software Engineering and Measurement,Strength of evidence in systematic reviews in software engineering,"Systematic reviews are only as good as the evidence they are based on. It is important, therefore, that users of systematic reviews know how much confidence they can place in the conclusions and recommendations arising from such reviews. In this paper we present an overview of some of the most influential systems for assessing the quality of individual primary studies and for grading the overall strength of a body of evidence. We also present an example of the use of such systems based on a systematic review of empirical studies of agile software development. Our findings suggest that the systems used in other disciplines for grading the strength of evidence for and reporting of systematic reviews, especially those that take account of qualitative and observational studies are of particular relevance for software engineering.",fullPaper,cp47
p773,073c37ddc266657a9bc21ebad7abc97991237539,j147,IEEE Software,The Rise and Evolution of Agile Software Development,"Agile software development has dominated the second half of the past 50 years of software engineering. Retrospectives, one of the most common agile practices, enables reflection on past performance, discussion of current progress, and charting forth directions for future improvement. Because of agile’s burgeoning popularity as the software development model of choice and a significant research subdomain of software engineering, it demands a retrospective of its own. This article provides a historical overview of agile’s main focus areas and a holistic synthesis of its trends, their evolution over the past two decades, agile’s current status, and, forecast from these, agile’s likely future. This article is part of a theme issue on software engineering’s 50th anniversary.",fullPaper,jv147
p774,30e741a0330cdcaf6a6466eaca2f09c8bd604b57,j142,IEEE Transactions on Software Engineering,A survey of controlled experiments in software engineering,"The classical method for identifying cause-effect relationships is to conduct controlled experiments. This paper reports upon the present state of how controlled experiments in software engineering are conducted and the extent to which relevant information is reported. Among the 5,453 scientific articles published in 12 leading software engineering journals and conferences in the decade from 1993 to 2002, 103 articles (1.9 percent) reported controlled experiments in which individuals or teams performed one or more software engineering tasks. This survey quantitatively characterizes the topics of the experiments and their subjects (number of subjects, students versus professionals, recruitment, and rewards for participation), tasks (type of task, duration, and type and size of application) and environments (location, development tools). Furthermore, the survey reports on how internal and external validity is addressed and the extent to which experiments are replicated. The gathered data reflects the relevance of software engineering experiments to industrial practice and the scientific maturity of software engineering research.",fullPaper,jv142
p775,3fbac6cafc36078a957edca033b92e643f69765a,c43,ACM Symposium on Applied Computing,Software engineering: theory and practice,Abstract content,poster,cp43
p776,8aa6361fc9b7876cddb3bb103a419cd8443adf60,c56,European Conference on Software Process Improvement,A Software Engineering Lifecycle Standard for Very Small Enterprises,Abstract content,fullPaper,cp56
p777,ecb8f480e369346d899b6c2ec935bea898fa499a,c100,ACM SIGMOD Conference,08031 -- Software Engineering for Self-Adaptive Systems: A Research Road Map,"Software's ability to adapt at run-time to changing user needs, system intrusions or faults, changing operational environment, and resource variability has been proposed as a means to cope with the complexity of today's software-intensive systems. Such self-adaptive systems can configure and reconfigure themselves, augment their functionality, continually optimize themselves, protect themselves, and recover themselves, while keeping most of their complexity hidden from the user and administrator. In this paper, we present research road map for software engineering of self-adaptive systems focusing on four views, which we identify as essential: requirements, modelling, engineering, and assurances.",poster,cp100
p778,57317f7b349f1d02153e54603e3988376ef4724f,j152,IEEE Annals of the History of Computing,A Brief History of Software Engineering,"This personal perspective on the art of programming begins with a look at the state of programming from about 1960, and it follows programming's development through the present day. The article examines key contributions to the field of software engineering and identifies major obstacles, which persist even today.",fullPaper,jv152
p779,afdfec4b9a0a65ac246b96f747b8aa07dda642c1,j142,IEEE Transactions on Software Engineering,Problem Oriented Software Engineering: Solving the Package Router Control Problem,"Problem orientation is gaining interest as a way of approaching the development of software intensive systems, and yet, a significant example that explores its use is missing from the literature. In this paper, we present the basic elements of Problem Oriented Software Engineering (POSE), which aims at bringing both nonformal and formal aspects of software development together in a single framework. We provide an example of a detailed and systematic POSE development of a software problem: that of designing the controller for a package router. The problem is drawn from the literature, but the analysis presented here is new. The aim of the example is twofold: to illustrate the main aspects of POSE and how it supports software engineering design and to demonstrate how a nontrivial problem can be dealt with by the approach.",fullPaper,jv142
p780,a9b42c3b3a22861ebd7289621b9e36be1b31699c,c107,British Machine Vision Conference,Replication's Role in Software Engineering,Abstract content,poster,cp107
p781,fe627947c9f0467e9e06fedb6b35ee8dc77c17c8,c45,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",Towards individualized software engineering: empirical studies should collect psychometrics,"Even though software is developed by humans, research in software engineering primarily focuses on the technologies, methods and processes they use while disregarding the importance of the humans themselves. In this paper we argue that most studies in software engineering should give much more weight to human factors. In particular empirical software engineering studies involving human developers should always consider collecting psychometric data on the humans involved. We focus on personality as one important psychometric factor and present initial results from an empirical study investigating correlations between personality and attitudes to software engineering processes and tools. We discuss what are currently hindering a more wide-spread use of psychometrics and how overcoming these hurdles could lead to a more individualized software engineering.",fullPaper,cp45
p782,f96d53e1132476a86f8b5a4ed2c9cd07345c9da3,c74,IEEE International Conference on Tools with Artificial Intelligence,A survey of social software engineering,"Software engineering is a complex socio-technical activity, due to the need for discussing and sharing knowledge among team members. This has raised the need for effective ways of sharing ideas, knowledge, and artifacts among groups and their members. The social aspect of software engineering process also demands computer support to facilitate the development by means of collaborative tools, applications and environments. In this paper, we present a survey of relevant works from psychology, mathematics and computer science studies. The combination of these fields provides the required infrastructure for engineering social and collaborative applications as well as the software engineering process. We also discuss possible solutions for the encountered shortcomings, and how they can improve software development.",poster,cp74
p783,2f9f3311be9d707bdcb0eb2d995d711f9138e614,c84,The Web Conference,Metamodelling for software engineering,"Interestingly, metamodelling for software engineering that you really wait for now is coming. It's significant to wait for the representative and beneficial books to read. Every book that is provided in better way and utterance will be expected by many peoples. Even you are a good reader or not, feeling to read this book will always appear when you find it. But, when you feel hard to find it as yours, what to do? Borrow to your friends and don't know when to give back it to her or him.",poster,cp84
p784,9663ab413e84763b6818522055431f5ddd9543db,c57,IEEE International Conference on Engineering of Complex Computer Systems,An Environment to Support Large Scale Experimentation in Software Engineering,"Experimental studies have been used as a mechanism to acquire knowledge through a scientific approach based on measurement of phenomena in different areas. However it is hard to run such studies when they require models (simulation), produce amount of information, and explore science in scale. In this case, a computerized infrastructure is necessary and constitutes a complex system to be built. In this paper we discuss an experimentation environment that has being built to support large scale experimentation and scientific knowledge management in software engineering.",fullPaper,cp57
p785,60cef66236eee4cc7c124abfa04d27cbc379362f,c95,IEEE International Conference on Computer Vision,Missing Data in Software Engineering,Abstract content,poster,cp95
p786,ea19a008e08f36121480553ab51cbcf4d48daa31,c9,Pacific Symposium on Biocomputing,Agile Software Engineering,Abstract content,poster,cp9
p787,ac2235b6e3af4a74670c3d31f922692784bd7063,j141,Empirical Software Engineering,Evaluating guidelines for reporting empirical software engineering studies,Abstract content,fullPaper,jv141
p788,9af3e5ec703f44ce80d1e0271474419c12c72122,c74,IEEE International Conference on Tools with Artificial Intelligence,Rationale-Based Software Engineering,Abstract content,poster,cp74
p789,f4df7f8cd5e5a2f00010856eaa8e5a2223b68203,c29,International Conference on Software Engineering,A view of 20th and 21st century software engineering,"George Santayana's statement, ""Those who cannot remember the past are condemned to repeat it,"" is only half true. The past also includes successful histories. If you haven't been made aware of them, you're often condemned not to repeat their successes.In a rapidly expanding field such as software engineering, this happens a lot. Extensive studies of many software projects such as the Standish Reports offer convincing evidence that many projects fail to repeat past successes.This paper tries to identify at least some of the major past software experiences that were well worth repeating, and some that were not. It also tries to identify underlying phenomena influencing the evolution of software engineering practices that have at least helped the author appreciate how our field has gotten to where it has been and where it is.A counterpart Santayana-like statement about the past and future might say, ""In an era of rapid change, those who repeat the past are condemned to a bleak future."" (Think about the dinosaurs, and think carefully about software engineering maturity models that emphasize repeatability.)This paper also tries to identify some of the major sources of change that will affect software engineering practices in the next couple of decades, and identifies some strategies for assessing and adapting to these sources of change. It also makes some first steps towards distinguishing relatively timeless software engineering principles that are risky not to repeat, and conditions of change under which aging practices will become increasingly risky to repeat.",fullPaper,cp29
p790,d300348e57198af44ede8f70e3eabe30eb013dac,j147,IEEE Software,Knowledge management in software engineering,"Software organizations' main assets are not plants, buildings, or expensive machines. A software organization's main asset is its intellectual capital, as it is in sectors such asconsulting, law, investment banking, and advertising. The major problem with intellectual capital is that it has legs and walks home every day. At the same rate experience walks out the door, inexperience walks in the door. Whether or not many software organizations admit it, they face the challenge ofsustaining the level of competence needed to win contracts and fulfill undertakings.",fullPaper,jv147
p791,daebea219735324da56cb5c181aee67193c7ea55,c58,Australian Software Engineering Conference,Ontology-Based Software Engineering- Software Engineering 2.0,"This paper describes the use of ontologies in different aspects of software engineering. This use of ontologies varies from support for software developers at multiple sites to the use of an ontology to provide semantics in different categories of software, particularly on the Web. The world's first and only software engineering ontology and a project management ontology in conjunction with a domain ontology are used to provide support for software development that is taking place at multiple sites. Ontologies are used to provide semantics to deal with heterogeneity in the representation of multiple information sources, enable the selection and composition of web services and grid resources, provide the shared knowledge base for multiagent systems, provide semantics and structure for trust and reputation systems and privacy based systems and codification of shared knowledge within different domains in business, science, manufacturing, engineering and utilities. They, therefore, bring a new paradigm to software engineering through the use of semantics as a central mechanism which will revolutionize the way software is developed and consumed in the future leading to the development of software as a service bringing about the dawn of software engineering 2.0.",fullPaper,cp58
p792,f7cdedbebc51c913f7a7b566bc1eac296df296ad,c49,International Symposium on Search Based Software Engineering,Challenges in automotive software engineering,"Developing and integrating automotive embedded software is a complex undertaking. The software is large. It is developed by many contributors. It is distributed over many control units connected by a variety of in-vehicle buses. Often much of the equipment or functions in a car are optional and regulatory requirements also vary between markets, leading to large combinatorial variations of software features. Targets running the software have to be cheap. Errors can be extremely expensive. New software and system features are demanded by the market and also by governmental regulations. Model-based design (MBD) of functional behaviour has been a big help in the recent past on the one hand, and on the other hand has by itself created new complexity by allowing relatively quick development of ever more features, especially when combined with autocoding. All this creates new challenges that did not exist a few years ago when feature development was slow. Major new challenges now are to tame all the complexity, get a system view on top of the individual functions, and to leverage executable system models to put more comprehensive testing into early phases of a development. Tools are required which really help those engineers and software developers. Their needs may not ask for a lot of computer science glamour. They can be quite basic and sophisticated concepts from computer science may find it difficult to find acceptance outside some niches. This presentation will outline the achievements, the current challenges and will point to upcoming tools and approaches that help meeting those challenges.",poster,cp49
p793,2d69014c0d601c202f0c33dde2f646e8f31183c5,c43,ACM Symposium on Applied Computing,The Future of Empirical Methods in Software Engineering Research,"We present the vision that for all fields of software engineering (SE), empirical research methods should enable the development of scientific knowledge about how useful different SE technologies are for different kinds of actors, performing different kinds of activities, on different kinds of systems. It is part of the vision that such scientific knowledge will guide the development of new SE technology and is a major input to important SE decisions in industry. Major challenges to the pursuit of this vision are: more SE research should be based on the use of empirical methods; the quality, including relevance, of the studies using such methods should be increased; there should be more and better synthesis of empirical evidence; and more theories should be built and tested. Means to meet these challenges include (1) increased competence regarding how to apply and combine alternative empirical methods, (2) tighter links between academia and industry, (3) the development of common research agendas with a focus on empirical methods, and (4) more resources for empirical research.",poster,cp43
p794,c575eb25feb0f06d2702fcf7751f6b4f61b892ee,c29,International Conference on Software Engineering,The Emerging Role of Data Scientists on Software Development Teams,"Creating and running software produces large amounts of raw data about the development process and the customer usage, which can be turned into actionable insight with the help of skilled data scientists. Unfortunately, data scientists with the analytical and software engineering skills to analyze these large data sets have been hard to come by; only recently have software companies started to develop competencies in software-oriented data analytics. To understand this emerging role, we interviewed data scientists across several product groups at Microsoft. In this paper, we describe their education and training background, their missions in software engineering contexts, and the type of problems on which they work. We identify five distinct working styles of data scientists: (1) Insight Providers, who work with engineers to collect the data needed to inform decisions that managers make; (2) Modeling Specialists, who use their machine learning expertise to build predictive models; (3) Platform Builders, who create data platforms, balancing both engineering and data analysis concerns; (4) Polymaths, who do all data science activities themselves; and (5) Team Leaders, who run teams of data scientists and spread best practices. We further describe a set of strategies that they employ to increase the impact and actionability of their work.",fullPaper,cp29
p795,53e0cea00e0653faba15d91f9b5673576af65967,c21,Grid Computing Environments,Software Engineering Data Collection for Field Studies,Abstract content,poster,cp21
p796,4e6f0d57c71f61062fd2a7beaf04cfde5ebe47a1,j141,Empirical Software Engineering,The role of replications in empirical software engineering—a word of warning,Abstract content,fullPaper,jv141
p797,932365acebbedd0be8da6245d537f031a38e7523,c91,Workshop on Algorithms and Models for the Web-Graph,The Focus Group Method as an Empirical Tool in Software Engineering,Abstract content,poster,cp91
p798,95a48bcb1e7c5c6875f7ce30b7bcbdf215c3df1a,c6,Americas Conference on Information Systems,Software Engineering for Automotive Systems: A Roadmap,"The first pieces of software were introduced into cars in 1976. By 2010, premium class vehicles are expected to contain one gigabyte of on-board software. We present research challenges in the domain of automotive software engineering.",poster,cp6
p799,3efa8779c215c6d8131d34ca0f9dc9d08e8665ba,c24,Decision Support Systems,Collaboration in Software Engineering: A Roadmap,"Software engineering projects are inherently cooperative, requiring many software engineers to coordinate their efforts to produce a large software system. Integral to this effort is developing shared understanding surrounding multiple artifacts, each artifact embodying its own model, over the entire development process. This focus on model- oriented collaboration embedded within a larger process is what distinguishes collaboration research in software engineering from broader collaboration research, which tends to address artifact-neutral coordination technologies and toolkits. This article first presents a list of goals for software engineering collaboration, then surveys existing collaboration support tools in software engineering. The survey covers both tools that focus on a single artifact or stage in the development process (requirements support tools, UML collaboration tools), and tools that support the representation and execution of an entire software process. Important collaboration standards are also described. Several possible future directions for collaboration in software engineering are presented, including tight integration between web and desktop development environments, broader participation by customers and end users in the entire development process, capturing argumentation surrounding design rationale, and use of massively multiplayer online (MMO) game technology as a collaboration medium. The article concludes by noting a problem in performing research on collaborative systems, that of assessing how well certain artifacts, models, and embedded processes work, and whether they are better than other approaches.",poster,cp24
p800,71f956a80edf979e508968568bc545308fd3ca8a,c86,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,Thermodynamics : An Engineering Approach,"Basic concepts of thermodynamics properties of pure substances the first law of thermodynamics - closed systems, control volumes the second law of thermodynamics entropy - a measure of disorder energy - a measure of work potential gas power cycles vapour and combined power cycles refrigeration cycles thermodynamics property gas mixtures gas vapour mixtures and air conditioning chemical reactions chemical and phase equilibrium thermodynamics of high-speed fluid flow property tables and charts (SI units, English units) about the software.",poster,cp86
p801,f038d65d3971c4be5529aadd7d89d5529cef54bf,c79,International Conference on Medical Image Computing and Computer-Assisted Intervention,Software Engineering Foundations: A Software Science Perspective,"To deal with the difficulties inherent in large-scale software development, the foundations of software engineering are yet to be explored. This comprehensive text is the first book to cover the theoretical and empirical foundations of software engineering. It provides a framework of software engineering methodologies and covers a wide range of foundations such as philosophy, informatics, and engineering economics. Self-contained and requiring only basic programming experience, this book is filled with in-depth comments, annotated references, real-world problems and heuristic questions. Software Engineering Foundations is an important book for software engineers and students alike.",poster,cp79
p802,421306786efd93f82a69f9acacc44a4863a86aba,j142,IEEE Transactions on Software Engineering,A Systematic Review of Theory Use in Software Engineering Experiments,"Empirically based theories are generally perceived as foundational to science. However, in many disciplines, the nature, role and even the necessity of theories remain matters for debate, particularly in young or practical disciplines such as software engineering. This article reports a systematic review of the explicit use of theory in a comprehensive set of 103 articles reporting experiments, from of a total of 5,453 articles published in major software engineering journals and conferences in the decade 1993-2002. Of the 103 articles, 24 use a total of 40 theories in various ways to explain the cause-effect relationship(s) under investigation. The majority of these use theory in the experimental design to justify research questions and hypotheses, some use theory to provide post hoc explanations of their results, and a few test or modify theory. A third of the theories are proposed by authors of the reviewed articles. The interdisciplinary nature of the theories used is greater than that of research in software engineering in general. We found that theory use and awareness of theoretical issues are present, but that theory-driven research is, as yet, not a major issue in empirical software engineering. Several articles comment explicitly on the lack of relevant theory. We call for an increased awareness of the potential benefits of involving theory, when feasible. To support software engineering researchers who wish to use theory, we show which of the reviewed articles on which topics use which theories for what purposes, as well as details of the theories' characteristics",fullPaper,jv142
p803,138238544443175e1238610cd7a1acac7cb91fad,c26,PS,Feature-Oriented Software Product Lines: Concepts and Implementation,"While standardization has empowered the software industry to substantially scale software development and to provide affordable software to a broad market, it often does not address smaller market segments, nor the needs and wishes of individual customers. Software product lines reconcile mass production and standardization with mass customization in software engineering. Ideally, based on a set of reusable parts, a software manufacturer can generate a software product based on the requirements of its customer. The concept of features is central to achieving this level of automation, because features bridge the gap between the requirements the customer has and the functionality a product provides. Thus features are a central concept in all phases of product-line development. The authors take a developers viewpoint, focus on the development, maintenance, and implementation of product-line variability, and especially concentrate on automated product derivation based on a users feature selection. The book consists of three parts. Part I provides a general introduction to feature-oriented software product lines, describing the product-line approach and introducing the product-line development process with its two elements of domain and application engineering. The pivotal part II covers a wide variety of implementation techniques including design patterns, frameworks, components, feature-oriented programming, and aspect-oriented programming, as well as tool-based approaches including preprocessors, build systems, version-control systems, and virtual separation of concerns. Finally, part III is devoted to advanced topics related to feature-oriented product lines like refactoring, feature interaction, and analysis tools specific to product lines. In addition, an appendix lists various helpful tools for software product-line development, along with a description of how they relate to the topics covered in this book. To tie the book together, the authors use two running examples that are well documented in the product-line literature: data management for embedded systems, and variations of graph data structures. They start every chapter by explicitly stating the respective learning goals and finish it with a set of exercises; additional teaching material is also available online. All these features make the book ideally suited for teaching both for academic classes and for professionals interested in self-study.",poster,cp26
p804,7f13651629224af4959a6e3ac888e26cdbbce6ca,j29,British Journal of Educational Technology,An application of games-based learning within software engineering,"For some time now, computer games have played an important role in both children and adults' leisure activities. While there has been much written on the negative aspects of computer games, it has also been recognised that they have potential advantages and benefits. There is no doubt that computer games are highly engaging and incorporate features that are extremely compelling. It is these highly engaging features of computer games that have attracted the interests of educationalists. The use of games-based learning has been growing for many years now; however, within software engineering, there is still a dearth of empirical evidence to support this approach. In this paper, we examine the literature on the use of computer games to teach software engineering concepts and describe a computer game we have been developing to teach these concepts.",fullPaper,jv29
p805,658a89f31cd662e78aa0c4236b090894f07c0ba2,j144,Information and Software Technology,A systematic review of effect size in software engineering experiments,Abstract content,fullPaper,jv144
p806,ba94ea2cdadd4f6741f3af37d7ce96abff845baf,c36,Conference on Software Engineering Education and Training,Comprehensive Evaluation of an Educational Software Engineering Simulation Environment,"Software engineering educational approaches are often evaluated only anecdotally, or in informal pilot studies. We describe a more comprehensive approach to evaluating a software engineering educational technique (SimSE, a graphical, interactive, customizable, game-based software engineering simulation environment). Our method for evaluating SimSE went above and beyond anecdotal experience and approached evaluation from a number of different angles through a family of studies designed to assess SimSE's effectiveness and guide its development. In this paper, we demonstrate the insights and lessons that can be gained when using such a multi-angled evaluation approach. Our hope is that, from this paper, educators will: (1) learn ideas about how to more comprehensively evaluate their own approaches, and (2) be provided with evidence about the educational effectiveness of SimSE.",fullPaper,cp36
p807,da93f65bcceb9ebbd31d41f10167a7780e1fc901,j147,IEEE Software,A Software Chasm: Software Engineering and Scientific Computing,"Some time ago, a chasm opened between the scientific-computing community and the software engineering community. Originally, computing meant scientific computing. Today, science and engineering applications are at the heart of software systems such as environmental monitoring systems, rocket guidance systems, safety studies for nuclear stations, and fuel injection systems. Failures of such health-, mission-, or safety-related systems have served as examples to promote the use of software engineering best practices. Yet, the bulk of the software engineering community's research is on anything but scientific-application software. This chasm has many possible causes. In this article, we look at the impact of one particular contributor in industry.",fullPaper,jv147
p808,8f38aa370c0a09f5f26d68a3ffe7a06eb1844809,c47,International Symposium on Empirical Software Engineering and Measurement,Checklists for Software Engineering Case Study Research,"Case study is an important research methodology for software engineering. We have identified the need for checklists supporting researchers and reviewers in conducting and reviewing case studies. We derived checklists for researchers and reviewers respectively, using systematic qualitative procedures. Based on nine sources on case studies, checklists are derived and validated, and hereby presented for further use and improvement.",fullPaper,cp47
p809,bb83408e1e4116a37bb5b14a045b0bb49b02145d,c41,Software Product Lines Conference,Software Design and Architecture The once and future focus of software engineering,"The design of software has been a focus of software engineering research since the field's beginning. This paper explores key aspects of this research focus and shows why design will remain a principal focus. The intrinsic elements of software design, both process and product, are discussed: concept formation, use of experience, and means for representation, reasoning, and directing the design activity. Design is presented as being an activity engaged by a wide range of stakeholders, acting throughout most of a system's lifecycle, making a set of key choices which constitute the application's architecture. Directions for design research are outlined, including: (a) drawing lessons, inspiration, and techniques from design fields outside of computer science, (b) emphasizing the design of application ""character"" (functionality and style) as well as the application's structure, and (c) expanding the notion of software to encompass the design of additional kinds of intangible complex artifacts.",poster,cp41
p810,f91d47bbd10afc8fff7bc73b0fa5ce63dc975e1c,c94,Vision,Enhancing software engineering education using teaching aids in 3-D online virtual worlds,"Three-dimensional online virtual worlds such as second life support avatar-based communications, a wide spectrum of online activities, and development of various in-world teaching and learning tools. We have experimented with second life in two computer science classes, one at Ohio University, the other at the University of Mary Washington, to enhance software engineering education. We used Second Life as an innovative collaboration and communication tool both in and outside classroom to help facilitate teamwork and interactions among student project team members. Second Life was also used as the virtual office for instructors and teaching assistants to answer students' questions during office hours. In addition, we developed two multi-player online software engineering educational games in second life, one based on the Groupthink software specification exercise developed at M.I.T., and the other based on the SimSE game (a 2-D single player game) developed at UC Irvine. By playing these two games, students learned fundamentals of software specification activities and principles of software development processes. In the paper, we will share our experience of using second life in two software engineering classes, and discuss its pros and cons based on the data collected from student surveys.",poster,cp94
p811,a8392509227625ce76425af04d61b9a65f283d09,c72,Intelligent Systems in Molecular Biology,Mining software engineering data,"Software engineering data (such as code bases, execution traces, historical code changes, mailing lists, and bug databases) contains a wealth of information about a project's status, progress, and evolution. Using well-established data mining techniques, practitioners and researchers have started exploring the potential of this valuable data in order to better manage their projects and to produce higher quality software systems that are delivered on time and within budget. This tutorial presents the latest research in mining software engineering data, discusses challenges associated with mining software engineering data, highlights success stories of mining software engineering data, and outlines future research directions. Attendees will acquire the knowledge and skills needed to integrate the mining of software engineering data in their own research or practice. This tutorial builds on several successful offerings at ICSE since 2007.",poster,cp72
p812,cd888f88f8227ceebe1a551de4fc9cdf6bee6e54,c59,British Computer Society Conference on Human-Computer Interaction,Agile human-centered software engineering,"We seek to close the gap between software engineering (SE) and human-computer interaction (HCI) by indicating interdisciplinary interfaces throughout the different phases of SE and HCI lifecycles. As agile representatives of SE, Extreme Programming (XP) and Agile Modeling (AM) contribute helpful principles and practices for a common engineering approach. We present a cross-discipline user interface design lifecycle that integrates SE and HCI under the umbrella of agile development. Melting IT budgets, pressure of time and the demand to build better software in less time must be supported by traveling as light as possible. We did, therefore, choose not just to mediate both disciplines. Following our surveys, a rather radical approach best fits the demands of engineering organizations.",fullPaper,cp59
p813,807abec1d67585e64efa2c4a097325ceee69e2c1,c41,Software Product Lines Conference,"Empirical Software Engineering Issues. Critical Assessment and Future Directions, International Workshop, Dagstuhl Castle, Germany, June 26-30, 2006. Revised Papers",Abstract content,poster,cp41
p814,f88ce0c94f9e7ce8e126e250c91c93332238c37f,c111,International Society for Music Information Retrieval Conference,What Every Engineer Should Know about Software Engineering,"THE PROFESSION OF SOFTWARE ENGINEERING Introduction Software Engineering as an Engineering Profession Standards and Certifications Misconceptions about Software Engineering Further Reading SOFTWARE PROPERTIES, PROCESSES, AND STANDARDS Introduction Characteristics of Software Software Processes and Methodologies Software Standards Further Reading SOFTWARE REQUIREMENTS SPECIFICATION Introduction Requirements Engineering Concepts Requirements Specifications Requirements Elicitation Requirements Modeling Requirements Documentation Recommendations on Requirements Further Reading DESIGNING SOFTWARE Introduction Software Design Concepts Software Design Modeling Pattern-Based Design Design Documentation Further Reading BUILDING SOFTWARE Introduction Programming Languages Software Construction Tools Becoming a Better Code Developer Further Reading SOFTWARE QUALITY ASSURANCE Introduction Quality Models and Standards Software Testing Metrics Fault Tolerance Maintenance and Reusability Further Reading MANAGING SOFTWARE PROJECTS AND SOFTWARE ENGINEERS Introduction Software Engineers Are People Too Project Management Basics Tracking and Reporting Progress Software Cost Estimation Project Cost Justification Risk Management Further Reading THE FUTURE OF SOFTWARE ENGINEERING Introduction Open Source Outsourcing and Offshoring Global Software Development Further Reading APPENDIX A: SOFTWARE REQUIREMENTS FOR A WASTEWATER PUMPING STATION WET WELL CONTROL SYSTEM (REV. 01.01.00) Introduction Overall Description Specific Requirements References APPENDIX B: SOFTWARE DESIGN FOR A WASTEWATER PUMPING STATION WET WELL CONTROL SYSTEM (REV. 01.01.00) Introduction Overall Description Design Decomposition References APPENDIX C: OBJECT MODELS FOR A WASTEWATER PUMPING STATION WET WELL CONTROL SYSTEM INDEX",poster,cp111
p815,576c3b0ca9f1df60ea95f430b4d2c5348abb2be3,c60,IEEE International Conference on Software Engineering and Formal Methods,Problem Oriented Software Engineering: A design-theoretic framework for software engineering,"A key challenge for software engineering is to learn how to reconcile the formal world of the machine and its software with the non-formal real world. In this paper, we discuss elements of problem oriented software engineering (POSE), an approach that brings both non- formal and formal aspects of software development together in a single theoretical framework for software engineering design. POSE presents development as the representation and step-wise transformation of software problems. It allows for the identification and clarification of system requirements, the understanding and structuring of the problem world, the structuring and specification of a hardware/software machine that can ensure satisfaction of the requirements in the problem world, and the construction of adequacy arguments, convincing both to developers and to customers, users and other interested parties, that the system will provide what is needed. Examples are used throughout the paper to illustrate how formal and non-formal descriptions are reconciled under POSE.",fullPaper,cp60
p816,11e2c3bfe1dd68446180f17e476addc947dad095,c50,International Conference on Automated Software Engineering,Applications of Ontologies in Software Engineering,"The emerging field of semantic web technologies promises new stimulus for Software Engineering research. However, since the underlying concepts of the semantic web have a long tradition in the knowledge engineering field, it is sometimes hard for software engineers to overlook the variety of ontology-enabled approaches to Software Engineering. In this paper we therefore present some examples of ontology applications throughout the Software Engineering lifecycle. We discuss the advantages of ontologies in each case and provide a framework for classifying the usage of ontologies in Software Engineering.",poster,cp50
p817,0cbfd89fa22878037f75612200b2417b47110da6,c113,International Conference on Image Analysis and Processing,"Software engineering, 8th Edition",Abstract content,poster,cp113
p818,2ad27a03e7c120c3d1fe31368a7a897960afd8cb,j142,IEEE Transactions on Software Engineering,Data Quality: Some Comments on the NASA Software Defect Datasets,"Background--Self-evidently empirical analyses rely upon the quality of their data. Likewise, replications rely upon accurate reporting and using the same rather than similar versions of datasets. In recent years, there has been much interest in using machine learners to classify software modules into defect-prone and not defect-prone categories. The publicly available NASA datasets have been extensively used as part of this research. Objective--This short note investigates the extent to which published analyses based on the NASA defect datasets are meaningful and comparable. Method--We analyze the five studies published in the IEEE Transactions on Software Engineering since 2007 that have utilized these datasets and compare the two versions of the datasets currently in use. Results--We find important differences between the two versions of the datasets, implausible values in one dataset and generally insufficient detail documented on dataset preprocessing. Conclusions--It is recommended that researchers 1) indicate the provenance of the datasets they use, 2) report any preprocessing in sufficient detail to enable meaningful replication, and 3) invest effort in understanding the data prior to applying machine learners.",fullPaper,jv142
p819,f73d04c6def6327cba25959f1bd8fca9f45feb93,c29,International Conference on Software Engineering,Challenges in automotive software engineering,"The amount of software in cars grows exponentially. Driving forces of this development are cheaper and more powerful hardware and the demand for innovations by new functions. The rapid increase of software and software based functionality brings various challenges (see [21], [23], [25], [26]) for the automotive industries, for their organization, key competencies, processes, methods, tools, models, product structures, division of work, logistics, maintenance, and long term strategies. From a software engineering perspective, the automotive industry is an ideal and fascinating application domain for advanced techniques. Although the automotive industry may adopt general results and solutions from the software engineering body of knowledge gained in other domains, the specific constraints and domain specific requirements in the automotive industry ask for individual solutions and bring various challenges for automotive software engineering. In cars we find literally all interesting problems and challenging issues of software and systems engineering.",fullPaper,cp29
p820,aad9c139227cc970646486109f7b8485a60d5c07,c46,Brazilian Symposium on Software Engineering,Rationale Management in Software Engineering,Abstract content,poster,cp46
p821,4c6b43f5c68b82bdb814312b019af561eac5c6bc,c56,European Conference on Software Process Improvement,A Perspective on the Future of Middleware-based Software Engineering,"Middleware is a software layer that stands between the networked operating system and the application and provides well known reusable solutions to frequently encountered problems like heterogeneity, interoperability, security, dependability. Further, with networks becoming increasingly pervasive, middleware appears as a major building block for the development of future software systems. Starting with the impact of pervasive networking on computing models, manifested by now common grid and ubiquitous computing, this paper surveys related challenges for the middleware and related impact on the software development. Indeed, future applications will need to cope with advanced non-functional properties such as context- awareness and mobility, for which adequate middleware support must be devised together with accompanying software development notations, methods and tools. This leads us to introduce our view on next generation middleware, considering both technological advances in the networking area but also the need for closer integration with software engineering best practices, to ultimately suggest middleware-based software processes.",poster,cp56
p822,4d6191fb1c4bcfef93ba7ce9a46f71f04b11e41b,c98,North American Chapter of the Association for Computational Linguistics,Changing the paradigm of software engineering,"Software evolution, iterative, and agile development represent a fundamental departure from the previous waterfall-based paradigm of software engineering.",poster,cp98
p823,5b5e66d34a1df332706f861724ff48894ad28040,c29,International Conference on Software Engineering,Software engineering for adaptive and self-managing systems,"The objective of this workshop is to consolidate the interest in the software engineering community on autonomic, self-managing, self-healing, self-optimizing, self-configuring, and self-adaptive systems. The workshop will provide a forum for researchers to share new results, raise awareness of new adaptive concerns, and promote collaboration among the community. This workshop will be the first of several to assess progress and identify challenges in this important area.",fullPaper,cp29
p824,cd5a4236691953cb024fdc34f496511d2ab555a5,j162,Systems Engineering,Some future trends and implications for systems and software engineering processes,"In response to the increasing criticality of software within systems and the increasing demands being put onto 21st century systems, systems and software engineering processes will evolve significantly over the next two decades. This paper identifies eight relatively surprise‐free trends—the increasing interaction of software engineering and systems engineering; increased emphasis on users and end value; increased emphasis on systems and software dependability; increasingly rapid change; increasing global connectivity and need for systems to interoperate; increasingly complex systems of systems; increasing needs for COTS, reuse, and legacy systems and software integration; and computational plenty. It also identifies two “wild card” trends: increasing software autonomy and combinations of biology and computing. It then discusses the likely influences of these trends on systems and software engineering processes between now and 2025, and presents an emerging scalable spiral process model for coping with the resulting challenges and opportunities of developing 21st century software‐intensive systems and systems of systems. © 2006 Wiley Periodicals, Inc. Syst Eng 9: 1–19, 2006",fullPaper,jv162
p825,6ad63dc45d5e9d9f0a6fc2abd9f3999b532ee9e5,c76,International Conference on Artificial Neural Networks,Value-Based Software Engineering: Overview and Agenda,Abstract content,poster,cp76
p826,6b8af3eafe7a26e65e49da04c67062af9ffd4df3,c61,Jahrestagung der Gesellschaft für Informatik,"Software Engineering, 8. Auflage",Abstract content,fullPaper,cp61
p827,f4f5136d20a905ff52c36de24afb1f7b61e1c5df,j147,IEEE Software,SE2004: Recommendations for Undergraduate Software Engineering Curricula,"Universities throughout the world have established undergraduate programs in software engineering, which complement existing programs in computer science and computer engineering. To provide guidance in designing an effective curriculum, the IEEE Computer Society and the ACM have developed the Software Engineering 2004 (SE2004) set of recommendations. The SE2004 document guides universities and colleges regarding the knowledge they should teach in undergraduate software engineering programs. It also provides sample courses and curriculum patterns. SE2004 begins with an overview of software engineering, explaining how it is both a computing and an engineering discipline. It then outlines the principles that drove the document's development and describes expected student outcomes. Next, SE2004 details the knowledge that universities and colleges should teach, known as SEEK (software engineering education knowledge), in a software engineering program. These recommendations are followed by general pedagogical guidelines, sample courses, and sample curriculum patterns",fullPaper,jv147
p828,9c243366103777ce1d029164bcc769f4f4231308,j79,Computer,The unspoken revolution in software engineering,In this article the author describes the outsourcing aspects of software engineering. The author finds outsourcing so fascinating partly because it serves as a magnifier and revelator of just about everything in software engineering. The development of offshoring also raises a new challenge for those of us entrusted with educating future software professionals in the industrialized world.,fullPaper,jv79
p829,cbd95a8750014a8c86bd9beb8f2ccacab85a2227,c14,International Conference on Exploring Services Science,The Role of Controlled Experiments in Software Engineering Research,Abstract content,poster,cp14
p830,7c9fd54e37cb362ece082906c8b8f0787ff4bdcc,c9,Pacific Symposium on Biocomputing,Using Ontologies in Software Engineering and Technology,Abstract content,poster,cp9
p831,3fad976398c0aef498d064ff6428ec761cd3a1e9,c101,International Conference on Automatic Face and Gesture Recognition,Essentials of software engineering,"Updated with new case studies and content, the fully revised Third Edition of Essentials of Software Engineering offers a comprehensive, accessible, and concise introduction to core topics and methodologies of software development. Designed for undergraduate students in introductory courses, the text covers all essential topics emphasized by the IEEE Computer Society-sponsored Software Engineering Body of Knowledge (SWEBOK). In-depth coverage of key issues, combined with a strong focus on software quality, makes Essentials of Software Engineering, Third Edition the perfect text for students entering the fast-growing and lucrative field of software development. The text includes thorough overviews of programming concepts, system analysis and design, principles of software engineering, development and support processes, methodologies, and product management. The revised and updated Third Edition includes all-new sections on SCRUM and HTML-Script-SQL Design Examples, as well as expanded discussions of User-Interface Design, Flow of Interactions, Cognitive Models, and other UI Design issues. Covering all phases of the software production lifecycle and emphasizing quality throughout, Essentials of Software Engineering is a superb resource for students of software engineering. Key Features: Revised and fully updated throughout, with all-new sections on SCRUM and HTML-Script-SQL Design Examples, as well as expanded discussions of other central topics Provides coverage of all essential topics emphasized by SWEBOK Covers essential topics required for students to complete individual and team projects in an affordable and accessible paperback format. Contains an all-new Appendix with examples of Essential Software Development Plan (SDP), Essential Software Requirements Specifications (SRS), Essential Software Design, and Essential Test Plan",poster,cp101
p832,907dac144f6435d882c84e8d3626b8e27d4355a3,c15,International Conference on Conceptual Structures,An Initial Theory of Value-Based Software Engineering,Abstract content,poster,cp15
p833,57e648f1cea23e2d11625990de2510ddbede39d4,c97,Interspeech,Status of Empirical Research in Software Engineering,Abstract content,poster,cp97
p834,5d609b86d8f5eec0c420d08b57e2d140a03a77b5,c74,IEEE International Conference on Tools with Artificial Intelligence,Software engineering - theory and practice (3. ed.),"Keep your method to be right here and read this page completed. You could delight in searching guide software engineering theory and practice%0D that you really describe obtain. Right here, getting the soft data of guide software engineering theory and practice%0D can be done easily by downloading and install in the web link web page that we supply below. Of course, the software engineering theory and practice%0D will be your own quicker. It's no should await the book software engineering theory and practice%0D to receive some days later after purchasing. It's no have to go outside under the heats at mid day to visit guide store.",poster,cp74
p835,7fb861b7fbe87d5ad2e62306286c383a9f2a1573,j147,IEEE Software,Reflections on software engineering education,"The ""engineering"" focus in software engineering education leaves instructors vulnerable to several traps. It also misleads students as to SE's essential human and social dimensions. Here, the author discusses how this limited conception of SE contributes to five assumptions that can trap SE educators: (i) an SE course needs an industrial project. (ii) SE is like other branches of engineering. (iii) Planning in SE is poorly done relative to other fields. (iv) The user interface is part of low-level design. (v) SWEBOK represents the state of the practice",fullPaper,jv147
p836,dacd22e7d9a77fc98fdd62a5b11b89993c972378,c46,Brazilian Symposium on Software Engineering,Integrating Security and Software Engineering: Advances and Future Visions,A Sample of Contents: Integrating Security and Software Engineering A Methodology to Develop Secure Systems Using Patterns Extending Security in Agile Software Development Methods Access Control Specification in UML.,poster,cp46
p837,b5527bcf459fb62f280a969ce9c75fc743411f10,c105,Biometrics and Identity Management,Systematic Review in Software Engineering,Abstract content,poster,cp105
p838,40f08c6412a287e6708496d8ae1a10c2f9e72a21,j147,IEEE Software,Evidence-Based Software Engineering for Practitioners,"Software managers and practitioners often must make decisions about what technologies to employ on their projects. They might be aware of problems with their current development practices (for example, production bottlenecks or numerous defect reports from customers) and want to resolve them. Or, they might have read about a new technology and want to take advantage of its promised benefits. However, practitioners can have difficulty making informed decisions about whether to adopt a new technology because there's little objective evidence to confirm its suitability, limits, qualities, costs, and inherent risks. This can lead to poor decisions about technology adoption. Software engineers might make incorrect decisions about adopting new techniques it they don't consider scientific evidence about the techniques' efficacy. They should consider using procedures similar to ones developed for evidence-based medicine. Software companies are often under pressure to adopt immature technologies because of market and management pressures. We suggest that practitioners consider evidence-based software engineering as a mechanism to support and improve their technology adoption decisions.",fullPaper,jv147
p839,fb170f5d7a9fe71b2f47ba6ad969ef410e402943,c82,Workshop on Interdisciplinary Software Engineering Research,Software Engineering (7th Edition),Abstract content,poster,cp82
p840,d37d42464fc68f14d4d23628bd8e5a7def386332,c105,Biometrics and Identity Management,Proceedings 25th International Conference on Software Engineering,The following topics are dealt with: software components; software testing; formal methods; software design; program analysis; software architecture; software engineering education; software fault correction.,poster,cp105
p841,9bf16f5cb870825a4ae6b902b197513a74395871,c24,Decision Support Systems,Value-Based Software Engineering,Abstract content,poster,cp24
p842,578981b3b4ab08742ec65324cee65ce1d1590b11,c83,International Conference on Computer Graphics and Interactive Techniques,Software engineering - principles and practice,"Software Engineering: Principles and Practice challenges the reader to appreciate the issues, design trade-offs and teamwork required for successful software development. This new edition has been brought fully up to date, with complete coverage of all aspects of the software lifecycle and a strong focus on all the skills needed to carry out software projects on time and within budget. Highlights of the third edition include: * Fully updated chapters on requirements engineering and software architecture. * New chapters on component-based software engineering, service orientation and global software development. * Extensive coverage of the human and social aspects of software development. * Balanced coverage of both traditional, heavyweight development and agile, lightweight development approaches such as Extreme Programming (XP). Written to support both introductory and advanced software engineering courses, this book is invaluable for everyone in software development and maintenance who wants an accessible account of the problems incurred in large-scale software development and the proposed solutions. A companion website with additional resources for students and instructors can be found at www.wileyeurope.com/college/van vliet",poster,cp83
p843,071e7c5701b73563117316880df4a31fa141f9ab,c70,International Conference on Intelligent Robotics and Applications,Software Engineering with Reusable Components,Abstract content,poster,cp70
p844,241b537eb8a8149273d0f3095973aada3a0ac66c,j141,Empirical Software Engineering,A survey on the use of topic models when mining software repositories,Abstract content,fullPaper,jv141
p845,988613a84f434a0ec95b0150103304aac7ad8119,j144,Information and Software Technology,A systematic review of statistical power in software engineering experiments,Abstract content,fullPaper,jv144
p846,b529562ca19d954b2e1d46b3b3660a7e89edd1c1,c109,International Conference on Mobile Data Management,Obfuscator-LLVM -- Software Protection for the Masses,"Software security with respect to reverse-engineering is a challenging discipline that has been researched for several years and which is still active. At the same time, this field is inherently practical, and thus of industrial relevance: indeed, protecting a piece of software against tampering, malicious modifications or reverse-engineering is a very difficult task. In this paper, we present and discuss a software obfuscation prototype tool based on the LLVM compilation suite. Our tool is built as different passes, where some of them have been open-sourced and are freely available, that work on the LLVM Intermediate Representation (IR) code. This approach brings several advantages, including the fact that it is language-agnostic and mostly independent of the target architecture. Our current prototype supports basic instruction substitutions, insertion of bogus control-flow constructs mixed with opaque predicates, control-flow flattening, procedures merging as well as a code tamper-proofing algorithm embedding code and data checksums directly in the control-flow flattening mechanism.",poster,cp109
p847,98eb83152df40549f5b39c9e245690ba65047114,j147,IEEE Software,Software Bots,"Although the development and widespread adoption of software bots has occurred in just a few years, bots have taken on many diverse tasks and roles. This article discusses current bot technology and presents a practical case study on how to use bots in software engineering.",fullPaper,jv147
p848,b2f8ea4b7195153124c9c099c45cb1e9054674ea,c29,International Conference on Software Engineering,Empirical studies of software engineering: a roadmap,"In this article we summarize the strengths and weaknesses of empirical research in software engineering. We argue that in order to improve the current situation we must create better studies and draw more credible interpretations from them. We finally present a roadmap for this improvement, which includes a general structure for software empirical studies and concrete steps for achieving these goals: designing better studies, collecting data more effectively, and involving others in our empirical enterprises.",fullPaper,cp29
p849,37b7f67b3a71d144ea1f0295a8899f75146255a5,c29,International Conference on Software Engineering,Software engineering for security: a roadmap,"Is there such a thing anymore as a software system that doesn’t need to be secure? Almost every softwarecontrolled system faces threats from potential adversaries, from Internet-aware client applications running on PCs, to complex telecommunications and power systems accessible over the Internet, to commodity software with copy protection mechanisms. Software engineers must be cognizant of these threats and engineer systems with credible defenses, while still delivering value to customers. In this paper, we present our perspectives on the research issues that arise in the interactions between software engineering and security.",fullPaper,cp29
p850,1d5c22408f0203d3b052b1aeac7083d9d5529427,c99,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,Encyclopedia of Software Engineering,"From the Publisher: 
Encompasses the field of software development process--from design to transpiration to testing and everything in between. Includes all functional disciplines, software tools and languages associated with software engineering of large and/or complex projects. Organized alphabetically--every major area contains an overview article that defines the topic. Each sub-discipline has a specific article covering history, current practice, practical data and projections about future practice.",poster,cp99
p851,2be0d59a55e24f3bef5e2ca0efbcab42d5ae2f15,c76,International Conference on Artificial Neural Networks,"Software Engineering 3 - Domains, Requirements, and Software Design",Abstract content,poster,cp76
p852,05acb25fccb689cb74b347929f425a6934e284bc,c64,Experimental Software Engineering Network,Sustainability Design and Software: The Karlskrona Manifesto,"Sustainability has emerged as a broad concern for society. Many engineering disciplines have been grappling with challenges in how we sustain technical, social and ecological systems. In the software engineering community, for example, maintainability has been a concern for a long time. But too often, these issues are treated in isolation from one another. Misperceptions among practitioners and research communities persist, rooted in a lack of coherent understanding of sustainability, and how it relates to software systems research and practice. This article presents a cross-disciplinary initiative to create a common ground and a point of reference for the global community of research and practice in software and sustainability, to be used for effectively communicating key issues, goals, values and principles of sustainability design for software-intensive systems.The centrepiece of this effort is the Karlskrona Manifesto for Sustainability Design, a vehicle for a much needed conversation about sustainability within and beyond the software community, and an articulation of the fundamental principles underpinning design choices that affect sustainability. We describe the motivation for developing this manifesto, including some considerations of the genre of the manifesto as well as the dynamics of its creation. We illustrate the collaborative reflective writing process and present the current edition of the manifesto itself. We assess immediate implications and applications of the articulated principles, compare these to current practice, and suggest future steps.",poster,cp64
p853,c50bb783b466f0a32f1c70a0d2398549690ee2d7,c64,Experimental Software Engineering Network,The \{PROMISE\} Repository of Software Engineering Databases.,Abstract content,poster,cp64
p854,f99c245f2a7c011c38360b051a3058ca539bcfd0,j142,IEEE Transactions on Software Engineering,Variability in Software Systems—A Systematic Literature Review,"Context: Variability (i.e., the ability of software systems or artifacts to be adjusted for different contexts) became a key property of many systems. Objective: We analyze existing research on variability in software systems. We investigate variability handling in major software engineering phases (e.g., requirements engineering, architecting). Method: We performed a systematic literature review. A manual search covered 13 premium software engineering journals and 18 premium conferences, resulting in 15,430 papers searched and 196 papers considered for analysis. To improve reliability and to increase reproducibility, we complemented the manual search with a targeted automated search. Results: Software quality attributes have not received much attention in the context of variability. Variability is studied in all software engineering phases, but testing is underrepresented. Data to motivate the applicability of current approaches are often insufficient; research designs are vaguely described. Conclusions: Based on our findings we propose dimensions of variability in software engineering. This empirically grounded classification provides a step towards a unifying, integrated perspective of variability in software systems, spanning across disparate or loosely coupled research themes in the software engineering community. Finally, we provide recommendations to bridge the gap between research and practice and point to opportunities for future research.",fullPaper,jv142
p855,e88f940ed5f93c698689b87f2f147b8f218a3cf8,c62,International Conference on Software Reuse,Concepts and Guidelines of Feature Modeling for Product Line Software Engineering,Abstract content,fullPaper,cp62
p856,690e1d57b9c5ca3e7e348fcad257767a4a2c1011,c63,IEEE International Software Metrics Symposium,Experiences from conducting semi-structured interviews in empirical software engineering research,"Many phenomena related to software development are qualitative in nature. Relevant measures of such phenomena are often collected using semi-structured interviews. Such interviews involve high costs, and the quality of the collected data is related to how the interviews are conducted. Careful planning and conducting of the interviews are therefore necessary, and experiences from interview studies in software engineering should consequently be collected and analyzed to provide advice to other researchers. We have brought together experiences from 12 software engineering studies, in which a total of 280 interviews were conducted. Four areas were particularly challenging when planning and conducting these interviews; estimating the necessary effort, ensuring that the interviewer had the needed skills, ensuring good interaction between interviewer and interviewees, and using the appropriate tools and project artifacts. The paper gives advice on how to handle these areas and suggests what information about the interviews should be included when reporting studies where interviews have been used in data collection. Knowledge from other disciplines is included. By sharing experience, knowledge about the accomplishments of software engineering interviews is increased and hence, measures of high quality can be achieved",fullPaper,cp63
p857,008f3f4af65402cb9a552de3191cc8dfceaa3a34,j147,IEEE Software,Software-engineering research revisited,"The author discusses three major changes that he suggests are occurring as a result of the software engineering industry adopting the industry-as-laboratory approach, in which researchers identify problems through close involvement with industrial projects and create and evaluate solutions in an almost indivisible research activity. This approach emphasizes what people actually do or can do in practice, rather than what is possible in principle. The three changes are a greater reliance on empirical definition of problems, an emphasis on real case studies, and a greater emphasis on contextual issues.<<ETX>>",fullPaper,jv147
p858,bd24420218cac6f5039c7a40189568ff4fae6048,j163,Journal of the American Society for Information Science,Software Engineering as Seen through Its Research Literature: A Study in Co-Word Analysis,"This empirical research demonstrates the effectiveness of content analysis to map the research literature of the software engineering discipline. The results suggest that certain research themes in software engineering have remained constant, but with changing thrusts. Other themes have arisen, matured, and then faded as major research topics, while still others seem transient or immature. Co-word analysis is the specific technique used. This methodology identifies associations among publication descriptors (indexing terms) from the ACM Computing Classification System and produces networks of descriptors that reveal these underlying patterns. This methodology is applicable to other domains with a supporting corpus of textual data. While this study utilizes index terms from a fixed taxonomy, that restriction is not inherent; the descriptors can be generated from the corpus. Hence, co-word analysis and the supporting software tools employed here can provide unique insights into any discipline's evolution.",fullPaper,jv163
p859,4c4442001e000d1afaba8e50181e3fc9fa68f88d,c68,International Workshop on Software Engineering for Large-Scale Multi-Agent Systems,Object-oriented and classical software engineering,"From the Publisher: 
Classical and Object-Oriented Software Engineering is designed for an introductory software engineering course. This book provides an excellent introduction to software engineering fundamentals,covering both traditional and object-oriented techniques. 
Schach's unique organization and style makes it excellent for use in a classroom setting. It presents the underlying software engineering theory in Part I and follows it up with the more practical life-cycle material in Part II. Many software engineering books are more like reference books,which do not provide the appropriate fundamentals before inundating students with implementation details. 
In this edition,more practical material has been added to help students understand how to use what they are learning. This has been done through the use of ""How To"" boxes and greater implementation detail in the case study. Additionally,the new edition contains the references to the most current literature and includes an overview of extreme programmming. 
The website in this edition will be more extensive. It will include Solutions,PowerPoints that incorporate lecture notes,newly developed self-quiz questions,and source code for the term project and case study.",poster,cp68
p860,7e63e256311e956093f9bea27456bc4e7206325a,c2,International Symposium on Intelligent Data Analysis,Software Engineering Metrics: What Do They Measure and How Do We Know?,"Construct validity is about the question, how we know that we're measuring the attribute that we think we're measuring? This is discussed in formal, theoretical ways in the computing literature (in terms of the representational theory of measurement) but rarely in simpler ways that foster application by practitioners. Construct validity starts with a thorough analysis of the construct, the attribute we are attempting to measure. In the IEEE Standard 1061, direct measures need not be validated. ""Direct"" measurement of an attribute involves a metric that depends only on the value of the attribute, but few or no software engineering attributes or tasks are so simple that measures of them can be direct. Thus, all metrics should be validated. The paper continues with a framework for evaluating proposed metrics, and applies it to two uses of bug counts. Bug counts capture only a small part of the meaning of the attributes they are being used to measure. Multidimensional analyses of attributes appear promising as a means of capturing the quality of the attribute in question. Analysis fragments run throughout the paper, illustrating the breakdown of an attribute or task of interest into sub-attributes for grouped study.",poster,cp2
p861,286c57dd231dcac718f556f76ece89db7571d380,c22,International Conference on Data Technologies and Applications,A software engineering framework for context-aware pervasive computing,"There is growing interest in the use of context-awareness as a technique for developing pervasive computing applications that are flexible, adaptable, and capable of acting autonomously on behalf of users. However, context-awareness introduces various software engineering challenges, as well as privacy and usability concerns. In this paper, we present a conceptual framework and software infrastructure that together address known software engineering challenges, and enable further practical exploration of social and usability issues by facilitating the prototyping and fine-tuning of context-aware applications.",poster,cp22
p862,20d68fe79beb869193133d1cc850c22b53fcd5ad,c64,Experimental Software Engineering Network,Empirical Research Methods in Software Engineering,Abstract content,fullPaper,cp64
p863,12e4dad15a6b1056e3eb1446299b90d074731647,c105,Biometrics and Identity Management,Facts and fallacies of software engineering,"There's a problem with those facts—and, as you might imagine, those fallacies. Many of these fundamentally important facts are learned by a software engineer, but over the short lifespan of the software field, all too many of them have been forgotten. While reading Facts and Fallacies of Software Engineering, you may experience moments of ""Oh, yes, I had forgotten that,"" alongside some ""Is that really true?"" thoughts.",poster,cp105
p864,e349c69e9fca46f3bd08b44d396fdee647d9053d,c108,International Conference on Information Integration and Web-based Applications & Services,Categories for software engineering,Abstract content,poster,cp108
p865,78303d3ff0420644dc995458a7cb60fb09030a4e,c65,Formal Concept Analysis,A Survey of Formal Concept Analysis Support for Software Engineering Activities,Abstract content,fullPaper,cp65
p866,0f59e1a63f26ae5861075571fabf5fda0373a7db,c23,International Conference on Open and Big Data,"The role of experimentation in software engineering: past, current, and future",Software engineering needs to follow the model of other physical sciences and develop an experimental paradigm for the field. This paper proposes the approach towards developing an experimental component of such a paradigm. The approach is based upon a quality improvement paradigm that addresses the role of experimentation and process improvement in the content of industrial development. The paper outlines a classification scheme for characterizing such experiments.,poster,cp23
p867,6157c688e166cc34cdc5e421e69424a4167c0dbf,c66,Annual Conference on Innovation and Technology in Computer Science Education,Bayesian Analysis of Empirical Software Engineering Cost Models,"Many parametric software estimation models have evolved in the last two decades (L.H. Putnam and W. Myers, 1992; C. Jones, 1997; R.M. Park et al., 1992). Almost all of these parametric models have been empirically calibrated to actual data from completed software projects. The most commonly used technique for empirical calibration has been the popular classical multiple regression approach. As discussed in the paper, the multiple regression approach imposes a few assumptions frequently violated by software engineering datasets. The paper illustrates the problems faced by the multiple regression approach during the calibration of one of the popular software engineering cost models, COCOMO II. It describes the use of a pragmatic 10 percent weighted average approach that was used for the first publicly available calibrated version (S. Chulani et al., 1998). It then moves on to show how a more sophisticated Bayesian approach can be used to alleviate some of the problems faced by multiple regression. It compares and contrasts the two empirical approaches, and concludes that the Bayesian approach was better and more robust than the multiple regression approach.",poster,cp66
p868,948351f610bbaeaa1ca7db39eee56eb8146adbb0,j164,Autonomous Agents and Multi-Agent Systems,Challenges and Research Directions in Agent-Oriented Software Engineering,Abstract content,fullPaper,jv164
p869,33725ed48b124e6897860dcb1eb5ac7e06fe0611,c42,Joint Conference of International Workshop on Software Measurement and International Conference on Software Process and Product Measurement,Methodologies and software engineering for agent systems : the agent-oriented software engineering handbook,Concepts and Abstractions of Agent-Oriented Software Engineering.- Agent-Based Abstractions for Software Development.- On the Use of Agents as Components of Software Systems.- A Survey on Agent-Oriented Oriented Software Engineering Research.- Methodologies for Agent-Based Systems Development.- The Gaia Methodology.- The Tropos Methodology.- The MaSE Methodology.- A Comparative Evaluation of Agent-Oriented Methodologies.- Special-Purpose Methodologies.- The ADELFE Methodology.- The Message Methodology.- The SADDE Methodology.- The Prometheus Methodology.- Tools and Infrastructures for Agent-Oriented Software Engineering.- The AUML Approach.- FIPA-Compliant Agent Infrastructures.- Coordination Infrastructures in the Engineering of Multiagent Systems.- Non Traditional Approaches to Agent-Oriented Software Engineering.- Engineering Amorphous Computing Systems.- Making Self-Organising Adaptive Multiagent Systems Work.- Engineering Swarming Systems.- Online Engineering and Open Computational Systems.- Emerging Trends and Perspectives.- Agents for Ubiquitous Computing.- Agents and the Grid.- Roadmap of Agent-Oriented Software Engineering.,poster,cp42
p870,2e122c81cdf362e34b84bc5286a87bbf82e6d7d0,j37,Communications of the ACM,End-user software engineering,"End-user programming has become the most common form of programming in use today [2], but there has been little investigation into the dependability of the programs end users create. This is problematic because the dependability of these programs can be very important; in some cases, errors in end-user programs, such as formula errors in spreadsheets, have cost millions of dollars. (For example, see www.theregister.co.uk/content/67/31298.html or panko.cba.hawaii.edu/ssr/Mypapers/whatknow.htm.) We have been investigating ways to address this problem by developing a software engineering paradigm viable for end-user programming, an approach we call end-user software engineering.",fullPaper,jv37
p871,72876f852bdc5a80a9a2ab3cb71768a519c4deb5,j165,Journal of Software and Systems Modeling,"Models in software engineering – an introduction
",Abstract content,fullPaper,jv165
p872,74ea36e76ae8a208336d3cf79ce1482245264363,c66,Annual Conference on Innovation and Technology in Computer Science Education,Teaching software engineering through game design,"Many projects currently used in Software Engineering curricula lack both the ""fun factor"" needed to engage students, as well as the practical realism of engineering projects that include other computer science disciplines such as Software Engineering, Networks, or Human Computer Interaction. This paper reports on our endeavor to enhance interest and retention in an existing Software Engineering curriculum through the use of computer game-based projects. Specifically, a set of game-centric, project-based modules have been developed that enable students to: (1) actively participate in the different phases of the software lifecycle taking a single project from requirement elicitation to testing and maintenance; (2) expose students to real issues in project and team management over the course of a 2-semester project; and at the same time (3) introduce students to the different aspects of computer game design. Preliminary results suggest the merits of our approach, showing improved class participation and performance.",fullPaper,cp66
p873,1f55fcd9bfc08f66723683071c45718b1eda9f56,c54,International Workshop on Agent-Oriented Software Engineering,DESMET: a methodology for evaluating software engineering methods and tools,"DESMET was a DTI-backed project with the goal of developing and validating a methodology for evaluating software engineering methods and tools. The project identified nine methods of evaluation and a set of criteria to help evaluators select an appropriate method. Detailed guidelines were developed for three important evaluation methods: formal experiments, quantitative case studies and feature analysis evaluations. This article describes the way the DESMET project used the DESMET methodology both to evaluate the methodology itself and to provide direct assistance to the commercial organisations using it.",poster,cp54
p874,49be9d3df3b1e40f3f01443c38e9f92a643eaa0d,c74,IEEE International Conference on Tools with Artificial Intelligence,Using the focus group method in software engineering: obtaining practitioner and user experiences,"This paper reflects on three cases where the focus group method was used to obtain feedback and experiences from software engineering practitioners and application users. The focus group method and its background are presented, the method's weaknesses and strengths are discussed, and guidelines are provided for how to use the method in the software engineering context. Furthermore, the results of the three studies conducted are highlighted and the paper concludes in a discussion on the applicability of the method for this type of research. In summary, the focus group method is a cost-effective and quick empirical research approach for obtaining qualitative insights and feedback from practitioners. It can be used in several phases and types of research. However, a major limitation of the method is that it is useful only in studying concepts that can be understood by participants in a limited time. We also recommend that in the software engineering context, the method should be used with sufficient empirical rigor.",poster,cp74
p875,44c013666919e101aba7d57a72aba9267929919c,c93,Human Language Technology - The Baltic Perspectiv,Software engineering with Ada,"From the Publisher: 
Grady Booch, a renowned authority in software development, and Doug Bryan combined their Ada programming and software engineering expertise for the new edition of this best-selling book. Their up-to-date introduction to Ada programming provides a foundation for using the language with software engineering and object-oriented design. Programmers will find Software Engineering with Ada, Third Edition to be a complete reference for creating large-scale Ada systems and understanding the software engineering aspects of these systems. Features of the third edition include techniques for combining object-oriented design principles and software engineering to maximize the potential of Ada; extensive examples of small-sized code that will benefit new Ada programmers; six chapters devoted to design; five new large-scale programming exercises that build upon the software engineering principles developed in the design chapters; design projects on topics such as environment monitoring, database systems, and generic tree packages; an introduction to up-to-date object-oriented design methodology; and a new appendix on the Ada 9X program.",poster,cp93
p876,7ae824e3cd361937a3514a22983d9f2fcac3f81d,c37,Asia-Pacific Software Engineering Conference,Software engineering concepts,"Software engineering concepts , Software engineering concepts , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",poster,cp37
p877,286495f04fbea22693e45957186c919e6e09b8cc,c53,International Conference on Software Engineering and Knowledge Engineering,Writing good software engineering research papers,"Software engineering researchers solve problems of several different kinds. To do so, they produce several different kinds of results, and they should develop appropriate evidence to validate these results. They often report their research in conference papers. I analyzed the abstracts of research papers submitted to XSE 2002 in order to identify the types of research reported in the submitted and accepted papers, and I observed the program committee discussions about which papers to accept. This report presents the research paradigms of the papers, common concerns of the program committee, and statistics on success rates. This information should help researchers design better research projects and write papers that present their results to best advantage.",poster,cp53
p878,1f1e25367457f67d74ea2a3b497645898c5962a8,c32,International Conference on Software Technology: Methods and Tools,Using benchmarking to advance research: a challenge to software engineering,"Benchmarks have been used in computer science to compare the performance of computer systems, information retrieval algorithms, databases, and many other technologies. The creation and widespread use of a benchmark within a research area is frequently accompanied by rapid technical progress and community building. These observations have led us to formulate a theory of benchmarking within scientific disciplines. Based on this theory, we challenge software engineering research to become more scientific and cohesive by working as a community to define benchmarks. In support of this challenge, we present a case study of the reverse engineering community, where we have successfully used benchmarks to advance the state of research.",poster,cp32
p879,33419d151b854d6268d5608cffdf25a8395d4967,j79,Computer,Value-Based Software Engineering: A Case Study,"The information technology field's accelerating rate of change makes feedback control essential for organizations to sense, evaluate, and adapt to changing value propositions in their competitive marketplace. Although traditional project feedback control mechanisms can manage the development efficiency of stable projects in well-established value situations, they do little to address the project's actual value, and can lead to wasteful misuse of an organization's scarce resources. The value-based approach to software development integrates value considerations into current and emerging software engineering principles and practices, while developing an overall framework in which these techniques compatibly reinforce each other.",fullPaper,jv79
p880,f43cba75815b7e9d079286341d15c014982b7823,c13,International Conference on Data Science and Advanced Analytics,An experimental card game for teaching software engineering,"The typical software engineering course consists of lectures in which concepts and theories are conveyed, along with a small ""toy"" software engineering project which attempts to give students the opportunity to put this knowledge into practice. Although both of these components are essential, neither one provides students with adequate practical knowledge regarding the process of software engineering. Namely, lectures allow only passive learning, and projects are so constrained by the time and scope requirements of the academic environment that they cannot be large enough to exhibit many of the phenomena occurring in realworld software engineering processes. To address this problem, we have developed Problems and Programmers, an educational card game that simulates the software engineering process and is designed to teach those process issues that are not sufficiently highlighted by lectures and projects. We describe how the game is designed, the mechanics of its game play, and the results of an experiment we conducted involving students playing the game.",poster,cp13
p881,10885f36ed1cd87e735e25b32e4bdec9994b4ed7,c29,International Conference on Software Engineering,Software engineering and middleware: a roadmap,"The construction of a large class of distributed systems can be simplified by leveraging middleware, which is layered between network operating systems and application components. Middleware resolves heterogeneity, and facilitates communication and coordination of distributed components. Existing middleware products enable software engineers to build systems that are distributed across a local-area network. State-of-the-art middleware research aims to push this boundary towards Internet-scale distribution, adaptive and reconfigurable middleware and middleware for dependable and wireless systems. The challenge for software engineering research is to devise notations, techniques, methods and tools for distributed system construction that systematically build and exploit the capabilities that middleware deliver. 1 I N T R O D U C T I O N Various commercial trends have lead to an increasing demand for distributed systems. Firstly, the number of mergers between companies is continuing to increase. The different divisions of a newly merged company have to deliver unified services to their customers and this usually demands an integration of their IT systems. The time available for delivery of such an integration is often so short that building a new system is not an option and therefore existing system components have to be integrated into a distributed system that appears as an integrating computing facility. Secondly, the time available for providing new services are decreasing. Often this can only be achieved if components are procured off-the-shelf and then integrated into a system rather than built from scratch. Components to be integrated may have incompatible requirements for their hardware and operating system platforms; they have to be deployed on different hosts, forcing the resulting system to be distributed. Finally, the Internet provides new opportunities to offer products and services to a vast number of potential customers. In this setting, it is difficult to estimate the scalability requirements. Permission to make digital or hard copies of all or part of this work lbr personal or classroom use is granted without fee provided that copies are not made or distributed tbr profit or commercial advantage and that copies bear this notice and the lull citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a tee. Future of Sofware Engineering Limerick Ireland Copyright ACM 2000 1-58113-253-0/00/6...$5.00 An e-commerce site that was designed to cope with a given number of transactions per day may suddenly find itself exposed to demand that is by orders of magnitude larger. The required scalability cannot usually be achieved by centralized or client-server architectures but demands a distributed system. Distributed systems can integrate legacy components, thus preserving investment, they can decrease the time to market, they can be scalable and tolerant against failures. The caveat, however, is that the construction of a truly distributed systems is considerably more difficult than building a centralized or client/server system. This is because there are multiple points of failure in a distributed system, system components need to communicate with each other through a network, which complicates communication and opens the door for security attacks. Middleware has been devised in order to conceal these difficulties from application engineers as much as possible; As they solve a real problem and simplify distributed system construction, middleware products are rapidly being adopted in industry [6]. In order to build distributed systems that meet the requirements, software engineers have to know what middleware is available, which one is best suited to the problems at hand, and how middleware can be used in the architecture, design and implementation of distributed systems. The principal contribution of this paper is an assessment of both, the state-of-the-practice that current middleware products offer and the state-of-the-art in middleware research. Software engineers increasingly use middleware to build distributed systems. Any research into distributed software engineering that ignores this trend will only have limited impact. We, therefore, analyze the influence that the increasing use of middleware should have on the software engineering research agenda. We argue that requirements engineering techniques are needed that focus on non-functional requirements, as these influence the selection and use of middleware. We identify that software architecture research should produce methods that guide engineers towards selecting the right middleware and employing it so that it meets a set of non-functional requirements. We then highlight that the use of middleware is not transparent for system design and that design methods are needed that address this issue.",fullPaper,cp29
p882,e6165f19a32c649e637ccaaf704e7bbb179c5823,c58,Australian Software Engineering Conference,"Object Oriented Software Engineering, Conquering Complex and Changing Systems","From the Publisher: 
This book is based on object-oriented techniques applied to software engineering. Employing the latest technologies such as UML, Patterns, and Java, Bernd Bruegge and Allen H. Dutoit offer a cohesive, class-tested presentation of object-oriented software engineering in a step-by-step format based on ten years of teaching and real-world software engineering experience. This text teaches practical experience in developing complex software appropriate for software engineering project courses, as well as industry R & D practitioners. The reader benefits from timely exposure to state-of-the-art tools and methods. 
 
Unlike other texts based on the teaching premise of multiple classes or developing multiple systems, this book focuses on techniques and applications in a reasonably complex environment, such as multi-team development projects including 20 to 60 participants. The book is based on concrete examples from real applications such as accident management, emissions modeling, facility management, and centralized traffic control. 
 
Provides an integrated communication infrastructure for distributed development 
Shows the state of the art in Software Engineering: UML, Java, Design Patterns, Distributed Development, and Multiproject Management 
Illustrates how the reader learns to develop in a distributed team with hands-on experience on real system development problems 
Offers a CD-ROM containing the materials used in courses taught by the authors-problem statements, requirement analysis documents, system design documents, test manuals, prototypes, and all the artifacts produced during the development of a facility management system 
Presents Companion Website (www.prenhall.com/bruegge) with supplemental material such as problem statements, requirement analysis documents, system design documents, test manuals, and solutions to exercises",poster,cp58
p883,c039b9a4080428eca9a685c2cfe269f4b8a4a5e7,c105,Biometrics and Identity Management,Conducting realistic experiments in software engineering,"An important goal of most empirical software engineering research is the transfer of research results to industrial applications. Two important obstacles for this transfer are the lack of control of variables of case studies, i.e., the lack of explanatory power, and the lack of realism of controlled experiments. While it may be difficult to increase the explanatory power of case studies, there is a large potential for increasing the realism of controlled software engineering experiments. To convince industry about the validity and applicability of the experimental results, the tasks, subjects and the environments of the experiments should be as realistic as practically possible. Such experiments are, however, more expensive than experiments involving students, small tasks and pen-and-paper environments. Consequently, a change towards more realistic experiments requires a change in the amount of resources spent on software engineering experiments. This paper argues that software engineering researchers should apply for resources enabling expensive and realistic software engineering experiments similar to how other researchers apply for resources for expensive software and hardware that are necessary for their research. The paper describes experiences from recent experiments that varied in size from involving one software professional for 5 days to 130 software professionals, from 9 consultancy companies, for one day each.",poster,cp105
p884,ed96e3b61e6f6a682fee07f8aa8cdf246fab44b6,c29,International Conference on Software Engineering,The use of program dependence graphs in software engineering,"This paper describes a language-independent program representation-the program dependence graph-and discusses how program dependence graphs, together with operations such as program slicing, can provide the basis for powerful programmmg tools that address important software-engineering problems, such as understanding what an existing program does and how it works, understanding the differences between several versions of a program, and creating new programs by combining pieces of old pro- grams. The paper primarily surveys work in this area that has been carried out at the University of Wisconsin during the past five years.",fullPaper,cp29
p885,100437655ade3a6467d5100b7c23eb0e36050d83,c21,Grid Computing Environments,Component-based software engineering - new challenges in software development,"The primary role of component-based software engineering is to address the development of systems as an assembly of parts (components), the development of parts as reusable entities, and the maintenance and upgrading of systems by customising and replacing such parts. This requires established methodologies and tool support covering the entire component and system lifecycle including technological, organisational, marketing, legal, and other aspects. The traditional disciplines from software engineering need new methodologies to support component-based development.",poster,cp21
p886,7fd3d0a1d17a134ad1bbbf753451c227477db1e5,c46,Brazilian Symposium on Software Engineering,Process Models in Software Engineering,"Software systems come and go through a series of passages that account for their inception, initial development, productive operation, upkeep, and retirement from one generation to another. This article categorizes and examines a number of methods for describing or modeling how software systems are developed. It begins with background and definitions of traditional software life-cycle models that dominate most textbook discussions and current software development practices. This is followed by a more comprehensive review of the alternative models of software evolution that are of current use as the basis for organizing software engineering projects and technologies. 
 
 
Keywords: 
 
software process model; 
definition; 
software life-cycle models; 
development models; 
production process models",poster,cp46
p887,14cae0857b066adfd72137481296cc9f81741d2a,c12,International Conference on Statistical and Scientific Database Management,"Software engineering education in the era of outsourcing, distributed development, and open source software: challenges and opportunities",Abstract content,poster,cp12
p888,3a0535cf9abb931646e07cdf875a4ae081e0dafb,c45,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",Tool Integration in Software Engineering Environments,"This article presents doctoral research on tool int egration within software engineering environments. Tool int egration concerns the techniques used to form coalitions of to ls that provide an environment supporting some, or all, act ivities within the software engineering process. Some inte res ing phenomena have been observed, such as the ad hoc na ture of tool integration in one particular software enginee ring company. This observation is at variance to the com m n perception of widespread integration suggested by t ool vendors and some previous academic literature. Ini tial results suggest that integration must be implemented for bu siness reasons, not for its own sake.",poster,cp45
p889,1c2f786b0a8930835fe014f6aaa861df1ea4f4e4,c67,Enterprise Application Integration,Ontologies in the Software Engineering Process,"The term ontology has become popular in several fields of Informatics like Artificial Intelligence, Agent systems, Database or Web Technology. Deviating from its original philosophical meaning, in the context of Computer Sciences the term ontology stands for a formal explicit specification of a shared conceptualization. Software Engineering (SE) is a field where conceptualisation plays a major role, e.g. in the early phases of software development, in the definition, use and re-use of software components and as a basis for their integration. Thus, ontologies are likely to invade the SE field as well soon. In this contribution, conceptual modeling as practiced in SE and Information Systems projects is contrasted with the ontology approach. The corresponding life cycle models for their development are compared. Finally, some perspectives of an Ontology-based Software Engineering (OBSE) approach are outlined.",fullPaper,cp67
p890,7044cf26d6f6ffe4804e0aaf7c18d6c6da5e3766,c89,Conference on Uncertainty in Artificial Intelligence,The challenges of software engineering education,"We discuss the technical skills that a software engineer should possess. We take the viewpoint of a school of engineering and put the software engineer's education in the wider context of engineering education. We stress both the common aspects that crosscut all engineering fields and the specific issues that pertain to software engineering. We believe that even in a continuously evolving field like software, education should emphasize principles and recognize what are the stable and long-lasting design concepts. Even though the more mundane technological solutions cannot be ignored, the students should be equipped with skills that allow them to dominate the evolution of technology.",poster,cp89
p891,a6f3a7288fc5f5e976427118abd354c8f602d9d4,c46,Brazilian Symposium on Software Engineering,Thinking on the Development of Software Engineering Technology,"The paper gives some thinking according to the following four aspects: 1) from the law of things development, revealing the development history of software engineering technology; 2) from the point of software natural characteristic, analyzing the construction of every abstraction layer of virtual machine; 3) from the point of software development, proposing the research content of software engineering discipline, and research the pattern of industrialized software production; 4) based on the appearance of Internet technology, exploring the development trend of software technology.",poster,cp46
p892,9ae81c7d36be6cd4c541e824f6886b33bae43b20,c50,International Conference on Automated Software Engineering,Models in software engineering - an introduction,Abstract content,poster,cp50
p893,66a09f63535221b05a1b802dd9f6a75fd743521e,c112,Very Large Data Bases Conference,Software engineering (6th ed.),Abstract content,poster,cp112
p894,968b8b3162b62e4c91cf4244ca9bf6f42c2256b5,c109,International Conference on Mobile Data Management,Conducting on-line surveys in software engineering,"One purpose of empirical software engineering is to enable an understanding of factors that influence software development. Surveys are an appropriate empirical strategy to gather data from a large population (e.g., about methods, tools, developers, companies) and to achieve an understanding of that population. Although surveys are quite often performed, for example, in social sciences and marketing research, they are underrepresented in empirical software engineering research, which most often uses controlled experiments and case studies. Consequently, also the methodological support how to perform such studies in software engineering is rather low. However, with the increasing pervasion of the Internet it is possible to perform surveys easily and cost-effectively over Internet pages (i.e., on-line), while at the same time the interest in performing surveys is growing. The purpose of this paper is twofold. First we want to arise the awareness of on-line surveys and discuss methods how to perform these in the context of software engineering. Second, we report our experience in performing on-line surveys in the form of lessons learned and guidelines.",poster,cp109
p895,27d062db677355d1f98d280e61ea8c068dcb0a0d,c33,International Conference on Agile Software Development,A Ranking of Software Engineering Measures Based on Expert Opinion,"This research proposes a framework based on expert opinion elicitation, developed to select the software engineering measures which are the best software reliability indicators. The current research is based on the top 30 measures identified in an earlier study conducted by Lawrence Livermore National Laboratory. A set of ranking criteria and their levels were identified. The score of each measure for each ranking criterion was elicited through expert opinion and then aggregated into a single score using multiattribute utility theory. The basic aggregation scheme selected was a linear additive scheme. A comprehensive sensitivity analysis was carried out. The sensitivity analysis included: variation of the ranking criteria levels, variation of the weights, variation of the aggregation schemes. The top-ranked measures were identified. Use of these measures in each software development phase can lead to a more reliable quantitative prediction of software reliability.",poster,cp33
p896,a646c876e9bec9923bb374e886c89c458b248f8f,j144,Information and Software Technology,Research in software engineering: an analysis of the literature,Abstract content,fullPaper,jv144
p897,482dd72ce24ec08528bd97cbbe93d239a377fb0a,j142,IEEE Transactions on Software Engineering,Experimentation in software engineering,"A framework is presented for analyzing most of the experimental work performed in software engineering over the past several years. The framework of experimentation consists of four categories corresponding to phases of the experimentation process: definition, planning, operation, and interpretation. A variety of experiments are described within the framework and their contribution to the software engineering discipline is discussed. Some recommendations for the application of the experimental process in software engineering are included.",fullPaper,jv142
p898,ff713b2904f72c58efb39de6e805861d0d4bd638,c40,IEEE International Conference on Software Maintenance and Evolution,Principles of software engineering management,"From time to time books emerge which become part of APL's folklore; the obvious example being ""APL-An Interactive Approach"" by Gilman and Roses. Some are more recent and more tangentical in their relevance; exam-pies are Myers ""Refiable Software Through Composite Design"" and Tufte's ""Visualization of Quantitative Data"". I believe that this most recent book of Mr. Gilb's may be destined to become part of this select group. A long-standing belief in the APL world is, that we can deliver ""productivity""-recent explorations of topics like Function Point Analysis are (belatedly) showing good quality evidence to back this belief. At heart I feel that Mr. Gilb is saying that even though we're moving in the fight direction we're still a very long way from the goal we ought to be shooting for-but that we can achieve the ultimate objective if we take a wider view of the world. Point one-the term 'software engineering' is being hijacked and is in imminent danger of becoming the vogue term for a much smaller range of topics than it purports to cover. Point two-APL applications have long tended to solve broader and more strategic problems than the more established languages. For all of his 442 pages, Mr. Gilb's messages axe brief, relevant and, I believe, achievable. They also embody common sense-that rarest of commodities-it is hard to believe that so many people can have been neglecting the principles for so long. I'd like to summarise his message in three ways. [1] Measure what you're being asked to do in terms meaningful to the business, user or application. And measure what you achieve. [2] Deliver your solution in many incremental stages-at all times delivering the highest value at least cost in the shortest time (remember, you just quantified all of this). [3] Early detection and solution of problems costs orders of magnitude less to fix than late detection and remedy. Think about it-its not novel is it? And doesn't the seemed message have a familiar ring7 What Mr. Glib is attempting to do is to bring rigour into the process. I think that if we were to adopt these methods in a similarly serious-minded vein we might easily discover a lot about ourselves and just possibly find a new way of getting our message into wider environments. I offer a challenge to the APL COmmunity in general and the APL90 organisers in particular. Lets find at least …",poster,cp40
p899,9ac8eda3552e02e23ca63715bffe7fcb98d5b4c9,j166,International Journal on Software Tools for Technology Transfer (STTT),What makes good research in software engineering?,Abstract content,fullPaper,jv166
p900,ff713b2904f72c58efb39de6e805861d0d4bd638,c98,North American Chapter of the Association for Computational Linguistics,Principles of software engineering management,"From time to time books emerge which become part of APL's folklore; the obvious example being ""APL-An Interactive Approach"" by Gilman and Roses. Some are more recent and more tangentical in their relevance; exam-pies are Myers ""Refiable Software Through Composite Design"" and Tufte's ""Visualization of Quantitative Data"". I believe that this most recent book of Mr. Gilb's may be destined to become part of this select group. A long-standing belief in the APL world is, that we can deliver ""productivity""-recent explorations of topics like Function Point Analysis are (belatedly) showing good quality evidence to back this belief. At heart I feel that Mr. Gilb is saying that even though we're moving in the fight direction we're still a very long way from the goal we ought to be shooting for-but that we can achieve the ultimate objective if we take a wider view of the world. Point one-the term 'software engineering' is being hijacked and is in imminent danger of becoming the vogue term for a much smaller range of topics than it purports to cover. Point two-APL applications have long tended to solve broader and more strategic problems than the more established languages. For all of his 442 pages, Mr. Gilb's messages axe brief, relevant and, I believe, achievable. They also embody common sense-that rarest of commodities-it is hard to believe that so many people can have been neglecting the principles for so long. I'd like to summarise his message in three ways. [1] Measure what you're being asked to do in terms meaningful to the business, user or application. And measure what you achieve. [2] Deliver your solution in many incremental stages-at all times delivering the highest value at least cost in the shortest time (remember, you just quantified all of this). [3] Early detection and solution of problems costs orders of magnitude less to fix than late detection and remedy. Think about it-its not novel is it? And doesn't the seemed message have a familiar ring7 What Mr. Glib is attempting to do is to bring rigour into the process. I think that if we were to adopt these methods in a similarly serious-minded vein we might easily discover a lot about ourselves and just possibly find a new way of getting our message into wider environments. I offer a challenge to the APL COmmunity in general and the APL90 organisers in particular. Lets find at least …",poster,cp98
p901,482dd72ce24ec08528bd97cbbe93d239a377fb0a,j142,IEEE Transactions on Software Engineering,Experimentation in software engineering,"A framework is presented for analyzing most of the experimental work performed in software engineering over the past several years. The framework of experimentation consists of four categories corresponding to phases of the experimentation process: definition, planning, operation, and interpretation. A variety of experiments are described within the framework and their contribution to the software engineering discipline is discussed. Some recommendations for the application of the experimental process in software engineering are included.",fullPaper,jv142
p902,3934337068a750e2eca6632546dab076e17c15ed,j147,IEEE Software,Reliability Engineering,"Reliability engineering dates back to reliability studies in the 20th century; since then, various models have been defined and used. Software engineering plays a key role from several viewpoints, but the main concern is that we're moving toward a more connected world, including enterprises and mobile devices. The three articles in this special issue illustrate current trends in this domain.",fullPaper,jv147
p903,ec9196b9edd46c5c59124ca776ca71538655fbc6,c91,Workshop on Algorithms and Models for the Web-Graph,Empirical Data Modeling in Software Engineering Using Radical Basis Functions,"Many empirical studies in software engineering involve relationships between various process and product characteristics derived via linear regression analysis. We propose an alternative modeling approach using radial basis functions (RBFs) which provide a flexible way to generalize linear regression function. Further, RBF models possess strong mathematical properties of universal and best approximation. We present an objective modeling methodology for determining model parameters using our recent SG algorithm, followed by a model selection procedure based on generalization ability. Finally, we describe a detailed RBF modeling study for software effort estimation using a well-known NASA dataset.",poster,cp91
p904,ddfa898b4044f5e7e36d39e99f035f9cf95cede1,c69,International Conference on Parallel Processing,Software engineering for large-scale multi-agent systems - SELMAS'05,Abstract content,poster,cp69
p905,25831b14ff3201ba595ee47c9347b576a34b15cd,j141,Empirical Software Engineering,Knowledge-Sharing Issues in Experimental Software Engineering,Abstract content,fullPaper,jv141
p906,27aa2b0aa6cfb97d2ac4044b28421700371e6027,j141,Empirical Software Engineering,On the application of measurement theory in software engineering,Abstract content,fullPaper,jv141
p907,2ea49387024835ac28527c66afc3265dc957475b,j141,Empirical Software Engineering,Repeatable software engineering experiments for comparing defect-detection techniques,Abstract content,fullPaper,jv141
p908,49900e8f8328adfee4818fa4ee959c2529966636,c27,ACM-SIAM Symposium on Discrete Algorithms,Knowledge management in software engineering - describing the process,"The management of knowledge and experience are key means by which systematic software development and process improvement occur. Within the domain of software engineering (SE), quality continues to remain an issue of concern. Although remedies such as fourth generation programming languages, structured techniques and object-oriented technology have been promoted, a ""silver bullet"" has yet to be found. Knowledge management (KM) gives organisations the opportunity to appreciate the challenges and complexities inherent in software development. We report on two case studies that investigate KM in SE at two IT organisations. Structured interviews were conducted, with the assistance of a qualitative questionnaire. The results were used to describe current practices for KM in SE, to investigate the nature of KM activities in these organisations, and to explain the impact of leadership, technology, culture and measurement as enablers of the KM process for SE.",poster,cp27
p909,4fe3be9b66f6a71aefe9963ec14cb56c62dca4f2,c1,Technical Symposium on Computer Science Education,A risk management framework for software engineering practice,"Formal risk analysis and management in software engineering is still an emerging part of project management. We provide a brief introduction to the concepts of risk management for software development projects, and then an overview of a new risk management framework. Risk management for software projects is intended to minimize the chances of unexpected events, or more specifically to keep all possible outcomes under tight management control. Risk management is also concerned with making judgments about how risk events are to be treated, valued, compared and combined. The ProRisk management framework is intended to account for a number of the key risk management principles required for managing the process of software development. It also provides a support environment to operationalize these management tasks.",poster,cp1
p910,38e26d517cc5a1e609a90391d35c3f25ee84089f,j167,IEEE transactions on computers,Software Engineering,"This paper provides a definition of the term ""software engineering"" and a survey of the current state of the art and likely future trends in the field. The survey covers the technology available in the various phases of the software life cycle—requirements engineering, design, coding, test, and maintenance—and in the overall area of software management and integrated technology-management approaches. It is oriented primarily toward discussing the domain of applicability of techniques (where and when they work), rather than how they work in detail. To cover the latter, an extensive set of 104 references is provided.",fullPaper,jv167
p911,ba3d605a080e906233b38cec6e879ac44fee4ac9,c55,Annual Workshop of the Psychology of Programming Interest Group,Value-based software engineering: reinventing,"The Value-Based Software Engineering (VBSE) agenda described in the preceding article has the objectives of integrating value considerations into current and emerging software engineering principles and practices, and of developing an overall framework in which they compatibly reinforce each other. In this paper, we provide a case study illustrating some of the key VBSE practices, and focusing on a particular anomaly in the monitoring and control area: the ""Earned Value Management System."" This is a most useful technique for monitoring and controlling the cost, schedule, and progress of a complex project. But it has absolutely nothing to say about the stakeholder value of the system being developed. The paper introduces an example order-processing software project, and shows how the use of Benefits Realization Analysis, stake-holder value proposition elicitation and reconciliation, and business case analysis provides a framework for stakeholder-earned-value monitoring and control.",poster,cp55
p912,18a28e73f39a58194da82e559227809f0e0260dc,c82,Workshop on Interdisciplinary Software Engineering Research,A practical approach of teaching Software Engineering,"In today's software industry a software engineer is not only expected to successfully cope with technical challenges, but also to deal with non-technical issues arising from difficult project situations. These issues typically include understanding the customer's domain and requirements, working in a team, organizing the division of work, and coping with time pressure and hard deadlines. Thus, in our opinion teaching Software Engineering, (SE) not only requires studying theory using text books, but also providing students with the experience of typical non-technical issues in a software project. This article reports experiences with the concept of a course focusing on providing practical know-how.",poster,cp82
p913,1a0e21c3bde997153a59f800ce2babe0779fffd3,c89,Conference on Uncertainty in Artificial Intelligence,Automotive software engineering,"Information technology has become the driving force of innovation in many areas of technology and also in cars. Embedded software controls the functions of cars, supports and assists the driver and realizes systems for information and entertainment. Software in automobiles is today one of the great challenges for software engineering. On modem cars we find all issues of software systems in a nutshell. It is a challenge for software and systems engineering.",poster,cp89
p914,8e34ac04a2de79ecd784f8374de20ecd9746cdcf,c75,International Conference on Machine Learning,Software Engineering Measurement,THE GOALS OF SOFTWARE ENGINEERING MEASUREMENT Software Engineering Measurement The Rationale for Effective Measurement Measurement across the Life Cycle Model Reasonable and Attainable Goals for Software Measurement Summary THE CONDUCT OF SCIENTIFIC INVESTIGATIONS The Principals of Scientific Investigation Measurement Measurement Issues Measurement Standards Principles of Experimentation MEASURING SOFTWARE DEVELOPMENT Measurement Domains Modeling: Mapping among Measurement Domains The Process of Software Measurement Summary VALIDATION OF SOFTWARE MEASURES Understanding What Is Being Measured Criterion-Oriented Validity Content Validity Construct Validity Empirical Validity Reliability STATIC SOFTWARE MEASUREMENT Introduction Primitive Measures of Source Code Measures of Software Quality Summary DERIVED SOFTWARE MEASURES Introduction Software Science Metrics Sources of Variation The Principal Components of Measurement Principal Components Analysis as a Validation Tool Discovering New Sources of Variation Domain Metrics A Unitary Measure of Software Complexity Summary MODELING WITH METRICS Introduction Simple Linear Regression Non-Linear Models Problems Associated with Multicollinearity Regression as a Metric Validation Tool Canonical Correlation MEASURING SOFTWARE EVOLUTION Introduction Measuring Evolving Software Measuring Changes to Modules across Builds Summary SOFTWARE SPECIFICATION AND DESIGN Introduction Software Operational Requirements Specification Software Functional Requirements Specification Software Module Requirements Specification A Formal Description of Program Operation Configuration Control for the Requirements Measuring Software Design Alternatives Maintainability DYNAMIC SOFTWARE MEASUREMENT Introduction A Stochastic Description of Program Operation The Profiles of Software Dynamics Estimates for Profiles Code Instrumentation Instrumenting for the Profiles Partial Complexity A Measure of Cohesion Entropy Testability Revisited THE MEASUREMENT OF SOFTWARE TESTING ACTIVITY Introduction Static and Dynamic Measurement A Metaphor for Test Activity Measurement Based Testing Fractional Measures Introduction to Statistical Testing SOFTWARE AVAILABILITY Introduction Software Reliability Availability Security Maintainability IMPLEMENTING A SOFTWARE MEASUREMENT PLAN The Software Measurement Process Building a Measurement Process Measurement Process Improvement Institutionalizing Measurement Process Improvement A Network Based Measurement System IMPLEMENTING A SOFTWARE RESEARCH PLAN What Is Software Research? Implementing a Research Plan Defining Software Research Objectives Budgeting for Software Research Research Pays APPENDIXES REVIEW OF MATHEMATICAL FUNDAMENTALS Matrix Algebra Some Notions of Probability Discrete Probability Distributions Continuous Probability Distributions Statistics Tests of Hypotheses Introduction to Modeling A STANDARD FOR THE MEASUREMENT OF C PROGRAMMING LANGUAGE ATTRIBUTES Introduction Compiler Directives Style and Statement Metrics Lexical Metrics Control Flowgraph Metrics Coupling Metrics Definitions Tokens,poster,cp75
p915,363677f867ae75808b5e1a9371b81bdfa648e726,c89,Conference on Uncertainty in Artificial Intelligence,The software engineering impacts of cultural factors on multi-cultural software development teams,"This paper is based on our experiences in trying to apply software engineering practices to development projects staffed by developers from three distinct cultures; Japan, India, and the United States. The development of commercial software products has always been difficult. The standard balancing act that occurs between features, schedules, and resources is at the core of the difficulty. We found that cultural differences also had a large impact on our software engineering work Much has been written and said about software engineering methods that can be applied to development projects to reduce and control these core difficulties. Methods that were thought to be ""best practices"" turned out to be ineffective or very difficult to implement. Our understanding of the possible root causes for these difficulties greatly increased when we began to study some of the cultural dynamics within the team. This paper describes our observations in terms of how these cultural factors impacted the software engineering techniques used on the projects.",poster,cp89
p916,7319d1e88da5cd1c707a71ce4cdc3e8c5c05b97c,c22,International Conference on Data Technologies and Applications,Formulating software engineering as a search problem,"Metaheuristic techniques such as genetic algorithms, simulated annealing and tabu search have found wide application in most areas of engineering. These techniques have also been applied in business, financial and economic modelling. Metaheuristics have been applied to three areas of software engineering: test data generation, module clustering and cost/effort prediction, yet there remain many software engineering problems which have yet to be tackled using metaheuristics. It is surprising that metaheuristics have not been more widely applied to software engineering; many problems in software engineering are characterised by precisely the features which make metaheuristics search applicable. In the paper it is argued that the features which make metaheuristics applicable for engineering and business applications outside software engineering also suggest that there is great potential for the exploitation of metaheuristics within software engineering. The paper briefly reviews the principal metaheuristic search techniques and surveys existing work on the application of metaheuristics to the three software engineering areas of test data generation, module clustering and cost/effort prediction. It also shows how metaheuristic search techniques can be applied to three additional areas of software engineering: maintenance/evolution system integration and requirements scheduling. The software engineering problem areas considered thus span the range of the software development process, from initial planning, cost estimation and requirements analysis through to integration, maintenance and evolution of legacy systems. The aim is to justify the claim that many problems in software engineering can be reformulated as search problems, to which metaheuristic techniques can be applied. The goal of the paper is to stimulate greater interest in metaheuristic search as a tool of optimisation of software engineering problems and to encourage the investigation and exploitation of these technologies in finding near optimal solutions to the complex constraint-based scenarios which arise so frequently in software engineering.",poster,cp22
p917,2e15acf6fcf62d034e52caffcc1b4b74c00c3d70,c31,International Conference on Evaluation & Assessment in Software Engineering,Formulation and preliminary test of an empirical theory of coordination in software engineering,"Motivated by evidence that coordination and dependencies among engineering decisions in a software project are key to better understanding and better methods of software creation, we set out to create empirically testable theory to characterize and make predictions about coordination of engineering decisions. We demonstrate that our theory is capable of expressing some of the main ideas about coordination in software engineering, such as Conway's law and the effects of information hiding in modular design. We then used software project data to create measures and test two hypotheses derived from our theory. Our results provide preliminary support for our formulations.",poster,cp31
p918,a7230b20e195a00de416c367a1e6a56a250bf43a,c13,International Conference on Data Science and Advanced Analytics,End-user software engineering with assertions in the spreadsheet paradigm,"There has been little research on end-user program development beyond the activity of programming. Devising ways to address additional activities related to end-user program development may be critical, however, because research shows that a large proportion of the programs written by end users contain faults. Toward this end, we have been working on ways to provide formal ""software engineering"" methodologies to end-user programmers. This paper describes an approach we have developed for supporting assertions in end-user software, focusing on the spreadsheet paradigm. We also report the results of a controlled experiment, with 59 end-user subjects, to investigate the usefulness of this approach. Our results show that the end users were able to use the assertions to reason about their spreadsheets, and that doing so was tied to both greater correctness and greater efficiency.",poster,cp13
p919,31889d6e3a251f25f10e391c3e96728e828fb8b1,c29,International Conference on Software Engineering,Writing good software engineering research papers: minitutorial,"Software engineering researchers solve problems of several different kinds. To do so, they produce several different kinds of results, and they should develop appropriate evidence to validate these results. They often report their research in conference papers. I analyzed the abstracts of research papers submitted to ICSE 2002 in order to identify the types of research reported in the submitted and accepted papers, and I observed the program committee discussions about which papers to accept. This report presents the research paradigms of the papers, common concerns of the program committee, and statistics on success rates. This information should help researchers design better research projects and write papers that present their results to best advantage.",fullPaper,cp29
p920,f05805ea64818288312ca915b923b3e94b24028d,c41,Software Product Lines Conference,Problems and Programmers: an educational software engineering card game,"Problems and Programmers is an educational card game that we have developed to help teach software engineering. It is based on the observation that students, in a typical software engineering course, gain little practical experience in issues regarding the software process. The underlying problem is time: any course faces the practical constraint of only being able to involve students in at most a few small software development projects. Problems and Programmers overcomes this limitation by providing a simulation of the software process. In playing the game, students become aware of not only general lessons, such as the fact that they must continuously make tradeoffs among multiple potential next steps, but also specific issues such as the fact that inspections improve the quality of code but delay its delivery time. We describe game play of Problems and Programmers, discuss its underlying design, and report on the results of a small experiment in which twenty-eight students played the game.",poster,cp41
p921,bc77f5407e5dcd818d7d51c5b059735c4939699e,c74,IEEE International Conference on Tools with Artificial Intelligence,Software engineering: a practitioner's approach (2nd ed.),Abstract content,poster,cp74
p922,925c964beff1cd42af961373c5dd672cc107c30b,c29,International Conference on Software Engineering,Software engineering education: a roadmap,"Software’s increasingly critical role in systems of widespread significance presents new challenges for the education of software engineers. Not only is our dependence on software increasing, but the character of software production is itself changing ‐ and with it the demands on the software developers. Four challenges for educators of software developers help identify aspirations for software engineering education.",fullPaper,cp29
p923,2621ca6891f5d369c340b470b11d35a0c2a5c6be,c57,IEEE International Conference on Engineering of Complex Computer Systems,A software engineering experiment in software component generation,"The paper presents results of a software engineering experiment in which a new technology for constructing program generators from domain-specific specification languages has been compared with a reuse technology that employs sets of reusable Ada program templates. Both technologies were applied to a common problem domain, constructing message translation and validation modules for military command, control, communications and information systems (C/sup 3/I). The experiment employed four subjects to conduct trials of use of the two technologies on a common set of test examples. The experiment was conducted with personnel supplied and supervised by an independent contractor. Test cases consisted of message specifications taken from Air Force C/sup 3/I systems. The main results are that greater productivity was achieved and fewer error were introduced when subjects used the program generator than when they used Ada templates to implement software modules from sets of specifications. The differences in the average performance of the subjects are statistically significant at confidence levels exceeding 99 percent.",poster,cp57
p924,8e95c6989b2521760bd5a4dd08c65c803f92c62b,c98,North American Chapter of the Association for Computational Linguistics,Software Engineering Processes: Principles and Applications,Fundamentals of the Software Engineering Process Introduction A Unified Framework of the Software Engineering Process Process Algebra Process-Based Software Engineering Software Engineering Process System Modeling The CMM Model The ISO 9001 Model The BOOTSTRAP Model The ISO/IEC 15504 (SPICE) Model The Software Engineering Process Reference Model: SEPRM Software Engineering Process System Analysis Benchmarking the SEPRM Processes Comparative Analysis of Current Process Models Transformation of Capability Levels Between Current Process Models Software Engineering Process Establishment Software Process Establishment Methodologies An Extension of ISO/IEC TR 15504 Model Software Engineering Process Assessment Software Process Assessment Methodologies Software Process Assessment Supporting Tools Software Engineering Process Improvement Software Process Improvement Methodologies Case Studies in Software Process Improvement Review And Perspectives Bibliography Appendices Index,poster,cp98
p925,a8b6d236039983df527c9e0f3cc5e713cf5dd709,j75,Lecture Notes in Computer Science,Search Based Software Engineering,Abstract content,fullPaper,jv75
p926,615909cc602358945c2fe6a48ef4037ac4a38798,c54,International Workshop on Agent-Oriented Software Engineering,Software engineering risk management,"Welcome to Software Engineering Risk Management (SERIM). As a professional associated with the development of software, you are well aware that the software development process can truly be a jungle, filled with hazards that lie in wait to sabotage your projects. These hazards (risks) are numerous and often complex. The purpose of this application is to help you find a safer path through this jungle by assessing risk factors, analyzing risks from several different perspectives, and developing focused action plans to manage risks before they sabotage your projects. I have used the mathematics of probability to design the formulas to help you assess and manage risks in the complex software development environment (Complete information on the SERIM ModelOs equations is included in this application.)",poster,cp54
p927,102a737e06904566914c52ee2f2cd182cb98542a,j168,Proceedings of the IEEE,Measurement and experimentation in software engineering,"The contributions of measurement and experimentation to the state of the art in software engineering are reviewed. The role of measurement in developing theoretical models is discussed, and concerns for reliability and validity are stressed. Current approaches to measuring software characteristics are presented as examples. In particular, software complexity metrics related to control flow, module interconnectedness, and Halstead's Software Science are discussed. The use of experimental methods in evaluating cause-effect relationships is also discussed. Example programs of experimental research which investigated conditional statements and control flow are reviewed. The conclusion argues that many advances in software engineering will be related to improvements in the measurement and experimental evaluation of software techniques and practices.",fullPaper,jv168
p928,97613fe1ff2bf6883cfe80196c576eee10ef7ada,c2,International Symposium on Intelligent Data Analysis,"Component-based software engineering: technologies, development frameworks, and quality assurance schemes","Component-based software development approach is based on the idea to develop software systems by selecting appropriate off-the-shelf components and then to assemble them with a well-defined software architecture. Because the new software development paradigm is very different from the traditional approach, quality assurance (QA) for component-based software development is a new topic in the software engineering community. In this paper, we survey current component-based software technologies, describe their advantages and disadvantages, and discuss the features they inherit. We also address QA issues for component-based software. As a major contribution, we propose a QA model for component-based software which covers component requirement analysis, component development, component certification, component customization, and system architecture design, integration, testing and maintenance.",poster,cp2
p929,56615aff0dda1a5ddb51344cba2734906edfc88c,c63,IEEE International Software Metrics Symposium,A method for assessing the software engineering capability of contractors,"This document provides guidelines and procedures for assessing the ability of potential DoD contractors to develop software in accordance with modem software engineering methods. It includes spl-:ific questions and a method for evaluating the results. ,I General Introduction The purpose of this document is to facilitate objective and consistent assessments of the ability of potential DoD contractors to dovelop software in accordance with modem software engineering methods. Such assessments would be conducted either In the pre-solicitation qualification process, in the formal source selection process, or both. While this doc~ument Is Intended to guide the assessment of a contractor's overall software engineering capability, it can also be valuable in the assessment ."", a specific project team's software engineering capability. Alternatively, this document can be used as an aid to software development organizations in conducting an internal as issment cf their own softvare engineering capability. The document is designed to help An assessment team define the highest priorldy steps for the Improvement of an organization's capability. Because an understanding of proper software engineering practice is only now developing, standard, well-accepted measures do not yet exist. The assessment questions listed in the body of this .)•cum,'nt are phrased so that an affirmative answer indicates that an organization has a desirable characteristic. Some of the questions pertain to advanced concepts of software engineering that may not yet be sufficiently refined or disseminated to be incorporated in a contractor's standard practice; therefore, not all assessment questions need be answered affirmatively for an organization to be considered to have a modern software engineering capability. The capability of a contractor to perform software engineering has been divided into three areas: 1. organization and resource management 2. software engine.)rlng process and its management 3. tools and technology. The qualities that the questions assess are different for each of these areas acnd are described in the introductions to the questions for each area. 093087 SEt Assessment Methodology A full assessment of software engineering capability' Includes some evaluation of the experience level of the software development personnel. Addendum A contains suggested questions for use In this evaluation.",poster,cp63
p930,edc534e2fb855269db5c6a202412171cd2c2a242,j147,IEEE Software,Toward a Discipline of Software Engineering,"Despite rapid changes in computing and software development, some fundamental ideas have remained constant. This article describes eight such concepts that together constitute a viable foundation for a software engineering discipline: abstraction, analysis and design methods and notations, user interface prototyping, modularity and architecture, software life cycle and process, reuse, metrics, and automated support.",fullPaper,jv147
p931,43615f4460ff880acfa78a84b14ecc79cd0c1de9,c34,IEEE Working Conference on Mining Software Repositories,Software Engineering: An Engineering Approach,"From the Publisher: 
A clear-cut, practical approach to software development! Emphasizing both the design and analysis of the technology, Peters and Pedrycz have written a comprehensive and complete text on a quantitative approach to software engineering. As you read the text, youll learn the software design practices that are standard practice in the industry today. Practical approaches to specifying, designing and testing software as well as the foundations of Software Engineering are also presented. Key Features 
 
*Thorough coverage is provided on the quantitative aspects of software Engineering including software measures, software quality, software costs and software reliability. 
*A complete case study allows students to trace the application of methods and practices in each chapter. 
*Examples found throughout the text are in C++ and Java. 
*A wide range of elementary and intermediate problems as well as more advanced research problems are available at the end of each chapter. 
*Students are given the opportunity to expand their horizons through frequent references to related web pages.",poster,cp34
p932,79f81a4b99e50841d3d114a99ae22532a3df7cbe,j142,IEEE Transactions on Software Engineering,Evaluating Software Engineering Technologies,"Many new software development practices, tools, and techniques have been introduced in recent years. Few, however, have been empirically evaluated. The objectives of this study were to measure technology use in a production environment, develop a statistical model for evaluating the effectiveness of technologies, and evaluate the effects of some specific technologies on productivity and reliability. A carefully matched sample of 22 projects from the Software Engineering Laboratory database was studied using an analysis-of-covariance procedure. Limited use of the technologies considered in the analysis produced approximately a 30 percent increase in software reliability. These technologies did not demonstrate any direct effect on development productivity.",fullPaper,jv142
p933,54a26878f0c5fe82582466e7c788cf6ddf100599,c29,International Conference on Software Engineering,Using a behavioral theory of program comprehension in software engineering,"A theory is presented of how a programmer goes about understanding a program. The theory is based on a representation of knowledge about programs as a succession of knowledge domains which bridge between the problem domain and the executing program. A hypothesis and verify process is used by programmers to reconstruct these domains when they seek to understand a program.
 The theory is useful in several ways in software engineering: It makes accurate predictions about the effectiveness of documentation; it can be used to systematically evaluate and critique other claims about documentation, and it may even be a useful guideline to a programmer in actually constructing documentation.",fullPaper,cp29
p934,6d52e9f70c3495df590df52392aa892905e11682,c89,Conference on Uncertainty in Artificial Intelligence,Agent-oriented software engineering,"The ATAL workshops focus on the links between the theory and practice of intelligent agents. One aspect of this, which is steadily growing in importance, is the idea of agent technology as a software engineering paradigm. Previous ATAL workshops have had special tracks on programming languages for agent-oriented development, and methodologies for agent system development. ATAL-99 aims to build on this experience by focussing on the wider issues of agents as a software engineering paradigm.",poster,cp89
p935,5bf3c1fa92728cd2f83885cc40ebea3b050f21da,c55,Annual Workshop of the Psychology of Programming Interest Group,"Software Engineering: Design, Reliability, and Management","Software engineering: design, reliability, and management , Software engineering: design, reliability, and management , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",poster,cp55
p936,193071cfbadc10719ee44ff4cb5f2982fb1ee467,c68,International Workshop on Software Engineering for Large-Scale Multi-Agent Systems,Advances in Software Engineering and Knowledge Engineering,"The papers collected in this book were invited by the editors as tutorial courses or keynote speeches for the Fourth International Conference on Software Engineering and Knowledge Engineering. The book offers wide coverage of the main topics involved with the specifications, prototyping, development and maintenance of software systems and knowledge-based systems. The main issues in the area of software engineering and knowledge engineering are addressed and for each analyzed topic the corresponding research state is reported.",poster,cp68
p937,377cc56f8258a44cb7ab38172e8161ba69766650,c29,International Conference on Software Engineering,The software engineering laboratory - an operational software experience factory,"For 15 years, the Software Engineering Laboratory (SEL) has been carrying out studies and experiments for the purpose of understand- ing, assessing, and improving software and software processes within a production software development environment at the National Aeronautics and Space Administration/Goddard Space Flight Center (NASA/GSFC). The SEL comprises three major organizations: NASA/GSFC, Flight Dynamics Division University of Maryland, Department of Computer Science Computer Sciences Corporation, Flight Dynamics Technology Group - These organizations have jointly carried out several hundred software studies, producing hundreds of reports, papers, and documents, all of which de scribe some aspect of the software engineering technology that has been analyzed in the flight dynamics environment at NASA. The studies range from small, controlled experiments (such as analyzing the effectiveness of code readingversus that of functional testing) tolarge, multiple- project studies (such as assessing the impacts of Ada on a production environment). The organization's driving goal is to improve the software process continually, so that sustained improvement may be observed in the resulting products. This paper discusses the SEL as a functioning example of an operational software experience factory and summarizes the characteristics of and major lessons learned from 15 years of SEL operations.",fullPaper,cp29
p938,dd096e01562e32aecb99a7cfa943af042866b562,c29,International Conference on Software Engineering,Software engineering tools and environments: a roadmap,"Tools and environments to aid developers in producing software have existed, in one form or another, since the early days of computer programming. They are becoming increasingly crucial as the demand for software increases, time-to-market decreases, and diversity and complexity grow beyond anything imagined a few decades ago. In this paper, we briefly review some of the history of tools and environments in software engineering, and then discuss some key challenges that we believe the field faces over the next decade.",fullPaper,cp29
p939,a9f5231542bcc6b9162c3d63ca1b1bbc72f1f386,c45,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",A Mature Profession of Software Engineering.,"Abstract : A model is presented that allows the characterization of the maturity of a profession in terms of eight infrastructure components: initial professional education, accreditation, skills development, certification, licensing, professional development, a code of ethics, and a professional society. Several mature professions are examined to provide examples of the nature of these components. The current states of the components of software engineering are described, and predictions are made for the evolution of those components as the profession matures.",poster,cp45
p940,614ad6dac9a02863c47b434999c8a09ed5ed49f9,c67,Enterprise Application Integration,Software engineering with ada,"Now, we come to offer you the right catalogues of book to open. software engineering with ada is one of the literary work in this world in suitable to be reading material. That's not only this book gives reference, but also it will show you the amazing benefits of reading a book. Developing your countless minds is needed; moreover you are kind of people with great curiosity. So, the book is very appropriate for you.",poster,cp67
p941,556655dd56ff0216552d345afd80786255e86379,c78,Neural Information Processing Systems,Cleanroom software engineering: technology and process,"Cleanroom software engineering is a process for developing and certifying high-reliability software. Combining theory-based engineering technologies in project management, incremental development, software specification and design, correctness verification, and statistical quality certification, the Cleanroom process answers today's call for more reliable software and provides methods for more cost-effective software development.Cleanroom originated with Harlan D. Mills, an IBM Fellow and a visionary in software engineering. Written by colleagues of Mills and some of the most experienced developers and practitioners of Cleanroom, Cleanroom Software Engineering provides a roadmap for software management, development, and testing as disciplined engineering practices. This book serves both as an introduction for those new to Cleanroom and as a reference guide for the growing practitioner community. Readers will discover a proven way to raise both quality and productivity in their software-intensive products, while reducing costs.Highlights Explains basic Cleanroom theory Introduces the sequence-based specification method Elaborates the full management, development, and certification process in a Cleanroom Reference Model (CRM) Shows how the Cleanroom process dovetails with the SEI's Capability Maturity Model for Software (CMM) Includes a large case study to illustrate how Cleanroom methods scale up to large projects.",poster,cp78
p942,36951d0cfb7aae0ef2ab77404e7d0dee75ac997e,c87,European Conference on Computer Vision,Software Pioneers: Contributions to Software Engineering,Abstract content,poster,cp87
p943,a71d66e4a6ef674ede43de952cbc90f26cb10cfd,j169,Annals of Software Engineering,Experimental design and analysis in software engineering,Abstract content,fullPaper,jv169
p944,a4006f42f842f299a66b6613bf145367b62dd771,c71,IEEE International Conference on Information Reuse and Integration,Cleanroom software engineering for zero-defect software,"Cleanroom software engineering is a theory-based, team-oriented process for developing very high quality software under statistical control. Cleanroom combines formal methods of object-based box structure specification and design, function-theoretic correctness verification, and statistical usage testing for quality certification to produce software that has zero defects with high probability. The process of cleanroom development and certification is carried out incrementally. Interface and design errors are rare because at each stage the harmonious operation of future increments at the next level of refinement is predefined by increments already in execution. The cleanroom process is being successfully applied in IBM and other applications. Quality results from several cleanroom projects are summarized.<<ETX>>",poster,cp71
p945,4356d5ff160881c48aab153fc0f6b99cb4420397,c66,Annual Conference on Innovation and Technology in Computer Science Education,Understanding the Philosophical Underpinnings of Software Engineering Research in Information Systems,Abstract content,poster,cp66
p946,8e22c6ffc70744031171237c6471587a4331d660,j79,Computer,Toward computer-supported concurrent software engineering,"An experimental software engineering environment called the flexible environment for collaborative software engineering (Flecse), which supports concurrent software engineering, is discussed. Flecse features tools designed to surmount collaboration problems that software engineers are increasingly encountering. The implementation of five important themes of concurrent software engineering in Flecse tools, concepts, life cycles, integration, and sharing, is examined.<<ETX>>",fullPaper,jv79
p947,a4e886711ae1f65a83c99c5ef43a55d997d37b5d,j141,Empirical Software Engineering,Hints for Reviewing Empirical Work in Software Engineering,Abstract content,fullPaper,jv141
p948,a8a9839b171c8ff0c27b4e53f7ca7abd5662d154,c43,ACM Symposium on Applied Computing,Software Engineering Project Management,"From the Publisher: 
The 2nd edition of Thayer's popular, bestselling book presents a top-down practical view of managing a successful software engineering project. The book builds on a framework for project managements activities based on the planning, organizing, staffing, directing, and controlling model. Thayer provides information designed to help readers understand and successfully perform the unique role of a project manager. 400 pp. Pub: 8/97.",poster,cp43
p949,ce71e163e181f22231db9bc7eb2c8a0f8ae6976e,j169,Annals of Software Engineering,"Process-Centered Software Engineering Environments, A Brief History and Future Challenges",Abstract content,fullPaper,jv169
p950,f5d39747f15bb4476d3de01420326f9884d11ff3,c110,IEEE International Conference on Automatic Face & Gesture Recognition,Software engineering code of ethics,"T he Board of Governors of the IEEE Computer Society established a steering committee in May 1993 for evaluating, planning, and coordinating actions related to establishing software engineering as a profession. In that same year the ACM Council endorsed the establishment of a Commission on Software Engineering. By January 1994, both societies formed a joint steering committee “to establish the appropriate set(s) of standards for professional practice of software engineering upon which industrial decisions, professional certification, and educational curricula can be based.” To accomplish these tasks they made the following recommendations: ACM and the IEEE Computer Society join forces to create a code of professional practices within our industry. Now, we ask for your comments.",poster,cp110
p951,19aee12e61ca1fe953b7e85433b4a6274f341678,c70,International Conference on Intelligent Robotics and Applications,Predicate Logic for Software Engineering,"The interpretations of logical expressions found in most introductory textbooks are not suitable for use in software engineering applications because they do not deal with partial functions. More advanced papers and texts deal with partial functions in a variety of complex ways. This paper proposes a very simple change to the classic interpretation of predicate expressions, one that defines their value for all values of all variables, yet is almost identical to the standard definitions. It then illustrates the application of this interpretation in software documentation. >",poster,cp70
p952,4e5f14131db1dada5eab8c5d4bf1317e611bb107,c89,Conference on Uncertainty in Artificial Intelligence,A Pattern Recognition Approach for Software Engineering Data Analysis,"In order to plan, control, and evaluate the software development process, one needs to collect and analyze data in a meaningful way. Classical techniques for such analysis are not always well suited to software engineering data. A pattern recognition approach for analyzing software engineering data, called optimized set reduction (OSR), that addresses many of the problems associated with the usual approaches is described. Methods are discussed for using the technique for prediction, risk management, and quality evaluation. Experimental results are provided to demonstrate the effectiveness of the technique for the particular application of software cost estimation. >",poster,cp89
p953,f8efcdd2055e0772e5b1ba2132104fec96c4ec20,c100,ACM SIGMOD Conference,Computational intelligence in software engineering,"The paper provides a unified view of computational intelligence in the context of software engineering. Technologies such as fuzzy sets, neural and evolutionary computing useful in software development are considered. The links between software engineering and computational intelligence are identified. An illustration is given in terms of a fuzzy software quality model.",poster,cp100
p954,ea747e976790e3e91eb1f1f293d48f09cc53e999,c29,International Conference on Software Engineering,The State Of Software Engineering Practice: A Preliminary Report,"This is the first in a series of SEI reports to provide periodic updates on the state of software engineering practice in the DoD software community. The SEI has developed, and is refining, a process framework and assessment methodology for characterizing the processes used by software organizations to develop and evolve software products. This report provides a brief overview of the process framework and assessment ap- proach, describes assessment results obtained to date, and dis- cusses implications of the current state of the practice for both customers and suppliers of DoD software.",fullPaper,cp29
p955,40f3a037c0c75c7e11f02f97c284dc58f0e9fb61,c32,International Conference on Software Technology: Methods and Tools,Perspectives in Software Engineering,"Software engineering refers to the process of creating software systems. It applies loosely to techniques which reduce high software cost and complexity while increasing reliability and mochfiability. This paper outlines the procedures used in the development of computer software, emphasizing large-scale software development, and pmpomtmg areas where problems exist and solutions have been proposed Solutions from both the management and the programmer points of vtew are then given for many of these problem areas.",poster,cp32
p956,8b589bc4a9f00221c67e002cc8550cf8715fec09,c27,ACM-SIAM Symposium on Discrete Algorithms,Outline of a Paradigm Change in Software Engineering,Abstract content,poster,cp27
p957,e9415d9a6e0065b46acd99ba4ff8b89bd1435fc8,c53,International Conference on Software Engineering and Knowledge Engineering,Machine Learning and Software Engineering,Abstract content,poster,cp53
p958,161928af38d0323a1821e8ac42c3494a15ff1e40,c99,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,Ethical Issues in Empirical Studies of Software Engineering,"The popularity of empirical methods in software engineering research is on the rise. Surveys, experiments, metrics, case studies, and field studies are examples of empirical methods used to investigate both software engineering processes and products. The increased application of empirical methods has also brought about an increase in discussions about adapting these methods to the peculiarities of software engineering. In contrast, the ethical issues raised by empirical methods have received little, if any, attention in the software engineering literature. This article is intended to introduce the ethical issues raised by empirical research to the software engineering research community and to stimulate discussion of how best to deal with these ethical issues. Through a review of the ethical codes of several fields that commonly employ humans and artifacts as research subjects, we have identified major ethical issues relevant to empirical studies of software engineering. These issues are illustrated with real empirical studies of software engineering.",poster,cp99
p959,cea798850b3331f0f37e0c763e3f22cc3d72ec9b,j147,IEEE Software,Software Engineering Programs Are Not Computer Science Programs,"Software Engineering programs have become a source of contention in many universities. Computer Science departments, many of which have used that phrase to describe individual courses for decades, claim SE as part of their discipline. Yet some engineering faculties claim it as a new specialty among the engineering disciplines. This article discusses the differences between traditional CS programs and most engineering programs, and argues that we need SE programs that follow the traditional engineering approach to professional education.",fullPaper,jv147
p960,b1bdae74be47faeeb04492c05ec0f4a75c25e430,j170,Knowledge engineering review (Print),Agent orientation in software engineering,"Agent-Oriented Software Engineering (AOSE) is rapidly emerging in response to urgent needs in both software engineering and agent-based computing. While these two disciplines coexisted without remarkable interaction until some years ago, today there is rich and fruitful interaction among them and various approaches are available that bring together techniques, concepts and ideas from both sides. This article offers a guide to the broad body of literature on AOSE. The guide, which is intended to be of value to both researchers and practitioners, is structured according to key issues and key topics that arise when dealing with AOSE: methods and frameworks for requirements engineering, analysis, design, and implementation; languages for programming, communication and coordination and ontology specification; and development tools and platforms.",fullPaper,jv170
p961,e520818326cee6fd9aca87d545c611f0781c94b9,c71,IEEE International Conference on Information Reuse and Integration,Simulation in software engineering training,"Simulation is frequently used for training in many application areas like aviation and economics, but not in software engineering. We present the SESAM project which focuses on software engineering education using simulation. In the SESAM project a simulator was developed. Using this simulator, a student can take the role of a software project manager. The simulated software project can be finished within a couple of hours because it is simulated in ""quick-motion"" mode. The background and goals of the SESAM project are presented. A new simulation model, the so called QA model, is introduced. The model behavior is demonstrated by investigating and comparing different strategies for software development. The results of experiments based on the QA model are reported. Finally, conclusions are drawn from the experiments and future work is outlined.",poster,cp71
p962,a22c336bbd84abdb02de3b19f7dc549a0ee2d95f,c6,Americas Conference on Information Systems,A survey of Agent-Oriented Software Engineering,"Agent-Oriented Software Engineering is the one of the most recent contributions to the field of Software Engineering. It has several benefits compared to existing development approaches, in particular the ability to let agents represent high-level abstractions of active entities in a software system. This paper gives an overview of recent research and industrial applications of both general high-level methodologies and on more specific design methodologies for industry-strength software engineering.",poster,cp6
p963,0dc01dd9dcb0f2be2332d4f6a7b492847a3e653b,j171,Software Engineering Journal,"Software engineering, the software process and their support","Computers are being applied more and more widely, penetrating ever deeper into the very fabric of society. Mankind is becoming increasingly dependent on the availability of software and its continuing validity. To achieve this consistently and reliably, in an operational domain that is forever changing, requires disciplined execution of the software development and evolution process and its effective management. That is the goal of advanced software engineering [1]. This paper summarises basic concepts of software engineering and of the software development process. This leads to a principle of uncertainty, analysis of its implications for the software development process, an overview of computer-assisted software engineering (CASE) and brief comments on the societal relevance of these topics. For researchers in the field and practitioners familiar with individual concepts, issues and specific solutions, the paper provides a unifying framework, a basis for conceptual advance. Those without a significant practical software engineering background and experienced graduate students will extend general familiarity with fresh insights, new concepts and additional detail. Undergraduate and graduate students without significant experience may treat the paper as an introductory text.",fullPaper,jv171
p964,3929570c1348ec7f9cb0f3fc4b0e2e4c2529e375,c68,International Workshop on Software Engineering for Large-Scale Multi-Agent Systems,Taming Agents and Objects in Software Engineering,Abstract content,fullPaper,cp68
p965,91b866c4a949bb5de3fb75475d629566cc224204,c9,Pacific Symposium on Biocomputing,Assessing process-centered software engineering environments,"Process-centered software engineering environments (PSEEs) are the most recent generation of environments supporting software development activities. They exploit an representation of the process (called the process model that specifies how to carry out software development activities, the roles and tasks of software developers, and how to use and control software development tools. A process model is therefore a vehicle to better understand and communicate the process. If it is expressed in a formal notation, it can be used to support a variety of activities such as process analysis, process simulation, and process enactment. PSEEs provide automatic support for these activities. They exploit languages based on different paradigms, such as Petri nets and rule-based systems. They include facilities to edit and analyze process models. By enacting the process model, a PSEE provides a variety of services, such as assistance for software developers, automation of routine tasks, invocation and control of software development tools, and enforcement of mandatory rules and practices. Several PSEEs have been developed, both as research projects and as commercial products. The initial deployment and exploitation of this technology have made it possible to produce a significant amount of experiences, comments, evaluations, and feedback. We still lack, however, consistent and comprehensive assessment methods that can be used to collect and organize this information. This article aims at contributing to the definition of such methods, by providing a systematic comparison grid and by accomplishing an initial evaluation of the state of the art in the field. This evaluation takes into account the systems that have been developed by the authors in the past five years, as well as the main characteristics of other well-known environments",poster,cp9
p966,4bee88ed87fd6298e7bb709e62e820e62a7ab1b6,c85,International Conference on Graph Transformation,Software Engineering Process Group Guide,"Abstract : Improving the process of software systems development and maintenance is the most reliable way to improve product quality. This document offers guidance on how to establish a software engineering process group (SEPG) and related software engineering process improvement functions. The process group works with line organizations to improve process quality by helping to assess current status, plan and implement improvements, and transfer technology to facilitate improvement in practice.",poster,cp85
p967,eb545656fb93b04d9c1acae8a21207ea1fa95f5f,c100,ACM SIGMOD Conference,A survey of the relevance of computer science and software engineering education,"We describe a study of 168 software professionals to determine how relevant their education has been to their careers. Starting with a list of 57 topics, we asked the participants to indicate how much they learned in university, how much they know now, how useful the material has been and whether they would like to learn more. We conclude from the results that certain software engineering topics should be given more emphasis, while the emphasis on certain mathematics topics should be changed.",poster,cp100
p968,ae7e51e8255946b0801d060ec456c5c4d6813132,c26,PS,Component Metadata for Software Engineering Tasks,Abstract content,poster,cp26
p969,3a6eff8c5e72c0ac2fa6b6d72214aebbec0ed996,c4,Annual Conference on Genetic and Evolutionary Computation,Reuse and Productivity in Integrated Computer-Aided Software Engineering: An Empirical Study,"Growing competition in the investment banking industry has given rise to increasing demand for high functionality software applications that can be developed in a short period of time. Yet delivering such applications creates a bottleneck in software development activities. This dilemma can be addressed when firms shift to development methods that emphasize software reusability. This article examines the productivity implications of object and repository-based integrated computer-aided software engineering (ICASE) software development in the context of a major investment bank's information systems strategy. The strategy emphasizes software reusability. Our empirical results, based on data from 20 projects that delivered software for the bank's New Trades Processing Architecture (NTPA), indicate an order of magnitude gain in software development productivity and the importance of reuse as a driver in realizing this result. In addition, results are presented on the extent of the learning that occurred over a two-year period after ICASE was introduced, and on the influence of the link between application characteristics and the ICASE tool set in achieving development performance. This work demonstrates the viability of the firm's IS strategy and offers new ideas for code reuse and software development productivity measurement that can be applied in development environments that emphasize reuse.",poster,cp4
p970,c39fe2bade1123e7d6742e572e7261c84c12e601,c16,Knowledge Discovery and Data Mining,Representing Software Engineering Models: The TAME Goal Oriented Approach,"A methodology and a knowledge representation and reasoning framework for top-down goal-oriented characterization, modeling, and execution of software engineering activities is presented. A prototype system (ES-TAME) which demonstrates the underlying knowledge representation and reasoning principles is described. ES-TAME provides an object-oriented metamodel concept that provides support for tailorable and reusable software engineering models (SEMs). It provides the basic mechanisms, functions, and attributes for all the other models. It is based on interobject relationships, dynamic viewpoints, and selective inheritance in addition to traditional object-oriented mechanisms. Descriptive SEMs include representations for basic software engineering activities. They are controlled and made operational by active GQM (goal-question-metric paradigm) models which are built by a systematic mechanism for defining and evaluating project and corporate goals and using measurement to provide feedback in real-time. >",poster,cp16
p971,51c5b153032d9d4fe190dedcebc9546c7ffc46bd,c19,ACM Conference on Economics and Computation,Classical and Object Oriented Software Engineering,"From the Publisher: 
Classical and Object-Oriented Software Engineering is designed for an introductory software engineering course. This book provides an excellent introduction to software engineering fundamentals,covering both traditional and object-oriented techniques. 
Schach's unique organization and style makes it excellent for use in a classroom setting. It presents the underlying software engineering theory in Part I and follows it up with the more practical life-cycle material in Part II. Many software engineering books are more like reference books,which do not provide the appropriate fundamentals before inundating students with implementation details. 
In this edition,more practical material has been added to help students understand how to use what they are learning. This has been done through the use of ""How To"" boxes and greater implementation detail in the case study. Additionally,the new edition contains the references to the most current literature and includes an overview of extreme programmming. 
The website in this edition will be more extensive. It will include Solutions,PowerPoints that incorporate lecture notes,newly developed self-quiz questions,and source code for the term project and case study.",poster,cp19
p972,c8c10e47cc3ae9e6a67e2f30eb99696a5ea02cf5,j1,IEEE Transactions on Knowledge and Data Engineering,A Knowledge-Based Environment for Modeling and Simulating Software Engineering Processes,"The design and representation schemes used in constructing a prototype computational environment for modeling and simulating multiagent software engineering processes are described. This environment is called the articulator. An overview of the articulator's architecture identifying five principal components is provided. Three of the components, the knowledge metamodel, the software process behavior simulator, and a knowledge base querying mechanism, are detailed and examples are included. The conclusion reiterates what is unique to this approach in applying knowledge engineering techniques to the problems of understanding the statics and dynamics of complex software engineering processes. >",fullPaper,jv1
p973,ae1c0a28c1d6b18a7b6fb8aafa77c0dcb3874a0d,c59,British Computer Society Conference on Human-Computer Interaction,"Reuse-based software engineering: techniques, organization, and controls",Reuse-Based Software Engineering offers an in-depth discussion of the fundamental issues and total coverage of the state-of-the-art. The inclusion of review questions and exercises makes it an excellent tutorial for both academics and professionals.,poster,cp59
p974,be416b6798c1a3216105fb319fb160704fd6a770,c71,IEEE International Conference on Information Reuse and Integration,Towards a software engineering approach to Web site development,"The World Wide Web (WWW) has become ""the"" global infrastructure for delivering information and services. The demands and expectations of information providers and consumers are pushing WWW technology towards higher-level quality of presentation, including active contents and improved usability of the hypermedia distributed infrastructure. This technological evolution, however, is not supported by adequate Web design methodologies. Web site development is usually carried out without following a well-defined process and lacks suitable tool support. In addition, Web technologies are quite powerful but rather low-level and their semantics is often left largely unspecified. As a consequence, understanding the conceptual structure of a complex Web site and managing its evolution are complex and difficult tasks. The approach we advocate here is based on sound software engineering principles. The Web site development process goes through requirements analysis, design, and implementation in a high-level language. We define an object-oriented modeling framework, called WOOM, which provides constructs and abstractions for a high-level implementation of a Web site. An important feature of WOOM is that it clearly separates the data that are presented through the site from the context in which the user accesses such data. This feature not only enhances separation of concerns in the design stage, but also favors its subsequent evolution. The paper provides a view of the approach and of its current prototype implementation.",poster,cp71
p975,1f534ffbb7645df74cebd534554afa5c9cf8f77b,j147,IEEE Software,Wisdom: A Software Engineering Method for Small Software Development Companies,"Wisdom is a new software engineering method addressing the specific needs of small teams that develop and maintain interactive systems. Because Wisdom defines a process, notation, and project philosophy, it can smoothly be applied in small companies leveraging on their communication, speed, and flexibility.",fullPaper,jv147
p976,6acda628eedf8317d0be2f4b4e6025c29ca05233,c15,International Conference on Conceptual Structures,Has twenty-five years of empirical software engineering made a difference?,"Our activities in software engineering typically fall into one of three categories, (1) to invent new phenomena, (2) to understand existing phenomena, and (3) to facilitate inspirational education. This paper explores the place of empirical software engineering in the first two of these activities. In this exploration evidence is drawn from the empirical literature in the areas of software inspections and software cost modelling and estimation. This research is then compared with the literature published in the Journal of Empirical Software Engineering. This evidence throws light on aspects of theory derivation, experimental methods and analysis, and also the challenges that we face as empirical software engineering evolves into the future.",poster,cp15
p977,6c658528361dce6ed8a3b2f7fc2411ca86c78cec,c11,Hawaii International Conference on System Sciences,A flexible transaction model for software engineering,"It is generally recognized that the classical transaction model, providing atomicity and serializability, is too strong for certain application areas since it unnecessarily restricts concurrency. The author is concerned with supporting cooperative work in multiuser design environments, particularly teams of programmers cooperating to develop and maintain software systems. An extended transaction model that meets the special requirements of software engineering projects is presented, possible implementation techniques are described, and a number of issues regarding the incorporation of such a model into multiuser software development environments are discussed.<<ETX>>",poster,cp11
p978,5ff6034f558caafbe7a7aef80a42cd0fadd051e2,j147,IEEE Software,An Experience in Collaborative Software Engineering Education,"Large-scale software development requires the interaction of specialists from different fields who must communicate their decisions and coordinate their activities. As global software development becomes mainstream, software engineers face new challenges for which they have received little or no training. To help a new generation of software developers better understand the industry's globalization and familiarize them with distributed, collaborative development, we designed a course entitled the Distributed Software Engineering Laboratory. In the class, pairs of students from different countries work as a virtual organization overseeing the whole software development process. We describe the lessons we have learned in this course and propose a framework useful in dealing with some of the difficulties participants face.",fullPaper,jv147
p979,2f470c6516cf4210f906410c14611dc03e51fe3c,c74,IEEE International Conference on Tools with Artificial Intelligence,Computer-Aided Software Engineering in a distributed workstation environment,"Computer-Aided Software Engineering environments are becoming essential for complex software projects, just as CAD systems have become essential for complex hardware projects. DSEE, the DOMAIN Software Engineering Environment, is a distributed, production quality, software development environment that runs on Apollo workstations. DSEE provides source code control, configuration management, release control, advice management, task management, and user-defined dependency tracking with automatic notification.
 DSEE incorporates some of the best ideas from existing systems. This paper describes DSEE, contrasts it other systems, and discusses some of the technical issues involved in the construction of a highly-reliable, safe, efficient, and distributed development environment.",poster,cp74
p980,9599aa996302c3ece477edc4122f0dee5e78f0ca,c36,Conference on Software Engineering Education and Training,Research synthesis in software engineering: a case for meta-analysis,"The use of meta-analytic techniques to summarize empirical software engineering research results is illustrated using a set of 5 published experiments from the literature. The intent of the analysis is to guide future work in this area through objective summarization of the literature to date. A focus on effect magnitude, in addition to statistical significance is championed, and the reader is provided with an illustration of simple methods for computing effect magnitudes.",poster,cp36
p981,8427d8c6425fe00b7c66f383dabe0e69745bd1f1,j75,Lecture Notes in Computer Science,Experimental Software Engineering Issues: Critical Assessment and Future Directions,Abstract content,fullPaper,jv75
p982,3a33a419b902517a8ec4e8d8b1e32a0772742672,c59,British Computer Society Conference on Human-Computer Interaction,A critique of diffusion theory as a managerial framework for understanding adoption of software engineering innovations,"The authors provide a brief overview of classical diffusion theory and suggest the potential applicability of this theory to problems related to predicting the adoption of technological innovations, including those related to software engineering. They critically evaluate the theory, identifying elements that must be extended and modified before it can be applied to technology transition, in general, and software engineering, specifically. They offer suggestions on ways in which these limitations might be overcome.<<ETX>>",poster,cp59
p983,9941b0548c85c86e063c3b17d80077574ecfb726,c95,IEEE International Conference on Computer Vision,Object-oriented software engineering - practical software development using UML and Java,1. Software and Software Engineering. 2. Review of Object Orientation and Java. 3. Basing Software Development on Reusable Technology. 4. Developing Requirements. 5. Modelling with Classes. 6. Using Design Patterns. 7. Focusing on Users and Their Tasks. 8. Modelling Interactions and Behaviour. 9. Architecting and Designing Software. 10. Testing and Inspecting to Ensure High Quality. 11. Managing the Software Process. 12. Review. Appendix A: Summary of UML Notation used in this Book. Appendix B: Summary of the Documentation Formats Recommended in this Book. Appendix C: System Descriptions. Appendix D: Answers to Selected Exercises. Glossary. Index.,poster,cp95
p984,c68fbf44dd0cf27d6620584e89783046c7863c71,c104,IEEE International Conference on Multimedia and Expo,Distributed component technologies and their software engineering implications,"In this state-of-the-art report, we review advances in distributed component technologies, such as the Enterprise JavaBeans (EJB) specification and the CORBA component model (CCM). We assess the state of industrial practice in the use of distributed components. We show several architectural styles for whose implementation distributed components have been used successfully. We review the use of iterative and incremental development processes and the notion of a model-driven architecture. We then assess the state of the art in research into novel software engineering methods and tools for the modelling, reasoning and deployment of distributed components. The open problems identified during this review result in the formulation of a research agenda that will contribute to the systematic engineering of distributed systems based on component technologies.",poster,cp104
p985,4f46715772306b0845e7db9140eb4137b95d7a84,c99,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,HANDBOOK OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING,"The field of multimedia software engineering is still in an inmature state. Significant research and development has been dedicated towards multimedia services and systems technology such as networking or database systems. Multimedia document formats have been standardized. But when it comes to multimedia application development, the development process is truncated to an implement-and-test method. Either specialized multimedia authoring systems or multime-dia frameworks or toolkits complementing programming languages or system software are directly used for implementation. No preceding modeling phases for requirements specification, analysis, or design of the system to build are enforced. The development of sophisticated multimedia process models and established , usable graphical notations tailored to the specification of multimedia systems is still underway. In order to fill this gap, it is the purpose of this chapter to show current achievements in object-oriented modeling of multimedia applications. Based on an analysis of the state of the art in multimedia application development, we shortly present approaches to object-oriented hypermedia modeling and extensions of the Unified Modeling Language (UML) for hypermedia and interactive systems. The main part of the chapter is dedicated towards a recent approach to the Object-oriented Modeling of MultiMedia Applications (OMMMA).",poster,cp99
p986,5e6fd96bb7b4e0554e758a454fe1df3a41304c27,c59,British Computer Society Conference on Human-Computer Interaction,Extreme programming for software engineering education?,"The eXtreme Programming (XP) software development methodology, has received considerable attention in recent years. The adherents of XP anecdotally extol its benefits, particularly as a method that is highly responsive to changing customer's desires. While XP has acquired numerous vocal advocates, the interactions and dependencies between XP practices have not been adequately studied. Good software engineering practice requires expertise in a complex set of activities that involve the intellectual skills of planning, designing, evaluating, and revising. The authors explore the practices of XP in the context of software engineering education. To do so, one must examine the practices of XP as they influence the acquisition of software engineering skills. The practices of XP, in combination or isolation, may provide critical features to aid or hinder the development of increasingly capable practitioners. This paper evaluates the practices of XP in the context of acquiring these necessary software engineering skills.",poster,cp59
p987,652d75d2571c7945cc378933f56797780b5b87c9,c22,International Conference on Data Technologies and Applications,Modeling Articulation Work in Software Engineering Processes,"Current software process modeling techniques do not generally support articulation work. Articulation work is the diagnosis, recovery and resumption of development activities that unexpectedly fail. It is an integral part of software process enactment since software processes can sometimes fail or breakdown. This paper presents a knowledge-based model of articulation work in software engineering processes. I t uses empirically-grounded heuristics t o address three problems in articulation work: diagnosing failed development activities, determining appropriate recovery, and resuming software processes. We first investigate the role and importance of articulation work with respect t o planned software development activities. We then outline a knowledge-based model of articulation work. The model has been implemented in a knowledgebased software process modeling environment called the Articulator. Combining the available software process modeling techniques and the model of articulation leads to a better foundation in process improvement and evolution.",poster,cp22
p988,29131c413a3a49368b429865b0966b5daa257ae5,c70,International Conference on Intelligent Robotics and Applications,Agent-Oriented Software Engineering for Internet Applications,Abstract content,poster,cp70
p989,ee225bb6b97ece25146f9cdfe0279370d79a72c9,c4,Annual Conference on Genetic and Evolutionary Computation,CASE productivity perceptions of software engineering professionals,Computer-aided software engineering (CASE) is moving into the problem-solving domain of the systems analyst. The authors undertook a study to investigate the various functional and behavioral aspects of CASE and determine the impact it has over manual methods of software engineering productivity.,poster,cp4
p990,a199cf452adcbf964fd65443d6dd0af4d6f1532c,c104,IEEE International Conference on Multimedia and Expo,Thinking objectively: software engineering in the small,"In 1968, the NATO Software Engineering Conference in Garmisch, Germany [6] initiated the concept of software engineering, identifying the problems with producing large, high-quality software applications. In 1975, De Remer [2] introduced the terms, “programming in the small” and “programming in the large” to differentiate the development characteristics of large-scale software development from detailed programming (for example, data structures and algorithms). The principal source of large-scale software at the time was development contracts issued by the U.S. Department of Defense. Since then, virtually all software engineering literature has concentrated explicitly and implicitly on the model of DoD contract software development. Since the late 1970s, the microcomputer revolution has dramatically increased the quantity of software produced, the average size of programs, and the number of companies involved in software development. Much more software is produced for internal use, commercial applications, and the mass-market than for deep-pocketed government and large industry. Using the number of units sold, mass-market software dwarfs the other forms of software sales. The growth of the software industry has produced many small companies that do not do contract software, but rather compete in other areas. This gives rise to at least four significant development issues that have not been adequately addressed in software engineering literature: company size, development mode, development size, and development speed. We discuss these issues and then discuss some of the shortcomings of current software engineering thinking for small companies.",poster,cp104
p991,9ba73feb6685945e88bebb19468b0177d4cae1a8,c17,International Conference on Enterprise Information Systems,in Software Engineering,"Software engineers work on multidisciplinary teams to identify and develop software solutions and to maintain software intensive systems of all sizes. The focus of this program is on the rigorous engineering practices necessary to build, maintain, and protect modern software intensive systems. Consistent with this focus, the software engineering baccalaureate program consists of a rigorous curriculum of science, math, computer science, and software engineering courses.",poster,cp17
p992,ad7227d76df78c0807e4d14d0f7b18040ea091c2,c29,International Conference on Software Engineering,Database system support for software engineering,"The activity of Computer-Aided Software Engineering (CASE) generates much data. Although there are few, if any, database system products with features that attend to the requirements of CASE environments, there is a flurry of research work in the database field to develop technology to meet these requirements. The purpose of this paper is to raise the level of awareness in the software engineering community about this research on database systems for design applications. It describes technical problems and proposed solutions, and provides an extensive bibliography.",fullPaper,cp29
p993,d1bbd3fdc052c330e7d201b9cd3f65e424373379,c0,International Conference on Human Factors in Computing Systems,Knowledge-based software engineering,"Knowledge-based software engineering emphasizes the fact that creating software is a knowledge-intensive activity, and proposes that making more knowledge available will facilitate the timely production of high-quality software. The author gives four reasons for software engineering being an interesting area for AI research. He also stipulates that KBSE researchers must answer several crucial questions: what part of the software process is targeted; what knowledge is applicable and how can it be represented, acquired and maintained; and how can one present the knowledge to developers to improve the quality and cost of software development?.<<ETX>>",poster,cp0
p994,fe492c64031d000119298b656c61c672cfac1dd9,c28,International Conference on Collaboration Technologies and Systems,Design rationale for software engineering: a survey,"The authors provide an introduction to design rationale and why it is important in software engineering. They look at the recent history of argumentation methods. They survey a number of the major systems developed for the support of design rationale, comparing their features and discussing their differences. They look at advantages and disadvantages of the various approaches to design rationale with special attention paid to how they can be used in the process software engineering. They conclude with a discussion of some open issues which are important for the inclusion of design rationale systems in the software engineering process.<<ETX>>",poster,cp28
p995,2f92f51d26a6fcea9f3bc237990bc35e55f0995e,c29,International Conference on Software Engineering,Software engineering: a roadmap,"This paper provides a roadmap for software engineering. It 
identifies the principal research challenges being faced by 
the discipline and brings together the threads derived from 
the key research specialisations within software 
engineering. The paper draws heavily on the roadmaps 
covering specific areas of software engineering research 
collected in this volume.",fullPaper,cp29
p996,e41e18f1ae2dbf5794688bf4971cf8d1c580ad9a,c30,IEEE Aerospace Conference,Ginger2: An Environment for Computer-Aided Empirical Software Engineering,"Empirical software engineering can be viewed as a series of actions to obtain knowledge and a better understanding about some aspects of software development, given a set of problem statements in the form of issues, questions or hypotheses. Experience has made us aware of the criticality of integrating the various types of data that are collected and analyzed as well as the criticality of integrating the various types of activities that take place, such as experiment design and the experiment itself. This has led us to develop a Computer-Aided Empirical Software Engineering (CAESE) framework to support the empirical software engineering lifecycle. The paper first presents the CAESE framework that consists of three elements: (1) a process model for the ""lifecycle"" of empirical software engineering studies, including needs analysis, experiment design, actual experimentation, and analyzing and packaging results; (2) a model that helps empirical software engineers decide how to look at the ""world"" to be studied in a coherent manner; (3) an architecture, based on which CAESE environments can be built, consisting of tool sets for each phase of the process model, a process management mechanism, and the two types of integration mechanism that are vital for handling multiple types of data: data integration and control integration. Next, the paper describes the Ginger2 environment as an instantiation of our framework. It concludes with reports on case studies using Ginger2, which dealt with a variety of empirical data types including mouse and keystrokes, eye traces, 3D movement, skin resistance level, and videotaped data.",poster,cp30
p997,2588db39ca2c65c389de64ed18d1c5da4ee1d34c,c2,International Symposium on Intelligent Data Analysis,Iterative Software Engineering for Multiagent Systems: The MASSIVE Method,"Agents, Multiagent Systems and Software Engineering.- Basic Concepts in Software Engineering.- The Conceptual Framework of Massive.- Massive Views.- Further Case Studies.- Conclusion.- Toolkits for Agent-Based Applications.- Basic Problem Solving Capabilities of TCS Agents.- Protoz Specification of the Contract-Net Protocol.",poster,cp2
p998,116090263e6a3b90bcb7d1aa8922bc2407cc3924,c74,IEEE International Conference on Tools with Artificial Intelligence,DAMOKLES - A Database System for Software ENgineering Environments,Abstract content,poster,cp74
p999,58c5072b2602428a178d034105362235c06cf86a,c81,IEEE Annual Symposium on Foundations of Computer Science,Software Engineering: An Object-Oriented Perspective,"From the Publisher: 
This book has been written to communicate the complexity of software engineering, a field that is on the rise. Braude has combined practical industrial experience with up-to-date academic experience to give the reader a feel for the complexity and important issues of real-world development. A longitudinal case study using IEEE standards is implemented throughout the book, along with many other examples, which enables the reader to understand the implications of quality factors, proper requirements documents, appropriate design, and appropriate project management techniques.",poster,cp81
p1000,57e7a7323f58a35f5e2cc33bf17d4ac9cdcafdd4,j172,Nature Protocols,Systematic and integrative analysis of large gene lists using DAVID bioinformatics resources,Abstract content,fullPaper,jv172
p1001,fa60b6806050255a77699bd0f9f5d824884c5162,j102,Nucleic Acids Research,Bioinformatics enrichment tools: paths toward the comprehensive functional analysis of large gene lists,"Functional analysis of large gene lists, derived in most cases from emerging high-throughput genomic, proteomic and bioinformatics scanning approaches, is still a challenging and daunting task. The gene-annotation enrichment analysis is a promising high-throughput strategy that increases the likelihood for investigators to identify biological processes most pertinent to their study. Approximately 68 bioinformatics enrichment tools that are currently available in the community are collected in this survey. Tools are uniquely categorized into three major classes, according to their underlying enrichment algorithms. The comprehensive collections, unique tool classifications and associated questions/issues will provide a more comprehensive and up-to-date view regarding the advantages, pitfalls and recent trends in a simpler tool-class level rather than by a tool-by-tool approach. Thus, the survey will help tool designers/developers and experienced end users understand the underlying algorithms and pertinent details of particular tool categories/tools, enabling them to make the best choices for their particular research interests.",fullPaper,jv102
p1002,fd495d6cf7c3169bc58550fdf32be6e16e2800f8,j5,Genome Biology,Bioconductor: open software development for computational biology and bioinformatics,Abstract content,fullPaper,jv5
p1003,90485e0ce54c1ad12a2d01362a007ab107d71063,c89,Conference on Uncertainty in Artificial Intelligence,Biopython: freely available Python tools for computational molecular biology and bioinformatics,"Summary: The Biopython project is a mature open source international collaboration of volunteer developers, providing Python libraries for a wide range of bioinformatics problems. Biopython includes modules for reading and writing different sequence file formats and multiple sequence alignments, dealing with 3D macro molecular structures, interacting with common tools such as BLAST, ClustalW and EMBOSS, accessing key online databases, as well as providing numerical methods for statistical learning. Availability: Biopython is freely available, with documentation and source code at www.biopython.org under the Biopython license. Contact: All queries should be directed to the Biopython mailing lists, see www.biopython.org/wiki/_Mailing_listspeter.cock@scri.ac.uk.",poster,cp89
p1004,8002fefd7bcc66c23a8d49aa9a7ae8d4a9885ad3,c105,Biometrics and Identity Management,"Expasy, the Swiss Bioinformatics Resource Portal, as designed by its users","Abstract The SIB Swiss Institute of Bioinformatics (https://www.sib.swiss) creates, maintains and disseminates a portfolio of reliable and state-of-the-art bioinformatics services and resources for the storage, analysis and interpretation of biological data. Through Expasy (https://www.expasy.org), the Swiss Bioinformatics Resource Portal, the scientific community worldwide, freely accesses more than 160 SIB resources supporting a wide range of life science and biomedical research areas. In 2020, Expasy was redesigned through a user-centric approach, known as User-Centred Design (UCD), whose aim is to create user interfaces that are easy-to-use, efficient and targeting the intended community. This approach, widely used in other fields such as marketing, e-commerce, and design of mobile applications, is still scarcely explored in bioinformatics. In total, around 50 people were actively involved, including internal stakeholders and end-users. In addition to an optimised interface that meets users' needs and expectations, the new version of Expasy provides an up-to-date and accurate description of high-quality resources based on a standardised ontology, allowing to connect functionally-related resources.",poster,cp105
p1005,a41d8c4eddf4054ef080c7edec21b39c492892ee,j0,Nature Biotechnology,"Nanopore sequencing technology, bioinformatics and applications",Abstract content,fullPaper,jv0
p1006,34d405eaecab40a932108a7ff97e92fb8fd1ae4e,c112,Very Large Data Bases Conference,A review of feature selection techniques in bioinformatics,"Feature selection techniques have become an apparent need in many bioinformatics applications. In addition to the large pool of techniques that have already been developed in the machine learning and data mining fields, specific applications in bioinformatics have led to a wealth of newly proposed techniques. In this article, we make the interested reader aware of the possibilities of feature selection, providing a basic taxonomy of feature selection techniques, and discussing their use, variety and potential in a number of both common as well as upcoming bioinformatics applications.",poster,cp112
p1007,1ff4bd599b950218f0517fb76ee49ad0599e1c53,j173,Journal of Molecular Biology,A Completely Reimplemented MPI Bioinformatics Toolkit with a New HHpred Server at its Core.,Abstract content,fullPaper,jv173
p1008,7d7735582cfa14efb00d967e9af4d725579e8746,c78,Neural Information Processing Systems,The PATRIC Bioinformatics Resource Center: expanding data and analysis capabilities,"The PathoSystems Resource Integration Center (PATRIC) is the bacterial Bioinformatics Resource Center funded by the National Institute of Allergy and Infectious Diseases (https://www.patricbrc.org). PATRIC supports bioinformatic analyses of all bacteria with a special emphasis on pathogens, offering a rich comparative analysis environment that provides users with access to over 250 000 uniformly annotated and publicly available genomes with curated metadata. PATRIC offers web-based visualization and comparative analysis tools, a private workspace in which users can analyze their own data in the context of the public collections, services that streamline complex bioinformatic workflows and command-line tools for bulk data analysis. Over the past several years, as genomic and other omics-related experiments have become more cost-effective and widespread, we have observed considerable growth in the usage of and demand for easy-to-use, publicly available bioinformatic tools and services. Here we report the recent updates to the PATRIC resource, including new web-based comparative analysis tools, eight new services and the release of a command-line interface to access, query and analyze data.",poster,cp78
p1009,d6425d11904920cf6fafe895e6073e2131978e60,c69,International Conference on Parallel Processing,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Abstract content,fullPaper,cp69
p1010,17190dc2a51b4f0edd05753a4cea2a2540059933,j0,Nature Biotechnology,The nf-core framework for community-curated bioinformatics pipelines,Abstract content,fullPaper,jv0
p1011,766d3c1f67728737a255bd88e272f2bacb92d6e9,c51,Conference of the Centre for Advanced Studies on Collaborative Research,Protein Sequence Analysis Using the MPI Bioinformatics Toolkit,"The MPI Bioinformatics Toolkit (https://toolkit.tuebingen.mpg.de) provides interactive access to a wide range of the best‐performing bioinformatics tools and databases, including the state‐of‐the‐art protein sequence comparison methods HHblits and HHpred. The Toolkit currently includes 35 external and in‐house tools, covering functionalities such as sequence similarity searching, prediction of sequence features, and sequence classification. Due to this breadth of functionality, the tight interconnection of its constituent tools, and its ease of use, the Toolkit has become an important resource for biomedical research and for teaching protein sequence analysis to students in the life sciences. In this article, we provide detailed information on utilizing the three most widely accessed tools within the Toolkit: HHpred for the detection of homologs, HHpred in conjunction with MODELLER for structure prediction and homology modeling, and CLANS for the visualization of relationships in large sequence datasets. © 2020 The Authors.",poster,cp51
p1012,f22e039275da9512fd59109165b99abf7c6910f9,c66,Annual Conference on Innovation and Technology in Computer Science Education,Snakemake - a scalable bioinformatics workflow engine,Summary: Snakemake is a workflow engine that provides a readable Python-based workflow definition language and a powerful execution environment that scales from single-core workstations to compute clusters without modifying the workflow. It is the first system to support the use of automatically inferred multiple named wildcards (or variables) in input and output filenames.,poster,cp66
p1013,bc54798db4962ec9ba6a5e87be3dce156454cfef,j174,Journal of Open Source Software,Augur: a bioinformatics toolkit for phylogenetic analyses of human pathogens,"Summary and statement of need The analysis of human pathogens requires a diverse collection of bioinformatics tools. These tools include standard genomic and phylogenetic software and custom software developed to handle the relatively numerous and short genomes of viruses and bacteria. Researchers increasingly depend on the outputs of these tools to infer transmission dynamics of human diseases and make actionable recommendations to public health officials (Black et al., 2020; Gardy et al., 2015). In order to enable real-time analyses of pathogen evolution, bioinformatics tools must scale rapidly with the number of samples and be flexible enough to adapt to a variety of questions and organisms. To meet these needs, we developed Augur, a bioinformatics toolkit designed for phylogenetic analyses of human pathogens.",fullPaper,jv174
p1014,0ff76dd78e47f4534ee148b644f1f2707bc70df5,c46,Brazilian Symposium on Software Engineering,"Improvements to PATRIC, the all-bacterial Bioinformatics Database and Analysis Resource Center","The Pathosystems Resource Integration Center (PATRIC) is the bacterial Bioinformatics Resource Center (https://www.patricbrc.org). Recent changes to PATRIC include a redesign of the web interface and some new services that provide users with a platform that takes them from raw reads to an integrated analysis experience. The redesigned interface allows researchers direct access to tools and data, and the emphasis has changed to user-created genome-groups, with detailed summaries and views of the data that researchers have selected. Perhaps the biggest change has been the enhanced capability for researchers to analyze their private data and compare it to the available public data. Researchers can assemble their raw sequence reads and annotate the contigs using RASTtk. PATRIC also provides services for RNA-Seq, variation, model reconstruction and differential expression analysis, all delivered through an updated private workspace. Private data can be compared by ‘virtual integration’ to any of PATRIC's public data. The number of genomes available for comparison in PATRIC has expanded to over 80 000, with a special emphasis on genomes with antimicrobial resistance data. PATRIC uses this data to improve both subsystem annotation and k-mer classification, and tags new genomes as having signatures that indicate susceptibility or resistance to specific antibiotics.",poster,cp46
p1015,c67f7d629c4500bd47056b03bdfaf2df2f3c9346,j102,Nucleic Acids Research,Human Splicing Finder: an online bioinformatics tool to predict splicing signals,"Thousands of mutations are identified yearly. Although many directly affect protein expression, an increasing proportion of mutations is now believed to influence mRNA splicing. They mostly affect existing splice sites, but synonymous, non-synonymous or nonsense mutations can also create or disrupt splice sites or auxiliary cis-splicing sequences. To facilitate the analysis of the different mutations, we designed Human Splicing Finder (HSF), a tool to predict the effects of mutations on splicing signals or to identify splicing motifs in any human sequence. It contains all available matrices for auxiliary sequence prediction as well as new ones for binding sites of the 9G8 and Tra2-β Serine-Arginine proteins and the hnRNP A1 ribonucleoprotein. We also developed new Position Weight Matrices to assess the strength of 5′ and 3′ splice sites and branch points. We evaluated HSF efficiency using a set of 83 intronic and 35 exonic mutations known to result in splicing defects. We showed that the mutation effect was correctly predicted in almost all cases. HSF could thus represent a valuable resource for research, diagnostic and therapeutic (e.g. therapeutic exon skipping) purposes as well as for global studies, such as the GEN2PHEN European Project or the Human Variome Project.",fullPaper,jv102
p1016,13226c692dd6908c64555fb095c4ee968a539a83,c91,Workshop on Algorithms and Models for the Web-Graph,Trends in the development of miRNA bioinformatics tools,"Abstract MicroRNAs (miRNAs) are small noncoding RNAs that regulate gene expression via recognition of cognate sequences and interference of transcriptional, translational or epigenetic processes. Bioinformatics tools developed for miRNA study include those for miRNA prediction and discovery, structure, analysis and target prediction. We manually curated 95 review papers and ∼1000 miRNA bioinformatics tools published since 2003. We classified and ranked them based on citation number or PageRank score, and then performed network analysis and text mining (TM) to study the miRNA tools development trends. Five key trends were observed: (1) miRNA identification and target prediction have been hot spots in the past decade; (2) manual curation and TM are the main methods for collecting miRNA knowledge from literature; (3) most early tools are well maintained and widely used; (4) classic machine learning methods retain their utility; however, novel ones have begun to emerge; (5) disease-associated miRNA tools are emerging. Our analysis yields significant insight into the past development and future directions of miRNA tools.",poster,cp91
p1017,cc384cff27d8a609f89f9e915c26bf31c39749a1,c64,Experimental Software Engineering Network,Deep learning in bioinformatics,"In the era of big data, transformation of biomedical big data into valuable knowledge has been one of the most important challenges in bioinformatics. Deep learning has advanced rapidly since the early 2000s and now demonstrates state-of-the-art performance in various fields. Accordingly, application of deep learning in bioinformatics to gain insight from data has been emphasized in both academia and industry. Here, we review deep learning in bioinformatics, presenting examples of current research. To provide a useful and comprehensive perspective, we categorize research both by the bioinformatics domain (i.e. omics, biomedical imaging, biomedical signal processing) and deep learning architecture (i.e. deep neural networks, convolutional neural networks, recurrent neural networks, emergent architectures) and present brief descriptions of each study. Additionally, we discuss theoretical and practical issues of deep learning in bioinformatics and suggest future research directions. We believe that this review will provide valuable insights and serve as a starting point for researchers to apply deep learning approaches in their bioinformatics studies.",poster,cp64
p1018,053e5c1f175cf385fb8ab551e68446e24b3475a5,c70,International Conference on Intelligent Robotics and Applications,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Abstract content,fullPaper,cp70
p1019,5c511ed1011c42a92fe1c4c5fe1fbc7190a5fde9,j175,Nature reviews genetics,Piercing the dark matter: bioinformatics of long-range sequencing and mapping,Abstract content,fullPaper,jv175
p1020,219bee9fde1303f423fac52ff373f19552c60d53,j176,Experimental and Molecular Medicine,Single-cell RNA sequencing technologies and bioinformatics pipelines,Abstract content,fullPaper,jv176
p1021,740c502c5596a570abfcffc1eefea160c2d6112c,j130,Scientific Reports,BATMAN-TCM: a Bioinformatics Analysis Tool for Molecular mechANism of Traditional Chinese Medicine,Abstract content,fullPaper,jv130
p1022,c464387a83a573c5154d551214628c760ff1a2b9,c0,International Conference on Human Factors in Computing Systems,Unipro UGENE: a unified bioinformatics toolkit,"UNLABELLED
Unipro UGENE is a multiplatform open-source software with the main goal of assisting molecular biologists without much expertise in bioinformatics to manage, analyze and visualize their data. UGENE integrates widely used bioinformatics tools within a common user interface. The toolkit supports multiple biological data formats and allows the retrieval of data from remote data sources. It provides visualization modules for biological objects such as annotated genome sequences, Next Generation Sequencing (NGS) assembly data, multiple sequence alignments, phylogenetic trees and 3D structures. Most of the integrated algorithms are tuned for maximum performance by the usage of multithreading and special processor instructions. UGENE includes a visual environment for creating reusable workflows that can be launched on local resources or in a High Performance Computing (HPC) environment. UGENE is written in C++ using the Qt framework. The built-in plugin system and structured UGENE API make it possible to extend the toolkit with new functionality.


AVAILABILITY AND IMPLEMENTATION
UGENE binaries are freely available for MS Windows, Linux and Mac OS X at http://ugene.unipro.ru/download.html. UGENE code is licensed under the GPLv2; the information about the code licensing and copyright of integrated tools can be found in the LICENSE.3rd_party file provided with the source bundle.",poster,cp0
p1023,4050c80ba36c6f86b0684a59be70182557b0770c,j177,Nature Machine Intelligence,Ensemble deep learning in bioinformatics,Abstract content,fullPaper,jv177
p1024,1e05e8c73a099b7ce11d4764706ba657c0e0d37b,c89,Conference on Uncertainty in Artificial Intelligence,The bioinformatics toolbox for circRNA discovery and analysis,"Abstract Circular RNAs (circRNAs) are a unique class of RNA molecule identified more than 40 years ago which are produced by a covalent linkage via back-splicing of linear RNA. Recent advances in sequencing technologies and bioinformatics tools have led directly to an ever-expanding field of types and biological functions of circRNAs. In parallel with technological developments, practical applications of circRNAs have arisen including their utilization as biomarkers of human disease. Currently, circRNA-associated bioinformatics tools can support projects including circRNA annotation, circRNA identification and network analysis of competing endogenous RNA (ceRNA). In this review, we collected about 100 circRNA-associated bioinformatics tools and summarized their current attributes and capabilities. We also performed network analysis and text mining on circRNA tool publications in order to reveal trends in their ongoing development.",poster,cp89
p1025,12467886cc2ab7aa5b0108001144ffaa3e67ded0,c8,The Compass,Deep learning-based clustering approaches for bioinformatics,"Abstract Clustering is central to many data-driven bioinformatics research and serves a powerful computational method. In particular, clustering helps at analyzing unstructured and high-dimensional data in the form of sequences, expressions, texts and images. Further, clustering is used to gain insights into biological processes in the genomics level, e.g. clustering of gene expressions provides insights on the natural structure inherent in the data, understanding gene functions, cellular processes, subtypes of cells and understanding gene regulations. Subsequently, clustering approaches, including hierarchical, centroid-based, distribution-based, density-based and self-organizing maps, have long been studied and used in classical machine learning settings. In contrast, deep learning (DL)-based representation and feature learning for clustering have not been reviewed and employed extensively. Since the quality of clustering is not only dependent on the distribution of data points but also on the learned representation, deep neural networks can be effective means to transform mappings from a high-dimensional data space into a lower-dimensional feature space, leading to improved clustering results. In this paper, we review state-of-the-art DL-based approaches for cluster analysis that are based on representation learning, which we hope to be useful, particularly for bioinformatics research. Further, we explore in detail the training procedures of DL-based clustering algorithms, point out different clustering quality metrics and evaluate several DL-based approaches on three bioinformatics use cases, including bioimaging, cancer genomics and biomedical text mining. We believe this review and the evaluation results will provide valuable insights and serve a starting point for researchers wanting to apply DL-based unsupervised methods to solve emerging bioinformatics research problems.",poster,cp8
p1026,fef2bd94ea0ece037c7f983b9d51a6a305482f07,j178,International Journal of Molecular Sciences,Bioinformatics Methods for Mass Spectrometry-Based Proteomics Data Analysis,"Recent advances in mass spectrometry (MS)-based proteomics have enabled tremendous progress in the understanding of cellular mechanisms, disease progression, and the relationship between genotype and phenotype. Though many popular bioinformatics methods in proteomics are derived from other omics studies, novel analysis strategies are required to deal with the unique characteristics of proteomics data. In this review, we discuss the current developments in the bioinformatics methods used in proteomics and how they facilitate the mechanistic understanding of biological processes. We first introduce bioinformatics software and tools designed for mass spectrometry-based protein identification and quantification, and then we review the different statistical and machine learning methods that have been developed to perform comprehensive analysis in proteomics studies. We conclude with a discussion of how quantitative protein data can be used to reconstruct protein interactions and signaling networks.",fullPaper,jv178
p1027,32dc2a2ea7d1735b3c0cf5f782200453215fe6cd,c101,International Conference on Automatic Face and Gesture Recognition,The EMBL-EBI bioinformatics web and programmatic tools framework,"Since 2009 the EMBL-EBI Job Dispatcher framework has provided free access to a range of mainstream sequence analysis applications. These include sequence similarity search services (https://www.ebi.ac.uk/Tools/sss/) such as BLAST, FASTA and PSI-Search, multiple sequence alignment tools (https://www.ebi.ac.uk/Tools/msa/) such as Clustal Omega, MAFFT and T-Coffee, and other sequence analysis tools (https://www.ebi.ac.uk/Tools/pfa/) such as InterProScan. Through these services users can search mainstream sequence databases such as ENA, UniProt and Ensembl Genomes, utilising a uniform web interface or systematically through Web Services interfaces (https://www.ebi.ac.uk/Tools/webservices/) using common programming languages, and obtain enriched results with novel visualisations. Integration with EBI Search (https://www.ebi.ac.uk/ebisearch/) and the dbfetch retrieval service (https://www.ebi.ac.uk/Tools/dbfetch/) further expands the usefulness of the framework. New tools and updates such as NCBI BLAST+, InterProScan 5 and PfamScan, new categories such as RNA analysis tools (https://www.ebi.ac.uk/Tools/rna/), new databases such as ENA non-coding, WormBase ParaSite, Pfam and Rfam, and new workflow methods, together with the retirement of depreciated services, ensure that the framework remains relevant to today's biological community.",poster,cp101
p1028,6966d2b49fa885897e0b2e18ce144831c55f3e5f,j153,bioRxiv,"Deep learning in bioinformatics: introduction, application, and perspective in big data era","Deep learning, which is especially formidable in handling big data, has achieved great success in various fields, including bioinformatics. With the advances of the big data era in biology, it is foreseeable that deep learning will become increasingly important in the field and will be incorporated in vast majorities of analysis pipelines. In this review, we provide both the exoteric introduction of deep learning, and concrete examples and implementations of its representative applications in bioinformatics. We start from the recent achievements of deep learning in the bioinformatics field, pointing out the problems which are suitable to use deep learning. After that, we introduce deep learning in an easy-to-understand fashion, from shallow neural networks to legendary convolutional neural networks, legendary recurrent neural networks, graph neural networks, generative adversarial networks, variational autoencoder, and the most recent state-of-the-art architectures. After that, we provide eight examples, covering five bioinformatics research directions and all the four kinds of data type, with the implementation written in Tensorflow and Keras. Finally, we discuss the common issues, such as overfitting and interpretability, that users will encounter when adopting deep learning methods and provide corresponding suggestions. The implementations are freely available at https://github.com/lykaust15/Deep_learning_examples.",fullPaper,jv153
p1029,d0da9ce3ca989bce2579b64be9aed518265a8994,c32,International Conference on Software Technology: Methods and Tools,Bioinformatics and Computational Biology Solutions Using R and Bioconductor,"the difficulty of assessing utilities, “a formal decision model can. . . synthesize current best evidence and clinical judgment. . . and via its utility component, link this evidence to clinical decisions. . . Decision makers are. . . reluctant to delegate [decisions]. . . to model-based formalisms [but are]. . . increasingly. . . relying on structured approaches to make more informed decisions” (p. 69). The “Decision Making” chapter continues with a presentation of the main mathematical details of maximizing SEU that includes as an example a costeffectiveness analysis of stroke interventions combining QUALYs and the Markov model developed in earlier sections. The chapter concludes with a presentation of hypothesis testing and a brief mention of sample size selection. The final chapter of the “Methods” section, “Simulation,” introduces Markov chain Monte Carlo and includes a section on Monte Carlo estimation of expected utility. The second section, “Case Studies,” has chapters titled “Meta-Analysis,” “Decision Trees,” and “Chronic Disease Modeling.” In the first of these, hierarchical Bayesian meta-analysis of exchangeable studies is developed in some detail, including WinBUGS-style directed graphs and code. Topics include combination of continuous and dichotomous endpoints using latent variables and sensitivity to prior specification. The chapter summary (p. 124) suggests that the resulting posterior distributions of effect magnitudes, “can. . . become components of a formal decision analysis [or]. . . of a comprehensive decision model.” However, no such examples are presented, and this chapter, while an excellent, practical introduction to Bayesian meta-analysis, is not well integrated with the main topic of the book. The “Decision Trees” chapter is organized around a case study of the decision between axillary lymph node dissection or not followed by the choice of none or one of three adjuvant therapies in the treatment of early-stage breast cancer. It is a good, clear presentation of backward induction and uses most of the machinery set up in the methods chapters: posterior distributions, predictive distributions, QUALYs, and SEU maximization. The final case study, “Chronic Disease Modeling,” deals with optimizing the frequency of radiological screening for breast cancer. The core models are a continuous-time four-state Markov model (healthy, preclinical, clinical, and dead) for which the inferential component is the estimation of age-dependent transition densities (through sojourn-time distributions), a sensitivity model that may depend on age and tumor size, a model of the number of auxiliary lymph nodes involved that depends on age and tumor size, and a survival model that depends on node involvement and a vector of other prognostic variables. The decision to be made is the screening schedule as a function of age, and in theory, the trade-off is between the cost of screening and the expected gain in QUALYs. Needless to say, this case study is only sketched, but the sketch is sufficiently detailed to give an idea of the modeling strategies and how the backward induction was implemented. I have one small quibble: regarding the statement on page 33: “to speak of a probability distribution for [the parameter] β we need to imagine a metapopulation, or a universe of possible populations, each with a different [value of the parameter],” I believe that this is not at all what Bayesians mean by probability. Rather, Bayesian inference treats probabilities as epistemic—referring to states of uncertainty based on incomplete information, not to alternate universes. To say, for example, that the speed of light, C, has a distribution is to say that I am uncertain about the exact value of C in this universe and that my uncertainty is described by a probability distribution. Speaking as a statistician, Modeling in Medical Decision Making: A Bayesian Approach would be good to use as one component in a graduate course on decision making following an introductory Bayesian course. It assumes an understanding of calculus, or at minimum, calculus notation. For established statisticians and biostatisticians, the book is a good way to get up to speed on Bayesian decision analysis in health care and could serve as an entry point into the large published literature alluded to at the beginning of this review.",poster,cp32
p1030,424b9225684cf54ef6d3939e2a4d7514bc90e67e,j179,Protein Science,The Bio3D packages for structural bioinformatics,"Bio3D is a family of R packages for the analysis of biomolecular sequence, structure, and dynamics. Major functionality includes biomolecular database searching and retrieval, sequence and structure conservation analysis, ensemble normal mode analysis, protein structure and correlation network analysis, principal component, and related multivariate analysis methods. Here, we review recent package developments, including a new underlying segregation into separate packages for distinct analysis, and introduce a new method for structure analysis named ensemble difference distance matrix analysis (eDDM). The eDDM approach calculates and compares atomic distance matrices across large sets of homologous atomic structures to help identify the residue wise determinants underlying specific functional processes. An eDDM workflow is detailed along with an example application to a large protein family. As a new member of the Bio3D family, the Bio3D‐eddm package supports both experimental and theoretical simulation‐generated structures, is integrated with other methods for dissecting sequence‐structure–function relationships, and can be used in a highly automated and reproducible manner. Bio3D is distributed as an integrated set of platform independent open source R packages available from: http://thegrantlab.org/bio3d/.",fullPaper,jv179
p1031,bfada4dd6e30ed53835b0ab326f16140922ebf31,c77,Networks,ExPASy: SIB bioinformatics resource portal,"ExPASy (http://www.expasy.org) has worldwide reputation as one of the main bioinformatics resources for proteomics. It has now evolved, becoming an extensible and integrative portal accessing many scientific resources, databases and software tools in different areas of life sciences. Scientists can henceforth access seamlessly a wide range of resources in many different domains, such as proteomics, genomics, phylogeny/evolution, systems biology, population genetics, transcriptomics, etc. The individual resources (databases, web-based and downloadable software tools) are hosted in a ‘decentralized’ way by different groups of the SIB Swiss Institute of Bioinformatics and partner institutions. Specifically, a single web portal provides a common entry point to a wide range of resources developed and operated by different SIB groups and external institutions. The portal features a search function across ‘selected’ resources. Additionally, the availability and usage of resources are monitored. The portal is aimed for both expert users and people who are not familiar with a specific domain in life sciences. The new web interface provides, in particular, visual guidance for newcomers to ExPASy.",poster,cp77
p1032,fe540ba862a10e06c8b75b751745e8b2d6161c73,c31,International Conference on Evaluation & Assessment in Software Engineering,Perseus: A Bioinformatics Platform for Integrative Analysis of Proteomics Data in Cancer Research.,Abstract content,poster,cp31
p1033,4246613a89c19dbd326eee6cbadf6a933de21eb9,c106,Chinese Conference on Biometric Recognition,BMC Bioinformatics,"BMC Bioinformatics is part of the BMC series which publishes subject-specific journals focused on the needs of individual research communities across all areas of biology and medicine. We do not make editorial decisions on the basis of the interest of a study or its likely impact. Studies must be scientifically valid; for research articles this includes a scientifically sound research question, the use of suitable methods and analysis, and following community-agreed standards relevant to the research field.  Specific criteria for other article types can be found in the submission guidelines.  BMC series open, inclusive and trusted.",poster,cp106
p1034,10b40befe5942e997f88ab40bac1acd931147893,c14,International Conference on Exploring Services Science,"PATRIC, the bacterial bioinformatics database and analysis resource","The Pathosystems Resource Integration Center (PATRIC) is the all-bacterial Bioinformatics Resource Center (BRC) (http://www.patricbrc.org). A joint effort by two of the original National Institute of Allergy and Infectious Diseases-funded BRCs, PATRIC provides researchers with an online resource that stores and integrates a variety of data types [e.g. genomics, transcriptomics, protein–protein interactions (PPIs), three-dimensional protein structures and sequence typing data] and associated metadata. Datatypes are summarized for individual genomes and across taxonomic levels. All genomes in PATRIC, currently more than 10 000, are consistently annotated using RAST, the Rapid Annotations using Subsystems Technology. Summaries of different data types are also provided for individual genes, where comparisons of different annotations are available, and also include available transcriptomic data. PATRIC provides a variety of ways for researchers to find data of interest and a private workspace where they can store both genomic and gene associations, and their own private data. Both private and public data can be analyzed together using a suite of tools to perform comparative genomic or transcriptomic analysis. PATRIC also includes integrated information related to disease and PPIs. All the data and integrated analysis and visualization tools are freely available. This manuscript describes updates to the PATRIC since its initial report in the 2007 NAR Database Issue.",poster,cp14
p1035,38080784dcde5f8bdb86af9a186b47117db37133,j180,Journal of Clinical Medicine,Bioinformatics and Computational Tools for Next-Generation Sequencing Analysis in Clinical Genetics,"Clinical genetics has an important role in the healthcare system to provide a definitive diagnosis for many rare syndromes. It also can have an influence over genetics prevention, disease prognosis and assisting the selection of the best options of care/treatment for patients. Next-generation sequencing (NGS) has transformed clinical genetics making possible to analyze hundreds of genes at an unprecedented speed and at a lower price when comparing to conventional Sanger sequencing. Despite the growing literature concerning NGS in a clinical setting, this review aims to fill the gap that exists among (bio)informaticians, molecular geneticists and clinicians, by presenting a general overview of the NGS technology and workflow. First, we will review the current NGS platforms, focusing on the two main platforms Illumina and Ion Torrent, and discussing the major strong points and weaknesses intrinsic to each platform. Next, the NGS analytical bioinformatic pipelines are dissected, giving some emphasis to the algorithms commonly used to generate process data and to analyze sequence variants. Finally, the main challenges around NGS bioinformatics are placed in perspective for future developments. Even with the huge achievements made in NGS technology and bioinformatics, further improvements in bioinformatic algorithms are still required to deal with complex and genetically heterogeneous disorders.",fullPaper,jv180
p1036,a6409a9192845e3f321edb0c147d263d9fad783c,j68,Frontiers in Genetics,Recent Advances of Deep Learning in Bioinformatics and Computational Biology,"Extracting inherent valuable knowledge from omics big data remains as a daunting problem in bioinformatics and computational biology. Deep learning, as an emerging branch from machine learning, has exhibited unprecedented performance in quite a few applications from academia and industry. We highlight the difference and similarity in widely utilized models in deep learning studies, through discussing their basic structures, and reviewing diverse applications and disadvantages. We anticipate the work can serve as a meaningful perspective for further development of its theory, algorithm and application in bioinformatic and computational biology.",fullPaper,jv68
p1037,6fd80c1f69da5ffae7b395e9d9d85ff8c061f885,c46,Brazilian Symposium on Software Engineering,VectorBase: an updated bioinformatics resource for invertebrate vectors and other organisms related with human diseases,"VectorBase is a National Institute of Allergy and Infectious Diseases supported Bioinformatics Resource Center (BRC) for invertebrate vectors of human pathogens. Now in its 11th year, VectorBase currently hosts the genomes of 35 organisms including a number of non-vectors for comparative analysis. Hosted data range from genome assemblies with annotated gene features, transcript and protein expression data to population genetics including variation and insecticide-resistance phenotypes. Here we describe improvements to our resource and the set of tools available for interrogating and accessing BRC data including the integration of Web Apollo to facilitate community annotation and providing Galaxy to support user-based workflows. VectorBase also actively supports our community through hands-on workshops and online tutorials. All information and data are freely available from our website at https://www.vectorbase.org/.",poster,cp46
p1038,224a97657b897bbae36494025e4aeef5bb18d155,c20,Research Conference of the South African Institute of Computer Scientists and Information Technologists,Programmatic access to bioinformatics tools from EMBL-EBI update: 2017,"Abstract Since 2009 the EMBL-EBI provides free and unrestricted access to several bioinformatics tools via the user's browser as well as programmatically via Web Services APIs. Programmatic access to these tools, which is fundamental to bioinformatics, is increasingly important as more high-throughput data is generated, e.g. from proteomics and metagenomic experiments. Access is available using both the SOAP and RESTful approaches and their usage is reviewed regularly in order to ensure that the best, supported tools are available to all users. We present here an update describing the latest enhancement to the Job Dispatcher APIs as well as the governance under it.",poster,cp20
p1039,80e86c8ca16da71d7ff279619d7b72c82a14f564,c84,The Web Conference,A new bioinformatics analysis tools framework at EMBL–EBI,"The EMBL-EBI provides access to various mainstream sequence analysis applications. These include sequence similarity search services such as BLAST, FASTA, InterProScan and multiple sequence alignment tools such as ClustalW, T-Coffee and MUSCLE. Through the sequence similarity search services, the users can search mainstream sequence databases such as EMBL-Bank and UniProt, and more than 2000 completed genomes and proteomes. We present here a new framework aimed at both novice as well as expert users that exposes novel methods of obtaining annotations and visualizing sequence analysis results through one uniform and consistent interface. These services are available over the web and via Web Services interfaces for users who require systematic access or want to interface with customized pipe-lines and workflows using common programming languages. The framework features novel result visualizations and integration of domain and functional predictions for protein database searches. It is available at http://www.ebi.ac.uk/Tools/sss for sequence similarity searches and at http://www.ebi.ac.uk/Tools/msa for multiple sequence alignments.",poster,cp84
p1040,0369a5e1ab11f46979d4b61419884724a022d62e,c30,IEEE Aerospace Conference,DAVID Bioinformatics Resources: expanded annotation database and novel algorithms to better extract biology from large gene lists,"All tools in the DAVID Bioinformatics Resources aim to provide functional interpretation of large lists of genes derived from genomic studies. The newly updated DAVID Bioinformatics Resources consists of the DAVID Knowledgebase and five integrated, web-based functional annotation tool suites: the DAVID Gene Functional Classification Tool, the DAVID Functional Annotation Tool, the DAVID Gene ID Conversion Tool, the DAVID Gene Name Viewer and the DAVID NIAID Pathogen Genome Browser. The expanded DAVID Knowledgebase now integrates almost all major and well-known public bioinformatics resources centralized by the DAVID Gene Concept, a single-linkage method to agglomerate tens of millions of diverse gene/protein identifiers and annotation terms from a variety of public bioinformatics databases. For any uploaded gene list, the DAVID Resources now provides not only the typical gene-term enrichment analysis, but also new tools and functions that allow users to condense large gene lists into gene functional groups, convert between gene/protein identifiers, visualize many-genes-to-many-terms relationships, cluster redundant and heterogeneous terms into groups, search for interesting and related genes or terms, dynamically view genes from their lists on bio-pathways and more. With DAVID (http://david.niaid.nih.gov), investigators gain more power to interpret the biological mechanisms associated with large gene lists.",poster,cp30
p1041,0d67f9364948d1d48f3c6cb9b88e60afd5f6d460,c90,Computer Vision and Pattern Recognition,The MPI bioinformatics Toolkit as an integrative platform for advanced protein sequence and structure analysis,"The MPI Bioinformatics Toolkit (http://toolkit.tuebingen.mpg.de) is an open, interactive web service for comprehensive and collaborative protein bioinformatic analysis. It offers a wide array of interconnected, state-of-the-art bioinformatics tools to experts and non-experts alike, developed both externally (e.g. BLAST+, HMMER3, MUSCLE) and internally (e.g. HHpred, HHblits, PCOILS). While a beta version of the Toolkit was released 10 years ago, the current production-level release has been available since 2008 and has serviced more than 1.6 million external user queries. The usage of the Toolkit has continued to increase linearly over the years, reaching more than 400 000 queries in 2015. In fact, through the breadth of its tools and their tight interconnection, the Toolkit has become an excellent platform for experimental scientists as well as a useful resource for teaching bioinformatic inquiry to students in the life sciences. In this article, we report on the evolution of the Toolkit over the last ten years, focusing on the expansion of the tool repertoire (e.g. CS-BLAST, HHblits) and on infrastructural work needed to remain operative in a changing web environment.",poster,cp90
p1042,602ab24e8134815630da74c746405b3693836d6e,j181,Journal of medical systems,A Survey of Data Mining and Deep Learning in Bioinformatics,Abstract content,fullPaper,jv181
p1043,dfa2727c776fc5b8dcd4d9217e4564e578ddb5a5,c58,Australian Software Engineering Conference,Bioinformatics and Computational Biology Solutions Using R and Bioconductor (Statistics for Biology and Health),Abstract content,poster,cp58
p1044,d26f49d123b114ba7ec83164bd4edca15d398677,c98,North American Chapter of the Association for Computational Linguistics,Sequence clustering in bioinformatics: an empirical study,"Sequence clustering is a basic bioinformatics task that is attracting renewed attention with the development of metagenomics and microbiomics. The latest sequencing techniques have decreased costs and as a result, massive amounts of DNA/RNA sequences are being produced. The challenge is to cluster the sequence data using stable, quick and accurate methods. For microbiome sequencing data, 16S ribosomal RNA operational taxonomic units are typically used. However, there is often a gap between algorithm developers and bioinformatics users. Different software tools can produce diverse results and users can find them difficult to analyze. Understanding the different clustering mechanisms is crucial to understanding the results that they produce. In this review, we selected several popular clustering tools, briefly explained the key computing principles, analyzed their characters and compared them using two independent benchmark datasets. Our aim is to assist bioinformatics users in employing suitable clustering tools effectively to analyze big sequencing data. Related data, codes and software tools were accessible at the link http://lab.malab.cn/∼lg/clustering/.",poster,cp98
p1045,6be2e6dca6c66f3ad74eb01c168e3c104119f41c,j182,Medicinal chemistry,Impacts of bioinformatics to medicinal chemistry.,"Facing the explosive growth of biological sequence data, such as those of protein/peptide and DNA/RNA, generated in the post-genomic age, many bioinformatical and mathematical approaches as well as physicochemical concepts have been introduced to timely derive useful informations from these biological sequences, in order to stimulate the development of medical science and drug design. Meanwhile, because of the rapid penetrations from these disciplines, medicinal chemistry is currently undergoing an unprecedented revolution. In this minireview, we are to summarize the progresses by focusing on the following six aspects. (1) Use the pseudo amino acid composition or PseAAC to predict various attributes of protein/peptide sequences that are useful for drug development. (2) Use pseudo oligonucleotide composition or PseKNC to do the same for DNA/RNA sequences. (3) Introduce the multi-label approach to study those systems where the constituent elements bear multiple characters and functions. (4) Utilize the graphical rules and ""wenxiang"" diagrams to analyze complicated biomedical systems. (5) Recent development in identifying the interactions of drugs with its various types of target proteins in cellular networking. (6) Distorted key theory and its application in developing peptide drugs.",fullPaper,jv182
p1046,36789799e464aa7465125ff8e778939843a0e89b,c52,Workshop on Learning from Authoritative Security Experiment Results,Taverna: a tool for the composition and enactment of bioinformatics workflows,"MOTIVATION
In silico experiments in bioinformatics involve the co-ordinated use of computational tools and information repositories. A growing number of these resources are being made available with programmatic access in the form of Web services. Bioinformatics scientists will need to orchestrate these Web services in workflows as part of their analyses.


RESULTS
The Taverna project has developed a tool for the composition and enactment of bioinformatics workflows for the life sciences community. The tool includes a workbench application which provides a graphical user interface for the composition of workflows. These workflows are written in a new language called the simple conceptual unified flow language (Scufl), where by each step within a workflow represents one atomic task. Two examples are used to illustrate the ease by which in silico experiments can be represented as Scufl workflows using the workbench application.",poster,cp52
p1047,1279fe26020af034fd6f16b04a1c23427c127db3,c9,Pacific Symposium on Biocomputing,Data-driven advice for applying machine learning to bioinformatics problems,"As the bioinformatics field grows, it must keep pace not only with new data but with new algorithms. Here we contribute a thorough analysis of 13 state-of-the-art, commonly used machine learning algorithms on a set of 165 publicly available classification problems in order to provide data-driven algorithm recommendations to current researchers. We present a number of statistical and visual comparisons of algorithm performance and quantify the effect of model selection and algorithm tuning for each algorithm and dataset. The analysis culminates in the recommendation of five algorithms with hyperparameters that maximize classifier performance across the tested problems, as well as general guidelines for applying machine learning to supervised classification problems.",fullPaper,cp9
p1048,e8df0290e84a7c7ff4df03e648126eb99085e2f8,c2,International Symposium on Intelligent Data Analysis,Influenza Research Database: An integrated bioinformatics resource for influenza virus research,"The Influenza Research Database (IRD) is a U.S. National Institute of Allergy and Infectious Diseases (NIAID)-sponsored Bioinformatics Resource Center dedicated to providing bioinformatics support for influenza virus research. IRD facilitates the research and development of vaccines, diagnostics and therapeutics against influenza virus by providing a comprehensive collection of influenza-related data integrated from various sources, a growing suite of analysis and visualization tools for data mining and hypothesis generation, personal workbench spaces for data storage and sharing, and active user community support. Here, we describe the recent improvements in IRD including the use of cloud and high performance computing resources, analysis and visualization of user-provided sequence data with associated metadata, predictions of novel variant proteins, annotations of phenotype-associated sequence markers and their predicted phenotypic effects, hemagglutinin (HA) clade classifications, an automated tool for HA subtype numbering conversion, linkouts to disease event data and the addition of host factor and antiviral drug components. All data and tools are freely available without restriction from the IRD website at https://www.fludb.org.",poster,cp2
p1049,e6a5bcdb576f2b0d965a9f71f24514553966716b,j183,Journal of Infectious Diseases,"Next Generation Sequencing and Bioinformatics Methodologies for Infectious Disease Research and Public Health: Approaches, Applications, and Considerations for Development of Laboratory Capacity.","Next generation sequencing (NGS) combined with bioinformatics has successfully been used in a vast array of analyses for infectious disease research of public health relevance. For instance, NGS and bioinformatics approaches have been used to identify outbreak origins, track transmissions, investigate epidemic dynamics, determine etiological agents of a disease, and discover novel human pathogens. However, implementation of high-quality NGS and bioinformatics in research and public health laboratories can be challenging. These challenges mainly include the choice of the sequencing platform and the sequencing approach, the choice of bioinformatics methodologies, access to the appropriate computation and information technology infrastructure, and recruiting and retaining personnel with the specialized skills and experience in this field. In this review, we summarize the most common NGS and bioinformatics workflows in the context of infectious disease genomic surveillance and pathogen discovery, and highlight the main challenges and considerations for setting up an NGS and bioinformatics-focused infectious disease research public health laboratory. We describe the most commonly used sequencing platforms and review their strengths and weaknesses. We review sequencing approaches that have been used for various pathogens and study questions, as well as the most common difficulties associated with these approaches that should be considered when implementing in a public health or research setting. In addition, we provide a review of some common bioinformatics tools and procedures used for pathogen discovery and genome assembly, along with the most common challenges and solutions. Finally, we summarize the bioinformatics of advanced viral, bacterial, and parasite pathogen characterization, including types of study questions that can be answered when utilizing NGS and bioinformatics.",fullPaper,jv183
p1050,632df9a1308ec631143f2897f79eeaf04208319d,c80,International Conference on Learning Representations,Metabolomics technology and bioinformatics for precision medicine,"Precision medicine is rapidly emerging as a strategy to tailor medical treatment to a small group or even individual patients based on their genetics, environment and lifestyle. Precision medicine relies heavily on developments in systems biology and omics disciplines, including metabolomics. Combination of metabolomics with sophisticated bioinformatics analysis and mathematical modeling has an extreme power to provide a metabolic snapshot of the patient over the course of disease and treatment or classifying patients into subpopulations and subgroups requiring individual medical treatment. Although a powerful approach, metabolomics have certain limitations in technology and bioinformatics. We will review various aspects of metabolomics technology and bioinformatics, from data generation, bioinformatics analysis, data fusion and mathematical modeling to data management, in the context of precision medicine.",poster,cp80
p1051,9ade9dace9c2351280b8be050b69ad2abe5e7d9d,c13,International Conference on Data Science and Advanced Analytics,A brief history of bioinformatics,"It is easy for today's students and researchers to believe that modern bioinformatics emerged recently to assist next-generation sequencing data analysis. However, the very beginnings of bioinformatics occurred more than 50 years ago, when desktop computers were still a hypothesis and DNA could not yet be sequenced. The foundations of bioinformatics were laid in the early 1960s with the application of computational methods to protein sequence analysis (notably, de novo sequence assembly, biological sequence databases and substitution models). Later on, DNA analysis also emerged due to parallel advances in (i) molecular biology methods, which allowed easier manipulation of DNA, as well as its sequencing, and (ii) computer science, which saw the rise of increasingly miniaturized and more powerful computers, as well as novel software better suited to handle bioinformatics tasks. In the 1990s through the 2000s, major improvements in sequencing technology, along with reduced costs, gave rise to an exponential increase of data. The arrival of 'Big Data' has laid out new challenges in terms of data mining and management, calling for more expertise from computer science into the field. Coupled with an ever-increasing amount of bioinformatics tools, biological Big Data had (and continues to have) profound implications on the predictive power and reproducibility of bioinformatics results. To overcome this issue, universities are now fully integrating this discipline into the curriculum of biology students. Recent subdisciplines such as synthetic biology, systems biology and whole-cell modeling have emerged from the ever-increasing complementarity between computer science and biology.",poster,cp13
p1052,a885fc8bcd9f398acdfeb1d64c37b523063b585e,j184,Neurocomputing,A novel features ranking metric with application to scalable visual and bioinformatics data classification,Abstract content,fullPaper,jv184
p1053,2392f7f8b64c2016bf57957d51f32b7cef2acbcd,j185,SpringerPlus,An overview of topic modeling and its current applications in bioinformatics,Abstract content,fullPaper,jv185
p1054,0a27e9b4d0e03c6d0b03ec0b900b48b0e4079969,c109,International Conference on Mobile Data Management,Protein Bioinformatics Databases and Resources.,Abstract content,poster,cp109
p1055,8f6bf0f8ea557aec5376ba73eb39e674156c8bd1,j186,BMC Microbiology,A comparison of sequencing platforms and bioinformatics pipelines for compositional analysis of the gut microbiome,Abstract content,fullPaper,jv186
p1056,cd8156fc9f17146b39dfaf47fcd20f1e3ab70791,j168,Proceedings of the IEEE,Manual for Using Homomorphic Encryption for Bioinformatics,"Biological data science is an emerging field facing multiple challenges for hosting, sharing, computing on, and interacting with large data sets. Privacy regulations and concerns about the risks of leaking sensitive personal health and genomic data add another layer of complexity to the problem. Recent advances in cryptography over the last five years have yielded a tool, homomorphic encryption, which can be used to encrypt data in such a way that storage can be outsourced to an untrusted cloud, and the data can be computed on in a meaningful way in encrypted form, without access to decryption keys. This paper introduces homomorphic encryption to the bioinformatics community, and presents an informal “manual” for using the Simple Encrypted Arithmetic Library (SEAL), which we have made publicly available for bioinformatic, genomic, and other research purposes.",fullPaper,jv168
p1057,d7e1225452deed203a2f92d7cca29bcde27866e5,j68,Frontiers in Genetics,A Review of Bioinformatics Tools for Bio-Prospecting from Metagenomic Sequence Data,"The microbiome can be defined as the community of microorganisms that live in a particular environment. Metagenomics is the practice of sequencing DNA from the genomes of all organisms present in a particular sample, and has become a common method for the study of microbiome population structure and function. Increasingly, researchers are finding novel genes encoded within metagenomes, many of which may be of interest to the biotechnology and pharmaceutical industries. However, such “bioprospecting” requires a suite of sophisticated bioinformatics tools to make sense of the data. This review summarizes the most commonly used bioinformatics tools for the assembly and annotation of metagenomic sequence data with the aim of discovering novel genes.",fullPaper,jv68
p1058,526cdbcd0d3063a4743fcd8033d9e76bd356d9bd,j187,Genome Research,A cloud-compatible bioinformatics pipeline for ultrarapid pathogen identification from next-generation sequencing of clinical samples,"Unbiased next-generation sequencing (NGS) approaches enable comprehensive pathogen detection in the clinical microbiology laboratory and have numerous applications for public health surveillance, outbreak investigation, and the diagnosis of infectious diseases. However, practical deployment of the technology is hindered by the bioinformatics challenge of analyzing results accurately and in a clinically relevant timeframe. Here we describe SURPI (“sequence-based ultrarapid pathogen identification”), a computational pipeline for pathogen identification from complex metagenomic NGS data generated from clinical samples, and demonstrate use of the pipeline in the analysis of 237 clinical samples comprising more than 1.1 billion sequences. Deployable on both cloud-based and standalone servers, SURPI leverages two state-of-the-art aligners for accelerated analyses, SNAP and RAPSearch, which are as accurate as existing bioinformatics tools but orders of magnitude faster in performance. In fast mode, SURPI detects viruses and bacteria by scanning data sets of 7–500 million reads in 11 min to 5 h, while in comprehensive mode, all known microorganisms are identified, followed by de novo assembly and protein homology searches for divergent viruses in 50 min to 16 h. SURPI has also directly contributed to real-time microbial diagnosis in acutely ill patients, underscoring its potential key role in the development of unbiased NGS-based clinical assays in infectious diseases that demand rapid turnaround times.",fullPaper,jv187
p1059,042328a210e2a2301bf1bd81b238f8cc3bd82ba5,c6,Americas Conference on Information Systems,Performance measures in evaluating machine learning based bioinformatics predictors for classifications,Abstract content,poster,cp6
p1060,e785892cf00aea8b58e638994b31a366d31b794e,c108,International Conference on Information Integration and Web-based Applications & Services,Encyclopedia of Bioinformatics and Computational Biology: ABC of Bioinformatics,Abstract content,poster,cp108
p1061,02f42cf7470fbe514c6426e63631fc1ab5a2e350,j188,Analytical and Bioanalytical Chemistry,Bioinformatics and peptidomics approaches to the discovery and analysis of food-derived bioactive peptides,Abstract content,fullPaper,jv188
p1062,e67a2993f38c125539a17c16791a6c73dbcc586d,c82,Workshop on Interdisciplinary Software Engineering Research,A novel hierarchical selective ensemble classifier with bioinformatics application,Abstract content,poster,cp82
p1063,f54d849fb9a111c0113f6c362d82d484b489193c,j153,bioRxiv,CLIMB (the Cloud Infrastructure for Microbial Bioinformatics): an online resource for the medical microbiology community,"The increasing availability and decreasing cost of high-throughput sequencing has transformed academic medical microbiology, delivering an explosion in available genomes while also driving advances in bioinformatics. However, many microbiologists are unable to exploit the resulting large genomics datasets because they do not have access to relevant computational resources and to an appropriate bioinformatics infrastructure. Here, we present the Cloud Infrastructure for Microbial Bioinformatics (CLIMB) facility, a shared computing infrastructure that has been designed from the ground up to provide an environment where microbiologists can share and reuse methods and data. DATA SUMMARY The paper describes a new, freely available public resource and therefore no data has been generated. The resource can be accessed at http://www.climb.ac.uk. Source code for software developed for the project can be found at http://github.com/MRC-CLIMB/ I/We confirm all supporting data, code and protocols have been provided within the article or through supplementary data files. IMPACT STATEMENT Technological advances mean that genome sequencing is now relatively simple, quick, and affordable. However, handling large genome datasets remains a significant challenge for many microbiologists, with substantial requirements for computational resources and expertise in data storage and analysis. This has led to fragmentary approaches to software development and data sharing that reduce the reproducibility of research and limits opportunities for bioinformatics training. Here, we describe a nationwide electronic infrastructure that has been designed to support the UK microbiology community, providing simple mechanisms for accessing large, shared, computational resources designed to meet the bioinformatic needs of microbiologists.",fullPaper,jv153
p1064,0cc0a1ba893e35adf01342ed8f91bfa0ad940416,j189,Journal of Biomedical Informatics,An overview of bioinformatics tools for epitope prediction: Implications on vaccine development,Abstract content,fullPaper,jv189
p1065,cdfe58e8c16884bf01989d7c35b0b7544710208f,c107,British Machine Vision Conference,Snakemake - a scalable bioinformatics workflow engine,"SUMMARY
Snakemake is a workflow engine that provides a readable Python-based workflow definition language and a powerful execution environment that scales from single-core workstations to compute clusters without modifying the workflow. It is the first system to support the use of automatically inferred multiple named wildcards (or variables) in input and output filenames.


AVAILABILITY
http://snakemake.googlecode.com.


CONTACT
johannes.koester@uni-due.de.",poster,cp107
p1066,aca8d8cc3fec56b9b41dc1853a75c1f0c0bd7d91,c39,International Conference on Global Software Engineering,Network Inference and Reconstruction in Bioinformatics,Abstract content,poster,cp39
p1067,a60d075f1f31c25a7ed32ebeca877d19233dde26,c97,Interspeech,Text Mining for Bioinformatics Using Biomedical Literature,Abstract content,poster,cp97
p1068,e2991da06cec4ca1f8f9727279674cfccdeee93c,c40,IEEE International Conference on Software Maintenance and Evolution,A global perspective on evolving bioinformatics and data science training needs,"Abstract Bioinformatics is now intrinsic to life science research, but the past decade has witnessed a continuing deficiency in this essential expertise. Basic data stewardship is still taught relatively rarely in life science education programmes, creating a chasm between theory and practice, and fuelling demand for bioinformatics training across all educational levels and career roles. Concerned by this, surveys have been conducted in recent years to monitor bioinformatics and computational training needs worldwide. This article briefly reviews the principal findings of a number of these studies. We see that there is still a strong appetite for short courses to improve expertise and confidence in data analysis and interpretation; strikingly, however, the most urgent appeal is for bioinformatics to be woven into the fabric of life science degree programmes. Satisfying the relentless training needs of current and future generations of life scientists will require a concerted response from stakeholders across the globe, who need to deliver sustainable solutions capable of both transforming education curricula and cultivating a new cadre of trainer scientists.",poster,cp40
p1069,cca5e74bf5303d9504e78bc58d61d9fe653d160d,j68,Frontiers in Genetics,The Road to Metagenomics: From Microbiology to DNA Sequencing Technologies and Bioinformatics,"The study of microorganisms that pervade each and every part of this planet has encountered many challenges through time such as the discovery of unknown organisms and the understanding of how they interact with their environment. The aim of this review is to take the reader along the timeline and major milestones that led us to modern metagenomics. This new and thriving area is likely to be an important contributor to solve different problems. The transition from classical microbiology to modern metagenomics studies has required the development of new branches of knowledge and specialization. Here, we will review how the availability of high-throughput sequencing technologies has transformed microbiology and bioinformatics and how to tackle the inherent computational challenges that arise from the DNA sequencing revolution. New computational methods are constantly developed to collect, process, and extract useful biological information from a variety of samples and complex datasets, but metagenomics needs the integration of several of these computational methods. Despite the level of specialization needed in bioinformatics, it is important that life-scientists have a good understanding of it for a correct experimental design, which allows them to reveal the information in a metagenome.",fullPaper,jv68
p1070,8a8ec7b22ded7e455b1d156d0a57f3eba9cf38b0,j190,Methods,Feature selection methods for big data bioinformatics: A survey from the search perspective.,Abstract content,fullPaper,jv190
p1071,334c19412ba69307b5f25408c79aac4cb8f94906,j85,BMC Bioinformatics,Bioinformatics research at BGRS-2018,Abstract content,fullPaper,jv85
p1072,a0672457c2687759fe88a44541cc53c81738c0da,c67,Enterprise Application Integration,The development and application of bioinformatics core competencies to improve bioinformatics training and education,"Bioinformatics is recognized as part of the essential knowledge base of numerous career paths in biomedical research and healthcare. However, there is little agreement in the field over what that knowledge entails or how best to provide it. These disagreements are compounded by the wide range of populations in need of bioinformatics training, with divergent prior backgrounds and intended application areas. The Curriculum Task Force of the International Society of Computational Biology (ISCB) Education Committee has sought to provide a framework for training needs and curricula in terms of a set of bioinformatics core competencies that cut across many user personas and training programs. The initial competencies developed based on surveys of employers and training programs have since been refined through a multiyear process of community engagement. This report describes the current status of the competencies and presents a series of use cases illustrating how they are being applied in diverse training contexts. These use cases are intended to demonstrate how others can make use of the competencies and engage in the process of their continuing refinement and application. The report concludes with a consideration of remaining challenges and future plans.",poster,cp67
p1073,fd23cfb37f4f69932edce56a13578b60b029db3a,j191,Molecular Ecology Resources,PipeCraft: Flexible open‐source toolkit for bioinformatics analysis of custom high‐throughput amplicon sequencing data,"High‐throughput sequencing methods have become a routine analysis tool in environmental sciences as well as in public and private sector. These methods provide vast amount of data, which need to be analysed in several steps. Although the bioinformatics may be applied using several public tools, many analytical pipelines allow too few options for the optimal analysis for more complicated or customized designs. Here, we introduce PipeCraft, a flexible and handy bioinformatics pipeline with a user‐friendly graphical interface that links several public tools for analysing amplicon sequencing data. Users are able to customize the pipeline by selecting the most suitable tools and options to process raw sequences from Illumina, Pacific Biosciences, Ion Torrent and Roche 454 sequencing platforms. We described the design and options of PipeCraft and evaluated its performance by analysing the data sets from three different sequencing platforms. We demonstrated that PipeCraft is able to process large data sets within 24 hr. The graphical user interface and the automated links between various bioinformatics tools enable easy customization of the workflow. All analytical steps and options are recorded in log files and are easily traceable.",fullPaper,jv191
p1074,8c5c522a0d02b0487929412be41eb385ff1eb991,j178,International Journal of Molecular Sciences,Aptamer Bioinformatics,"Aptamers are short nucleic acid sequences capable of specific, high-affinity molecular binding. They are isolated via SELEX (Systematic Evolution of Ligands by Exponential Enrichment), an evolutionary process that involves iterative rounds of selection and amplification before sequencing and aptamer characterization. As aptamers are genetic in nature, bioinformatic approaches have been used to improve both aptamers and their selection. This review will discuss the advancements made in several enclaves of aptamer bioinformatics, including simulation of aptamer selection, fragment-based aptamer design, patterning of libraries, identification of lead aptamers from high-throughput sequencing (HTS) data and in silico aptamer optimization.",fullPaper,jv178
p1075,4edf4880e93783901df1118ff174c839ffe2986f,j153,bioRxiv,Enabling the democratization of the genomics revolution with a fully integrated web-based bioinformatics platform,"Continued advancements in sequencing technologies have fueled the development of new sequencing applications and promise to flood current databases with raw data. A number of factors prevent the seamless and easy use of these data, including the breadth of project goals, the wide array of tools that individually perform fractions of any given analysis, the large number of associated software/hardware dependencies, and the detailed expertise required to perform these analyses. To address these issues, we have developed an intuitive web-based environment with a wide assortment of integrated and cutting-edge bioinformatics tools. These preconfigured workflows provide even novice next-generation sequencing users with the ability to perform many complex analyses with only a few mouse clicks, and, within the context of the same environment, to visualize and further interrogate their results. This bioinformatics platform is an initial attempt at Empowering the Development of Genomics Expertise (EDGE) in a wide range of applications.",fullPaper,jv153
p1076,a541fb091828a8a52ab0bc5914c9f68ca89df3e5,j187,Genome Research,"H3ABioNet, a sustainable pan-African bioinformatics network for human heredity and health in Africa","The application of genomics technologies to medicine and biomedical research is increasing in popularity, made possible by new high-throughput genotyping and sequencing technologies and improved data analysis capabilities. Some of the greatest genetic diversity among humans, animals, plants, and microbiota occurs in Africa, yet genomic research outputs from the continent are limited. The Human Heredity and Health in Africa (H3Africa) initiative was established to drive the development of genomic research for human health in Africa, and through recognition of the critical role of bioinformatics in this process, spurred the establishment of H3ABioNet, a pan-African bioinformatics network for H3Africa. The limitations in bioinformatics capacity on the continent have been a major contributory factor to the lack of notable outputs in high-throughput biology research. Although pockets of high-quality bioinformatics teams have existed previously, the majority of research institutions lack experienced faculty who can train and supervise bioinformatics students. H3ABioNet aims to address this dire need, specifically in the area of human genetics and genomics, but knock-on effects are ensuring this extends to other areas of bioinformatics. Here, we describe the emergence of genomics research and the development of bioinformatics in Africa through H3ABioNet.",fullPaper,jv187
p1077,bf4e29b5b8afe5aa7dbf091f11d488a9b34c7e90,c88,Symposium on the Theory of Computing,The Impact of Bioinformatics on Vaccine Design and Development,"Vaccines are the pharmaceutical products that offer the best cost‐benefit ratio in the pre‐ vention or treatment of diseases. In that a vaccine is a pharmaceutical product, vaccine development and production are costly and it takes years for this to be accomplished. Several approaches have been applied to reduce the times and costs of vaccine develop‐ ment, mainly focusing on the selection of appropriate antigens or antigenic structures, carriers, and adjuvants. One of these approaches is the incorporation of bioinformatics methods and analyses into vaccine development. This chapter provides an overview of the application of bioinformatics strategies in vaccine design and development, supply‐ ing some successful examples of vaccines in which bioinformatics has furnished a cutting edge in their development. Reverse vaccinology, immunoinformatics, and structural vac ‐ cinology are described and addressed in the design and development of specific vaccines against infectious diseases caused by bacteria, viruses, and parasites. These include some emerging or re‐emerging infectious diseases, as well as therapeutic vaccines to fight can‐ cer, allergies, and substance abuse, which have been facilitated and improved by using bioinformatics tools or which are under development based on bioinformatics strategies. antigenic B‐cell (IEDB) and CTL epitopes (NetCTL.1.2 server). They determined, by in silico studies, surface accessibility, surface flexibility, hydrophilicity, homology modeling (MODELLER ver. 9.12, CHARMM, WhatIF, PROCHECK, Verify 3D), and structure‐based epitope prediction for E protein, NS3, and NS5. They performed molecular docking of the ZIKV‐E protein with HLA‐A0201, of the ZIKV‐NS3 protein with HLA‐B2705, and of the ZIKV‐NS5 protein with HLA‐C0801 (PatchDock rigid‐body docking server, FireDock server). these",poster,cp88
p1078,27464183a5d63c06d4a8fafb0ba4e61bbdb59b70,j68,Frontiers in Genetics,Single-Cell Transcriptomics Bioinformatics and Computational Challenges,"The emerging single-cell RNA-Seq (scRNA-Seq) technology holds the promise to revolutionize our understanding of diseases and associated biological processes at an unprecedented resolution. It opens the door to reveal intercellular heterogeneity and has been employed to a variety of applications, ranging from characterizing cancer cells subpopulations to elucidating tumor resistance mechanisms. Parallel to improving experimental protocols to deal with technological issues, deriving new analytical methods to interpret the complexity in scRNA-Seq data is just as challenging. Here, we review current state-of-the-art bioinformatics tools and methods for scRNA-Seq analysis, as well as addressing some critical analytical challenges that the field faces.",fullPaper,jv68
p1079,b48c077e8f368857a2453a5ac6a62108308dbe89,c87,European Conference on Computer Vision,Systems Bioinformatics: increasing precision of computational diagnostics and therapeutics through network-based approaches,"Abstract Systems Bioinformatics is a relatively new approach, which lies in the intersection of systems biology and classical bioinformatics. It focuses on integrating information across different levels using a bottom-up approach as in systems biology with a data-driven top-down approach as in bioinformatics. The advent of omics technologies has provided the stepping-stone for the emergence of Systems Bioinformatics. These technologies provide a spectrum of information ranging from genomics, transcriptomics and proteomics to epigenomics, pharmacogenomics, metagenomics and metabolomics. Systems Bioinformatics is the framework in which systems approaches are applied to such data, setting the level of resolution as well as the boundary of the system of interest and studying the emerging properties of the system as a whole rather than the sum of the properties derived from the system’s individual components. A key approach in Systems Bioinformatics is the construction of multiple networks representing each level of the omics spectrum and their integration in a layered network that exchanges information within and between layers. Here, we provide evidence on how Systems Bioinformatics enhances computational therapeutics and diagnostics, hence paving the way to precision medicine. The aim of this review is to familiarize the reader with the emerging field of Systems Bioinformatics and to provide a comprehensive overview of its current state-of-the-art methods and technologies. Moreover, we provide examples of success stories and case studies that utilize such methods and tools to significantly advance research in the fields of systems biology and systems medicine.",poster,cp87
p1080,75c52a70ec235ceb8a522a1e7e884c9792211d47,j153,bioRxiv,Bioinformatics core competencies for undergraduate life sciences education,"Bioinformatics is becoming increasingly central to research in the life sciences. However, despite its importance, bioinformatics skills and knowledge are not well integrated in undergraduate biology education. This curricular gap prevents biology students from harnessing the full potential of their education, limiting their career opportunities and slowing genomic research innovation. To advance the integration of bioinformatics into life sciences education, a framework of core bioinformatics competencies is needed. To that end, we here report the results of a survey of life sciences faculty in the United States about teaching bioinformatics to undergraduate life scientists. Responses were received from 1,260 faculty representing institutions in all fifty states with a combined capacity to educate hundreds of thousands of students every year. Results indicate strong, widespread agreement that bioinformatics knowledge and skills are critical for undergraduate life scientists, as well as considerable agreement about which skills are necessary. Perceptions of the importance of some skills varied with the respondent’s degree of training, time since degree earned, and/or the Carnegie classification of the respondent’s institution. To assess which skills are currently being taught, we analyzed syllabi of courses with bioinformatics content submitted by survey respondents. Finally, we used the survey results, the analysis of syllabi, and our collective research and teaching expertise to develop a set of bioinformatics core competencies for undergraduate life sciences students. These core competencies are intended to serve as a guide for institutions as they work to integrate bioinformatics into their life sciences curricula. Significance Statement Bioinformatics, an interdisciplinary field that uses techniques from computer science and mathematics to store, manage, and analyze biological data, is becoming increasingly central to modern biology research. Given the widespread use of bioinformatics and its impacts on societal problem-solving (e.g., in healthcare, agriculture, and natural resources management), there is a growing need for the integration of bioinformatics competencies into undergraduate life sciences education. Here, we present a set of bioinformatics core competencies for undergraduate life scientists developed using the results of a large national survey and the expertise of our working group of bioinformaticians and educators. We also present results from the survey on the importance of bioinformatics skills and the current state of integration of bioinformatics into biology education.",fullPaper,jv153
p1081,4332d830f827f3b59b829b523f0e130aad75baae,j192,Cancer Informatics,"Review of Current Methods, Applications, and Data Management for the Bioinformatics Analysis of Whole Exome Sequencing","The advent of next-generation sequencing technologies has greatly promoted advances in the study of human diseases at the genomic, transcriptomic, and epigenetic levels. Exome sequencing, where the coding region of the genome is captured and sequenced at a deep level, has proven to be a cost-effective method to detect disease-causing variants and discover gene targets. In this review, we outline the general framework of whole exome sequence data analysis. We focus on established bioinformatics tools and applications that support five analytical steps: raw data quality assessment, preprocessing, alignment, post-processing, and variant analysis (detection, annotation, and prioritization). We evaluate the performance of open-source alignment programs and variant calling tools using simulated and benchmark datasets, and highlight the challenges posed by the lack of concordance among variant detection tools. Based on these results, we recommend adopting multiple tools and resources to reduce false positives and increase the sensitivity of variant calling. In addition, we briefly discuss the current status and solutions for big data management, analysis, and summarization in the field of bioinformatics.",fullPaper,jv192
p1082,fbae68c4166d297cb4cc7b014906d559dfce484a,j193,GigaScience,Bioinformatics applications on Apache Spark,"Abstract With the rapid development of next-generation sequencing technology, ever-increasing quantities of genomic data pose a tremendous challenge to data processing. Therefore, there is an urgent need for highly scalable and powerful computational systems. Among the state-of–the-art parallel computing platforms, Apache Spark is a fast, general-purpose, in-memory, iterative computing framework for large-scale data processing that ensures high fault tolerance and high scalability by introducing the resilient distributed dataset abstraction. In terms of performance, Spark can be up to 100 times faster in terms of memory access and 10 times faster in terms of disk access than Hadoop. Moreover, it provides advanced application programming interfaces in Java, Scala, Python, and R. It also supports some advanced components, including Spark SQL for structured data processing, MLlib for machine learning, GraphX for computing graphs, and Spark Streaming for stream computing. We surveyed Spark-based applications used in next-generation sequencing and other biological domains, such as epigenetics, phylogeny, and drug discovery. The results of this survey are used to provide a comprehensive guideline allowing bioinformatics researchers to apply Spark in their own fields.",fullPaper,jv193
p1083,aa8cd9b6126da24d6e772a6973e82b5443537780,j194,Current Topics in Medicinal Chemistry,Bioinformatics and Drug Discovery,"Bioinformatic analysis can not only accelerate drug target identification and drug candidate screening and refinement, but also facilitate characterization of side effects and predict drug resistance. High-throughput data such as genomic, epigenetic, genome architecture, cistromic, transcriptomic, proteomic, and ribosome profiling data have all made significant contribution to mechanism-based drug discovery and drug repurposing. Accumulation of protein and RNA structures, as well as development of homology modeling and protein structure simulation, coupled with large structure databases of small molecules and metabolites, paved the way for more realistic protein-ligand docking experiments and more informative virtual screening. I present the conceptual framework that drives the collection of these high-throughput data, summarize the utility and potential of mining these data in drug discovery, outline a few inherent limitations in data and software mining these data, point out news ways to refine analysis of these diverse types of data, and highlight commonly used software and databases relevant to drug discovery.",fullPaper,jv194
p1084,a066d6337183fba874dd6f90805df9178f00d847,c69,International Conference on Parallel Processing,Designing a course model for distance-based online bioinformatics training in Africa: The H3ABioNet experience,"Africa is not unique in its need for basic bioinformatics training for individuals from a diverse range of academic backgrounds. However, particular logistical challenges in Africa, most notably access to bioinformatics expertise and internet stability, must be addressed in order to meet this need on the continent. H3ABioNet (www.h3abionet.org), the Pan African Bioinformatics Network for H3Africa, has therefore developed an innovative, free-of-charge “Introduction to Bioinformatics” course, taking these challenges into account as part of its educational efforts to provide on-site training and develop local expertise inside its network. A multiple-delivery–mode learning model was selected for this 3-month course in order to increase access to (mostly) African, expert bioinformatics trainers. The content of the course was developed to include a range of fundamental bioinformatics topics at the introductory level. For the first iteration of the course (2016), classrooms with a total of 364 enrolled participants were hosted at 20 institutions across 10 African countries. To ensure that classroom success did not depend on stable internet, trainers pre-recorded their lectures, and classrooms downloaded and watched these locally during biweekly contact sessions. The trainers were available via video conferencing to take questions during contact sessions, as well as via online “question and discussion” forums outside of contact session time. This learning model, developed for a resource-limited setting, could easily be adapted to other settings.",poster,cp69
p1085,1a8bfb3b927778f643ae612562e0b9b915f614d3,j195,Journal of B.U.ON. : official journal of the Balkan Union of Oncology,Microarray bioinformatics in cancer- a review.,"Bioinformatics is one of the newest fields of biological research, and should be viewed broadly as the use of mathematical, statistical, and computational methods for the processing and analysis of biological data. Over the last decade, the rapid growth of information and technology in both ""genomics"" and ""omics"" eras has been overwhelming for the laboratory scientists to process experimental results. Traditional gene-by-gene approaches in research are insufficient to meet the growth and demand of biological research in understanding the true biology. The massive amounts of data generated by new technologies as genomic sequencing and microarray chips make the management of data and the integration of multiple platforms of high importance; this is then followed by data analysis and interpretation to achieve biological understanding and therapeutic progress. Global views of analyzing the magnitude of information are necessary and traditional approaches to lab work have steadily been changing towards a bioinformatics era. Research is moving from being restricted to a laboratory environment to working with computers in a ""virtual lab"" environment. The present review article shall put light on this emerging field and its applicability towards cancer research.",fullPaper,jv195
p1086,13043cbc3317220e57f89b70344d9b3f63d3a347,c6,Americas Conference on Information Systems,Bioinformatics for precision oncology,"Abstract Molecular profiling of tumor biopsies plays an increasingly important role not only in cancer research, but also in the clinical management of cancer patients. Multi-omics approaches hold the promise of improving diagnostics, prognostics and personalized treatment. To deliver on this promise of precision oncology, appropriate bioinformatics methods for managing, integrating and analyzing large and complex data are necessary. Here, we discuss the specific requirements of bioinformatics methods and software that arise in the setting of clinical oncology, owing to a stricter regulatory environment and the need for rapid, highly reproducible and robust procedures. We describe the workflow of a molecular tumor board and the specific bioinformatics support that it requires, from the primary analysis of raw molecular profiling data to the automatic generation of a clinical report and its delivery to decision-making clinical oncologists. Such workflows have to various degrees been implemented in many clinical trials, as well as in molecular tumor boards at specialized cancer centers and university hospitals worldwide. We review these and more recent efforts to include other high-dimensional multi-omics patient profiles into the tumor board, as well as the state of clinical decision support software to translate molecular findings into treatment recommendations.",poster,cp6
p1087,841dbff787e715ffb5b4c6b5a9841e0a2da0f1a0,c9,Pacific Symposium on Biocomputing,The European Bioinformatics Institute in 2016: Data growth and integration,"New technologies are revolutionising biological research and its applications by making it easier and cheaper to generate ever-greater volumes and types of data. In response, the services and infrastructure of the European Bioinformatics Institute (EMBL-EBI, www.ebi.ac.uk) are continually expanding: total disk capacity increases significantly every year to keep pace with demand (75 petabytes as of December 2015), and interoperability between resources remains a strategic priority. Since 2014 we have launched two new resources: the European Variation Archive for genetic variation data and EMPIAR for two-dimensional electron microscopy data, as well as a Resource Description Framework platform. We also launched the Embassy Cloud service, which allows users to run large analyses in a virtual environment next to EMBL-EBI's vast public data resources.",poster,cp9
p1088,3174838e6064eece0d05c0a29d42d6284d594b2d,j153,bioRxiv,Reproducible bioinformatics project: a community for reproducible bioinformatics analysis pipelines,Abstract content,fullPaper,jv153
p1089,6696681640fb9c9dc5f81ea73a433f09bfe9886a,j85,BMC Bioinformatics,Developing reproducible bioinformatics analysis workflows for heterogeneous computing environments to support African genomics,Abstract content,fullPaper,jv85
p1090,262758686200e7d847e5489fff55b6a5919fcb1d,c64,Experimental Software Engineering Network,"Bioinformatics approaches, prospects and challenges of food bioactive peptide research",Abstract content,poster,cp64
p1091,8263465748cb85e0945d5c530812b87f2b962bde,j196,Microbes and Environments,Metagenomics and Bioinformatics in Microbial Ecology: Current Status and Beyond,"Metagenomic approaches are now commonly used in microbial ecology to study microbial communities in more detail, including many strains that cannot be cultivated in the laboratory. Bioinformatic analyses make it possible to mine huge metagenomic datasets and discover general patterns that govern microbial ecosystems. However, the findings of typical metagenomic and bioinformatic analyses still do not completely describe the ecology and evolution of microbes in their environments. Most analyses still depend on straightforward sequence similarity searches against reference databases. We herein review the current state of metagenomics and bioinformatics in microbial ecology and discuss future directions for the field. New techniques will allow us to go beyond routine analyses and broaden our knowledge of microbial ecosystems. We need to enrich reference databases, promote platforms that enable meta- or comprehensive analyses of diverse metagenomic datasets, devise methods that utilize long-read sequence information, and develop more powerful bioinformatic methods to analyze data from diverse perspectives.",fullPaper,jv196
p1092,dfdd9d364b6c7860fd9e2df37b7970a230a646df,j153,bioRxiv,Barriers to integration of bioinformatics into undergraduate life sciences education: A national study of US life sciences faculty uncover significant barriers to integrating bioinformatics into undergraduate instruction,"Bioinformatics, a discipline that combines aspects of biology, statistics, and computer science, is increasingly important for biological research. However, bioinformatics instruction is rarely integrated into life sciences curricula at the undergraduate level. To understand why, the Network for Integrating Bioinformatics into Life Sciences Education (NIBLSE, “nibbles”) recently undertook an extensive survey of life sciences faculty in the United States. The survey responses to open-ended questions about barriers to integration were subjected to keyword analysis. The barrier most frequently reported by the ~1,260 respondents was lack of faculty training. Faculty at associate’s-granting institutions report the least training in bioinformatics and the least integration of bioinformatics into their teaching. Faculty from underrepresented minority groups (URMs) in STEM reported training barriers at a higher rate than others, although the number of URM respondents was small. Interestingly, the cohort of faculty with the most recently awarded PhD degrees reported the most training but were teaching bioinformatics at a lower rate than faculty who earned their degrees in previous decades. Other barriers reported included lack of student interest in bioinformatics; lack of student preparation in mathematics, statistics, and computer science; already overly full curricula; and limited access to resources, including hardware, software, and vetted teaching materials. The results of the survey, the largest to date on bioinformatics education, will guide efforts to further integrate bioinformatics instruction into undergraduate life sciences education.",fullPaper,jv153
p1093,a164044fe875d7d9d23a9f569c5b2c82660add4e,j197,Bioscience Reports,Bioinformatics in translational drug discovery,"Bioinformatics approaches are becoming ever more essential in translational drug discovery both in academia and within the pharmaceutical industry. Computational exploitation of the increasing volumes of data generated during all phases of drug discovery is enabling key challenges of the process to be addressed. Here, we highlight some of the areas in which bioinformatics resources and methods are being developed to support the drug discovery pipeline. These include the creation of large data warehouses, bioinformatics algorithms to analyse ‘big data’ that identify novel drug targets and/or biomarkers, programs to assess the tractability of targets, and prediction of repositioning opportunities that use licensed drugs to treat additional indications.",fullPaper,jv197
p1094,6e1ca21d92705c43a285bfbd01391b45c6140ead,j85,BMC Bioinformatics,Proceedings of the 16th Annual UT-KBRIN Bioinformatics Summit 2016: bioinformatics,Abstract content,fullPaper,jv85
p1095,b779b41898931373f6e1f302b0a0d9a38313956a,j198,Wiley Interdisciplinary Reviews Data Mining and Knowledge Discovery,Overview of random forest methodology and practical guidance with emphasis on computational biology and bioinformatics,"The random forest (RF) algorithm by Leo Breiman has become a standard data analysis tool in bioinformatics. It has shown excellent performance in settings where the number of variables is much larger than the number of observations, can cope with complex interaction structures as well as highly correlated variables and return measures of variable importance. This paper synthesizes 10 years of RF development with emphasis on applications to bioinformatics and computational biology. Special attention is paid to practical aspects such as the selection of parameters, available RF implementations, and important pitfalls and biases of RF and its variable importance measures (VIMs). The paper surveys recent developments of the methodology relevant to bioinformatics as well as some representative examples of RF applications in this context and possible directions for future research. © 2012 Wiley Periodicals, Inc.",fullPaper,jv198
p1096,78dbf5d24f1788ec47b723904e85dc68d74d77b7,j199,Clinical Chemistry,Bioinformatics for clinical next generation sequencing.,"BACKGROUND
Next generation sequencing (NGS)-based assays continue to redefine the field of genetic testing. Owing to the complexity of the data, bioinformatics has become a necessary component in any laboratory implementing a clinical NGS test.


CONTENT
The computational components of an NGS-based work flow can be conceptualized as primary, secondary, and tertiary analytics. Each of these components addresses a necessary step in the transformation of raw data into clinically actionable knowledge. Understanding the basic concepts of these analysis steps is important in assessing and addressing the informatics needs of a molecular diagnostics laboratory. Equally critical is a familiarity with the regulatory requirements addressing the bioinformatics analyses. These and other topics are covered in this review article.


SUMMARY
Bioinformatics has become an important component in clinical laboratories generating, analyzing, maintaining, and interpreting data from molecular genetics testing. Given the rapid adoption of NGS-based clinical testing, service providers must develop informatics work flows that adhere to the rigor of clinical laboratory standards, yet are flexible to changes as the chemistry and software for analyzing sequencing data mature.",fullPaper,jv199
p1097,02f62fe778f1cdcdeb1e975f84ba054895202c26,j85,BMC Bioinformatics,"Knowledge Discovery and interactive Data Mining in Bioinformatics - State-of-the-Art, future challenges and research directions",Abstract content,fullPaper,jv85
p1098,f16757ccc0ece0955b59c8ad6f79215ec04b3ccd,j85,BMC Bioinformatics,An internet-based bioinformatics toolkit for plant biosecurity diagnosis and surveillance of viruses and viroids,Abstract content,fullPaper,jv85
p1099,d78797ea88f50994e97358fedbc9922baaffba8d,c43,ACM Symposium on Applied Computing,Bioinformatics For Geneticists,"Thank you for reading bioinformatics for geneticists. As you may know, people have search numerous times for their favorite novels like this bioinformatics for geneticists, but end up in harmful downloads. Rather than reading a good book with a cup of coffee in the afternoon, instead they are facing with some malicious virus inside their laptop. bioinformatics for geneticists is available in our book collection an online access to it is set as public so you can download it instantly. Our digital library saves in multiple countries, allowing you to get the most less latency time to download any of our books like this one. Merely said, the bioinformatics for geneticists is universally compatible with any devices to read.",poster,cp43
p1100,d78797ea88f50994e97358fedbc9922baaffba8d,c50,International Conference on Automated Software Engineering,Bioinformatics For Geneticists,"Thank you for reading bioinformatics for geneticists. As you may know, people have search numerous times for their favorite novels like this bioinformatics for geneticists, but end up in harmful downloads. Rather than reading a good book with a cup of coffee in the afternoon, instead they are facing with some malicious virus inside their laptop. bioinformatics for geneticists is available in our book collection an online access to it is set as public so you can download it instantly. Our digital library saves in multiple countries, allowing you to get the most less latency time to download any of our books like this one. Merely said, the bioinformatics for geneticists is universally compatible with any devices to read.",poster,cp50
p1101,ad02572df6c57604703ed35d6f7a161b14adf540,j175,Nature reviews genetics,Applied bioinformatics for the identification of regulatory elements,Abstract content,fullPaper,jv175
p1102,8fdfb72555c7ca65b00e0496f36c9639e19d351f,c59,British Computer Society Conference on Human-Computer Interaction,In the loop: promoter–enhancer interactions and bioinformatics,"Enhancer–promoter regulation is a fundamental mechanism underlying differential transcriptional regulation. Spatial chromatin organization brings remote enhancers in contact with target promoters in cis to regulate gene expression. There is considerable evidence for promoter–enhancer interactions (PEIs). In the recent years, genome-wide analyses have identified signatures and mapped novel enhancers; however, being able to precisely identify their target gene(s) requires massive biological and bioinformatics efforts. In this review, we give a short overview of the chromatin landscape and transcriptional regulation. We discuss some key concepts and problems related to chromatin interaction detection technologies, and emerging knowledge from genome-wide chromatin interaction data sets. Then, we critically review different types of bioinformatics analysis methods and tools related to representation and visualization of PEI data, raw data processing and PEI prediction. Lastly, we provide specific examples of how PEIs have been used to elucidate a functional role of non-coding single-nucleotide polymorphisms. The topic is at the forefront of epigenetic research, and by highlighting some future bioinformatics challenges in the field, this review provides a comprehensive background for future PEI studies.",poster,cp59
p1103,81cc8e468ef4fc555b5bb89697f587b2e2969f97,j200,Current Protocols in Human Genetics,Getting Started with Microbiome Analysis: Sample Acquisition to Bioinformatics,"Historically, in order to study microbes, it was necessary to grow them in the laboratory. It was clear though that many microbe communities were refractory to study because none of the members could be grown outside of their native habitat. The development of culture‐independent methods to study microbiota using high‐throughput sequencing of the 16S ribosomal RNA gene variable regions present in all prokaryotic organisms has provided new opportunities to investigate complex microbial communities. In this unit, the process for a microbiome analysis is described. Many of the components required for this process may already exist. A pipeline is described for acquisition of samples from different sites on the human body, isolation of microbial DNA, and DNA sequencing using the Illumina MiSeq sequencing platform. Finally, a new analytical workflow for basic bioinformatics data analysis, QWRAP, is described, which can be used by clinical and basic science investigators. Curr. Protoc. Hum. Genet. 82:18.8.1‐18.8.29. © 2014 by John Wiley & Sons, Inc.",fullPaper,jv200
p1104,3a3256ab94a4af91c2ddaf2388e56d2dcaa4e5d6,c28,International Conference on Collaboration Technologies and Systems,Big Data Analytics in Bioinformatics: A Machine Learning Perspective,"Bioinformatics research is characterized by voluminous and incremental datasets and complex data analytics methods. The machine learning methods used in bioinformatics are iterative and parallel. These methods can be scaled to handle big data using the distributed and parallel computing technologies. 
Usually big data tools perform computation in batch-mode and are not optimized for iterative processing and high data dependency among operations. In the recent years, parallel, incremental, and multi-view machine learning algorithms have been proposed. Similarly, graph-based architectures and in-memory big data tools have been developed to minimize I/O cost and optimize iterative processing. 
However, there lack standard big data architectures and tools for many important bioinformatics problems, such as fast construction of co-expression and regulatory networks and salient module identification, detection of complexes over growing protein-protein interaction data, fast analysis of massive DNA, RNA, and protein sequence data, and fast querying on incremental and heterogeneous disease networks. This paper addresses the issues and challenges posed by several big data problems in bioinformatics, and gives an overview of the state of the art and the future research opportunities.",poster,cp28
p1105,71d7233f4e490dcd1e221cf0d4b7584a6806b6b2,c36,Conference on Software Engineering Education and Training,A selective review of robust variable selection with applications in bioinformatics,"A drastic amount of data have been and are being generated in bioinformatics studies. In the analysis of such data, the standard modeling approaches can be challenged by the heavy-tailed errors and outliers in response variables, the contamination in predictors (which may be caused by, for instance, technical problems in microarray gene expression studies), model mis-specification and others. Robust methods are needed to tackle these challenges. When there are a large number of predictors, variable selection can be as important as estimation. As a generic variable selection and regularization tool, penalization has been extensively adopted. In this article, we provide a selective review of robust penalized variable selection approaches especially designed for high-dimensional data from bioinformatics and biomedical studies. We discuss the robust loss functions, penalty functions and computational algorithms. The theoretical properties and implementation are also briefly examined. Application examples of the robust penalization approaches in representative bioinformatics and biomedical studies are also illustrated.",poster,cp36
p1106,f994acdaa9282f28fcad091fbe9aa16f0c2bad50,j102,Nucleic Acids Research,Design and bioinformatics analysis of genome-wide CLIP experiments,"The past decades have witnessed a surge of discoveries revealing RNA regulation as a central player in cellular processes. RNAs are regulated by RNA-binding proteins (RBPs) at all post-transcriptional stages, including splicing, transportation, stabilization and translation. Defects in the functions of these RBPs underlie a broad spectrum of human pathologies. Systematic identification of RBP functional targets is among the key biomedical research questions and provides a new direction for drug discovery. The advent of cross-linking immunoprecipitation coupled with high-throughput sequencing (genome-wide CLIP) technology has recently enabled the investigation of genome-wide RBP–RNA binding at single base-pair resolution. This technology has evolved through the development of three distinct versions: HITS-CLIP, PAR-CLIP and iCLIP. Meanwhile, numerous bioinformatics pipelines for handling the genome-wide CLIP data have also been developed. In this review, we discuss the genome-wide CLIP technology and focus on bioinformatics analysis. Specifically, we compare the strengths and weaknesses, as well as the scopes, of various bioinformatics tools. To assist readers in choosing optimal procedures for their analysis, we also review experimental design and procedures that affect bioinformatics analyses.",fullPaper,jv102
p1107,8b1b5d208cc81fa7a194a843b8140f8f5d45ecb5,c114,IEEE International Conference on Robotics and Automation,"EDAM: an ontology of bioinformatics operations, types of data and identifiers, topics and formats","Motivation: Advancing the search, publication and integration of bioinformatics tools and resources demands consistent machine-understandable descriptions. A comprehensive ontology allowing such descriptions is therefore required. Results: EDAM is an ontology of bioinformatics operations (tool or workflow functions), types of data and identifiers, application domains and data formats. EDAM supports semantic annotation of diverse entities such as Web services, databases, programmatic libraries, standalone tools, interactive applications, data schemas, datasets and publications within bioinformatics. EDAM applies to organizing and finding suitable tools and data and to automating their integration into complex applications or workflows. It includes over 2200 defined concepts and has successfully been used for annotations and implementations. Availability: The latest stable version of EDAM is available in OWL format from http://edamontology.org/EDAM.owl and in OBO format from http://edamontology.org/EDAM.obo. It can be viewed online at the NCBO BioPortal and the EBI Ontology Lookup Service. For documentation and license please refer to http://edamontology.org. This article describes version 1.2 available at http://edamontology.org/EDAM_1.2.owl. Contact: jison@ebi.ac.uk",poster,cp114
p1108,d05cf07b2cb746f812a7268dc842ff23bd38f928,c59,British Computer Society Conference on Human-Computer Interaction,Bioinformatics programs are 31-fold over-represented among the highest impact scientific papers of the past two decades,"MOTIVATION
To analyze the relative proportion of bioinformatics papers and their non-bioinformatics counterparts in the top 20 most cited papers annually for the past two decades.


RESULTS
When defining bioinformatics papers as encompassing both those that provide software for data analysis or methods underlying data analysis software, we find that over the past two decades, more than a third (34%) of the most cited papers in science were bioinformatics papers, which is approximately a 31-fold enrichment relative to the total number of bioinformatics papers published. More than half of the most cited papers during this span were bioinformatics papers. Yet, the average 5-year JIF of top 20 bioinformatics papers was 7.7, whereas the average JIF for top 20 non-bioinformatics papers was 25.8, significantly higher (P < 4.5 × 10(-29)). The 20-year trend in the average JIF between the two groups suggests the gap does not appear to be significantly narrowing. For a sampling of the journals producing top papers, bioinformatics journals tended to have higher Gini coefficients, suggesting that development of novel bioinformatics resources may be somewhat 'hit or miss'. That is, relative to other fields, bioinformatics produces some programs that are extremely widely adopted and cited, yet there are fewer of intermediate success.


CONTACT
jdwren@gmail.com


SUPPLEMENTARY INFORMATION
Supplementary data are available at Bioinformatics online.",poster,cp59
p1109,ae2164e0358f21a22f47d01a58745f09ac0c4dea,j201,Network Modeling Analysis in Health Informatics and Bioinformatics,"Big data analytics in bioinformatics: architectures, techniques, tools and issues",Abstract content,fullPaper,jv201
p1110,1c2ee9858ac6ebbfbb21f8a4a8f8ac8d281b97e1,j202,Journal of Cellular Physiology,Big Data Bioinformatics,"Recent technological advances allow for high throughput profiling of biological systems in a cost‐efficient manner. The low cost of data generation is leading us to the “big data” era. The availability of big data provides unprecedented opportunities but also raises new challenges for data mining and analysis. In this review, we introduce key concepts in the analysis of big data, including both “machine learning” algorithms as well as “unsupervised” and “supervised” examples of each. We note packages for the R programming language that are available to perform machine learning analyses. In addition to programming based solutions, we review webservers that allow users with limited or no programming background to perform these analyses on large data compendia. J. Cell. Physiol. 229: 1896–1900, 2014. © 2014 Wiley Periodicals, Inc.",fullPaper,jv202
p1111,f3940978ecf60c554b654bd1afbb3ab03f660ff8,c66,Annual Conference on Innovation and Technology in Computer Science Education,Bioinformatics Curriculum Guidelines: Toward a Definition of Core Competencies,"Rapid advances in the life sciences and in related information technologies necessitate the ongoing refinement of bioinformatics educational programs in order to maintain their relevance. As the discipline of bioinformatics and computational biology expands and matures, it is important to characterize the elements that contribute to the success of professionals in this field. These individuals work in a wide variety of settings, including bioinformatics core facilities, biological and medical research laboratories, software development organizations, pharmaceutical and instrument development companies, and institutions that provide education, service, and training. In response to this need, the Curriculum Task Force of the International Society for Computational Biology (ISCB) Education Committee seeks to define curricular guidelines for those who train and educate bioinformaticians. The previous report of the task force summarized a survey that was conducted to gather input regarding the skill set needed by bioinformaticians [1]. The current article details a subsequent effort, wherein the task force broadened its perspectives by examining bioinformatics career opportunities, surveying directors of bioinformatics core facilities, and reviewing bioinformatics education programs. 
 
The bioinformatics literature provides valuable perspectives on bioinformatics education by defining skill sets needed by bioinformaticians, presenting approaches for providing informatics training to biologists, and discussing the roles of bioinformatics core facilities in training and education. 
 
The skill sets required for success in the field of bioinformatics are considered by several authors: Altman [2] defines five broad areas of competency and lists key technologies; Ranganathan [3] presents highlights from the Workshops on Education in Bioinformatics, discussing challenges and possible solutions; Yale's interdepartmental PhD program in computational biology and bioinformatics is described in [4], which lists the general areas of knowledge of bioinformatics; in a related article, a graduate of Yale's PhD program reflects on the skills needed by a bioinformatician [5]; Altman and Klein [6] describe the Stanford Biomedical Informatics (BMI) Training Program, presenting observed trends among BMI students; the American Medical Informatics Association defines competencies in the related field of biomedical informatics in [7]; and the approaches used in several German universities to implement bioinformatics education are described in [8]. 
 
Several approaches to providing bioinformatics training for biologists are described in the literature. Tan et al. [9] report on workshops conducted to identify a minimum skill set for biologists to be able to address the informatics challenges of the “-omics” era. They define a requisite skill set by analyzing responses to questions about the knowledge, skills, and abilities that biologists should possess. The authors in [10] present examples of strategies and methods for incorporating bioinformatics content into undergraduate life sciences curricula. Pevzner and Shamir [11] propose that undergraduate biology curricula should contain an additional course, “Algorithmic, Mathematical, and Statistical Concepts in Biology.” Wingren and Botstein [12] present a graduate course in quantitative biology that is based on original, pathbreaking papers in diverse areas of biology. Johnson and Friedman [13] evaluate the effectiveness of incorporating biological informatics into a clinical informatics program. The results reported are based on interviews of four students and informal assessments of bioinformatics faculty. 
 
The challenges and opportunities relevant to training and education in the context of bioinformatics core facilities are discussed by Lewitter et al. [14]. Relatedly, Lewitter and Rebhan [15] provide guidance regarding the role of a bioinformatics core facility in hiring biologists and in furthering their education in bioinformatics. Richter and Sexton [16] describe a need for highly trained bioinformaticians in core facilities and provide a list of requisite skills. Similarly, Kallioniemi et al. [17] highlight the roles of bioinformatics core units in education and training. 
 
This manuscript expands the body of knowledge pertaining to bioinformatics curriculum guidelines by presenting the results from a broad set of surveys (of core facility directors, of career opportunities, and of existing curricula). Although there is some overlap in the findings of the surveys, they are reported separately, in order to avoid masking the unique aspects of each of the perspectives and to demonstrate that the same themes arise, even when different perspectives are considered. The authors derive from their surveys an initial set of core competencies and relate the competencies to three different categories of professions that have a need for bioinformatics training.",poster,cp66
p1112,001eeb49fd35bc035d53da5575ff41e2752d0f32,j203,Natural product reports (Print),Chemo- and bioinformatics resources for in silico drug discovery from medicinal plants beyond their traditional use: a critical review.,"In silico approaches have been widely recognised to be useful for drug discovery. Here, we consider the significance of available databases of medicinal plants and chemo- and bioinformatics tools for in silico drug discovery beyond the traditional use of folk medicines. This review contains a practical example of the application of combined chemo- and bioinformatics methods to study pleiotropic therapeutic effects (known and novel) of 50 medicinal plants from Traditional Indian Medicine.",fullPaper,jv203
p1113,50effa434362bba823d61ad1b24f1c3fb917685c,c98,North American Chapter of the Association for Computational Linguistics,From protein structure to function with bioinformatics,Abstract content,poster,cp98
p1114,5272f607ad45e5617168b9209b541dbc7f29ad57,c58,Australian Software Engineering Conference,Modern bioinformatics meets traditional Chinese medicine,"MOTIVATION
Traditional Chinese medicine (TCM) is gaining increasing attention with the emergence of integrative medicine and personalized medicine, characterized by pattern differentiation on individual variance and treatments based on natural herbal synergism. Investigating the effectiveness and safety of the potential mechanisms of TCM and the combination principles of drug therapies will bridge the cultural gap with Western medicine and improve the development of integrative medicine. Dealing with rapidly growing amounts of biomedical data and their heterogeneous nature are two important tasks among modern biomedical communities. Bioinformatics, as an emerging interdisciplinary field of computer science and biology, has become a useful tool for easing the data deluge pressure by automating the computation processes with informatics methods. Using these methods to retrieve, store and analyze the biomedical data can effectively reveal the associated knowledge hidden in the data, and thus promote the discovery of integrated information. Recently, these techniques of bioinformatics have been used for facilitating the interactional effects of both Western medicine and TCM. The analysis of TCM data using computational technologies provides biological evidence for the basic understanding of TCM mechanisms, safety and efficacy of TCM treatments. At the same time, the carrier and targets associated with TCM remedies can inspire the rethinking of modern drug development. This review summarizes the significant achievements of applying bioinformatics techniques to many aspects of the research in TCM, such as analysis of TCM-related '-omics' data and techniques for analyzing biological processes and pharmaceutical mechanisms of TCM, which have shown certain potential of bringing new thoughts to both sides.",poster,cp58
p1115,dc25529946abf23ce357d828a750ea99d8c36483,c11,Hawaii International Conference on System Sciences,"GOBLET: The Global Organisation for Bioinformatics Learning, Education and Training","In recent years, high-throughput technologies have brought big data to the life sciences. The march of progress has been rapid, leaving in its wake a demand for courses in data analysis, data stewardship, computing fundamentals, etc., a need that universities have not yet been able to satisfy—paradoxically, many are actually closing “niche” bioinformatics courses at a time of critical need. The impact of this is being felt across continents, as many students and early-stage researchers are being left without appropriate skills to manage, analyse, and interpret their data with confidence. This situation has galvanised a group of scientists to address the problems on an international scale. For the first time, bioinformatics educators and trainers across the globe have come together to address common needs, rising above institutional and international boundaries to cooperate in sharing bioinformatics training expertise, experience, and resources, aiming to put ad hoc training practices on a more professional footing for the benefit of all.",poster,cp11
p1116,26b3223735e316901bdadb831f012ab67b0456fe,j108,PLoS ONE,Genomics Virtual Laboratory: A Practical Bioinformatics Workbench for the Cloud,"Background Analyzing high throughput genomics data is a complex and compute intensive task, generally requiring numerous software tools and large reference data sets, tied together in successive stages of data transformation and visualisation. A computational platform enabling best practice genomics analysis ideally meets a number of requirements, including: a wide range of analysis and visualisation tools, closely linked to large user and reference data sets; workflow platform(s) enabling accessible, reproducible, portable analyses, through a flexible set of interfaces; highly available, scalable computational resources; and flexibility and versatility in the use of these resources to meet demands and expertise of a variety of users. Access to an appropriate computational platform can be a significant barrier to researchers, as establishing such a platform requires a large upfront investment in hardware, experience, and expertise. Results We designed and implemented the Genomics Virtual Laboratory (GVL) as a middleware layer of machine images, cloud management tools, and online services that enable researchers to build arbitrarily sized compute clusters on demand, pre-populated with fully configured bioinformatics tools, reference datasets and workflow and visualisation options. The platform is flexible in that users can conduct analyses through web-based (Galaxy, RStudio, IPython Notebook) or command-line interfaces, and add/remove compute nodes and data resources as required. Best-practice tutorials and protocols provide a path from introductory training to practice. The GVL is available on the OpenStack-based Australian Research Cloud (http://nectar.org.au) and the Amazon Web Services cloud. The principles, implementation and build process are designed to be cloud-agnostic. Conclusions This paper provides a blueprint for the design and implementation of a cloud-based Genomics Virtual Laboratory. We discuss scope, design considerations and technical and logistical constraints, and explore the value added to the research community through the suite of services and resources provided by our implementation.",fullPaper,jv108
p1117,9133a61f001b4cc59036b967c5bbaeba7486f06f,c17,International Conference on Enterprise Information Systems,Survey of MapReduce frame operation in bioinformatics,"Bioinformatics is challenged by the fact that traditional analysis tools have difficulty in processing large-scale data from high-throughput sequencing. The open source Apache Hadoop project, which adopts the MapReduce framework and a distributed file system, has recently given bioinformatics researchers an opportunity to achieve scalable, efficient and reliable computing performance on Linux clusters and on cloud computing services. In this article, we present MapReduce frame-based applications that can be employed in the next-generation sequencing and other biological domains. In addition, we discuss the challenges faced by this field as well as the future works on parallel computing in bioinformatics.",poster,cp17
p1118,995bbef8ff7bbbb7d994e3bfeed1ac169356db5f,c33,International Conference on Agile Software Development,Survey of Natural Language Processing Techniques in Bioinformatics,"Informatics methods, such as text mining and natural language processing, are always involved in bioinformatics research. In this study, we discuss text mining and natural language processing methods in bioinformatics from two perspectives. First, we aim to search for knowledge on biology, retrieve references using text mining methods, and reconstruct databases. For example, protein-protein interactions and gene-disease relationship can be mined from PubMed. Then, we analyze the applications of text mining and natural language processing techniques in bioinformatics, including predicting protein structure and function, detecting noncoding RNA. Finally, numerous methods and applications, as well as their contributions to bioinformatics, are discussed for future use by text mining and natural language processing researchers.",poster,cp33
p1119,c273e7be5e2c9a316efbd01687852bc3af364455,c5,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,Bioinformatics in Africa: The Rise of Ghana?,"Until recently, bioinformatics, an important discipline in the biological sciences, was largely limited to countries with advanced scientific resources. Nonetheless, several developing countries have lately been making progress in bioinformatics training and applications. In Africa, leading countries in the discipline include South Africa, Nigeria, and Kenya. However, one country that is less known when it comes to bioinformatics is Ghana. Here, I provide a first description of the development of bioinformatics activities in Ghana and how these activities contribute to the overall development of the discipline in Africa. Over the past decade, scientists in Ghana have been involved in publications incorporating bioinformatics analyses, aimed at addressing research questions in biomedical science and agriculture. Scarce research funding and inadequate training opportunities are some of the challenges that need to be addressed for Ghanaian scientists to continue developing their expertise in bioinformatics.",poster,cp5
p1120,c03fd5d4d6521c6398b73ff242d97fff4630aac4,c4,Annual Conference on Genetic and Evolutionary Computation,Bioinformatics Education—Perspectives and Challenges out of Africa,"The discipline of bioinformatics has developed rapidly since the complete sequencing of the first genomes in the 1990s. The development of many high-throughput techniques during the last decades has ensured that bioinformatics has grown into a discipline that overlaps with, and is required for, the modern practice of virtually every field in the life sciences. This has placed a scientific premium on the availability of skilled bioinformaticians, a qualification that is extremely scarce on the African continent. The reasons for this are numerous, although the absence of a skilled bioinformatician at academic institutions to initiate a training process and build sustained capacity seems to be a common African shortcoming. This dearth of bioinformatics expertise has had a knock-on effect on the establishment of many modern high-throughput projects at African institutes, including the comprehensive and systematic analysis of genomes from African populations, which are among the most genetically diverse anywhere on the planet. Recent funding initiatives from the National Institutes of Health and the Wellcome Trust are aimed at ameliorating this shortcoming. In this paper, we discuss the problems that have limited the establishment of the bioinformatics field in Africa, as well as propose specific actions that will help with the education and training of bioinformaticians on the continent. This is an absolute requirement in anticipation of a boom in high-throughput approaches to human health issues unique to data from African populations.",poster,cp4
p1121,cd238467fdd77d04902d1ceafc9d4b590093f659,j204,CBE - Life Sciences Education,A Survey of Scholarly Literature Describing the Field of Bioinformatics Education and Bioinformatics Educational Research,"This article provides an overview of the state of research in bioinformatics education in the years 1998 through 2013. It identifies current curricular approaches for integrating bioinformatics education, concepts and skills being taught, pedagogical approaches and methods of delivery, and educational research and evaluation results.",fullPaper,jv204
p1122,f09db1d5196ee5a18f583ac2e5f66c223ca0360e,c2,International Symposium on Intelligent Data Analysis,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"This special issue of the IEEE/ACM Transactions on Computational Biology and Bioinformatic includes a selection of papers presented at the Seventh Brazilian Symposium on Bioinformatics (BSB 2012) held 15-17 August 2012 in Campo Grande (Mato Grosso do Sul), Brazil. BSB is an international symposium that covers all aspects of bioinformatics and computational biology. The symposium is organized by the special interest group in Computational Biology of the Brazilian Computer Society (SBC).",poster,cp2
p1123,cabf20b781705bc68322972385ab2c6a4dc6b871,j85,BMC Bioinformatics,Verification and validation of bioinformatics software without a gold standard: a case study of BWA and Bowtie,Abstract content,fullPaper,jv85
p1124,638342beccc1922285c81499e3e67fb65b32d6e8,c89,Conference on Uncertainty in Artificial Intelligence,Random Forest for Bioinformatics,Abstract content,poster,cp89
p1125,7b4c5f3a1fdcc582f63796d238cfef3ef0b73192,j205,Nature Chemical Biology,A widespread self-cleaving ribozyme class is revealed by bioinformatics,Abstract content,fullPaper,jv205
p1126,03633f1e781670a023fb2cdc7e3cb0da6fd5591a,c6,Americas Conference on Information Systems,Challenges in RNA virus bioinformatics,"Abstract Motivation: Computer-assisted studies of structure, function and evolution of viruses remains a neglected area of research. The attention of bioinformaticians to this interesting and challenging field is far from commensurate with its medical and biotechnological importance. It is telling that out of >200 talks held at ISMB 2013, the largest international bioinformatics conference, only one presentation explicitly dealt with viruses. In contrast to many broad, established and well-organized bioinformatics communities (e.g. structural genomics, ontologies, next-generation sequencing, expression analysis), research groups focusing on viruses can probably be counted on the fingers of two hands. Results: The purpose of this review is to increase awareness among bioinformatics researchers about the pressing needs and unsolved problems of computational virology. We focus primarily on RNA viruses that pose problems to many standard bioinformatics analyses owing to their compact genome organization, fast mutation rate and low evolutionary conservation. We provide an overview of tools and algorithms for handling viral sequencing data, detecting functionally important RNA structures, classifying viral proteins into families and investigating the origin and evolution of viruses. Contact: manja@uni-jena.de Supplementary information: Supplementary data are available at Bioinformatics online. The references for this article can be found in the Supplementary Material.",poster,cp6
p1127,166334c026c49f557ecca04d47c17b5e54acb804,c100,ACM SIGMOD Conference,Buying in to bioinformatics: an introduction to commercial sequence analysis software,"Advancements in high-throughput nucleotide sequencing techniques have brought with them state-of-the-art bioinformatics programs and software packages. Given the importance of molecular sequence data in contemporary life science research, these software suites are becoming an essential component of many labs and classrooms, and as such are frequently designed for non-computer specialists and marketed as one-stop bioinformatics toolkits. Although beautifully designed and powerful, user-friendly bioinformatics packages can be expensive and, as more arrive on the market each year, it can be difficult for researchers, teachers and students to choose the right software for their needs, especially if they do not have a bioinformatics background. This review highlights some of the currently available and most popular commercial bioinformatics packages, discussing their prices, usability, features and suitability for teaching. Although several commercial bioinformatics programs are arguably overpriced and overhyped, many are well designed, sophisticated and, in my opinion, worth the investment. If you are just beginning your foray into molecular sequence analysis or an experienced genomicist, I encourage you to explore proprietary software bundles. They have the potential to streamline your research, increase your productivity, energize your classroom and, if anything, add a bit of zest to the often dry detached world of bioinformatics.",poster,cp100
p1128,cb3ee9880c252c23f2b473d4055f26df2a6f75a9,j85,BMC Bioinformatics,MEIGO: an open-source software suite based on metaheuristics for global optimization in systems biology and bioinformatics,Abstract content,fullPaper,jv85
p1129,4c0f1b81e4c53e3ae0bf3a7dad47fc26146a203a,c84,The Web Conference,An explosion of bioinformatics careers,"Big data is everywhere, and its influence and practical omnipresence across multiple industries will just continue to grow. For life scientists with expertise and an interest in bioinformatics, computer science, statistics, and related skill sets, the job outlook couldn’t be rosier. Big pharma, biotech, and software companies are clamoring to hire professionals with experience in bioinformatics and the identification, compilation, analysis, and visualization of huge amounts of biological and health care information. With the rapid development of new tools to make sense of life science research and outcomes, spurred by innovative research in bioinformatics itself, scientists who are entranced by data can pursue more career options than ever before. Read the Feature (Full-Text HTML) Read the Feature (PDF)",poster,cp84
p1130,6901570083ecd16ceaf296de88819f0105d309aa,j206,Analytical Chemistry,Bioinformatics: The Next Frontier of Metabolomics,"Bioinformatic tools are required to carry out essential functions such as statistical analyses and database functionalities. Now, they are also needed for one of the most difficult tasks, helping researchers decide which metabolites are the most biologically meaningful. This can be achieved through aiding the identification process, reducing feature redundancy, putting forward better candidates for tandem mass spectrometry (MS/MS), speeding up or automating the workflow, deconvolving the feature list through meta-analysis or multigroup analysis, or using stable isotopes and pathway mapping. This review thus focuses on the most recent and innovative bioinformatic advancements for identifying metabolites. 
 
A primary objective of metabolomics beyond biomarker discovery is to identify the most meaningful metabolites that correlate with disease pathogenesis or other perturbations of metabolism. Metabolites play important roles in biological pathways; their flux or differential regulation (dysregulation) can reveal novel insights into disease and environmental influences. Therefore, one of the most important goals of metabolomic analysis has been to assign metabolite identity so they can be used for further statistical and informed pathway analysis.1,2 Over the past few years, technologies for analyzing metabolites by untargeted or targeted metabolomics have undergone extensive improvements. Strides to establish the most efficient protocols for experimental design, sample extraction techniques, and data acquisition have paid off providing robust complex data sets.3−9 As more is being required of these data sets such as assigning identity and biological meaning to the features, bioinformatics is the area of metabolomics which is currently undergoing the most needed growth. 
 
It is often the case that metabolomic analysis results in a list of metabolites with low specificity for the disease or stimulus being studied (Figure ​(Figure1).1). Some of these metabolites seem to be dysregulated in a variety of diseases such as acylcarnitines10−13 and fatty acids.14−17 They may be more indicative of a perturbed systemic cause (appetite, physical activity, diurnal rhythm changes, etc..), sample contamination, or instrumental/bioinformatic noise, rather than a specific biomarker of disease. An example of this can be seen in the analysis of urinary biomarkers of ionizing radiation, where dicarboxylic acids were downregulated in the rat after radiation exposure. It was proven that this observation was actually caused by a decreased appetite after radiation exposure perturbing the β-oxidation pathway and not from radiation-induced cellular changes.18,19 Furthermore, dicarboxylic acids can leach out from plastics during the extraction process, further adding to the ambiguity of their role in ionizing radiation.20 
 
 
 
Figure 1 
 
Biomarkers that have high vs low disease specificity. 
 
 
 
As well as identifying the correct source of the biomarkers, it is also important to identify their physiological role and how to utilize them as therapeutic targets. This first has to start with the identification of the metabolite and is determined by filtering thresholds set by the user which is intrinsically biased. These thresholds include those for fold change and p-value, which are highly dependent on the experiment; in vitro experiments would exhibit lower variation between biological replicates than in vivo. The ease of identifying the metabolite is also determined by its concentration in the sample and previous annotation in metabolite databases. Filtering thresholds for metabolite intensity that are set too high may omit important biologically meaningful metabolites rather than noise. Furthermore, a metabolite that is novel or not curated in a database may not be taken into consideration based on the chemical knowledge of the researcher and what they deem as meaningful. 
 
In order to transform the complex list of identified metabolites into markers of disease, or assign what role they play, bioinformatic tools can aid in identifying the potential pathways that the metabolite may belong to. It is then that the researcher can use this knowledge surrounding the biology of the metabolite to probe the mechanism of the disease. Untargeted metabolomics has already been used in such a manner to find the source of neuropathic pain.21N,N-Dimethylsphingosine was dysregulated in a rat model of neuropathic pain, furthermore when dosed to control rats it induced mechanical hypersensitivity. This metabolite implicated the sphingomyelin-ceramide pathway as a potential therapeutic target. Antimetabolite inhibitors of enzymes in this pathway were tested and were able to ameliorate neuropathic pain (unpublished data). This study holds promise for other metabolomic studies to maximize the potential information contained within the data for finding therapeutics of disease rather than only providing lists of dysregulated metabolites.",fullPaper,jv206
p1131,4fb83e7c610fbaac17930e365a3faffe695c5628,j207,Biological Procedures Online,Integration of bioinformatics to biodegradation,Abstract content,fullPaper,jv207
p1132,1c0a72198b638f458f1f1ea0ebae05bef5ce005b,c46,Brazilian Symposium on Software Engineering,Crowdsourcing for bioinformatics,"MOTIVATION
Bioinformatics is faced with a variety of problems that require human involvement. Tasks like genome annotation, image analysis, knowledge-base population and protein structure determination all benefit from human input. In some cases, people are needed in vast quantities, whereas in others, we need just a few with rare abilities. Crowdsourcing encompasses an emerging collection of approaches for harnessing such distributed human intelligence. Recently, the bioinformatics community has begun to apply crowdsourcing in a variety of contexts, yet few resources are available that describe how these human-powered systems work and how to use them effectively in scientific domains.


RESULTS
Here, we provide a framework for understanding and applying several different types of crowdsourcing. The framework considers two broad classes: systems for solving large-volume 'microtasks' and systems for solving high-difficulty 'megatasks'. Within these classes, we discuss system types, including volunteer labor, games with a purpose, microtask markets and open innovation contests. We illustrate each system type with successful examples in bioinformatics and conclude with a guide for matching problems to crowdsourcing solutions that highlights the positives and negatives of different approaches.",poster,cp46
p1133,9ae641778971fd6e0bdf3755ef4658d40c218def,c56,European Conference on Software Process Improvement,Flow Cytometry Bioinformatics,"Flow cytometry bioinformatics is the application of bioinformatics to flow cytometry data, which involves storing, retrieving, organizing, and analyzing flow cytometry data using extensive computational resources and tools. Flow cytometry bioinformatics requires extensive use of and contributes to the development of techniques from computational statistics and machine learning. Flow cytometry and related methods allow the quantification of multiple independent biomarkers on large numbers of single cells. The rapid growth in the multidimensionality and throughput of flow cytometry data, particularly in the 2000s, has led to the creation of a variety of computational analysis methods, data standards, and public databases for the sharing of results. Computational methods exist to assist in the preprocessing of flow cytometry data, identifying cell populations within it, matching those cell populations across samples, and performing diagnosis and discovery using the results of previous steps. For preprocessing, this includes compensating for spectral overlap, transforming data onto scales conducive to visualization and analysis, assessing data for quality, and normalizing data across samples and experiments. For population identification, tools are available to aid traditional manual identification of populations in two-dimensional scatter plots (gating), to use dimensionality reduction to aid gating, and to find populations automatically in higher dimensional space in a variety of ways. It is also possible to characterize data in more comprehensive ways, such as the density-guided binary space partitioning technique known as probability binning, or by combinatorial gating. Finally, diagnosis using flow cytometry data can be aided by supervised learning techniques, and discovery of new cell types of biological importance by high-throughput statistical methods, as part of pipelines incorporating all of the aforementioned methods. Open standards, data, and software are also key parts of flow cytometry bioinformatics. Data standards include the widely adopted Flow Cytometry Standard (FCS) defining how data from cytometers should be stored, but also several new standards under development by the International Society for Advancement of Cytometry (ISAC) to aid in storing more detailed information about experimental design and analytical steps. Open data is slowly growing with the opening of the CytoBank database in 2010 and FlowRepository in 2012, both of which allow users to freely distribute their data, and the latter of which has been recommended as the preferred repository for MIFlowCyt-compliant data by ISAC. Open software is most widely available in the form of a suite of Bioconductor packages, but is also available for web execution on the GenePattern platform.",poster,cp56
p1134,b3f6b5d741c028fad39a692ee40dfd613ed36089,c113,International Conference on Image Analysis and Processing,A primer to frequent itemset mining for bioinformatics,"Over the past two decades, pattern mining techniques have become an integral part of many bioinformatics solutions. Frequent itemset mining is a popular group of pattern mining techniques designed to identify elements that frequently co-occur. An archetypical example is the identification of products that often end up together in the same shopping basket in supermarket transactions. A number of algorithms have been developed to address variations of this computationally non-trivial problem. Frequent itemset mining techniques are able to efficiently capture the characteristics of (complex) data and succinctly summarize it. Owing to these and other interesting properties, these techniques have proven their value in biological data analysis. Nevertheless, information about the bioinformatics applications of these techniques remains scattered. In this primer, we introduce frequent itemset mining and their derived association rules for life scientists. We give an overview of various algorithms, and illustrate how they can be used in several real-life bioinformatics application domains. We end with a discussion of the future potential and open challenges for frequent itemset mining in the life sciences.",poster,cp113
p1135,d07fea283ad69996b9d2b553e23dc61b872f18d6,j208,Influenza and Other Respiratory Viruses,Influenza Research Database: an integrated bioinformatics resource for influenza research and surveillance,"Please cite this paper as: Squires et al. (2012) Influenza research database: an integrated bioinformatics resource for influenza research and surveillance. Influenza and Other Respiratory Viruses 6(6), 404–416.",fullPaper,jv208
p1136,5961e5a906a614679356f348caffff5047cb9117,c70,International Conference on Intelligent Robotics and Applications,An Overview of Multiple Sequence Alignments and Cloud Computing in Bioinformatics,"Multiple sequence alignment (MSA) of DNA, RNA, and protein sequences is one of the most essential techniques in the fields of molecular biology, computational biology, and bioinformatics. Next-generation sequencing technologies are changing the biology landscape, flooding the databases with massive amounts of raw sequence data. MSA of ever-increasing sequence data sets is becoming a significant bottleneck. In order to realise the promise of MSA for large-scale sequence data sets, it is necessary for existing MSA algorithms to be run in a parallelised fashion with the sequence data distributed over a computing cluster or server farm. Combining MSA algorithms with cloud computing technologies is therefore likely to improve the speed, quality, and capability for MSA to handle large numbers of sequences. 
In this review, multiple sequence alignments are discussed, with a specific focus on the ClustalW and Clustal Omega algorithms. Cloud computing technologies and concepts are outlined, and the next generation of cloud base MSA algorithms is introduced.",poster,cp70
p1137,83e21271aa794fd03ec49c059ec6fb831307a3b5,c62,International Conference on Software Reuse,ViPR: an open bioinformatics database and analysis resource for virology research,"The Virus Pathogen Database and Analysis Resource (ViPR, www.ViPRbrc.org) is an integrated repository of data and analysis tools for multiple virus families, supported by the National Institute of Allergy and Infectious Diseases (NIAID) Bioinformatics Resource Centers (BRC) program. ViPR contains information for human pathogenic viruses belonging to the Arenaviridae, Bunyaviridae, Caliciviridae, Coronaviridae, Flaviviridae, Filoviridae, Hepeviridae, Herpesviridae, Paramyxoviridae, Picornaviridae, Poxviridae, Reoviridae, Rhabdoviridae and Togaviridae families, with plans to support additional virus families in the future. ViPR captures various types of information, including sequence records, gene and protein annotations, 3D protein structures, immune epitope locations, clinical and surveillance metadata and novel data derived from comparative genomics analysis. Analytical and visualization tools for metadata-driven statistical sequence analysis, multiple sequence alignment, phylogenetic tree construction, BLAST comparison and sequence variation determination are also provided. Data filtering and analysis workflows can be combined and the results saved in personal ‘Workbenches’ for future use. ViPR tools and data are available without charge as a service to the virology research community to help facilitate the development of diagnostics, prophylactics and therapeutics for priority pathogens and other viruses.",poster,cp62
p1138,14522a6e2c9cae34d61bf41f9dc9773db04e5c55,j85,BMC Bioinformatics,Bioinformatics analysis of the epitope regions for norovirus capsid protein,Abstract content,fullPaper,jv85
p1139,2316adc7f81213388c0f777dc2c68ff603d9b9e3,c114,IEEE International Conference on Robotics and Automation,Bioinformatics opportunities for identification and study of medicinal plants,"Plants have been used as a source of medicine since historic times and several commercially important drugs are of plant-based origin. The traditional approach towards discovery of plant-based drugs often times involves significant amount of time and expenditure. These labor-intensive approaches have struggled to keep pace with the rapid development of high-throughput technologies. In the era of high volume, high-throughput data generation across the biosciences, bioinformatics plays a crucial role. This has generally been the case in the context of drug designing and discovery. However, there has been limited attention to date to the potential application of bioinformatics approaches that can leverage plant-based knowledge. Here, we review bioinformatics studies that have contributed to medicinal plants research. In particular, we highlight areas in medicinal plant research where the application of bioinformatics methodologies may result in quicker and potentially cost-effective leads toward finding plant-based remedies.",poster,cp114
p1140,0111701e0eb217a99ec8df36d0a39c0526e37806,j104,Scientometrics,Detecting the knowledge structure of bioinformatics by mining full-text collections,Abstract content,fullPaper,jv104
p1141,14d46d1d2f7cd38f68f9fc68726b45db57e327b9,c78,Neural Information Processing Systems,Bioinformatics: Sequence and Genome Analysis,Preface Chapter 1. Historical introduction and overview Chapter 2. Collecting and storing sequences in the laboratory Chapter 3. Alignment of pairs of sequences Chapter 4. Introduction to probability and statistical analysis of sequence alignments Chapter 5. Multiple sequence alignment Chapter 6. Sequence database searching for similar sequences Chapter 7. Phylogenetic prediction Chapter 8. Prediction of RNA secondary structure Chapter 9. Gene prediction and regulation Chapter 10. Protein classification and structure prediction Chapter 11. Genome analysis Chapter 12. Bioinformatics programming using Perl and Perl modules Chapter 13. Analysis of microarrays,poster,cp78
p1142,feffbc145bc8481ba50c3ceedcee61a3017517fc,j75,Lecture Notes in Computer Science,Pattern Recognition in Bioinformatics,Abstract content,fullPaper,jv75
p1143,d33bc1af4b65696b4d76bbac13684a8dc0ae13e0,c24,Decision Support Systems,Computational and Bioinformatics Frameworks for Next-Generation Whole Exome and Genome Sequencing,"It has become increasingly apparent that one of the major hurdles in the genomic age will be the bioinformatics challenges of next-generation sequencing. We provide an overview of a general framework of bioinformatics analysis. For each of the three stages of (1) alignment, (2) variant calling, and (3) filtering and annotation, we describe the analysis required and survey the different software packages that are used. Furthermore, we discuss possible future developments as data sources grow and highlight opportunities for new bioinformatics tools to be developed.",poster,cp24
p1144,48a3c8c0a27802dfdca150a77cfa9bcd51381378,j209,Human Mutation,ALSoD: A user‐friendly online bioinformatics tool for amyotrophic lateral sclerosis genetics,"Amyotrophic lateral sclerosis (ALS) is the commonest adult onset motor neuron disease, with a peak age of onset in the seventh decade. With advances in genetic technology, there is an enormous increase in the volume of genetic data produced, and a corresponding need for storage, analysis, and interpretation, particularly as our understanding of the relationships between genotype and phenotype mature. Here, we present a system to enable this in the form of the ALS Online Database (ALSoD at http://alsod.iop.kcl.ac.uk), a freely available database that has been transformed from a single gene storage facility recording mutations in the SOD1 gene to a multigene ALS bioinformatics repository and analytical instrument combining genotype, phenotype, and geographical information with associated analysis tools. These include a comparison tool to evaluate genes side by side or jointly with user configurable features, a pathogenicity prediction tool using a combination of computational approaches to distinguish variants with nonfunctional characteristics from disease‐associated mutations with more dangerous consequences, and a credibility tool to enable ALS researchers to objectively assess the evidence for gene causation in ALS. Furthermore, integration of external tools, systems for feedback, annotation by users, and two‐way links to collaborators hosting complementary databases further enhance the functionality of ALSoD. Hum Mutat 33:1345–1351, 2012. © 2012 Wiley Periodicals, Inc.",fullPaper,jv209
p1145,0a59d6ecaad02de3c8c8d57e81fff765e93a08ca,j201,Network Modeling Analysis in Health Informatics and Bioinformatics,Role of bioinformatics and pharmacogenomics in drug discovery and development process,Abstract content,fullPaper,jv201
p1146,12ab5f1e159f71367ea967c4cb287a1dd5915518,j210,Frontiers in Oncology,Current Challenges in the Bioinformatics of Single Cell Genomics,"Single cell genomics is a rapidly growing field with many new techniques emerging in the past few years. However, few bioinformatics tools specific for single cell genomics analysis are available. Single cell DNA/RNA sequencing data usually have low genome coverage and high amplification bias, which makes bioinformatics analysis challenging. Many current bioinformatics tools developed for bulk cell sequencing do not work well with single cell sequencing data. Here, we summarize current challenges in the bioinformatics analysis of single cell genomic DNA sequencing and single cell transcriptomes. These challenges include calling copy number variations, identifying mutated genes in tumor samples, reconstructing cell lineages, recovering low abundant transcripts, and improving the accuracy of quantitative analysis of transcripts. Development in single cell genomics bioinformatics analysis will promote the application of this technology to basic biology and medical research.",fullPaper,jv210
p1147,3e48eb32868c583d1616f077d24f15f49341dac4,c72,Intelligent Systems in Molecular Biology,Milestones in graphical bioinformatics,"After reviewing the field of graphical bioinformatics, we have selected two dozen of the most significant publications that represent milestones of graphical bioinformatics. These publications can be viewed as forming the backbone of graphical bioinformatics, the branch of bioinformatics that initiates analysis of DNA, RNA, and proteins by considering various graphical representations of these sequences. Graphical bioinformatics, a division of bioinformatics that analyzes sequences of DNA, RNA, proteins, and proteomics maps by developing and using tools of discrete mathematics and graph theory in particular, has expanded since the year 2000, although pioneering contributions date back to Hamory (1983) and Jeffrey (1990). We chronologically follow the development of graphical bioinformatics, without assuming that readers are familiar with discrete mathematics or graph theory. Readers unfamiliar with graph theory may even have some advantage over those who have been only superficially exposed to graph theory, inview of wide misconceptions and misinformation about chemical graph theory among quantum chemists, physical chemists, and medicinal chemists in past decades. © 2013 Wiley Periodicals, Inc.",poster,cp72
p1148,fadf2d7f4e1f0dba9534468b60aeefd751880912,c9,Pacific Symposium on Biocomputing,Metscape 2 bioinformatics tool for the analysis and visualization of metabolomics and gene expression data,"MOTIVATION
Metabolomics is a rapidly evolving field that holds promise to provide insights into genotype-phenotype relationships in cancers, diabetes and other complex diseases. One of the major informatics challenges is providing tools that link metabolite data with other types of high-throughput molecular data (e.g. transcriptomics, proteomics), and incorporate prior knowledge of pathways and molecular interactions.


RESULTS
We describe a new, substantially redesigned version of our tool Metscape that allows users to enter experimental data for metabolites, genes and pathways and display them in the context of relevant metabolic networks. Metscape 2 uses an internal relational database that integrates data from KEGG and EHMN databases. The new version of the tool allows users to identify enriched pathways from expression profiling data, build and analyze the networks of genes and metabolites, and visualize changes in the gene/metabolite data. We demonstrate the applications of Metscape to annotate molecular pathways for human and mouse metabolites implicated in the pathogenesis of sepsis-induced acute lung injury, for the analysis of gene expression and metabolite data from pancreatic ductal adenocarcinoma, and for identification of the candidate metabolites involved in cancer and inflammation.


AVAILABILITY
Metscape is part of the National Institutes of Health-supported National Center for Integrative Biomedical Informatics (NCIBI) suite of tools, freely available at http://metscape.ncibi.org. It can be downloaded from http://cytoscape.org or installed via Cytoscape plugin manager.


CONTACT
metscape-help@umich.edu; akarnovs@umich.edu


SUPPLEMENTARY INFORMATION
Supplementary data are available at Bioinformatics online.",poster,cp9
p1149,430fd08176f58140b6a0b239902cb6575bcfc565,c106,Chinese Conference on Biometric Recognition,Are graph databases ready for bioinformatics?,Contact: Lars.Juhl.Jensen@gmail.com,poster,cp106
p1150,f950ef4d991c8a6109e174af09d3737796d4771a,j211,Biochimica et Biophysica Acta,Bioinformatics tools for secretome analysis.,Abstract content,fullPaper,jv211
p1151,a51badd5461c1650b2bfba7d0a5a89949186fb67,j189,Journal of Biomedical Informatics,Bio2RDF: Towards a mashup to build bioinformatics knowledge systems,Abstract content,fullPaper,jv189
p1152,8f124016fda610e3e9147dd046fb306f219a8c85,j212,Viruses,Coronavirus Genomics and Bioinformatics Analysis,"The drastic increase in the number of coronaviruses discovered and coronavirus genomes being sequenced have given us an unprecedented opportunity to perform genomics and bioinformatics analysis on this family of viruses. Coronaviruses possess the largest genomes (26.4 to 31.7 kb) among all known RNA viruses, with G + C contents varying from 32% to 43%. Variable numbers of small ORFs are present between the various conserved genes (ORF1ab, spike, envelope, membrane and nucleocapsid) and downstream to nucleocapsid gene in different coronavirus lineages. Phylogenetically, three genera, Alphacoronavirus, Betacoronavirus and Gammacoronavirus, with Betacoronavirus consisting of subgroups A, B, C and D, exist. A fourth genus, Deltacoronavirus, which includes bulbul coronavirus HKU11, thrush coronavirus HKU12 and munia coronavirus HKU13, is emerging. Molecular clock analysis using various gene loci revealed that the time of most recent common ancestor of human/civet SARS related coronavirus to be 1999–2002, with estimated substitution rate of 4×10−4 to 2×10−2 substitutions per site per year. Recombination in coronaviruses was most notable between different strains of murine hepatitis virus (MHV), between different strains of infectious bronchitis virus, between MHV and bovine coronavirus, between feline coronavirus (FCoV) type I and canine coronavirus generating FCoV type II, and between the three genotypes of human coronavirus HKU1 (HCoV-HKU1). Codon usage bias in coronaviruses were observed, with HCoV-HKU1 showing the most extreme bias, and cytosine deamination and selection of CpG suppressed clones are the two major independent biological forces that shape such codon usage bias in coronaviruses.",fullPaper,jv212
p1153,b29963f44d38d1ad765278cb2f3213d60d0d5d32,c112,Very Large Data Bases Conference,"Bioinformatics: Concepts, Methodologies, Tools, and Applications","Information Resources Management Association (IRMA) is a research-based professional organization dedicated to advancing the concepts and practices of information resources management in modern organizations. IRMA’s primary purpose is to promote the understanding, development and practice of managing information resources as key enterprise assets among IRM/IT professionals. IRMA brings together researchers, practitioners, academicians, and policy makers in information technology management from over 50 countries. Information Resources Management Association (USA)",poster,cp112
p1154,dbce33c58b187318292e93e578d0d025be0f2156,j213,Current Bioinformatics,Bioinformatics Tools for Mass Spectroscopy-Based Metabolomic Data Processing and Analysis,"Biological systems are increasingly being studied in a holistic manner, using omics approaches, to provide quantitative and qualitative descriptions of the diverse collection of cellular components. Among the omics approaches, metabolomics, which deals with the quantitative global profiling of small molecules or metabolites, is being used extensively to explore the dynamic response of living systems, such as organelles, cells, tissues, organs and whole organisms, under diverse physiological and pathological conditions. This technology is now used routinely in a number of applications, including basic and clinical research, agriculture, microbiology, food science, nutrition, pharmaceutical research, environmental science and the development of biofuels. Of the multiple analytical platforms available to perform such analyses, nuclear magnetic resonance and mass spectrometry have come to dominate, owing to the high resolution and large datasets that can be generated with these techniques. The large multidimensional datasets that result from such studies must be processed and analyzed to render this data meaningful. Thus, bioinformatics tools are essential for the efficient processing of huge datasets, the characterization of the detected signals, and to align multiple datasets and their features. This paper provides a state-of-the-art overview of the data processing tools available, and reviews a collection of recent reports on the topic. Data conversion, pre-processing, alignment, normalization and statistical analysis are introduced, with their advantages and disadvantages, and comparisons are made to guide the reader.",fullPaper,jv213
p1155,668f58972329633c8fcdb45dd40959759fcf54cb,j156,Springer US,Bioinformatics: Databases and Systems,Abstract content,fullPaper,jv156
p1156,fd6761a938813f6124be2a4f9f9093ab27f4c01c,c51,Conference of the Centre for Advanced Studies on Collaborative Research,Making it personal: translational bioinformatics,"One of the most exciting research areas in Translational Bioinformatics1 ,2 is related to the redefinition of fundamental notions of what constitutes a ‘disease.’ Nosology, the systematic classification of diseases, dates back to Carl Linnaeus, with the Genera Morborum 3 Today, the improvement in our abilities to make molecular measurements related to health and disease has largely driven the revolution towards personalized medicine. For example, in diseases like non-small cell lung cancer or breast cancer, standard-of-care is now including sequencing of genes such as EGFR or quantitating panels of RNA such as those included in Oncotype DX, respectively, to drive therapeutic decisions for new subtypes of patients. While experts, including those at the National Research Council, are seeing the potential of scaling beyond these early case examples towards redefining our entire nosology,4 it is in the field of cancer where personalized or precision medicine has had best traction. It is no coincidence that many contributions to this special issue of JAMIA focus on cancer. Personalized medicine, also known as precision medicine, has often been equated with the use of molecular measurements to characterize disease. The special feature in this issue of JAMIA challenges this limited view.

Personalized medicine starts even before a disease is manifested in an individual, many times at a point when the disease or condition is preventable. Researchers use data from different sources to develop preventive models. For example, smoking is still the strongest preventable risk factor for many cancers, most notably lung cancer, yet it is hard to extract this information from …",poster,cp51
p1157,8114a26d698a67e95ad1c4b02472219f95f28264,c110,IEEE International Conference on Automatic Face & Gesture Recognition,Computational intelligence techniques in bioinformatics,Abstract content,poster,cp110
p1158,14b5aed2b042e4a1de41af3cd2e6c57da1073083,j85,BMC Bioinformatics,bioNerDS: exploring bioinformatics’ database and software use through literature mining,Abstract content,fullPaper,jv85
p1159,f42261d881dbac77870df168376bb2898cf46333,j85,BMC Bioinformatics,4273π: Bioinformatics education on low cost ARM hardware,Abstract content,fullPaper,jv85
p1160,ac12c9b9e35e58b55d85a97c47886a7371c14afa,c18,Conference on Innovative Data Systems Research,Data mining in bioinformatics using Weka,"UNLABELLED
The Weka machine learning workbench provides a general-purpose environment for automatic classification, regression, clustering and feature selection-common data mining problems in bioinformatics research. It contains an extensive collection of machine learning algorithms and data pre-processing methods complemented by graphical user interfaces for data exploration and the experimental comparison of different machine learning techniques on the same problem. Weka can process data given in the form of a single relational table. Its main objectives are to (a) assist users in extracting useful information from data and (b) enable them to easily identify a suitable algorithm for generating an accurate predictive model from it.


AVAILABILITY
http://www.cs.waikato.ac.nz/ml/weka.",poster,cp18
p1161,cb5756420f7ed71e49847223fdaf17d3ea9a511c,j214,Journal of chemical information and computer sciences,The Chemistry Development Kit (CDK): An Open-Source Java Library for Chemo-and Bioinformatics,"The Chemistry Development Kit (CDK) is a freely available open-source Java library for Structural Chemo-and Bioinformatics. Its architecture and capabilities as well as the development as an open-source project by a team of international collaborators from academic and industrial institutions is described. The CDK provides methods for many common tasks in molecular informatics, including 2D and 3D rendering of chemical structures, I/O routines, SMILES parsing and generation, ring searches, isomorphism checking, structure diagram generation, etc. Application scenarios as well as access information for interested users and potential contributors are given.",fullPaper,jv214
p1162,8f262d37490e1393123d2b3459f73dd06f27159a,j85,BMC Bioinformatics,Cloud BioLinux: pre-configured and on-demand bioinformatics computing for the genomics community,Abstract content,fullPaper,jv85
p1163,932ca0db2a4e3c722664cb9a2e5ca3a94aa0620e,j6,Nature Methods,A Bioinformatics Method Identifies Prominent Off-targeted Transcripts in RNAi Screens,Abstract content,fullPaper,jv6
p1164,b84cc17984f6f13bfadd2b61a605dabf9d9bfa8b,j85,BMC Bioinformatics,An overview of the Hadoop/MapReduce/HBase framework and its current applications in bioinformatics,Abstract content,fullPaper,jv85
p1165,f0cb15fe57fc7c0df374e09c2f415b4f45a46ee1,j215,Biology Direct,Bioinformatics clouds for big data manipulation,Abstract content,fullPaper,jv215
p1166,48b355e447287134c88c5b7f84ae7b6055503a4b,j85,BMC Bioinformatics,Provenance in bioinformatics workflows,Abstract content,fullPaper,jv85
p1167,ab9fe0fe3b39e6f36dcf74a715e7adcd218325b2,c93,Human Language Technology - The Baltic Perspectiv,Bioinformatics challenges for genome-wide association studies,"Motivation: The sequencing of the human genome has made it possible to identify an informative set of >1 million single nucleotide polymorphisms (SNPs) across the genome that can be used to carry out genome-wide association studies (GWASs). The availability of massive amounts of GWAS data has necessitated the development of new biostatistical methods for quality control, imputation and analysis issues including multiple testing. This work has been successful and has enabled the discovery of new associations that have been replicated in multiple studies. However, it is now recognized that most SNPs discovered via GWAS have small effects on disease susceptibility and thus may not be suitable for improving health care through genetic testing. One likely explanation for the mixed results of GWAS is that the current biostatistical analysis paradigm is by design agnostic or unbiased in that it ignores all prior knowledge about disease pathobiology. Further, the linear modeling framework that is employed in GWAS often considers only one SNP at a time thus ignoring their genomic and environmental context. There is now a shift away from the biostatistical approach toward a more holistic approach that recognizes the complexity of the genotype–phenotype relationship that is characterized by significant heterogeneity and gene–gene and gene–environment interaction. We argue here that bioinformatics has an important role to play in addressing the complexity of the underlying genetic basis of common human diseases. The goal of this review is to identify and discuss those GWAS challenges that will require computational methods. Contact: jason.h.moore@dartmouth.edu",poster,cp93
p1168,39e5b01e94d64b5b54e65f59d31a748ef4417663,c19,ACM Conference on Economics and Computation,Bpipe: a tool for running and managing bioinformatics pipelines,"SUMMARY
Bpipe is a simple, dedicated programming language for defining and executing bioinformatics pipelines. It specializes in enabling users to turn existing pipelines based on shell scripts or command line tools into highly flexible, adaptable and maintainable workflows with a minimum of effort. Bpipe ensures that pipelines execute in a controlled and repeatable fashion and keeps audit trails and logs to ensure that experimental results are reproducible. Requiring only Java as a dependency, Bpipe is fully self-contained and cross-platform, making it very easy to adopt and deploy into existing environments.


AVAILABILITY AND IMPLEMENTATION
Bpipe is freely available from http://bpipe.org under a BSD License.",poster,cp19
p1169,ba69a521bad7a913a9ee82d1c2e49f41771d5c00,j216,Infection and Immunity,PATRIC: the Comprehensive Bacterial Bioinformatics Resource with a Focus on Human Pathogenic Species,"ABSTRACT Funded by the National Institute of Allergy and Infectious Diseases, the Pathosystems Resource Integration Center (PATRIC) is a genomics-centric relational database and bioinformatics resource designed to assist scientists in infectious-disease research. Specifically, PATRIC provides scientists with (i) a comprehensive bacterial genomics database, (ii) a plethora of associated data relevant to genomic analysis, and (iii) an extensive suite of computational tools and platforms for bioinformatics analysis. While the primary aim of PATRIC is to advance the knowledge underlying the biology of human pathogens, all publicly available genome-scale data for bacteria are compiled and continually updated, thereby enabling comparative analyses to reveal the basis for differences between infectious free-living and commensal species. Herein we summarize the major features available at PATRIC, dividing the resources into two major categories: (i) organisms, genomes, and comparative genomics and (ii) recurrent integration of community-derived associated data. Additionally, we present two experimental designs typical of bacterial genomics research and report on the execution of both projects using only PATRIC data and tools. These applications encompass a broad range of the data and analysis tools available, illustrating practical uses of PATRIC for the biologist. Finally, a summary of PATRIC's outreach activities, collaborative endeavors, and future research directions is provided.",fullPaper,jv216
p1170,44fd2220345a3cb32140fac3819d78d6cc143551,j108,PLoS ONE,Identification of Plasma Lipid Biomarkers for Prostate Cancer by Lipidomics and Bioinformatics,"Background Lipids have critical functions in cellular energy storage, structure and signaling. Many individual lipid molecules have been associated with the evolution of prostate cancer; however, none of them has been approved to be used as a biomarker. The aim of this study is to identify lipid molecules from hundreds plasma apparent lipid species as biomarkers for diagnosis of prostate cancer. Methodology/Principal Findings Using lipidomics, lipid profiling of 390 individual apparent lipid species was performed on 141 plasma samples from 105 patients with prostate cancer and 36 male controls. High throughput data generated from lipidomics were analyzed using bioinformatic and statistical methods. From 390 apparent lipid species, 35 species were demonstrated to have potential in differentiation of prostate cancer. Within the 35 species, 12 were identified as individual plasma lipid biomarkers for diagnosis of prostate cancer with a sensitivity above 80%, specificity above 50% and accuracy above 80%. Using top 15 of 35 potential biomarkers together increased predictive power dramatically in diagnosis of prostate cancer with a sensitivity of 93.6%, specificity of 90.1% and accuracy of 97.3%. Principal component analysis (PCA) and hierarchical clustering analysis (HCA) demonstrated that patient and control populations were visually separated by identified lipid biomarkers. RandomForest and 10-fold cross validation analyses demonstrated that the identified lipid biomarkers were able to predict unknown populations accurately, and this was not influenced by patient's age and race. Three out of 13 lipid classes, phosphatidylethanolamine (PE), ether-linked phosphatidylethanolamine (ePE) and ether-linked phosphatidylcholine (ePC) could be considered as biomarkers in diagnosis of prostate cancer. Conclusions/Significance Using lipidomics and bioinformatic and statistical methods, we have identified a few out of hundreds plasma apparent lipid molecular species as biomarkers for diagnosis of prostate cancer with a high sensitivity, specificity and accuracy.",fullPaper,jv108
p1171,2045eaca03cd0a6b72cd44f616998f30ed3cf4b5,c39,International Conference on Global Software Engineering,BioJava: an open-source framework for bioinformatics in 2012,"Motivation: BioJava is an open-source project for processing of biological data in the Java programming language. We have recently released a new version (3.0.5), which is a major update to the code base that greatly extends its functionality. Results: BioJava now consists of several independent modules that provide state-of-the-art tools for protein structure comparison, pairwise and multiple sequence alignments, working with DNA and protein sequences, analysis of amino acid properties, detection of protein modifications and prediction of disordered regions in proteins as well as parsers for common file formats using a biologically meaningful data model. Availability: BioJava is an open-source project distributed under the Lesser GPL (LGPL). BioJava can be downloaded from the BioJava website (http://www.biojava.org). BioJava requires Java 1.6 or higher. All inquiries should be directed to the BioJava mailing lists. Details are available at http://biojava.org/wiki/BioJava:MailingLists Contact: andreas.prlic@gmail.com",poster,cp39
p1172,968516d4ded428ee3ca1ab08ba03486c335a18c4,j217,Journal of Inorganic Biochemistry,A bioinformatics view of zinc enzymes.,Abstract content,fullPaper,jv217
p1173,1fbac92306d62bd45a434cc88c6d84d70ab51df5,c52,Workshop on Learning from Authoritative Security Experiment Results,The Roots of Bioinformatics in Theoretical Biology,"From the late 1980s onward, the term “bioinformatics” mostly has been used to refer to computational methods for comparative analysis of genome data. However, the term was originally more widely defined as the study of informatic processes in biotic systems. In this essay, I will trace this early history (from a personal point of view) and I will argue that the original meaning of the term is re-emerging.",poster,cp52
p1174,bfc414bf2bb0c745b081137e7a69de266023aefe,c75,International Conference on Machine Learning,Bioinformatics challenges for personalized medicine,"Motivation: Widespread availability of low-cost, full genome sequencing will introduce new challenges for bioinformatics. Results: This review outlines recent developments in sequencing technologies and genome analysis methods for application in personalized medicine. New methods are needed in four areas to realize the potential of personalized medicine: (i) processing large-scale robust genomic data; (ii) interpreting the functional effect and the impact of genomic variation; (iii) integrating systems data to relate complex genetic interactions with phenotypes; and (iv) translating these discoveries into medical practice. Contact: russ.altman@stanford.edu Supplementary information: Supplementary data are available at Bioinformatics online.",poster,cp75
p1175,378ca8eac56e3b0372bee1eb746cfe05b848ccbe,j199,Clinical Chemistry,Proteomics and bioinformatics approaches for identification of serum biomarkers to detect breast cancer.,"BACKGROUND
Surface-enhanced laser desorption/ionization (SELDI) is an affinity-based mass spectrometric method in which proteins of interest are selectively adsorbed to a chemically modified surface on a biochip, whereas impurities are removed by washing with buffer. This technology allows sensitive and high-throughput protein profiling of complex biological specimens.


METHODS
We screened for potential tumor biomarkers in 169 serum samples, including samples from a cancer group of 103 breast cancer patients at different clinical stages [stage 0 (n = 4), stage I (n = 38), stage II (n = 37), and stage III (n = 24)], from a control group of 41 healthy women, and from 25 patients with benign breast diseases. Diluted serum samples were applied to immobilized metal affinity capture Ciphergen ProteinChip Arrays previously activated with Ni2+. Proteins bound to the chelated metal were analyzed on a ProteinChip Reader Model PBS II. Complex protein profiles of different diagnostic groups were compared and analyzed using the ProPeak software package.


RESULTS
A panel of three biomarkers was selected based on their collective contribution to the optimal separation between stage 0-I breast cancer patients and noncancer controls. The same separation was observed using independent test data from stage II-III breast cancer patients. Bootstrap cross-validation demonstrated that a sensitivity of 93% for all cancer patients and a specificity of 91% for all controls were achieved by a composite index derived by multivariate logistic regression using the three selected biomarkers.


CONCLUSIONS
Proteomics approaches such as SELDI mass spectrometry, in conjunction with bioinformatics tools, could greatly facilitate the discovery of new and better biomarkers. The high sensitivity and specificity achieved by the combined use of the selected biomarkers show great potential for the early detection of breast cancer.",fullPaper,jv199
p1176,1783e2da0f2a0bf97dd57c7b10e27b9d91116d23,j218,Molecular & Cellular Proteomics,"Probing Native Protein Structures by Chemical Cross-linking, Mass Spectrometry, and Bioinformatics*","Chemical cross-linking of reactive groups in native proteins and protein complexes in combination with the identification of cross-linked sites by mass spectrometry has been in use for more than a decade. Recent advances in instrumentation, cross-linking protocols, and analysis software have led to a renewed interest in this technique, which promises to provide important information about native protein structure and the topology of protein complexes. In this article, we discuss the critical steps of chemical cross-linking and its implications for (structural) biology: reagent design and cross-linking protocols, separation and mass spectrometric analysis of cross-linked samples, dedicated software for data analysis, and the use of cross-linking data for computational modeling. Finally, the impact of protein cross-linking on various biological disciplines is highlighted.",fullPaper,jv218
p1177,cb8491c54f0b1e63034343ac5c8325371089185e,c71,IEEE International Conference on Information Reuse and Integration,A review of the stability of feature selection techniques for bioinformatics data,"Feature selection is an important step in data mining and is used in various domains including genetics, medicine, and bioinformatics. Choosing the important features (genes) is essential for the discovery of new knowledge hidden within the genetic code as well as the identification of important biomarkers. Although feature selection methods can help sort through large numbers of genes based on their relevance to the problem at hand, the results generated tend to be unstable and thus cannot be reproduced in other experiments. Relatedly, research interest in the stability of feature ranking methods has grown recently and researchers have produced experimental designs for testing the stability of feature selection, creating new metrics for measuring stability and new techniques designed to improve the stability of the feature selection process. In this paper, we will introduce the role of stability in feature selection with DNA microarray data. We list various ways of improving feature ranking stability, and discuss feature selection techniques, specifically explaining ensemble feature ranking and presenting various ensemble feature ranking aggregation methods. Finally, we discuss experimental procedures such as dataset perturbation, fixed overlap partitioning, and cross validation procedures that help researchers analyze and measure the stability of feature ranking methods. Throughout this work, we investigate current research in the field and discuss possible avenues of continuing such research efforts.",fullPaper,cp71
p1178,17b07d8bb61a1462299053b66615f9eddd8f23d5,j108,PLoS ONE,Comprehensive Decision Tree Models in Bioinformatics,"Purpose Classification is an important and widely used machine learning technique in bioinformatics. Researchers and other end-users of machine learning software often prefer to work with comprehensible models where knowledge extraction and explanation of reasoning behind the classification model are possible. Methods This paper presents an extension to an existing machine learning environment and a study on visual tuning of decision tree classifiers. The motivation for this research comes from the need to build effective and easily interpretable decision tree models by so called one-button data mining approach where no parameter tuning is needed. To avoid bias in classification, no classification performance measure is used during the tuning of the model that is constrained exclusively by the dimensions of the produced decision tree. Results The proposed visual tuning of decision trees was evaluated on 40 datasets containing classical machine learning problems and 31 datasets from the field of bioinformatics. Although we did not expected significant differences in classification performance, the results demonstrate a significant increase of accuracy in less complex visually tuned decision trees. In contrast to classical machine learning benchmarking datasets, we observe higher accuracy gains in bioinformatics datasets. Additionally, a user study was carried out to confirm the assumption that the tree tuning times are significantly lower for the proposed method in comparison to manual tuning of the decision tree. Conclusions The empirical results demonstrate that by building simple models constrained by predefined visual boundaries, one not only achieves good comprehensibility, but also very good classification performance that does not differ from usually more complex models built using default settings of the classical decision tree algorithm. In addition, our study demonstrates the suitability of visually tuned decision trees for datasets with binary class attributes and a high number of possibly redundant attributes that are very common in bioinformatics.",fullPaper,jv108
p1179,84534c6fa2dab4768617205b302bfbce9b3bb376,j213,Current Bioinformatics,A Review of Ensemble Methods in Bioinformatics,"Ensemble learning is an intensively studies technique in machine learning and pattern recognition. Recent work in computational biology has seen an increasing use of ensemble learning methods due to their unique advantages in dealing with small sample size, high-dimensionality, and complexity data structures. The aim of this article is two-fold. First, it is to provide a review of the most widely used ensemble learning methods and their application in various bioinformatics problems, including the main topics of gene expression, mass spectrometry-based proteomics, gene-gene interaction identification from genome-wide association studies, and prediction of regulatory elements from DNA and protein sequences. Second, we try to identify and summarize future trends of ensemble methods in bioinformatics. Promising directions such as ensemble of support vector machine, meta-ensemble, and ensemble based feature selection are discussed.",fullPaper,jv213
p1180,035256a8a6d8a73af2adb38245f4130daa1f0535,c97,Interspeech,Machine learning in bioinformatics,"This article reviews machine learning methods for bioinformatics. It presents modelling methods, such as supervised classification, clustering and probabilistic graphical models for knowledge discovery, as well as deterministic and stochastic heuristics for optimization. Applications in genomics, proteomics, systems biology, evolution and text mining are also shown.",poster,cp97
p1181,bc881e788f09a3842612f522d4cfcc05bbe7d46b,c8,The Compass,Data Mining in Bioinformatics,Abstract content,poster,cp8
p1182,36cedf831ca23777472a7707ef7ff5bbb77b123c,c62,International Conference on Software Reuse,Application Of Data Mining In Bioinformatics,This article highlights some of the basic concepts of bioinformatics and data mining. The major research areas of bioinformatics are highlighted. The application of data mining in the domain of bioinformatics is explained. It also highlights some of the current challenges and opportunities of data mining in bioinformatics.,poster,cp62
p1183,abd71797e0c682fabcfc57f38104a086f1770379,j85,BMC Bioinformatics,Cancer bioinformatics: A new approach to systems clinical medicine,Abstract content,fullPaper,jv85
p1184,0e6fa04227f98914408b77b9d52eab7dcc194c83,c13,International Conference on Data Science and Advanced Analytics,Rise and Demise of Bioinformatics? Promise and Progress,"The field of bioinformatics and computational biology has gone through a number of transformations during the past 15 years, establishing itself as a key component of new biology. This spectacular growth has been challenged by a number of disruptive changes in science and technology. Despite the apparent fatigue of the linguistic use of the term itself, bioinformatics has grown perhaps to a point beyond recognition. We explore both historical aspects and future trends and argue that as the field expands, key questions remain unanswered and acquire new meaning while at the same time the range of applications is widening to cover an ever increasing number of biological disciplines. These trends appear to be pointing to a redefinition of certain objectives, milestones, and possibly the field itself.",poster,cp13
p1185,c42a04355ad3313e2a9d828d43e46fa348fd797f,c83,International Conference on Computer Graphics and Interactive Techniques,Bioinformatics for personal genome interpretation,"An international consortium released the first draft sequence of the human genome 10 years ago. Although the analysis of this data has suggested the genetic underpinnings of many diseases, we have not yet been able to fully quantify the relationship between genotype and phenotype. Thus, a major current effort of the scientific community focuses on evaluating individual predispositions to specific phenotypic traits given their genetic backgrounds. Many resources aim to identify and annotate the specific genes responsible for the observed phenotypes. Some of these use intra-species genetic variability as a means for better understanding this relationship. In addition, several online resources are now dedicated to collecting single nucleotide variants and other types of variants, and annotating their functional effects and associations with phenotypic traits. This information has enabled researchers to develop bioinformatics tools to analyze the rapidly increasing amount of newly extracted variation data and to predict the effect of uncharacterized variants. In this work, we review the most important developments in the field--the databases and bioinformatics tools that will be of utmost importance in our concerted effort to interpret the human variome.",poster,cp83
p1186,cdbe29845cfdaa320446223b9e5deae6fb10d6ef,j219,IEEE/ACM Transactions on Computational Biology & Bioinformatics,Fast Parallel Markov Clustering in Bioinformatics Using Massively Parallel Computing on GPU with CUDA and ELLPACK-R Sparse Format,"Markov clustering (MCL) is becoming a key algorithm within bioinformatics for determining clusters in networks. However, with increasing vast amount of data on biological networks, performance and scalability issues are becoming a critical limiting factor in applications. Meanwhile, GPU computing, which uses CUDA tool for implementing a massively parallel computing environment in the GPU card, is becoming a very powerful, efficient, and low-cost option to achieve substantial performance gains over CPU approaches. The use of on-chip memory on the GPU is efficiently lowering the latency time, thus, circumventing a major issue in other parallel computing environments, such as MPI. We introduce a very fast Markov clustering algorithm using CUDA (CUDA-MCL) to perform parallel sparse matrix-matrix computations and parallel sparse Markov matrix normalizations, which are at the heart of MCL. We utilized ELLPACK-R sparse format to allow the effective and fine-grain massively parallel processing to cope with the sparse nature of interaction networks data sets in bioinformatics applications. As the results show, CUDA-MCL is significantly faster than the original MCL running on CPU. Thus, large-scale parallel computation on off-the-shelf desktop-machines, that were previously only possible on supercomputing architectures, can significantly change the way bioinformaticians and biologists deal with their data.",fullPaper,jv219
p1187,d3b163b34b5f7ac25a727514c29b1d889ec27f1d,j220,Clinical pharmacology and therapy,Translational Bioinformatics: Linking the Molecular World to the Clinical World,Abstract content,fullPaper,jv220
p1188,00ff13cf1e7b109351550b22413ed06b7d58cba7,j85,BMC Bioinformatics,CaPSID: A bioinformatics platform for computational pathogen sequence identification in human genomes and transcriptomes,Abstract content,fullPaper,jv85
p1189,42b5e58dc252fb9009ff86fab8b77574c2a21705,j201,Network Modeling Analysis in Health Informatics and Bioinformatics,Threshold-based feature selection techniques for high-dimensional bioinformatics data,Abstract content,fullPaper,jv201
p1190,5b4c23484dea349a9f0f354e26a3f26c0ca6cc40,c89,Conference on Uncertainty in Artificial Intelligence,myExperiment: a repository and social network for the sharing of bioinformatics workflows,"myExperiment (http://www.myexperiment.org) is an online research environment that supports the social sharing of bioinformatics workflows. These workflows are procedures consisting of a series of computational tasks using web services, which may be performed on data from its retrieval, integration and analysis, to the visualization of the results. As a public repository of workflows, myExperiment allows anybody to discover those that are relevant to their research, which can then be reused and repurposed to their specific requirements. Conversely, developers can submit their workflows to myExperiment and enable them to be shared in a secure manner. Since its release in 2007, myExperiment currently has over 3500 registered users and contains more than 1000 workflows. The social aspect to the sharing of these workflows is facilitated by registered users forming virtual communities bound together by a common interest or research project. Contributors of workflows can build their reputation within these communities by receiving feedback and credit from individuals who reuse their work. Further documentation about myExperiment including its REST web service is available from http://wiki.myexperiment.org. Feedback and requests for support can be sent to bugs@myexperiment.org.",poster,cp89
p1191,a65e32fe0d11ac2f9f932d2f2fa01c42f31af2f9,c59,British Computer Society Conference on Human-Computer Interaction,Bioinformatics for the Human Microbiome Project,"Microbes inhabit virtually all sites of the human body, yet we know very little about the role they play in our health. In recent years, there has been increasing interest in studying human-associated microbial communities, particularly since microbial dysbioses have now been implicated in a number of human diseases [1]–[3]. Dysbiosis, the disruption of the normal microbial community structure, however, is impossible to define without first establishing what “normal microbial community structure” means within the healthy human microbiome. Recent advances in sequencing technologies have made it feasible to perform large-scale studies of microbial communities, providing the tools necessary to begin to address this question [4], [5]. This led to the implementation of the Human Microbiome Project (HMP) in 2007, an initiative funded by the National Institutes of Health Roadmap for Biomedical Research and constructed as a large, genome-scale community research project [6]. Any such project must plan for data analysis, computational methods development, and the public availability of tools and data; here, we provide an overview of the corresponding bioinformatics organization, history, and results from the HMP (Figure 1).",poster,cp59
p1192,9c1c4d1d5ef82f4b934f732183869c7109d01aa6,j221,Plant and Cell Physiology,Advances in Omics and Bioinformatics Tools for Systems Analyses of Plant Functions,"Omics and bioinformatics are essential to understanding the molecular systems that underlie various plant functions. Recent game-changing sequencing technologies have revitalized sequencing approaches in genomics and have produced opportunities for various emerging analytical applications. Driven by technological advances, several new omics layers such as the interactome, epigenome and hormonome have emerged. Furthermore, in several plant species, the development of omics resources has progressed to address particular biological properties of individual species. Integration of knowledge from omics-based research is an emerging issue as researchers seek to identify significance, gain biological insights and promote translational research. From these perspectives, we provide this review of the emerging aspects of plant systems research based on omics and bioinformatics analyses together with their associated resources and technological advances.",fullPaper,jv221
p1193,9854408831f344da39d4af2bd01922484eea80d1,j85,BMC Bioinformatics,Personalized cloud-based bioinformatics services for research and education: use cases and the elasticHPC package,Abstract content,fullPaper,jv85
p1194,3c45ee7f74d13efb13276d5a23ba4f9df6e48ca7,c50,International Conference on Automated Software Engineering,VectorBase: improvements to a bioinformatics resource for invertebrate vector genomics,"VectorBase (http://www.vectorbase.org) is a NIAID-supported bioinformatics resource for invertebrate vectors of human pathogens. It hosts data for nine genomes: mosquitoes (three Anopheles gambiae genomes, Aedes aegypti and Culex quinquefasciatus), tick (Ixodes scapularis), body louse (Pediculus humanus), kissing bug (Rhodnius prolixus) and tsetse fly (Glossina morsitans). Hosted data range from genomic features and expression data to population genetics and ontologies. We describe improvements and integration of new data that expand our taxonomic coverage. Releases are bi-monthly and include the delivery of preliminary data for emerging genomes. Frequent updates of the genome browser provide VectorBase users with increasing options for visualizing their own high-throughput data. One major development is a new population biology resource for storing genomic variations, insecticide resistance data and their associated metadata. It takes advantage of improved ontologies and controlled vocabularies. Combined, these new features ensure timely release of multiple types of data in the public domain while helping overcome the bottlenecks of bioinformatics and annotation by engaging with our user community.",poster,cp50
p1195,1f0919651f73af61805a284aa0a9ddad3bd273c9,j222,Springer: New York,Bioinformatics for High Throughput Sequencing,Abstract content,fullPaper,jv222
p1196,076d9e29b15c06b7ce1166562ebdb985a52003db,j223,Physiological Genomics,Aging and microRNA expression in human skeletal muscle: a microarray and bioinformatics analysis.,"A common characteristic of aging is loss of skeletal muscle (sarcopenia), which can lead to falls and fractures. MicroRNAs (miRNAs) are novel posttranscriptional modulators of gene expression with potential roles as regulators of skeletal muscle mass and function. The purpose of this study was to profile miRNA expression patterns in aging human skeletal muscle with a miRNA array followed by in-depth functional and network analysis. Muscle biopsy samples from 36 men [young: 31 ± 2 (n = 19); older: 73 ± 3 (n = 17)] were 1) analyzed for expression of miRNAs with a miRNA array, 2) validated with TaqMan quantitative real-time PCR assays, and 3) identified (and later validated) for potential gene targets with the bioinformatics knowledge base software Ingenuity Pathways Analysis. Eighteen miRNAs were differentially expressed in older humans (P < 0.05 and >500 expression level). Let-7 family members Let-7b and Let-7e were significantly elevated and further validated in older subjects (P < 0.05). Functional and network analysis from Ingenuity determined that gene targets of the Let-7s were associated with molecular networks involved in cell cycle control such as cellular proliferation and differentiation. We confirmed with real-time PCR that mRNA expression of cell cycle regulators CDK6, CDC25A, and CDC34 were downregulated in older compared with young subjects (P < 0.05). In addition, PAX7 mRNA expression was lower in older subjects (P < 0.05). These data suggest that aging is characterized by a higher expression of Let-7 family members that may downregulate genes related to cellular proliferation. We propose that higher Let-7 expression may be an indicator of impaired cell cycle function possibly contributing to reduced muscle cell renewal and regeneration in older human muscle.",fullPaper,jv223
p1197,27546dce4ee9a41dbac31a8c6a0b1f9b90f97a77,c3,Frontiers in Education Conference,Principal component analysis based methods in bioinformatics studies,"In analysis of bioinformatics data, a unique challenge arises from the high dimensionality of measurements. Without loss of generality, we use genomic study with gene expression measurements as a representative example but note that analysis techniques discussed in this article are also applicable to other types of bioinformatics studies. Principal component analysis (PCA) is a classic dimension reduction approach. It constructs linear combinations of gene expressions, called principal components (PCs). The PCs are orthogonal to each other, can effectively explain variation of gene expressions, and may have a much lower dimensionality. PCA is computationally simple and can be realized using many existing software packages. This article consists of the following parts. First, we review the standard PCA technique and their applications in bioinformatics data analysis. Second, we describe recent 'non-standard' applications of PCA, including accommodating interactions among genes, pathways and network modules and conducting PCA with estimating equations as opposed to gene expressions. Third, we introduce several recently proposed PCA-based techniques, including the supervised PCA, sparse PCA and functional PCA. The supervised PCA and sparse PCA have been shown to have better empirical performance than the standard PCA. The functional PCA can analyze time-course gene expression data. Last, we raise the awareness of several critical but unsolved problems related to PCA. The goal of this article is to make bioinformatics researchers aware of the PCA technique and more importantly its most recent development, so that this simple yet effective dimension reduction technique can be better employed in bioinformatics data analysis.",poster,cp3
p1198,d55f3a7e25a61e978cbff9651491123ed96c0b69,j224,Plant Physiology,"A Bioinformatics Approach to the Identification, Classification, and Analysis of Hydroxyproline-Rich Glycoproteins[W][OA]","Hydroxyproline-rich glycoproteins (HRGPs) are a superfamily of plant cell wall proteins that function in diverse aspects of plant growth and development. This superfamily consists of three members: hyperglycosylated arabinogalactan proteins (AGPs), moderately glycosylated extensins (EXTs), and lightly glycosylated proline-rich proteins (PRPs). Hybrid and chimeric versions of HRGP molecules also exist. In order to “mine” genomic databases for HRGPs and to facilitate and guide research in the field, the BIO OHIO software program was developed that identifies and classifies AGPs, EXTs, PRPs, hybrid HRGPs, and chimeric HRGPs from proteins predicted from DNA sequence data. This bioinformatics program is based on searching for biased amino acid compositions and for particular protein motifs associated with known HRGPs. HRGPs identified by the program are subsequently analyzed to elucidate the following: (1) repeating amino acid sequences, (2) signal peptide and glycosylphosphatidylinositol lipid anchor addition sequences, (3) similar HRGPs via Basic Local Alignment Search Tool, (4) expression patterns of their genes, (5) other HRGPs, glycosyl transferase, prolyl 4-hydroxylase, and peroxidase genes coexpressed with their genes, and (6) gene structure and whether genetic mutants exist in their genes. The program was used to identify and classify 166 HRGPs from Arabidopsis (Arabidopsis thaliana) as follows: 85 AGPs (including classical AGPs, lysine-rich AGPs, arabinogalactan peptides, fasciclin-like AGPs, plastocyanin AGPs, and other chimeric AGPs), 59 EXTs (including SP5 EXTs, SP5/SP4 EXTs, SP4 EXTs, SP4/SP3 EXTs, a SP3 EXT, “short” EXTs, leucine-rich repeat-EXTs, proline-rich extensin-like receptor kinases, and other chimeric EXTs), 18 PRPs (including PRPs and chimeric PRPs), and AGP/EXT hybrid HRGPs.",fullPaper,jv224
p1199,e8db4c01d32cad0694852a94052c1b7d88842ef6,c21,Grid Computing Environments,"Pseudo Amino Acid Composition and its Applications in Bioinformatics, Proteomics and System Biology","With the avalanche of protein sequences generated in the post-genomic age, it is highly desired to develop automated methods for efficiently identifying various attributes of uncharacterized proteins. This is one of the most im- portant tasks facing us today in bioinformatics, and the information thus obtained will have important impacts on the de- velopment of proteomics and system biology. To realize that, one of the keys is to find an effective model to represent the sample of a protein. The most straightforward model in this regard is its entire amino acid sequence; however, the entire sequence model would fail to work when the query protein did not have significant homology to proteins of known char- acteristics. Thus, various non-sequential models or discrete models were proposed. The simplest discrete model is the amino acid (AA) composition. Using it to represent a protein, however, all the sequence-order information would be com- pletely lost. To cope with such a dilemma, the concept of pseudo amino acid (PseAA) composition was introduced. Its es- sence is to keep using a discrete model to represent a protein yet without completely losing its sequence-order informa- tion. Therefore, in a broad sense, the PseAA composition of a protein is actually a set of discrete numbers that is de- rived from its amino acid sequence and that is different from the classical AA composition and able to harbour some sort of sequence order or pattern information. Ever since the first PseAA composition was formulated to predict protein sub- cellular localization and membrane protein types, it has stimulated many different modes of PseAA composition for studying various kinds of problems in proteins and proteins-related systems. In this review, we shall give a brief and sys- tematic introduction of various modes of PseAA composition and their applications. Meanwhile, the challenges for find- ing the optimal PseAA composition are also briefly discussed.",poster,cp21
p1200,d55f3a7e25a61e978cbff9651491123ed96c0b69,j224,Plant Physiology,"A Bioinformatics Approach to the Identification, Classification, and Analysis of Hydroxyproline-Rich Glycoproteins[W][OA]","Hydroxyproline-rich glycoproteins (HRGPs) are a superfamily of plant cell wall proteins that function in diverse aspects of plant growth and development. This superfamily consists of three members: hyperglycosylated arabinogalactan proteins (AGPs), moderately glycosylated extensins (EXTs), and lightly glycosylated proline-rich proteins (PRPs). Hybrid and chimeric versions of HRGP molecules also exist. In order to “mine” genomic databases for HRGPs and to facilitate and guide research in the field, the BIO OHIO software program was developed that identifies and classifies AGPs, EXTs, PRPs, hybrid HRGPs, and chimeric HRGPs from proteins predicted from DNA sequence data. This bioinformatics program is based on searching for biased amino acid compositions and for particular protein motifs associated with known HRGPs. HRGPs identified by the program are subsequently analyzed to elucidate the following: (1) repeating amino acid sequences, (2) signal peptide and glycosylphosphatidylinositol lipid anchor addition sequences, (3) similar HRGPs via Basic Local Alignment Search Tool, (4) expression patterns of their genes, (5) other HRGPs, glycosyl transferase, prolyl 4-hydroxylase, and peroxidase genes coexpressed with their genes, and (6) gene structure and whether genetic mutants exist in their genes. The program was used to identify and classify 166 HRGPs from Arabidopsis (Arabidopsis thaliana) as follows: 85 AGPs (including classical AGPs, lysine-rich AGPs, arabinogalactan peptides, fasciclin-like AGPs, plastocyanin AGPs, and other chimeric AGPs), 59 EXTs (including SP5 EXTs, SP5/SP4 EXTs, SP4 EXTs, SP4/SP3 EXTs, a SP3 EXT, “short” EXTs, leucine-rich repeat-EXTs, proline-rich extensin-like receptor kinases, and other chimeric EXTs), 18 PRPs (including PRPs and chimeric PRPs), and AGP/EXT hybrid HRGPs.",fullPaper,jv224
p1201,e8db4c01d32cad0694852a94052c1b7d88842ef6,c96,USENIX Symposium on Operating Systems Design and Implementation,"Pseudo Amino Acid Composition and its Applications in Bioinformatics, Proteomics and System Biology","With the avalanche of protein sequences generated in the post-genomic age, it is highly desired to develop automated methods for efficiently identifying various attributes of uncharacterized proteins. This is one of the most im- portant tasks facing us today in bioinformatics, and the information thus obtained will have important impacts on the de- velopment of proteomics and system biology. To realize that, one of the keys is to find an effective model to represent the sample of a protein. The most straightforward model in this regard is its entire amino acid sequence; however, the entire sequence model would fail to work when the query protein did not have significant homology to proteins of known char- acteristics. Thus, various non-sequential models or discrete models were proposed. The simplest discrete model is the amino acid (AA) composition. Using it to represent a protein, however, all the sequence-order information would be com- pletely lost. To cope with such a dilemma, the concept of pseudo amino acid (PseAA) composition was introduced. Its es- sence is to keep using a discrete model to represent a protein yet without completely losing its sequence-order informa- tion. Therefore, in a broad sense, the PseAA composition of a protein is actually a set of discrete numbers that is de- rived from its amino acid sequence and that is different from the classical AA composition and able to harbour some sort of sequence order or pattern information. Ever since the first PseAA composition was formulated to predict protein sub- cellular localization and membrane protein types, it has stimulated many different modes of PseAA composition for studying various kinds of problems in proteins and proteins-related systems. In this review, we shall give a brief and sys- tematic introduction of various modes of PseAA composition and their applications. Meanwhile, the challenges for find- ing the optimal PseAA composition are also briefly discussed.",poster,cp96
p1202,e8c7c2c6400a19929f693b1db482890810fbfeb7,c64,Experimental Software Engineering Network,Current Protocols in Bioinformatics,"1. Please read the rough pages and mark any changes right in the text. 2. If you have large inserts to add, please supply us with a disk and hard copy of the insert(s) and indicate where they should go.",poster,cp64
p1203,5cd0b6d48ab997f351b39b614b02e512d0fb590a,j85,BMC Bioinformatics,Clinical Bioinformatics: challenges and opportunities,Abstract content,fullPaper,jv85
p1204,12d8861eab0a6aabd1e1774a48ebbdc30a81b315,c68,International Workshop on Software Engineering for Large-Scale Multi-Agent Systems,"Proteins : Structure , Function , and Bioinformatics","After printing the PDF file, please read the page proofs carefully and: 1) indicate changes or corrections in the margin of the page proofs; 2) answer all queries (footnotes A,B,C, etc.) on the last page of the PDF proof; 3) proofread any tables and equations carefully; 4) check that any Greek, especially ""mu"", has translated correctly. Within 48 hours, please return the following to the address given below: 1) original PDF set of page proofs, 2) Reprint Order form, 3) Return fax form Return to: Your article will be published online via our EarlyView service within a few days of correction receipt. Your prompt attention to and return of page proofs is crucial to faster publication of your work. If you experience technical problems, please contact Doug Frank (",poster,cp68
p1205,f34c30f7836233da7277c2f2f2153004a10598a6,j218,Molecular & Cellular Proteomics,"GProX, a User-Friendly Platform for Bioinformatics Analysis and Visualization of Quantitative Proteomics Data*","Recent technological advances have made it possible to identify and quantify thousands of proteins in a single proteomics experiment. As a result of these developments, the analysis of data has become the bottleneck of proteomics experiment. To provide the proteomics community with a user-friendly platform for comprehensive analysis, inspection and visualization of quantitative proteomics data we developed the Graphical Proteomics Data Explorer (GProX)1. The program requires no special bioinformatics training, as all functions of GProX are accessible within its graphical user-friendly interface which will be intuitive to most users. Basic features facilitate the uncomplicated management and organization of large data sets and complex experimental setups as well as the inspection and graphical plotting of quantitative data. These are complemented by readily available high-level analysis options such as database querying, clustering based on abundance ratios, feature enrichment tests for e.g. GO terms and pathway analysis tools. A number of plotting options for visualization of quantitative proteomics data is available and most analysis functions in GProX create customizable high quality graphical displays in both vector and bitmap formats. The generic import requirements allow data originating from essentially all mass spectrometry platforms, quantitation strategies and software to be analyzed in the program. GProX represents a powerful approach to proteomics data analysis providing proteomics experimenters with a toolbox for bioinformatics analysis of quantitative proteomics data. The program is released as open-source and can be freely downloaded from the project webpage at http://gprox.sourceforge.net.",fullPaper,jv218
p1206,92555f78bf48e48605507485a0b8b0c7db18d5cc,c54,International Workshop on Agent-Oriented Software Engineering,International Journal of Knowledge Discovery in Bioinformatics,Abstract content,poster,cp54
p1207,6c049c9258e31f5f196445ba8be04a735022dacf,j10,Chemical Reviews,Bioinformatics and systems biology of the lipidome.,"Lipids play an important role in physiology and pathophysiology of living systems. Until a few decades ago, the number of lipid molecules that were chemically characterized was a few hundred at most and were catalogued in monographs and compendia.1 Since the advent of the era of the genome and the proteome, there has been increasing recognition that other macromolecules like lipids and polysaccharides in living systems display considerable structural diversity and systematic efforts are underway to identify, characterize and catalog these molecules. With mass spectrometric techniques coming of age, several thousand distinct molecular species have been identified from living species and the roles of several of these are beginning to be characterized.2 Unlike genes and proteins, whose defined alphabets provide the framework for ontologies and classification at the sequence level, lipids and polysaccharides have been characterized for the large part by popular names, with no foundations for systematic classification. 
 
The past two decades have witnessed two major advances in lipid biology. In the first, mass spectrometry has enabled the identification of thousands of lipid molecular species from cells and tissues and this has pointed to the important need for developing a systematic ontology that can rationally name and catalog the molecules. Second, the ability to investigate the functional roles of lipid molecules through systematic phenotypic studies has led to the identification of lipids as extremely important players in physiology and pathophysiology of living species.3 In combination with proteins and nucleic acids, lipids are integrally involved in biochemical networks that lead to phenotypes such as homeostasis, differentiation, and death of cells and tissues. Any approach to systems characterization of living systems, of necessity, has to include lipids along with other macromolecules and all complex cellular pathways involving lipid molecular species. Systems biology now extends in its scope to identify biosynthetic and metabolic lipid networks, cellular signaling networks that explicitly include lipid molecules and transcriptional and epigenetic networks where lipids play an integral role.4 
 
Several large scale projects to characterize lipids and their functional roles have been initiated as exemplified by the LIPID MAPS5 effort. The LIPID MAPS is an exemplar systems biology project that measures cell-wide lipid changes in an attempt to reconstruct biochemical pathways associated with lipid processing and signaling. The cell-wide measurements of components of these pathways include mass spectrometric measurements of lipid changes in response to stimulus in mammalian cells, changes in transcription profiles in response to stimulus and in select cases proteomic changes in response to stimulus. Figure 1 shows a schematic of the LIPID MAPS experiments related to different lipid categories/pathways and the subsequent processing of the experimental data generated. Network reconstruction efforts rely on organization, analysis and integration of these data and this requires a strong bioinformatics and systems biology effort. The former has to include development of a systematic and universal classification and nomenclature system, design and development of lipid and lipid-gene, lipid-protein databases with appropriate functional annotations, and efficient query and analysis systems that can be broadly useful to the biology research community. The latter has to include methods for analysis of large scale lipid measurements in cells, reconstruction of lipid metabolic and biosynthetic pathways, and quantitative models of lipid fluxes in cells under varied perturbations. In this review, we will provide a comprehensive summary of extant developments in lipid bioinformatics and systems biology and discuss the outlook for the future integration of lipidomics into cellular and organismic biology. The sections that follow are delineated into the informatics approaches specific to lipid biology followed by an overview and exemplar approach to analysis of large scale lipidomic data towards a systems description of mammalian cells. 
 
 
 
Figure 1 
 
Overview of the process of performing a quantitative lipid analysis of macrophage cell sample (in this example, a time-course experiment using bone marrow derived macrophages). Extraction methods, LC/GC purification methods, MS acquisition strategies ... 
 
 
 
 
2. Classification, Ontology, Nomenclature and Structure Representation of Lipid Molecules 
The first step towards classification of lipids is the establishment of an ontology that is extensible, flexible and scalable. One must be able to classify, name and represent these molecules in a logical manner which is amenable to data basing and computational manipulation. Lipids have been loosely defined as biological substances that are generally hydrophobic in nature and in many cases soluble in organic solvents.6 These chemical features are present in a broad range of molecules such as fatty acids, phospholipids, sterols, sphingolipids, terpenes and others. In view of the fact that lipids comprise an extremely heterogeneous collection of molecules from a structural and functional standpoint, it is not surprising that there are significant differences with regard to the scope and organization of current classification schemes. 
 
2.1. Classification, Ontology and Nomenclature 
In order to address the lack of a consistent classification and nomenclature methodology for lipids, LIPID MAPS consortium members have developed a comprehensive classification system for lipids.7 The consortium has taken a more chemistry-based approach and defines lipids as hydrophobic or amphipathic small molecules that may originate entirely or in part by carbanion based condensations of thioesters (such as fatty acids and polyketides) and/or by carbocation based condensations of isoprene units (such as prenols and sterols). Figure 2 shows the mechanisms of lipid biosynthesis.8 Based on this classification system, lipids have been divided into eight categories: Fatty acyls, Glycerolipids, Glycerophospholipids, Sphingolipids, Sterol lipids, Prenol lipids, Saccharolipids, and Polyketides. Each category is further divided into classes and subclasses. Additionally, following the existing rules and recommendations proposed by the International Union of Biochemistry and Applied Chemists and the International Union of Biochemistry and Molecular Biology (IUPAC-IUBMB) commission on Biochemical Nomenclature, a consistent nomenclature scheme has also been developed to provide systematic names for various classes and subclasses of lipids.7 
 
 
 
Figure 2 
 
Mechanisms of lipid biosynthesis. Biosynthesis of ketoacyl- and isoprene-containing lipids proceeds by carbanion and carbocation-mediated chain extension, respectively.8 
 
 
 
All lipids in the LIPID MAPS Structure Database (LMSD) are classified and annotated using this comprehensive classification and nomenclature system developed by the LIPID MAPS consortium.",fullPaper,jv10
p1208,36d44cb9c907defeecc3b72f2076393156bc7dad,c93,Human Language Technology - The Baltic Perspectiv,Multiobjective Genetic Algorithms for Clustering - Applications in Data Mining and Bioinformatics,Abstract content,poster,cp93
p1209,2a4d5bb3e8c61d1bb86dfa093869564ac94ac613,j225,Advances in Experimental Medicine and Biology,Decision tree and ensemble learning algorithms with their applications in bioinformatics.,Abstract content,fullPaper,jv225
p1210,b29c9548e5a39e8a956da075e4f6fa375318f3e9,c23,International Conference on Open and Big Data,BioStar: An Online Question & Answer Resource for the Bioinformatics Community,"Although the era of big data has produced many bioinformatics tools and databases, using them effectively often requires specialized knowledge. Many groups lack bioinformatics expertise, and frequently find that software documentation is inadequate while local colleagues may be overburdened or unfamiliar with specific applications. Too often, such problems create data analysis bottlenecks that hinder the progress of biological research. In order to help address this deficiency, we present BioStar, a forum based on the Stack Exchange platform where experts and those seeking solutions to problems of computational biology exchange ideas. The main strengths of BioStar are its large and active group of knowledgeable users, rapid response times, clear organization of questions and responses that limit discussion to the topic at hand, and ranking of questions and answers that help identify their usefulness. These rankings, based on community votes, also contribute to a reputation score for each user, which serves to keep expert contributors engaged. The BioStar community has helped to answer over 2,300 questions from over 1,400 users (as of June 10, 2011), and has played a critical role in enabling and expediting many research projects. BioStar can be accessed at http://www.biostars.org/.",poster,cp23
p1211,28884a708632a94128a9a8c83473ed803c419a3a,j226,Journal of Clinical Bioinformatics,Clinical bioinformatics: a new emerging science,Abstract content,fullPaper,jv226
p1212,7915fd00117fcc2f380863b2a04ea6638c22951b,j24,Metabolomics,Bioinformatics tools for cancer metabolomics,Abstract content,fullPaper,jv24
p1213,61508fb4e1313d3baae055e0b56938b9f49c6ae7,j85,BMC Bioinformatics,dbOGAP - An Integrated Bioinformatics Resource for Protein O-GlcNAcylation,Abstract content,fullPaper,jv85
p1214,9c7fa07bfbb1c48c9be148afe804d9593575a03d,j178,International Journal of Molecular Sciences,Bioinformatics Tools and Novel Challenges in Long Non-Coding RNAs (lncRNAs) Functional Analysis,"The advent of next generation sequencing revealed that a fraction of transcribed RNAs (short and long RNAs) is non-coding. Long non-coding RNAs (lncRNAs) have a crucial role in regulating gene expression and in epigenetics (chromatin and histones remodeling). LncRNAs may have different roles: gene activators (signaling), repressors (decoy), cis and trans gene expression regulators (guides) and chromatin modificators (scaffolds) without the need to be mutually exclusive. LncRNAs are also implicated in a number of diseases. The huge amount of inhomogeneous data produced so far poses several bioinformatics challenges spanning from the simple annotation to the more complex functional annotation. In this review, we report and discuss several bioinformatics resources freely available and dealing with the study of lncRNAs. To our knowledge, this is the first review summarizing all the available bioinformatics resources on lncRNAs appeared in the literature after the completion of the human genome project. Therefore, the aim of this review is to provide a little guide for biologists and bioinformaticians looking for dedicated resources, public repositories and other tools for lncRNAs functional analysis.",fullPaper,jv178
p1215,bb5306987d169d36d4b217b0b0f6e13e2d5cb097,c63,IEEE International Software Metrics Symposium,The use of classification trees for bioinformatics,"Classification trees are nonparametric statistical learning methods that incorporate feature selection and interactions, possess intuitive interpretability, are efficient, and have high prediction accuracy when used in ensembles. This paper provides a brief introduction to the classification tree‐based methods, a review of the recent developments, and a survey of the applications in bioinformatics and statistical genetics. © 2011 John Wiley & Sons, Inc. WIREs Data Mining Knowl Discov 2011 1 55‐63 DOI: 10.1002/widm.14",poster,cp63
p1216,ed3617a2b4f46ace154bbdaf08d4618381b57a45,j227,Trends in Genetics,Bioinformatics challenges of new sequencing technology.,Abstract content,fullPaper,jv227
p1217,ad220a3283c6c5f81482a9554319151a22d4233f,c29,International Conference on Software Engineering,"Omics technologies, data and bioinformatics principles.",Abstract content,poster,cp29
p1218,da5e628776269bd89bbec28736dc420675dba8df,c84,The Web Conference,Microarray bioinformatics.,Abstract content,poster,cp84
p1219,5624297ba6c1c4e4f1691a27661687462f9d0265,j228,Nature Reviews Neurology,Multimodal monitoring and neurocritical care bioinformatics,Abstract content,fullPaper,jv228
p1220,c67a8498369c0f0affb98f984344b9b027826d88,c77,Networks,Using bioinformatics to predict the functional impact of SNVs,"MOTIVATION
The past decade has seen the introduction of fast and relatively inexpensive methods to detect genetic variation across the genome and exponential growth in the number of known single nucleotide variants (SNVs). There is increasing interest in bioinformatics approaches to identify variants that are functionally important from millions of candidate variants. Here, we describe the essential components of bioinformatics tools that predict functional SNVs.


RESULTS
Bioinformatics tools have great potential to identify functional SNVs, but the black box nature of many tools can be a pitfall for researchers. Understanding the underlying methods, assumptions and biases of these tools is essential to their intelligent application.",poster,cp77
p1221,59f827a4edbdd1ae328081c19f174c26822a214e,c32,International Conference on Software Technology: Methods and Tools,Tools and collaborative environments for bioinformatics research,"Advanced research requires intensive interaction among a multitude of actors, often possessing different expertise and usually working at a distance from each other. The field of collaborative research aims to establish suitable models and technologies to properly support these interactions. In this article, we first present the reasons for an interest of Bioinformatics in this context by also suggesting some research domains that could benefit from collaborative research. We then review the principles and some of the most relevant applications of social networking, with a special attention to networks supporting scientific collaboration, by also highlighting some critical issues, such as identification of users and standardization of formats. We then introduce some systems for collaborative document creation, including wiki systems and tools for ontology development, and review some of the most interesting biological wikis. We also review the principles of Collaborative Development Environments for software and show some examples in Bioinformatics. Finally, we present the principles and some examples of Learning Management Systems. In conclusion, we try to devise some of the goals to be achieved in the short term for the exploitation of these technologies.",poster,cp32
p1222,e1371bc7c700d3752ae645755967db504347d87f,j229,Molecules,Bioinformatics Resources and Tools for Phage Display,"Databases and computational tools for mimotopes have been an important part of phage display study. Five special databases and eighteen algorithms, programs and web servers and their applications are reviewed in this paper. Although these bioinformatics resources have been widely used to exclude target-unrelated peptides, characterize small molecules-protein interactions and map protein-protein interactions, a lot of problems are still waiting to be solved. With the improvement of these tools, they are expected to serve the phage display community better.",fullPaper,jv229
p1223,a937930aaf9f170e2ae2a80da2cbb3491acafa90,c59,British Computer Society Conference on Human-Computer Interaction,Translational bioinformatics: linking knowledge across biological and clinical realms,"Nearly a decade since the completion of the first draft of the human genome, the biomedical community is positioned to usher in a new era of scientific inquiry that links fundamental biological insights with clinical knowledge. Accordingly, holistic approaches are needed to develop and assess hypotheses that incorporate genotypic, phenotypic, and environmental knowledge. This perspective presents translational bioinformatics as a discipline that builds on the successes of bioinformatics and health informatics for the study of complex diseases. The early successes of translational bioinformatics are indicative of the potential to achieve the promise of the Human Genome Project for gaining deeper insights to the genetic underpinnings of disease and progress toward the development of a new generation of therapies.",poster,cp59
p1224,4ad78c487670caeeeec8bcbc13ed841268c39884,j230,Journal of Computing Science and Engineering,A Survey of Transfer and Multitask Learning in Bioinformatics,"Machine learning and data mining have found many applications in biological domains, where we look to build predictive models based on labeled training data. However, in practice, high quality labeled data is scarce, and to label new data incurs high costs. Transfer and multitask learning offer an attractive alternative, by allowing useful knowledge to be extracted and transferred from data in auxiliary domains helps counter the lack of data problem in the target domain. In this article, we survey recent advances in transfer and multitask learning for bioinformatics applications. In particular, we survey several key bioinformatics application areas, including sequence classification, gene expression data analysis, biological network reconstruction and biomedical applications. Category: Convergence computing",fullPaper,jv230
p1225,1b74c8798d6e6052bdeec1e9e51a46eafd41f06c,j226,Journal of Clinical Bioinformatics,Role of clinical bioinformatics in the development of network-based Biomarkers,Abstract content,fullPaper,jv226
p1226,3bc1a4b70d3f157139a048cb1ebe623464ce1b71,c97,Interspeech,Mobyle: a new full web bioinformatics framework,"Motivation: For the biologist, running bioinformatics analyses involves a time-consuming management of data and tools. Users need support to organize their work, retrieve parameters and reproduce their analyses. They also need to be able to combine their analytic tools using a safe data flow software mechanism. Finally, given that scientific tools can be difficult to install, it is particularly helpful for biologists to be able to use these tools through a web user interface. However, providing a web interface for a set of tools raises the problem that a single web portal cannot offer all the existing and possible services: it is the user, again, who has to cope with data copy among a number of different services. A framework enabling portal administrators to build a network of cooperating services would therefore clearly be beneficial. Results: We have designed a system, Mobyle, to provide a flexible and usable Web environment for defining and running bioinformatics analyses. It embeds simple yet powerful data management features that allow the user to reproduce analyses and to combine tools using a hierarchical typing system. Mobyle offers invocation of services distributed over remote Mobyle servers, thus enabling a federated network of curated bioinformatics portals without the user having to learn complex concepts or to install sophisticated software. While being focused on the end user, the Mobyle system also addresses the need, for the bioinfomatician, to automate remote services execution: PlayMOBY is a companion tool that automates the publication of BioMOBY web services, using Mobyle program definitions. Availability: The Mobyle system is distributed under the terms of the GNU GPLv2 on the project web site (http://bioweb2.pasteur.fr/projects/mobyle/). It is already deployed on three servers: http://mobyle.pasteur.fr, http://mobyle.rpbs.univ-paris-diderot.fr and http://lipm-bioinfo.toulouse.inra.fr/Mobyle. The PlayMOBY companion is distributed under the terms of the CeCILL license, and is available at http://lipm-bioinfo.toulouse.inra.fr/biomoby/PlayMOBY/. Contact: mobyle-support@pasteur.fr; mobyle-support@rpbs.univ-paris-diderot.fr; letondal@pasteur.fr Supplementary information:Supplementary data are available at Bioinformatics online.",poster,cp97
p1227,862e3ed2dedaf812d33ffcd24b21fe59de69c004,j231,Toxicological Sciences,The evolution of bioinformatics in toxicology: advancing toxicogenomics.,"As one reflects back through the past 50 years of scientific research, a significant accomplishment was the advance into the genomic era. Basic research scientists have uncovered the genetic code and the foundation of the most fundamental building blocks for the molecular activity that supports biological structure and function. Accompanying these structural and functional discoveries is the advance of techniques and technologies to probe molecular events, in time, across environmental and chemical exposures, within individuals, and across species. The field of toxicology has kept pace with advances in molecular study, and the past 50 years recognizes significant growth and explosive understanding of the impact of the compounds and environment to basic cellular and molecular machinery. The advancement of molecular techniques applied in a whole-genomic capacity to the study of toxicant effects, toxicogenomics, is no doubt a significant milestone for toxicological research. Toxicogenomics has also provided an avenue for advancing a joining of multidisciplinary sciences including engineering and informatics in traditional toxicological research. This review will cover the evolution of the field of toxicogenomics in the context of informatics integration its current promise, and limitations.",fullPaper,jv231
p1228,1e6d09bf67536c6e46ae70555d85cc7c81ba7c90,c35,EUROMICRO Conference on Software Engineering and Advanced Applications,Comparative bioinformatics analysis of the mammalian and bacterial glycomes,"A comparative analysis of bacterial and mammalian glycomes based on the statistical analysis of two major carbohydrate databases, Bacterial CarbohydrateStructure Data Base (BCSDB) and GLYCOSCIENCES.de (GS), is presented. An in-depth comparison of these two glycomes reveals both striking differences and unexpected similarities. Within the prokaryotic kingdom, we focus on the glycomes of seven classes of pathogenic bacteria with respect to (i) their most abundant monosaccharide units; (ii) disaccharide pairs; (iii) carbohydrate modifications; (iv) occurrence and use of sialic acids; and (v) class-specific monosaccharides. The aim of this work is to gain insights into unique carbohydrate patterns in bacteria. Data interpretation reveals significant trends in the composition of specific carbohydrate classes as result of evolution-driven structural adaptations of bacterial pathogens and symbionts to their mammalian hosts. The differences are discussed in light of their value for biomedical applications, such as the targeting of unique glycosyl transferases, vaccine development, and devising novel diagnostic tools.",poster,cp35
p1229,3f49fbb64ac1eec25f48146d24a8f94a790d9a99,j85,BMC Bioinformatics,Visual gene developer: a fully programmable bioinformatics software for synthetic gene optimization,Abstract content,fullPaper,jv85
p1230,56dff84c0afc7d03838027409a2df96487ea4677,c25,International Conference on Contemporary Computing,Kernel Methods in Bioinformatics,Abstract content,poster,cp25
p1231,15a10bbde0cc53be4508ebd04d0230e7fe425d7a,j232,Current Medicinal Chemistry,Structural bioinformatics and its impact to biomedical science.,"During the last two decades, the number of sequence-known proteins has increased rapidly. In contrast, the corresponding increment for structure-known proteins is much slower. The unbalanced situation has critically limited our ability to understand the molecular mechanism of proteins and conduct structure-based drug design by timely using the updated information of newly found sequences. Therefore, it is highly desired to develop an automated method for fast deriving the 3D (3-dimensional) structure of a protein from its sequence. Under such a circumstance, the structural bioinformatics was emerging naturally as the time required. In this review, three main strategies developed in structural bioinformatics, i.e., pure energetic approach, heuristic approach, and homology modeling approach, as well as their underlying principles, are briefly introduced. Meanwhile, a series of demonstrations are presented to show how the structural bioinformatics has been applied to timely derive the 3D structures of some functionally important proteins, helping to understand their action mechanisms and stimulating the course of drug discovery. Also, the limitation of these approaches and the future challenges of structural bioinformatics are briefly addressed.",fullPaper,jv232
p1232,e7d569fd76916cb44c54baf340eea8097fcde480,j85,BMC Bioinformatics,Bioinformatics analysis of disordered proteins in prokaryotes,Abstract content,fullPaper,jv85
p1233,45ffbb76c414b11ade4b8a7fea93296a966d3b25,j233,Biochemical Society Transactions,Bioinformatics of the TULIP domain superfamily.,"Proteins of the BPI (bactericidal/permeability-increasing protein)-like family contain either one or two tandem copies of a fold that usually provides a tubular cavity for the binding of lipids. Bioinformatic analyses show that, in addition to its known members, which include BPI, LBP [LPS (lipopolysaccharide)-binding protein)], CETP (cholesteryl ester-transfer protein), PLTP (phospholipid-transfer protein) and PLUNC (palate, lung and nasal epithelium clone) protein, this family also includes other, more divergent groups containing hypothetical proteins from fungi, nematodes and deep-branching unicellular eukaryotes. More distantly, BPI-like proteins are related to a family of arthropod proteins that includes hormone-binding proteins (Takeout-like; previously described to adopt a BPI-like fold), allergens and several groups of uncharacterized proteins. At even greater evolutionary distance, BPI-like proteins are homologous with the SMP (synaptotagmin-like, mitochondrial and lipid-binding protein) domains, which are found in proteins associated with eukaryotic membrane processes. In particular, SMP domain-containing proteins of yeast form the ERMES [ER (endoplasmic reticulum)-mitochondria encounter structure], required for efficient phospholipid exchange between these organelles. This suggests that SMP domains themselves bind lipids and mediate their exchange between heterologous membranes. The most distant group of homologues we detected consists of uncharacterized animal proteins annotated as TM (transmembrane) 24. We propose to group these families together into one superfamily that we term as the TULIP (tubular lipid-binding) domain superfamily.",fullPaper,jv233
p1234,9f4c3994c2600f15f3dbda8ba4f15cd5ab610b99,c33,International Conference on Agile Software Development,Systems Biology: The Next Frontier for Bioinformatics,"Biochemical systems biology augments more traditional disciplines, such as genomics, biochemistry and molecular biology, by championing (i) mathematical and computational modeling; (ii) the application of traditional engineering practices in the analysis of biochemical systems; and in the past decade increasingly (iii) the use of near-comprehensive data sets derived from ‘omics platform technologies, in particular “downstream” technologies relative to genome sequencing, including transcriptomics, proteomics and metabolomics. The future progress in understanding biological principles will increasingly depend on the development of temporal and spatial analytical techniques that will provide high-resolution data for systems analyses. To date, particularly successful were strategies involving (a) quantitative measurements of cellular components at the mRNA, protein and metabolite levels, as well as in vivo metabolic reaction rates, (b) development of mathematical models that integrate biochemical knowledge with the information generated by high-throughput experiments, and (c) applications to microbial organisms. The inevitable role bioinformatics plays in modern systems biology puts mathematical and computational sciences as an equal partner to analytical and experimental biology. Furthermore, mathematical and computational models are expected to become increasingly prevalent representations of our knowledge about specific biochemical systems.",poster,cp33
p1235,6806fe4a47310f6961cdaeca17fca6aed513d33f,c46,Brazilian Symposium on Software Engineering,BioRuby: bioinformatics software for the Ruby programming language,"Summary: The BioRuby software toolkit contains a comprehensive set of free development tools and libraries for bioinformatics and molecular biology, written in the Ruby programming language. BioRuby has components for sequence analysis, pathway analysis, protein modelling and phylogenetic analysis; it supports many widely used data formats and provides easy access to databases, external programs and public web services, including BLAST, KEGG, GenBank, MEDLINE and GO. BioRuby comes with a tutorial, documentation and an interactive environment, which can be used in the shell, and in the web browser. Availability: BioRuby is free and open source software, made available under the Ruby license. BioRuby runs on all platforms that support Ruby, including Linux, Mac OS X and Windows. And, with JRuby, BioRuby runs on the Java Virtual Machine. The source code is available from http://www.bioruby.org/. Contact: katayama@bioruby.org",poster,cp46
p1236,51485938a6859c593a2b4d3144d8d408c89c06f8,j221,Plant and Cell Physiology,Genomics and Bioinformatics Resources for Crop Improvement,"Recent remarkable innovations in platforms for omics-based research and application development provide crucial resources to promote research in model and applied plant species. A combinatorial approach using multiple omics platforms and integration of their outcomes is now an effective strategy for clarifying molecular systems integral to improving plant productivity. Furthermore, promotion of comparative genomics among model and applied plants allows us to grasp the biological properties of each species and to accelerate gene discovery and functional analyses of genes. Bioinformatics platforms and their associated databases are also essential for the effective design of approaches making the best use of genomic resources, including resource integration. We review recent advances in research platforms and resources in plant omics together with related databases and advances in technology.",fullPaper,jv221
p1237,7e39604a4b65b27da14200b23e950d350da649f5,c69,International Conference on Parallel Processing,CloudBLAST: Combining MapReduce and Virtualization on Distributed Resources for Bioinformatics Applications,"This paper proposes and evaluates an approach to the parallelization, deployment and management of bioinformatics applications that integrates several emerging technologies for distributed computing. The proposed approach uses the MapReduce paradigm to parallelize tools and manage their execution, machine virtualization to encapsulate their execution environments and commonly used data sets into flexibly deployable virtual machines, and network virtualization to connect resources behind firewalls/NATs while preserving the necessary performance and the communication environment. An implementation of this approach is described and used to demonstrate and evaluate the proposed approach. The implementation integrates Hadoop, Virtual Workspaces, and ViNe as the MapReduce, virtual machine and virtual network technologies, respectively, to deploy the commonly used bioinformatics tool NCBI BLAST on a WAN-based test bed consisting of clusters at two distinct locations, the University of Florida and the University of Chicago. This WAN-based implementation, called CloudBLAST, was evaluated against both non-virtualized and LAN-based implementations in order to assess the overheads of machine and network virtualization, which were shown to be insignificant. To compare the proposed approach against an MPI-based solution, CloudBLAST performance was experimentally contrasted against the publicly available mpiBLAST on the same WAN-based test bed. Both versions demonstrated performance gains as the number of available processors increased, with CloudBLAST delivering speedups of 57 against 52.4 of MPI version, when 64 processors on 2 sites were used. The results encourage the use of the proposed approach for the execution of large-scale bioinformatics applications on emerging distributed environments that provide access to computing resources as a service.",poster,cp69
p1238,d9433484c532ab2f14173afba786026fdee0af07,j234,Nature Reviews Microbiology,Detecting genomic islands using bioinformatics approaches,Abstract content,fullPaper,jv234
p1239,05aa2c49c418d2dc099cc945b6f51a845a6a316a,c93,Human Language Technology - The Baltic Perspectiv,BioJava: an open-source framework for bioinformatics,"Summary: BioJava is a mature open-source project that provides a framework for processing of biological data. BioJava contains powerful analysis and statistical routines, tools for parsing common file formats and packages for manipulating sequences and 3D structures. It enables rapid bioinformatics application development in the Java programming language. Availability: BioJava is an open-source project distributed under the Lesser GPL (LGPL). BioJava can be downloaded from the BioJava website (http://www.biojava.org). BioJava requires Java 1.5 or higher. Contact: andreas.prlic@gmail.com. All queries should be directed to the BioJava mailing lists. Details are available at http://biojava.org/wiki/BioJava:MailingLists.",poster,cp93
p1240,047e1fc0dda69819f480fc831882aa9a3b1b47e4,c29,International Conference on Software Engineering,Bioinformatics approaches for genomics and post genomics applications of next-generation sequencing,"Technical advances such as the development of molecular cloning, Sanger sequencing, PCR and oligonucleotide microarrays are key to our current capacity to sequence, annotate and study complete organismal genomes. Recent years have seen the development of a variety of so-called 'next-generation' sequencing platforms, with several others anticipated to become available shortly. The previously unimaginable scale and economy of these methods, coupled with their enthusiastic uptake by the scientific community and the potential for further improvements in accuracy and read length, suggest that these technologies are destined to make a huge and ongoing impact upon genomic and post-genomic biology. However, like the analysis of microarray data and the assembly and annotation of complete genome sequences from conventional sequencing data, the management and analysis of next-generation sequencing data requires (and indeed has already driven) the development of informatics tools able to assemble, map, and interpret huge quantities of relatively or extremely short nucleotide sequence data. Here we provide a broad overview of bioinformatics approaches that have been introduced for several genomics and functional genomics applications of next-generation sequencing.",poster,cp29
p1241,00d6bc10acc69bb10944bc3f7542b8f2461809cb,c110,IEEE International Conference on Automatic Face & Gesture Recognition,Combinatorial pattern discovery in biological sequences: The TEIRESIAS algorithm [published erratum appears in Bioinformatics 1998;14(2): 229],"MOTIVATION
The discovery of motifs in biological sequences is an important problem.


RESULTS
This paper presents a new algorithm for the discovery of rigid patterns (motifs) in biological sequences. Our method is combinatorial in nature and able to produce all patterns that appear in at least a (user-defined) minimum number of sequences, yet it manages to be very efficient by avoiding the enumeration of the entire pattern space. Furthermore, the reported patterns are maximal: any reported pattern cannot be made more specific and still keep on appearing at the exact same positions within the input sequences. The effectiveness of the proposed approach is showcased on a number of test cases which aim to: (i) validate the approach through the discovery of previously reported patterns; (ii) demonstrate the capability to identify automatically highly selective patterns particular to the sequences under consideration. Finally, experimental analysis indicates that the algorithm is output sensitive, i.e. its running time is quasi-linear to the size of the generated output.",poster,cp110
p1242,db5c742121d01d0fedd66dc33da0c1f5f38b2015,j235,Proteomics,PhosphoSite: A bioinformatics resource dedicated to physiological protein phosphorylation,"PhosphoSite™ is a curated, web‐based bioinformatics resource dedicated to physiologic sites of protein phosphorylation in human and mouse. PhosphoSite is populated with information derived from published literature as well as high‐throughput discovery programs. PhosphoSite provides information about the phosphorylated residue and its surrounding sequence, orthologous sites in other species, location of the site within known domains and motifs, and relevant literature references. Links are also provided to a number of external resources for protein sequences, structure, post‐translational modifications and signaling pathways, as well as sources of phospho‐specific antibodies and probes. As the amount of information in the underlying knowledgebase expands, users will be able to systematically search for the kinases, phosphatases, ligands, treatments, and receptors that have been shown to regulate the phosphorylation status of the sites, and pathways in which the phosphorylation sites function. As it develops into a comprehensive resource of known in vivo phosphorylation sites, we expect that PhosphoSite will be a valuable tool for researchers seeking to understand the role of intracellular signaling pathways in a wide variety of biological processes.",fullPaper,jv235
p1243,6383011826c14af5d63700b4a3dd6e33391c55d0,c47,International Symposium on Empirical Software Engineering and Measurement,Bioinformatics: the machine learning approach,"In this book Pierre Baldi and Soren Brunak present the key machine learning approaches and apply them to the computational problems encountered in the analysis of biological data. The book is aimed both at biologists and biochemists who need to understand new data-driven algorithms and at those with a primary background in physics, mathematics, statistics, or computer science who need to know more about applications in molecular biology.",poster,cp47
p1244,cf171d57f8232ba90a0696f8cb46144b39380d0b,c68,International Workshop on Software Engineering for Large-Scale Multi-Agent Systems,Bioinformatics - The Machine Learning Approach,Abstract content,poster,cp68
p1245,887e97d6da0fedd885c1c05dcc899919ab6d189d,j236,Methods of biochemical analysis,Bioinformatics - a practical guide to the analysis of genes and proteins,"Foreword (Lee Hood). Preface. Contributors. PART ONE: BIOLOGICAL DATABASES. 1. Sequence Databases (Rolf Apweiler). 2. Mapping Databases (Peter S. White and Tara C. Matise). 3. Information Retrieval from Biological Databases (Andreas D. Baxevanis). 4. Genomic Databases (Tyra G. Wolfsberg). PART TWO: ANALYSIS AT THE NUCLEOTIDE LEVEL. 5. Predictive Methods Using DNA Sequences (Enrique Blanco and Roderic Guigo). 6. Predictive Methods Using RNA Sequences (David Mathews and Michael Zuker). 7. Sequence Polymorphisms (James C. Mullikin and Stephen T. Sherry). PART THREE: ANALYSIS AT THE PROTEIN LEVEL. 8. Predictive Methods Using Protein Sequences (Yanay Ofran and Burkhard Rost). 9. Protein Structure Prediction and Analysis (David Wishart). 10. Intermolecular Interactions and Biological Pathways (Gary D. Bader and Anton J. Enright). PART FOUR: INFERRING RELATIONSHIPS. 11. Assessing Pairwise Sequence Similarity: BLAST and FASTA (Andreas D. Baxevanis). 12. Creation and Analysis of Protein Multiple Sequence Alignments (Geoffrey J. Barton). 13. Sequence Assembly and Finishing Methods (Nancy F. Hansen, Pamela Jacques Thomas and Gerard G. Bouffard). 14. Phylogenetic Analysis (Fiona S. L. Brinkman). 15. Computational Approaches in Comparative Genomics (Andreas D. Baxevanis). 16. Using DNA Microarrays to Assay Gene Expression (John Quackenbush). 17. Proteomics and Protein Identification (Mark R. Holmes, Kevin R. Ramkissoon and Morgan C. Giddings). PART FIVE: DEVELOPING TOOLS. 18. Using Perl to Facilitate Biological Analysis (Lincoln D. Stein). Appendices. Glossary. Index.",fullPaper,jv236
p1246,cb18c300bf209370736472b244e66911ac02a8f1,j189,Journal of Biomedical Informatics,State of the nation in data integration for bioinformatics,Abstract content,fullPaper,jv189
p1247,a8a294704715ec82ac91f129734bf05ddf9bde14,c10,Big Data,BUSCO: assessing genome assembly and annotation completeness with single-copy orthologs,"MOTIVATION
Genomics has revolutionized biological research, but quality assessment of the resulting assembled sequences is complicated and remains mostly limited to technical measures like N50.


RESULTS
We propose a measure for quantitative assessment of genome assembly and annotation completeness based on evolutionarily informed expectations of gene content. We implemented the assessment procedure in open-source software, with sets of Benchmarking Universal Single-Copy Orthologs, named BUSCO.


AVAILABILITY AND IMPLEMENTATION
Software implemented in Python and datasets available for download from http://busco.ezlab.org.


CONTACT
evgeny.zdobnov@unige.ch


SUPPLEMENTARY INFORMATION
Supplementary data are available at Bioinformatics online.",poster,cp10
p1248,ff8b25a72ca611a4138b48282277f6de78a02e05,c88,Symposium on the Theory of Computing,WIWS: a protein structure bioinformatics Web service collection,"The WHAT IF molecular-modelling and drug design program is widely distributed in the world of protein structure bioinformatics. Although originally designed as an interactive application, its highly modular design and inbuilt control language have recently enabled its deployment as a collection of programmatically accessible web services. We report here a collection of WHAT IF-based protein structure bioinformatics web services: these relate to structure quality, the use of symmetry in crystal structures, structure correction and optimization, adding hydrogens and optimizing hydrogen bonds and a series of geometric calculations. The freely accessible web services are based on the industry standard WS-I profile and the EMBRACE technical guidelines, and are available via both REST and SOAP paradigms. The web services run on a dedicated computational cluster; their function and availability is monitored daily.",poster,cp88
p1249,cc90910b6e31fe44cddc1e341f21eec0aaa5db44,c96,USENIX Symposium on Operating Systems Design and Implementation,Trimmomatic: a flexible trimmer for Illumina sequence data,"Motivation: Although many next-generation sequencing (NGS) read preprocessing tools already existed, we could not find any tool or combination of tools that met our requirements in terms of flexibility, correct handling of paired-end data and high performance. We have developed Trimmomatic as a more flexible and efficient preprocessing tool, which could correctly handle paired-end data. Results: The value of NGS read preprocessing is demonstrated for both reference-based and reference-free tasks. Trimmomatic is shown to produce output that is at least competitive with, and in many cases superior to, that produced by other tools, in all scenarios tested. Availability and implementation: Trimmomatic is licensed under GPL V3. It is cross-platform (Java 1.5+ required) and available at http://www.usadellab.org/cms/index.php?page=trimmomatic Contact: usadel@bio1.rwth-aachen.de Supplementary information: Supplementary data are available at Bioinformatics online.",poster,cp96
p1250,b2bd25d9dc805cdb1fd7f7b1349751d433da80c2,c16,Knowledge Discovery and Data Mining,Probabilistic Modeling in Bioinformatics and Medical Informatics,Abstract content,poster,cp16
p1251,f6ad89da462694bc17fb9e32e8b263e9ed7231ca,j60,PLoS Biology,Incorporating Genomics and Bioinformatics across the Life Sciences Curriculum,"Community Page Incorporating Genomics and Bioinformatics across the Life Sciences Curriculum Jayna L. Ditty 1 , Christopher A. Kvaal 2 , Brad Goodner 3 , Sharyn K. Freyermuth 4 , Cheryl Bailey 5 , Robert A. Britton 6 , Stuart G. Gordon 7 , Sabine Heinhorst 8 , Kelynne Reed 9 , Zhaohui Xu 10 , Erin R. Sanders-Lorenz 11 , Seth Axen 12 , Edwin Kim 12 , Mitrick Johns 13 , Kathleen Scott 14 , Cheryl A. Kerfeld 12,15 * 1 Department of Biology, University of St. Thomas, St. Paul, Minnesota, United States of America, 2 Department of Biological Sciences, St. Cloud State University, St. Cloud, Minnesota, United States of America, 3 Department of Biology, Hiram College, Hiram, Ohio, United States of America, 4 Biochemistry Department, University of Missouri- Columbia, Columbia, Missouri, United States of America, 5 Department of Biochemistry, University of Nebraska-Lincoln, Lincoln, Nebraska, United States of America, 6 Department of Microbiology and Molecular Genetics, Michigan State University, East Lansing, Michigan, United States of America, 7 Department of Biology, Presbyterian College, Clinton, South Carolina, United States of America, 8 Department of Chemistry and Biochemistry, The University of Southern Mississippi, Hattiesburg, Mississippi, United States of America, 9 Biology Department, Austin College, Sherman, Texas, United States of America, 10 Department of Biological Sciences, Bowling Green State University, Bowling Green, Ohio, United States of America, 11 Department of Microbiology, Immunology and Molecular Genetics, University of California – Los Angeles, Los Angeles, California, United States of America, 12 Department of Energy-Joint Genome Institute, Walnut Creek, California, United States of America, 13 Department of Biological Sciences, Northern Illinois University, DeKalb, Illinois, United States of America, 14 Department of Integrative Biology, University of South Florida, Tampa, Florida, United States of America, 15 Department of Plant and Microbial Biology, University of California Berkley, Berkeley, California, United States of America Introduction Undergraduate life sciences education needs an overhaul, as clearly described in the National Research Council of the National Academies’ publication BIO 2010: Transforming Undergraduate Education for Future Research Biologists. Among BIO 2010’s top recommendations is the need to involve students in working with real data and tools that reflect the nature of life sciences research in the 21st century [1]. Education research studies support the importance of utilizing primary literature, designing and implementing experiments, and analyzing results in the context of a bona fide scientific question [1–12] in cultivating the analytical skills necessary to become a scientist. Incorporating these basic scientific methodologies in under- graduate education leads to increased undergraduate and post-graduate reten- tion in the sciences [13–16]. Toward this end, many undergraduate teaching orga- nizations offer training and suggestions for faculty to update and improve their teaching approaches to help students learn as scientists, through design and discovery (e.g., Council of Undergraduate Research [www.cur.org] and Project Kaleidoscope [ www.pkal.org]). With the advent of genome sequencing and bioinformatics, many scientists now formulate biological questions and inter- pret research results in the context of genomic information. Just as the use of bioinformatic tools and databases changed the way scientists investigate problems, it must change how scientists teach to create new opportunities for students to gain experiences reflecting the influence of genomics, proteomics, and bioinformatics on modern life sciences research [17–41]. Educators have responded by incorpo- rating bioinformatics into diverse life science curricula [42–44]. While these published exercises in, and guidelines for, bioinformatics curricula are helpful and inspirational, faculty new to the area of bioinformatics inevitably need training in the theoretical underpinnings of the algo- rithms [45]. Moreover, effectively inte- grating bioinformatics into courses or independent research projects requires infrastructure for organizing and assessing student work. Here, we present a new platform for faculty to keep current with the rapidly changing field of bioinfor- matics, the Integrated Microbial Genomes Annotation Collaboration Toolkit (IMG- ACT) (Figure 1). It was developed by instructors from both research-intensive and predominately undergraduate institu- tions in collaboration with the Department of Energy-Joint Genome Institute (DOE- JGI) as a means to innovate and update undergraduate education and faculty de- velopment. The IMG-ACT program pro- vides a cadre of tools, including access to a clearinghouse of genome sequences, bioin- formatics databases, data storage, instruc- tor course management, and student notebooks for organizing the results of their bioinformatic investigations. In the process, IMG-ACT makes it feasible to provide undergraduate research opportu- nities to a greater number and diversity of students, in contrast to the traditional mentor-to-student apprenticeship model for undergraduate research, which can be too expensive and time-consuming to provide for every undergraduate. The IMG-ACT serves as the hub for the network of faculty and students that use the system for microbial genome analysis. Open access of the IMG-ACT infrastructure to participating schools en- sures that all types of higher education institutions can utilize it. With the infra- structure in place, faculty can focus their efforts on the pedagogy of bioinformatics, involvement of students in research, and use of this tool for their own research agenda. What the original faculty mem- bers of the IMG-ACT development team present here is an overview of how the IMG-ACT program has affected our Citation: Ditty JL, Kvaal CA, Goodner B, Freyermuth SK, Bailey C, et al. (2010) Incorporating Genomics and Bioinformatics across the Life Sciences Curriculum. PLoS Biol 8(8): e1000448. doi:10.1371/journal.pbio.1000448 Published August 10, 2010 Copyright: s 2010 Ditty et al. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. Funding: No specific funding was received for this work. The Community Page is a forum for organizations and societies to highlight their efforts to enhance the dissemination and value of scientific knowledge. Competing Interests: The authors have declared that no competing interests exist. Abbreviations: IMG-ACT; Integrated Microbial Genomes Annotation Collaboration Toolkit * E-mail: ckerfeld@lbl.gov PLoS Biology | www.plosbiology.org August 2010 | Volume 8 | Issue 8 | e1000448",fullPaper,jv60
p1252,2711117464ccbb23a310b9de727c9bcfec86ba2e,c28,International Conference on Collaboration Technologies and Systems,Geneious Basic: An integrated and extendable desktop software platform for the organization and analysis of sequence data,"Summary: The two main functions of bioinformatics are the organization and analysis of biological data using computational resources. Geneious Basic has been designed to be an easy-to-use and flexible desktop software application framework for the organization and analysis of biological data, with a focus on molecular sequences and related data types. It integrates numerous industry-standard discovery analysis tools, with interactive visualizations to generate publication-ready images. One key contribution to researchers in the life sciences is the Geneious public application programming interface (API) that affords the ability to leverage the existing framework of the Geneious Basic software platform for virtually unlimited extension and customization. The result is an increase in the speed and quality of development of computation tools for the life sciences, due to the functionality and graphical user interface available to the developer through the public API. Geneious Basic represents an ideal platform for the bioinformatics community to leverage existing components and to integrate their own specific requirements for the discovery, analysis and visualization of biological data. Availability and implementation: Binaries and public API freely available for download at http://www.geneious.com/basic, implemented in Java and supported on Linux, Apple OSX and MS Windows. The software is also available from the Bio-Linux package repository at http://nebc.nerc.ac.uk/news/geneiousonbl. Contact: peter@biomatters.com",poster,cp28
p1253,0fff44b9387f5708a3634f31b001976b251f28f7,c72,Intelligent Systems in Molecular Biology,myGrid: personalised bioinformatics on the information grid,"MOTIVATION
The (my)Grid project aims to exploit Grid technology, with an emphasis on the Information Grid, and provide middleware layers that make it appropriate for the needs of bioinformatics. (my)Grid is building high level services for data and application integration such as resource discovery, workflow enactment and distributed query processing. Additional services are provided to support the scientific method and best practice found at the bench but often neglected at the workstation, notably provenance management, change notification and personalisation.


RESULTS
We give an overview of these services and their metadata. In particular, semantically rich metadata expressed using ontologies necessary to discover, select and compose services into dynamic workflows.",fullPaper,cp72
p1254,3b81e9d3e78a3f57a502e8478e3433d5ab5530d8,c63,IEEE International Software Metrics Symposium,Over-optimism in bioinformatics: an illustration,"MOTIVATION
In statistical bioinformatics research, different optimization mechanisms potentially lead to 'over-optimism' in published papers. So far, however, a systematic critical study concerning the various sources underlying this over-optimism is lacking.


RESULTS
We present an empirical study on over-optimism using high-dimensional classification as example. Specifically, we consider a 'promising' new classification algorithm, namely linear discriminant analysis incorporating prior knowledge on gene functional groups through an appropriate shrinkage of the within-group covariance matrix. While this approach yields poor results in terms of error rate, we quantitatively demonstrate that it can artificially seem superior to existing approaches if we 'fish for significance'. The investigated sources of over-optimism include the optimization of datasets, of settings, of competing methods and, most importantly, of the method's characteristics. We conclude that, if the improvement of a quantitative criterion such as the error rate is the main contribution of a paper, the superiority of new algorithms should always be demonstrated on independent validation data.


AVAILABILITY
The R codes and relevant data can be downloaded from http://www.ibe.med.uni-muenchen.de/organisation/mitarbeiter/020_professuren/boulesteix/overoptimism/, such that the study is completely reproducible.",poster,cp63
p1255,e0fa4cccc48d3deda3414c7f62eda73b10722c4d,c79,International Conference on Medical Image Computing and Computer-Assisted Intervention,Machine learning: an indispensable tool in bioinformatics.,Abstract content,poster,cp79
p1256,8488dd6a52010c8ff6b5236978cd66c257a6bfc5,j237,Cell,Protein Diversity from Alternative Splicing A Challenge for Bioinformatics and Post-Genome Biology,Abstract content,fullPaper,jv237
p1257,d8792446c467a83d3bb14dcc342926c72e1f7dfb,j172,Nature Protocols,"The Phyre2 web portal for protein modeling, prediction and analysis",Abstract content,fullPaper,jv172
p1258,f06b2502434bf2b4c8536552a11b99bd6b646785,c3,Frontiers in Education Conference,Bioinformatics and molecular modeling in glycobiology,Abstract content,poster,cp3
p1259,0ba69357a59a556759fc09f09ce268b6b7b4f5c2,c53,International Conference on Software Engineering and Knowledge Engineering,Broader incorporation of bioinformatics in education: opportunities and challenges,"The major opportunities for broader incorporation of bioinformatics in education can be placed into three general categories: general applicability of bioinformatics in life science and related curricula; inherent fit of bioinformatics for promoting student learning in most biology programs; and the general experience and associated comfort students have with computers and technology. Conversely, the major challenges for broader incorporation of bioinformatics in education can be placed into three general categories: required infrastructure and logistics; instructor knowledge of bioinformatics and continuing education; and the breadth of bioinformatics, and the diversity of students and educational objectives. Broader incorporation of bioinformatics at all education levels requires overcoming the challenges to using transformative computer-requiring learning activities, assisting faculty in collecting assessment data on mastery of student learning outcomes, as well as creating more faculty development opportunities that span diverse skill levels, with an emphasis placed on providing resource materials that are kept up-to-date as the field and tools change.",poster,cp53
p1260,ca8e9c98ee59737c9e6ac10b11c9b412b3f07684,c27,ACM-SIAM Symposium on Discrete Algorithms,"Bioinformatics training: a review of challenges, actions and support requirements","As bioinformatics becomes increasingly central to research in the molecular life sciences, the need to train non-bioinformaticians to make the most of bioinformatics resources is growing. Here, we review the key challenges and pitfalls to providing effective training for users of bioinformatics services, and discuss successful training strategies shared by a diverse set of bioinformatics trainers. We also identify steps that trainers in bioinformatics could take together to advance the state of the art in current training practices. The ideas presented in this article derive from the first Trainer Networking Session held under the auspices of the EU-funded SLING Integrating Activity, which took place in November 2009.",poster,cp27
p1261,e4b10d45884ec3feb993cc5ca5ba3cf9858af939,c108,International Conference on Information Integration and Web-based Applications & Services,Protein Bioinformatics: From Sequence to Function,"One of the most pressing tasks in biotechnology today is to unlock the function of each of the thousands of new genes identified everyday. Scientists do this by analyzing and interpreting proteins, which are considered the task force of a gene. This single source reference covers all aspects of proteins, explaining fundamentals, synthesizing the latest literature, and demonstrating the most important bioinformatics tools available today for protein analysis, interpretation and prediction. Students and researchers of biotechnology, bioinformatics, proteomics, protein engineering, biophysics, computational biology, molecular modeling, and drug design will find this a ready reference for staying current and productive in this fast evolving interdisciplinary field. It explains all aspects of proteins including sequence and structure analysis, prediction of protein structures, protein folding, protein stability, and protein interactions. It teaches readers how to analyze their own datasets using available online databases, software tools, and web servers, which are listed and updated on the book's web companion page. It presents a cohesive and accessible overview of the field, using illustrations to explain key concepts and detailed exercises for students.",poster,cp108
p1262,bb38c180777326ed103ad0aa31545c8754da959f,c103,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,Machine Learning Approaches to Bioinformatics,"This book covers a wide range of subjects in applying machine learning approaches for bioinformatics projects. The book succeeds on two key unique features. First, it introduces the most widely used machine learning approaches in bioinformatics and discusses, with evaluations from real case studies, how they are used in individual bioinformatics projects. Second, it introduces state-of-the-art bioinformatics research methods. The theoretical parts and the practical parts are well integrated for readers to follow the existing procedures in individual research. Unlike most of the bioinformatics books on the market, the content coverage is not limited to just one subject. A broad spectrum of relevant topics in bioinformatics including systematic data mining and computational systems biology researches are brought together in this book, thereby offering an efficient and convenient platform for teaching purposes. An essential reference for both final year undergraduates and graduate students in universities, as well as a comprehensive handbook for new researchers, this book will also serve as a practical guide for software development in relevant bioinformatics projects.",poster,cp103
p1263,c700eee5f49366deed6d15f625083709cd323485,j238,Molecular biology and evolution,"MEGA5: molecular evolutionary genetics analysis using maximum likelihood, evolutionary distance, and maximum parsimony methods.","Comparative analysis of molecular sequence data is essential for reconstructing the evolutionary histories of species and inferring the nature and extent of selective forces shaping the evolution of genes and species. Here, we announce the release of Molecular Evolutionary Genetics Analysis version 5 (MEGA5), which is a user-friendly software for mining online databases, building sequence alignments and phylogenetic trees, and using methods of evolutionary bioinformatics in basic biology, biomedicine, and evolution. The newest addition in MEGA5 is a collection of maximum likelihood (ML) analyses for inferring evolutionary trees, selecting best-fit substitution models (nucleotide or amino acid), inferring ancestral states and sequences (along with probabilities), and estimating evolutionary rates site-by-site. In computer simulation analyses, ML tree inference algorithms in MEGA5 compared favorably with other software packages in terms of computational efficiency and the accuracy of the estimates of phylogenetic trees, substitution parameters, and rate variation among sites. The MEGA user interface has now been enhanced to be activity driven to make it easier for the use of both beginners and experienced scientists. This version of MEGA is intended for the Windows platform, and it has been configured for effective use on Mac OS X and Linux desktops. It is available free of charge from http://www.megasoftware.net.",fullPaper,jv238
p1264,1c2486bc6e46abe5cfac23eac852f3f38900b882,j20,Proceedings of the National Academy of Sciences of the United States of America,"DNA barcodes: Genes, genomics, and bioinformatics","It is not a coincidence that DNA barcoding has developed in concert with genomics-based investigations. DNA barcoding (a tool for rapid species identification based on DNA sequences) and genomics (which compares entire genome structure and expression) share an emphasis on large-scale genetic data acquisition that offers new answers to questions previously beyond the reach of traditional disciplines. DNA barcodes consist of a standardized short sequence of DNA (400–800 bp) that in principle should be easily generated and characterized for all species on the planet (1). A massive on-line digital library of barcodes will serve as a standard to which the DNA barcode sequence of an unidentified sample from the forest, garden, or market can be matched. Similar to genomics, which has accelerated the process of recognizing novel genes and comparing gene function, DNA barcoding will allow users to efficiently recognize known species and speed the discovery of species yet to be found in nature. DNA barcoding aims to use the information of one or a few gene regions to identify all species of life, whereas genomics, the inverse of barcoding, describes in one (e.g., humans) or a few selected species the function and interactions across all genes (Fig. 1). The work of Lahaye et al. (2) reported in a recent issue of PNAS brings the application of DNA barcoding one step closer to implementation in plants.",fullPaper,jv20
p1265,28a6a69125f826864770c0a1e0e6e33581deabf1,c73,Workshop on Algorithms in Bioinformatics,Complex heatmaps reveal patterns and correlations in multidimensional genomic data,"UNLABELLED
Parallel heatmaps with carefully designed annotation graphics are powerful for efficient visualization of patterns and relationships among high dimensional genomic data. Here we present the ComplexHeatmap package that provides rich functionalities for customizing heatmaps, arranging multiple parallel heatmaps and including user-defined annotation graphics. We demonstrate the power of ComplexHeatmap to easily reveal patterns and correlations among multiple sources of information with four real-world datasets.


AVAILABILITY AND IMPLEMENTATION
The ComplexHeatmap package and documentation are freely available from the Bioconductor project: http://www.bioconductor.org/packages/devel/bioc/html/ComplexHeatmap.html


CONTACT
m.schlesner@dkfz.de


SUPPLEMENTARY INFORMATION
Supplementary data are available at Bioinformatics online.",poster,cp73
p1266,4ca507758234dcf754a7adc458dd574b70a285d7,j0,Nature Biotechnology,Bioinformatics prediction of HIV coreceptor usage,Abstract content,fullPaper,jv0
p1267,43a15ba37c0e1c88ebf28ff7f5cbe7e4ad20d6cf,c53,International Conference on Software Engineering and Knowledge Engineering,CD-HIT: accelerated for clustering the next-generation sequencing data,"Summary: CD-HIT is a widely used program for clustering biological sequences to reduce sequence redundancy and improve the performance of other sequence analyses. In response to the rapid increase in the amount of sequencing data produced by the next-generation sequencing technologies, we have developed a new CD-HIT program accelerated with a novel parallelization strategy and some other techniques to allow efficient clustering of such datasets. Our tests demonstrated very good speedup derived from the parallelization for up to ∼24 cores and a quasi-linear speedup for up to ∼8 cores. The enhanced CD-HIT is capable of handling very large datasets in much shorter time than previous versions. Availability: http://cd-hit.org. Contact: liwz@sdsc.edu Supplementary information: Supplementary data are available at Bioinformatics online.",poster,cp53
p1268,5f04b1c953e9c2842039f9656b7e0c3364af4229,c83,International Conference on Computer Graphics and Interactive Techniques,Gene Expression Atlas at the European Bioinformatics Institute,"The Gene Expression Atlas (http://www.ebi.ac.uk/gxa) is an added-value database providing information about gene expression in different cell types, organism parts, developmental stages, disease states, sample treatments and other biological/experimental conditions. The content of this database derives from curation, re-annotation and statistical analysis of selected data from the ArrayExpress Archive of Functional Genomics Data. A simple interface allows the user to query for differential gene expression either (i) by gene names or attributes such as Gene Ontology terms, or (ii) by biological conditions, e.g. diseases, organism parts or cell types. The gene queries return the conditions where expression has been reported, while condition queries return which genes are reported to be expressed in these conditions. A combination of both query types is possible. The query results are ranked using various statistical measures and by how many independent studies in the database show the particular gene-condition association. Currently, the database contains information about more than 200 000 genes from nine species and almost 4500 biological conditions studied in over 30 000 assays from over 1000 independent studies.",poster,cp83
p1269,305cb07385180e4a2d8b1c7b44cf9dc85c334563,c73,Workshop on Algorithms in Bioinformatics,"Algorithms in Bioinformatics, 5th International Workshop, WABI 2005, Mallorca, Spain, October 3-6, 2005, Proceedings",Abstract content,fullPaper,cp73
p1270,39a42d51b862cfe2f5cf290e50c3c9b4975db6d3,c21,Grid Computing Environments,The EMBL-EBI search and sequence analysis tools APIs in 2019,"Abstract The EMBL-EBI provides free access to popular bioinformatics sequence analysis applications as well as to a full-featured text search engine with powerful cross-referencing and data retrieval capabilities. Access to these services is provided via user-friendly web interfaces and via established RESTful and SOAP Web Services APIs (https://www.ebi.ac.uk/seqdb/confluence/display/JDSAT/EMBL-EBI+Web+Services+APIs+-+Data+Retrieval). Both systems have been developed with the same core principles that allow them to integrate an ever-increasing volume of biological data, making them an integral part of many popular data resources provided at the EMBL-EBI. Here, we describe the latest improvements made to the frameworks which enhance the interconnectivity between public EMBL-EBI resources and ultimately enhance biological data discoverability, accessibility, interoperability and reusability.",poster,cp21
p1271,00c79e5668e5219ae33d6ddf6c0349c63b458a3f,j239,Pharmacogenomics (London),Pharmacogenomics and bioinformatics: PharmGKB.,"The NIH initiated the PharmGKB in April 2000. The primary mission was to create a repository of primary data, tools to track associations between genes and drugs, and to catalog the location and frequency of genetic variations known to impact drug response. Over the past 10 years, new technologies have shifted research from candidate gene pharmacogenetics to phenotype-based pharmacogenomics with a consequent explosion of data. PharmGKB has refocused on curating knowledge rather than housing primary genotype and phenotype data, and now, captures more complex relationships between genes, variants, drugs, diseases and pathways. Going forward, the challenges are to provide the tools and knowledge to plan and interpret genome-wide pharmacogenomics studies, predict gene-drug relationships based on shared mechanisms and support data-sharing consortia investigating clinical applications of pharmacogenomics.",fullPaper,jv239
p1272,51da1eab2d350b5aa0eeebf83fba7caae3a3bc29,j240,International Journal of Systematic and Evolutionary Microbiology,Introducing EzBioCloud: a taxonomically united database of 16S rRNA gene sequences and whole-genome assemblies,"The recent advent of DNA sequencing technologies facilitates the use of genome sequencing data that provide means for more informative and precise classification and identification of members of the Bacteria and Archaea. Because the current species definition is based on the comparison of genome sequences between type and other strains in a given species, building a genome database with correct taxonomic information is of paramount need to enhance our efforts in exploring prokaryotic diversity and discovering novel species as well as for routine identifications. Here we introduce an integrated database, called EzBioCloud, that holds the taxonomic hierarchy of the Bacteria and Archaea, which is represented by quality-controlled 16S rRNA gene and genome sequences. Whole-genome assemblies in the NCBI Assembly Database were screened for low quality and subjected to a composite identification bioinformatics pipeline that employs gene-based searches followed by the calculation of average nucleotide identity. As a result, the database is made of 61 700 species/phylotypes, including 13 132 with validly published names, and 62 362 whole-genome assemblies that were identified taxonomically at the genus, species and subspecies levels. Genomic properties, such as genome size and DNA G+C content, and the occurrence in human microbiome data were calculated for each genus or higher taxa. This united database of taxonomy, 16S rRNA gene and genome sequences, with accompanying bioinformatics tools, should accelerate genome-based classification and identification of members of the Bacteria and Archaea. The database and related search tools are available at www.ezbiocloud.net/.",fullPaper,jv240
p1273,cd0c8bc7a22efced33bdb84e5a9abbd30bbd863b,j219,IEEE/ACM Transactions on Computational Biology & Bioinformatics,Multiobjective Optimization in Bioinformatics and Computational Biology,"This paper reviews the application of multiobjective optimization in the fields of bioinformatics and computational biology. A survey of existing work, organized by application area, forms the main body of the review, following an introduction to the key concepts in multiobjective optimization. An original contribution of the review is the identification of five distinct ""contexts,"" giving rise to multiple objectives: These are used to explain the reasons behind the use of multiobjective optimization in each application area and also to point the way to potential future uses of the technique",fullPaper,jv219
p1274,39258b60531ff5aee0fca118b212b355c3e9821a,j85,BMC Bioinformatics,BLAST+: architecture and applications,Abstract content,fullPaper,jv85
p1275,d84ce894cc3b9114a6de910ce2e1eefe940eb63f,c11,Hawaii International Conference on System Sciences,The Roots of Bioinformatics,"Every new scientific discipline or methodology reaches a point in its maturation where it is fruitful for it to turn its gaze inward, as well as backward. Such introspection helps to clarify the essential structure of a field of study, facilitating communication, pedagogy, standardization, and the like, while retrospection aids this process by accounting for its beginnings and underpinnings. 
 
In this spirit, PLoS Computational Biology is launching a new series of themed articles tracing the roots of bioinformatics. Essays from prominent workers in the field will relate how selected scientific, technological, economic, and even cultural threads came to influence the development of the field we know today. These are not intended to be review articles, nor personal reminiscences, but rather narratives from individual perspectives about the origins and foundations of bioinformatics, and are expected to provide both historical and technical insights. Ideally, these articles will offer an archival record of the field's development, as well as a human face on an important segment of science, for the benefit of current and future workers. 
 
Upcoming articles, already commissioned, will cover the roots of bioinformatics in structural biology, in evolutionary biology, and in artificial intelligence, with more in the works. These topics are obviously very broad, and so are likely to be subdivided or otherwise revisited in future installments by authors with varying perspectives. Topics and authors will be chosen at the discretion of the editors along lines broadly corresponding to the usual content of this journal. 
 
The author, having been asked to serve as Series Editor by the Editor-in-Chief, will endeavor to maintain a uniform flow of articles solicited from luminaries in the field. As a starting point to the series, I offer below a few vignettes and reflections on some longer-term influences that have shaped the discipline. I first consider the unique status of bioinformatics vis-a-vis science and technology, and then explore historical trends in biology and related fields that anticipated and prepared the way for bioinformatics. Examining the context of key moments when computers were first taken up by early adopters reveals how deep the roots of bioinformatics go.",poster,cp11
p1276,99649284503c2abe6871ae45f3dbf394a3a86352,j241,Science Signaling,Integrative Analysis of Complex Cancer Genomics and Clinical Profiles Using the cBioPortal,"The cBioPortal enables integration, visualization, and analysis of multidimensional cancer genomic and clinical data. The cBioPortal for Cancer Genomics (http://cbioportal.org) provides a Web resource for exploring, visualizing, and analyzing multidimensional cancer genomics data. The portal reduces molecular profiling data from cancer tissues and cell lines into readily understandable genetic, epigenetic, gene expression, and proteomic events. The query interface combined with customized data storage enables researchers to interactively explore genetic alterations across samples, genes, and pathways and, when available in the underlying data, to link these to clinical outcomes. The portal provides graphical summaries of gene-level data from multiple platforms, network visualization and analysis, survival analysis, patient-centric queries, and software programmatic access. The intuitive Web interface of the portal makes complex cancer genomics profiles accessible to researchers and clinicians without requiring bioinformatics expertise, thus facilitating biological discoveries. Here, we provide a practical guide to the analysis and visualization features of the cBioPortal for Cancer Genomics.",fullPaper,jv241
p1277,6e6f0fb09cc0604e1dca938c1950f1a3e1550c4a,c53,International Conference on Software Engineering and Knowledge Engineering,"Anatomy Ontologies for Bioinformatics, Principles and Practice",Abstract content,poster,cp53
p1278,f618bbecc1abf333bdcda46a9c2f4938e3c79e0f,c28,International Conference on Collaboration Technologies and Systems,Metabolomics technology and bioinformatics,"Metabolomics is the global analysis of all or a large number of cellular metabolites. Like other functional genomics research, metabolomics generates large amounts of data. Handling, processing and analysis of this data is a clear challenge and requires specialized mathematical, statistical and bioinformatics tools. Metabolomics needs for bioinformatics span through data and information management, raw analytical data processing, metabolomics standards and ontology, statistical analysis and data mining, data integration and mathematical modelling of metabolic networks within a framework of systems biology. The major approaches in metabolomics, along with the modern analytical tools used for data generation, are reviewed in the context of these specific bioinformatics needs.",poster,cp28
p1279,984bb8680c30bbe8d2979486259efb373d108e62,c51,Conference of the Centre for Advanced Studies on Collaborative Research,Phylogenetic diversity (PD) and biodiversity conservation: some bioinformatics challenges,"Biodiversity conservation addresses information challenges through estimations encapsulated in measures of diversity. A quantitative measure of phylogenetic diversity, “PD”, has been defined as the minimum total length of all the phylogenetic branches required to span a given set of taxa on the phylogenetic tree (Faith 1992a). While a recent paper incorrectly characterizes PD as not including information about deeper phylogenetic branches, PD applications over the past decade document the proper incorporation of shared deep branches when assessing the total PD of a set of taxa. Current PD applications to macroinvertebrate taxa in streams of New South Wales, Australia illustrate the practical importance of this definition. Phylogenetic lineages, often corresponding to new, “cryptic”, taxa, are restricted to a small number of stream localities. A recent case of human impact causing loss of taxa in one locality implies a higher PD value for another locality, because it now uniquely represents a deeper branch. This molecular-based phylogenetic pattern supports the use of DNA barcoding programs for biodiversity conservation planning. Here, PD assessments side-step the contentious use of barcoding-based “species” designations. Bioinformatics challenges include combining different phylogenetic evidence, optimization problems for conservation planning, and effective integration of phylogenetic information with environmental and socioeconomic data.",poster,cp51
p1280,39dae53515afb42664369c291ec6d1ce34d778bd,j85,BMC Bioinformatics,WGCNA: an R package for weighted correlation network analysis,Abstract content,fullPaper,jv85
p1281,0e6aefc52cc98104fdd802cd4fa9878d9f5fbca5,c10,Big Data,Cytoscape 2.8: new features for data integration and network visualization,"Summary: Cytoscape is a popular bioinformatics package for biological network visualization and data integration. Version 2.8 introduces two powerful new features—Custom Node Graphics and Attribute Equations—which can be used jointly to greatly enhance Cytoscape's data integration and visualization capabilities. Custom Node Graphics allow an image to be projected onto a node, including images generated dynamically or at remote locations. Attribute Equations provide Cytoscape with spreadsheet-like functionality in which the value of an attribute is computed dynamically as a function of other attributes and network properties. Availability and implementation: Cytoscape is a desktop Java application released under the Library Gnu Public License (LGPL). Binary install bundles and source code for Cytoscape 2.8 are available for download from http://cytoscape.org. Contact: msmoot@ucsd.edu",poster,cp10
p1282,378b5e4f4305fa723a32eb489abba4a917589397,j242,Genome Medicine,Translational bioinformatics in the cloud: an affordable alternative,Abstract content,fullPaper,jv242
p1283,86c9b7b093210f8435cbc66dd6b0b16df570af59,j219,IEEE/ACM Transactions on Computational Biology & Bioinformatics,Cache-Oblivious Dynamic Programming for Bioinformatics,"We present efficient cache-oblivious algorithms for some well-studied string problems in bioinformatics including the longest common subsequence, global pairwise sequence alignment and three-way sequence alignment (or median), both with affine gap costs, and RNA secondary structure prediction with simple pseudoknots. For each of these problems, we present cache-oblivious algorithms that match the best-known time complexity, match or improve the best-known space complexity, and improve significantly over the cache-efficiency of earlier algorithms. We present experimental results which show that our cache-oblivious algorithms run faster than software and implementations based on previous best algorithms for these problems.",fullPaper,jv219
p1284,a114acb9c90d29d9611674824b01a007b6b7a115,c106,Chinese Conference on Biometric Recognition,Bismark: a flexible aligner and methylation caller for Bisulfite-Seq applications,"Summary: A combination of bisulfite treatment of DNA and high-throughput sequencing (BS-Seq) can capture a snapshot of a cell's epigenomic state by revealing its genome-wide cytosine methylation at single base resolution. Bismark is a flexible tool for the time-efficient analysis of BS-Seq data which performs both read mapping and methylation calling in a single convenient step. Its output discriminates between cytosines in CpG, CHG and CHH context and enables bench scientists to visualize and interpret their methylation data soon after the sequencing run is completed. Availability and implementation: Bismark is released under the GNU GPLv3+ licence. The source code is freely available from www.bioinformatics.bbsrc.ac.uk/projects/bismark/. Contact: felix.krueger@bbsrc.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.",poster,cp106
p1285,548fe25043edbe4539c68550383875466ed0d777,c31,International Conference on Evaluation & Assessment in Software Engineering,The Protein Data Bank,"The Protein Data Bank [PDB; Berman, Westbrook et al. (2000), Nucleic Acids Res. 28, 235-242; http://www.pdb.org/] is the single worldwide archive of primary structural data of biological macromolecules. Many secondary sources of information are derived from PDB data. It is the starting point for studies in structural bioinformatics. This article describes the goals of the PDB, the systems in place for data deposition and access, how to obtain further information and plans for the future development of the resource. The reader should come away with an understanding of the scope of the PDB and what is provided by the resource.",poster,cp31
p1286,c5a3be735cfcf83b6337bbb62c43433f2d9f4edc,j243,Journal of Antimicrobial Chemotherapy,ResFinder 4.0 for predictions of phenotypes from genotypes,"Abstract Objectives WGS-based antimicrobial susceptibility testing (AST) is as reliable as phenotypic AST for several antimicrobial/bacterial species combinations. However, routine use of WGS-based AST is hindered by the need for bioinformatics skills and knowledge of antimicrobial resistance (AMR) determinants to operate the vast majority of tools developed to date. By leveraging on ResFinder and PointFinder, two freely accessible tools that can also assist users without bioinformatics skills, we aimed at increasing their speed and providing an easily interpretable antibiogram as output. Methods The ResFinder code was re-written to process raw reads and use Kmer-based alignment. The existing ResFinder and PointFinder databases were revised and expanded. Additional databases were developed including a genotype-to-phenotype key associating each AMR determinant with a phenotype at the antimicrobial compound level, and species-specific panels for in silico antibiograms. ResFinder 4.0 was validated using Escherichia coli (n = 584), Salmonella spp. (n = 1081), Campylobacter jejuni (n = 239), Enterococcus faecium (n = 106), Enterococcus faecalis (n = 50) and Staphylococcus aureus (n = 163) exhibiting different AST profiles, and from different human and animal sources and geographical origins. Results Genotype–phenotype concordance was ≥95% for 46/51 and 25/32 of the antimicrobial/species combinations evaluated for Gram-negative and Gram-positive bacteria, respectively. When genotype–phenotype concordance was <95%, discrepancies were mainly linked to criteria for interpretation of phenotypic tests and suboptimal sequence quality, and not to ResFinder 4.0 performance. Conclusions WGS-based AST using ResFinder 4.0 provides in silico antibiograms as reliable as those obtained by phenotypic AST at least for the bacterial species/antimicrobial agents of major public health relevance considered.",fullPaper,jv243
p1287,fd0e0fb078a7be850b87ed148e85b8f69c3ac5da,c25,International Conference on Contemporary Computing,Over-optimism in bioinformatics research,"The problem of ”false research findings” in medical research has focused much attention in the last few years (Ioannidis, 2005). One of the main problems, termed as ”fishing for significance” in the present letter, is that researchers often (consciously or subconsciously) report results that are in fact the product of an intensive optimization, i.e. of multiple comparisons. Such results are typically unlikely to be reproduced in an independent study and have a high probability to be false (Ioannidis, 2005). The ”fishing for significance” problem is enhanced by the so-called ”publication bias”: positive results have a much higher chance to get published than negative results, as already acknowledged fifty years ago (Sterling, 1959). In a word, many false positive results are produced through multiple comparisons, and false positives have higher chance to get published than true negatives. Moreover, the difficulty to publish negative results obviously encourages authors to find something positive in their study by performing numerous analyses until one of them yields positive results by chance, i.e. to fish for significance. Although this issue is by far less acknowledged and publicly admitted than in the medical context, the same types of problems occur in biostatistics and bioinformatics research.",poster,cp25
p1288,e04796bee4f83e99250dce77c88d09ed1b8ecf82,j244,Genes,Bioinformatics for Next Generation Sequencing Data,"The emergence of next-generation sequencing (NGS) platforms imposes increasing demands on statistical methods and bioinformatic tools for the analysis and the management of the huge amounts of data generated by these technologies. Even at the early stages of their commercial availability, a large number of softwares already exist for analyzing NGS data. These tools can be fit into many general categories including alignment of sequence reads to a reference, base-calling and/or polymorphism detection, de novo assembly from paired or unpaired reads, structural variant detection and genome browsing. This manuscript aims to guide readers in the choice of the available computational tools that can be used to face the several steps of the data analysis workflow.",fullPaper,jv244
p1289,4fd99625a934559e30dc9b7a64f9f9237b15157c,j209,Human Mutation,"Pathogenic or not? And if so, then how? Studying the effects of missense mutations using bioinformatics methods","Many gene defects are relatively easy to identify experimentally, but obtaining information about the effects of sequence variations and elucidation of the detailed molecular mechanisms of genetic diseases will be among the next major efforts in mutation research. Amino acid substitutions may have diverse effects on protein structure and function; thus, a detailed analysis of the mutations is essential. Experimental study of the molecular effects of mutations is laborious, whereas useful and reliable information about the effects of amino acid substitutions can readily be obtained by theoretical methods. Experimentally defined structures and molecular modeling can be used as a basis for interpretation of the mutations. The effects of missense mutations can be analyzed even when the 3D structure of the protein has not been determined, although structure‐based analyses are more reliable. Structural analyses include studies of the contacts between residues, their implication for the stability of the protein, and the effects of the introduced residues. Investigations of steric and stereochemical consequences of substitutions provide insights on the molecular fit of the introduced residue. Mutations that change the electrostatic surface potential of a protein have wide‐ranging effects. Analyses of the effects of mutations on interactions with ligands and partners have been performed for elucidation of functional mutations. We have employed numerous methods for predicting the effects of amino acid substitutions. We discuss the applicability of these methods in the analysis of genes, proteins, and diseases to reveal protein structure–function relationships, which is essential to gain insights into disease genotype–phenotype correlations. Hum Mutat 0, 1–15, 2009. © 2009 Wiley‐Liss, Inc.",fullPaper,jv209
p1290,93315068530a331e50edba8618234c2da4e04761,c49,International Symposium on Search Based Software Engineering,Introduction to bioinformatics,"Bioinformatics is a multifaceted discipline combining many scientific fields including computational biology, statistics, mathematics, molecular biology, and genetics. Bioinformatics enables biomedical investigators to exploit existing and emerging computational technologies to seamlessly store, mine, retrieve, and analyze data from genomics and proteomics technologies. This is achieved by creating unified data models, standardizing data interfaces, developing structured vocabularies, generating new data visualization methods, and capturing detailed metadata that describes various aspects of the experimental design and analysis methods. Already there are a number of related undertakings that are dividing the field into more specialized groups. Clinical Bioinformatics and Biomedical Informatics are emerging as transitional fields to promote the utilization of genomics and proteomics data combined with medical history and demographic data towards personalized medicine, molecular diagnostics, pharmacogenomics and predicting outcomes of therapeutic interventions. The field of bioinformatics will continue to evolve through the incorporation of diverse technologies and methodologies that draw experts from disparate fields to create the latest computational and informational tools specifically design for the biomedical research enterprise.",poster,cp49
p1291,239b44fd21c3f949f9fdf43bfedd4ab2a3d314e3,j85,BMC Bioinformatics,pROC: an open-source package for R and S+ to analyze and compare ROC curves,Abstract content,fullPaper,jv85
p1292,584fd593dc69720afad8b1b5663cd9aea4da9c45,c59,British Computer Society Conference on Human-Computer Interaction,Cd-hit: a fast program for clustering and comparing large sets of protein or nucleotide sequences,"MOTIVATION
In 2001 and 2002, we published two papers (Bioinformatics, 17, 282-283, Bioinformatics, 18, 77-82) describing an ultrafast protein sequence clustering program called cd-hit. This program can efficiently cluster a huge protein database with millions of sequences. However, the applications of the underlying algorithm are not limited to only protein sequences clustering, here we present several new programs using the same algorithm including cd-hit-2d, cd-hit-est and cd-hit-est-2d. Cd-hit-2d compares two protein datasets and reports similar matches between them; cd-hit-est clusters a DNA/RNA sequence database and cd-hit-est-2d compares two nucleotide datasets. All these programs can handle huge datasets with millions of sequences and can be hundreds of times faster than methods based on the popular sequence comparison and database search tools, such as BLAST.",poster,cp59
p1293,b450dedf0079b80c490e57fd9a20464351e5cbf3,j245,IEEE Transactions on Visualization and Computer Graphics,An insight-based methodology for evaluating bioinformatics visualizations,"High-throughput experiments, such as gene expression microarrays in the life sciences, result in very large data sets. In response, a wide variety of visualization tools have been created to facilitate data analysis. A primary purpose of these tools is to provide biologically relevant insight into the data. Typically, visualizations are evaluated in controlled studies that measure user performance on predetermined tasks or using heuristics and expert reviews. To evaluate and rank bioinformatics visualizations based on real-world data analysis scenarios, we developed a more relevant evaluation method that focuses on data insight. This paper presents several characteristics of insight that enabled us to recognize and quantify it in open-ended user tests. Using these characteristics, we evaluated five microarray visualization tools on the amount and types of insight they provide and the time it takes to acquire it. The results of the study guide biologists in selecting a visualization tool based on the type of their microarray data, visualization designers on the key role of user interaction techniques, and evaluators on a new approach for evaluating the effectiveness of visualizations for providing insight. Though we used the method to analyze bioinformatics visualizations, it can be applied to other domains.",fullPaper,jv245
p1294,6654afe7bdb669fa109891a23b24159315ad1677,c29,International Conference on Software Engineering,The SEED and the Rapid Annotation of microbial genomes using Subsystems Technology (RAST),"In 2004, the SEED (http://pubseed.theseed.org/) was created to provide consistent and accurate genome annotations across thousands of genomes and as a platform for discovering and developing de novo annotations. The SEED is a constantly updated integration of genomic data with a genome database, web front end, API and server scripts. It is used by many scientists for predicting gene functions and discovering new pathways. In addition to being a powerful database for bioinformatics research, the SEED also houses subsystems (collections of functionally related protein families) and their derived FIGfams (protein families), which represent the core of the RAST annotation engine (http://rast.nmpdr.org/). When a new genome is submitted to RAST, genes are called and their annotations are made by comparison to the FIGfam collection. If the genome is made public, it is then housed within the SEED and its proteins populate the FIGfam collection. This annotation cycle has proven to be a robust and scalable solution to the problem of annotating the exponentially increasing number of genomes. To date, >12 000 users worldwide have annotated >60 000 distinct genomes using RAST. Here we describe the interconnectedness of the SEED database and RAST, the RAST annotation pipeline and updates to both resources.",poster,cp29
p1295,a445a855650696b008cac2f8caa4e58249512ee2,c6,Americas Conference on Information Systems,trimAl: a tool for automated alignment trimming in large-scale phylogenetic analyses,"Summary: Multiple sequence alignments are central to many areas of bioinformatics. It has been shown that the removal of poorly aligned regions from an alignment increases the quality of subsequent analyses. Such an alignment trimming phase is complicated in large-scale phylogenetic analyses that deal with thousands of alignments. Here, we present trimAl, a tool for automated alignment trimming, which is especially suited for large-scale phylogenetic analyses. trimAl can consider several parameters, alone or in multiple combinations, for selecting the most reliable positions in the alignment. These include the proportion of sequences with a gap, the level of amino acid similarity and, if several alignments for the same set of sequences are provided, the level of consistency across different alignments. Moreover, trimAl can automatically select the parameters to be used in each specific alignment so that the signal-to-noise ratio is optimized. Availability: trimAl has been written in C++, it is portable to all platforms. trimAl is freely available for download (http://trimal.cgenomics.org) and can be used online through the Phylemon web server (http://phylemon2.bioinfo.cipf.es/). Supplementary Material is available at http://trimal.cgenomics.org/publications. Contact: tgabaldon@crg.es",poster,cp6
p1296,c0450f920c178d25e7707fa00432516fa6a96ce5,c64,Experimental Software Engineering Network,ClueGO: a Cytoscape plug-in to decipher functionally grouped gene ontology and pathway annotation networks,"Summary: We have developed ClueGO, an easy to use Cytoscape plug-in that strongly improves biological interpretation of large lists of genes. ClueGO integrates Gene Ontology (GO) terms as well as KEGG/BioCarta pathways and creates a functionally organized GO/pathway term network. It can analyze one or compare two lists of genes and comprehensively visualizes functionally grouped terms. A one-click update option allows ClueGO to automatically download the most recent GO/KEGG release at any time. ClueGO provides an intuitive representation of the analysis results and can be optionally used in conjunction with the GOlorize plug-in. Availability: http://www.ici.upmc.fr/cluego/cluegoDownload.shtml Contact: jerome.galon@crc.jussieu.fr Supplementary information: Supplementary data are available at Bioinformatics online.",poster,cp64
p1297,e3831b9cf7292a29a87ba4574a61b78d0e555994,c104,IEEE International Conference on Multimedia and Expo,Penalized feature selection and classification in bioinformatics,"In bioinformatics studies, supervised classification with high-dimensional input variables is frequently encountered. Examples routinely arise in genomic, epigenetic and proteomic studies. Feature selection can be employed along with classifier construction to avoid over-fitting, to generate more reliable classifier and to provide more insights into the underlying causal relationships. In this article, we provide a review of several recently developed penalized feature selection and classification techniques--which belong to the family of embedded feature selection methods--for bioinformatics studies with high-dimensional input. Classification objective functions, penalty functions and computational algorithms are discussed. Our goal is to make interested researchers aware of these feature selection and classification methods that are applicable to high-dimensional bioinformatics data.",poster,cp104
p1298,31ff2cf6c374eccc85b3c188d96b547f05e19183,j85,BMC Bioinformatics,I-TASSER server for protein 3D structure prediction,Abstract content,fullPaper,jv85
p1299,3ed174d7751c27c489584f29ca3ff19084808426,c65,Formal Concept Analysis,Recent developments in the MAFFT multiple sequence alignment program,"The accuracy and scalability of multiple sequence alignment (MSA) of DNAs and proteins have long been and are still important issues in bioinformatics. To rapidly construct a reasonable MSA, we developed the initial version of the MAFFT program in 2002. MSA software is now facing greater challenges in both scalability and accuracy than those of 5 years ago. As increasing amounts of sequence data are being generated by large-scale sequencing projects, scalability is now critical in many situations. The requirement of accuracy has also entered a new stage since the discovery of functional noncoding RNAs (ncRNAs); the secondary structure should be considered for constructing a high-quality alignment of distantly related ncRNAs. To deal with these problems, in 2007, we updated MAFFT to Version 6 with two new techniques: the PartTree algorithm and the Four-way consistency objective function. The former improved the scalability of progressive alignment and the latter improved the accuracy of ncRNA alignment. We review these and other techniques that MAFFT uses and suggest possible future directions of MSA software as a basis of comparative analyses. MAFFT is available at http://align.bmr.kyushu-u.ac.jp/mafft/software/.",poster,cp65
p1300,31ff2cf6c374eccc85b3c188d96b547f05e19183,j85,BMC Bioinformatics,I-TASSER server for protein 3D structure prediction,Abstract content,fullPaper,jv85
p1301,00b7e1a7228c49492d0506f2165c593a3e52e2b1,j97,Science,"Structural Bioinformatics-Based Design of Selective, Irreversible Kinase Inhibitors","The active sites of 491 human protein kinase domains are highly conserved, which makes the design of selective inhibitors a formidable challenge. We used a structural bioinformatics approach to identify two selectivity filters, a threonine and a cysteine, at defined positions in the active site of p90 ribosomal protein S6 kinase (RSK). A fluoromethylketone inhibitor, designed to exploit both selectivity filters, potently and selectively inactivated RSK1 and RSK2 in mammalian cells. Kinases with only one selectivity filter were resistant to the inhibitor, yet they became sensitized after genetic introduction of the second selectivity filter. Thus, two amino acids that distinguish RSK from other protein kinases are sufficient to confer inhibitor sensitivity.",fullPaper,jv97
p1302,e64728f38ddeb4a134e92610f280a546c82bda91,j153,bioRxiv,Roary: rapid large-scale prokaryote pan genome analysis,"Summary A typical prokaryote population sequencing study can now consist of hundreds or thousands of isolates. Interrogating these datasets can provide detailed insights into the genetic structure of of prokaryotic genomes. We introduce Roary, a tool that rapidly builds large-scale pan genomes, identifying the core and dispensable accessory genes. Roary makes construction of the pan genome of thousands of prokaryote samples possible on a standard desktop without compromising on the accuracy of results. Using a single CPU Roary can produce a pan genome consisting of 1000 isolates in 4.5 hours using 13 GB of RAM, with further speedups possible using multiple processors. Availability and implementation Roary is implemented in Perl and is freely available under an open source GPLv3 license from http://sanger-pathogens.github.io/Roary Contact roary@sanger.ac.uk Supplementary information Supplementary data are available at Bioinformatics online.",fullPaper,jv153
p1303,f581ffaa6a409b36c249dd7453ccde16bb8cb89c,c103,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,MultiQC: summarize analysis results for multiple tools and samples in a single report,"Motivation: Fast and accurate quality control is essential for studies involving next-generation sequencing data. Whilst numerous tools exist to quantify QC metrics, there is no common approach to flexibly integrate these across tools and large sample sets. Assessing analysis results across an entire project can be time consuming and error prone; batch effects and outlier samples can easily be missed in the early stages of analysis. Results: We present MultiQC, a tool to create a single report visualising output from multiple tools across many samples, enabling global trends and biases to be quickly identified. MultiQC can plot data from many common bioinformatics tools and is built to allow easy extension and customization. Availability and implementation: MultiQC is available with an GNU GPLv3 license on GitHub, the Python Package Index and Bioconda. Documentation and example reports are available at http://multiqc.info Contact: phil.ewels@scilifelab.se",poster,cp103
p1304,770833aec0e6d8f6eaa642c948bb04b6e66fc036,j246,Methods of Information in Medicine,What is Bioinformatics? A Proposed Definition and Overview of the Field,"Summary Background: The recent flood of data from genome sequences and functional genomics has given rise to new field, bioinformatics, which combines elements of biology and computer science. Objectives: Here we propose a definition for this new field and review some of the research that is being pursued, particularly in relation to transcriptional regulatory systems. Methods: Our definition is as follows: Bioinformatics is conceptualizing biology in terms of macromolecules (in the sense of physical-chemistry) and then applying “informatics” techniques (derived from disciplines such as applied maths, computer science, and statistics) to understand and organize the information associated with these molecules, on a large-scale. Results and Conclusions: Analyses in bioinformatics predominantly focus on three types of large datasets available in molecular biology: macromolecular structures, genome sequences, and the results of functional genomics experiments (eg expression data). Additional information includes the text of scientific papers and “relationship data” from metabolic pathways, taxonomy trees, and protein-protein interaction networks. Bioinformatics employs a wide range of computational techniques including sequence and structural alignment, database design and data mining, macromolecular geometry, phylogenetic tree construction, prediction of protein structure and function, gene finding, and expression data clustering. The emphasis is on approaches integrating a variety of computational methods and heterogeneous data sources. Finally, bioinformatics is a practical discipline. We survey some representative applications, such as finding homologues, designing drugs, and performing large-scale censuses. Additional information pertinent to the review is available over the web at http://bioinfo.mbb.yale.edu/what-is-it.",fullPaper,jv246
p1305,8df59e797d3671b0dc75f2b6c1ca9af9533166cc,c58,Australian Software Engineering Conference,Multiple sequence alignment with the Clustal series of programs,"The Clustal series of programs are widely used in molecular biology for the multiple alignment of both nucleic acid and protein sequences and for preparing phylogenetic trees. The popularity of the programs depends on a number of factors, including not only the accuracy of the results, but also the robustness, portability and user-friendliness of the programs. New features include NEXUS and FASTA format output, printing range numbers and faster tree calculation. Although, Clustal was originally developed to run on a local computer, numerous Web servers have been set up, notably at the EBI (European Bioinformatics Institute) (http://www.ebi.ac.uk/clustalw/).",poster,cp58
p1306,bd1f14e7531220c39fad8f86985cce7b283f035d,c74,IEEE International Conference on Tools with Artificial Intelligence,Kernel Methods for Pattern Analysis,"Kernel methods provide a powerful and unified framework for pattern discovery, motivating algorithms that can act on general types of data (e.g. strings, vectors or text) and look for general types of relations (e.g. rankings, classifications, regressions, clusters). The application areas range from neural networks and pattern recognition to machine learning and data mining. This book, developed from lectures and tutorials, fulfils two major roles: firstly it provides practitioners with a large toolkit of algorithms, kernels and solutions ready to use for standard pattern discovery problems in fields such as bioinformatics, text analysis, image analysis. Secondly it provides an easy introduction for students and researchers to the growing field of kernel-based pattern analysis, demonstrating with examples how to handcraft an algorithm or a kernel for a new specific application, and covering all the necessary conceptual and mathematical tools to do so.",fullPaper,cp74
p1307,1f4d6c1d0a7191c677049444e05dc282d46a34e1,c112,Very Large Data Bases Conference,Bioinformatics for Whole-Genome Shotgun Sequencing of Microbial Communities,"The application of whole-genome shotgun sequencing to microbial communities represents a major development in metagenomics, the study of uncultured microbes via the tools of modern genomic analysis. In the past year, whole-genome shotgun sequencing projects of prokaryotic communities from an acid mine biofilm, the Sargasso Sea, Minnesota farm soil, three deep-sea whale falls, and deep-sea sediments have been reported, adding to previously published work on viral communities from marine and fecal samples. The interpretation of this new kind of data poses a wide variety of exciting and difficult bioinformatics problems. The aim of this review is to introduce the bioinformatics community to this emerging field by surveying existing techniques and promising new approaches for several of the most interesting of these computational problems.",poster,cp112
p1308,b18d3c40f373bff28d5cfe3d8d2a674c7aed6841,j20,Proceedings of the National Academy of Sciences of the United States of America,Bioinformatics identification of MurJ (MviN) as the peptidoglycan lipid II flippase in Escherichia coli,"Peptidoglycan is a cell-wall glycopeptide polymer that protects bacteria from osmotic lysis. Whereas in Gram-positive bacteria it also serves as scaffold for many virulence factors, in Gram-negative bacteria, peptidoglycan is an anchor for the outer membrane. For years, we have known the enzymes required for the biosynthesis of peptidoglycan; what was missing was the flippase that translocates the lipid-anchored precursors across the cytoplasmic membrane before their polymerization into mature peptidoglycan. Using a reductionist bioinformatics approach, I have identified the essential inner-membrane protein MviN (renamed MurJ) as a likely candidate for the peptidoglycan flippase in Escherichia coli. Here, I present genetic and biochemical data that confirm the requirement of MurJ for peptidoglycan biosynthesis and that are in agreement with a role of MurJ as a flippase. Because of its essential nature, MurJ could serve as a target in the continuing search for antimicrobial compounds.",fullPaper,jv20
p1309,d98d0d1900b13b87aa4ffd6b69c046beb63f0434,c22,International Conference on Data Technologies and Applications,"Graphical Models, Exponential Families, and Variational Inference","The formalism of probabilistic graphical models provides a unifying framework for capturing complex dependencies among random variables, and building large-scale multivariate statistical models. Graphical models have become a focus of research in many statistical, computational and mathematical fields, including bioinformatics, communication theory, statistical physics, combinatorial optimization, signal and image processing, information retrieval and statistical machine learning. Many problems that arise in specific instances — including the key problems of computing marginals and modes of probability distributions — are best studied in the general setting. Working with exponential family representations, and exploiting the conjugate duality between the cumulant function and the entropy for exponential families, we develop general variational representations of the problems of computing likelihoods, marginal probabilities and most probable configurations. We describe how a wide variety of algorithms — among them sum-product, cluster variational methods, expectation-propagation, mean field methods, max-product and linear programming relaxation, as well as conic programming relaxations — can all be understood in terms of exact or approximate forms of these variational representations. The variational approach provides a complementary alternative to Markov chain Monte Carlo as a general source of approximation methods for inference in large-scale statistical models.",poster,cp22
p1310,cc34993836f578db8a25befd0b266af36804f64f,j102,Nucleic Acids Research,High-throughput functional annotation and data mining with the Blast2GO suite,"Functional genomics technologies have been widely adopted in the biological research of both model and non-model species. An efficient functional annotation of DNA or protein sequences is a major requirement for the successful application of these approaches as functional information on gene products is often the key to the interpretation of experimental results. Therefore, there is an increasing need for bioinformatics resources which are able to cope with large amount of sequence data, produce valuable annotation results and are easily accessible to laboratories where functional genomics projects are being undertaken. We present the Blast2GO suite as an integrated and biologist-oriented solution for the high-throughput and automatic functional annotation of DNA or protein sequences based on the Gene Ontology vocabulary. The most outstanding Blast2GO features are: (i) the combination of various annotation strategies and tools controlling type and intensity of annotation, (ii) the numerous graphical features such as the interactive GO-graph visualization for gene-set function profiling or descriptive charts, (iii) the general sequence management features and (iv) high-throughput capabilities. We used the Blast2GO framework to carry out a detailed analysis of annotation behaviour through homology transfer and its impact in functional genomics research. Our aim is to offer biologists useful information to take into account when addressing the task of functionally characterizing their sequence data.",fullPaper,jv102
p1311,08b43d84e6747e370ef307e2ada50675b414514a,j247,IEEE Transactions on Neural Networks,Survey of clustering algorithms,"Data analysis plays an indispensable role for understanding various phenomena. Cluster analysis, primitive exploration with little or no prior knowledge, consists of research developed across a wide variety of communities. The diversity, on one hand, equips us with many tools. On the other hand, the profusion of options causes confusion. We survey clustering algorithms for data sets appearing in statistics, computer science, and machine learning, and illustrate their applications in some benchmark data sets, the traveling salesman problem, and bioinformatics, a new field attracting intensive efforts. Several tightly related topics, proximity measure, and cluster validation, are also discussed.",fullPaper,jv247
p1312,5d0e28c5a5cfe9820802ab6ec3c49f6927555af9,c22,International Conference on Data Technologies and Applications,GSDS 2.0: an upgraded gene feature visualization server,"Summary: Visualizing genes’ structure and annotated features helps biologists to investigate their function and evolution intuitively. The Gene Structure Display Server (GSDS) has been widely used by more than 60 000 users since its first publication in 2007. Here, we reported the upgraded GSDS 2.0 with a newly designed interface, supports for more types of annotation features and formats, as well as an integrated visual editor for editing the generated figure. Moreover, a user-specified phylogenetic tree can be added to facilitate further evolutionary analysis. The full source code is also available for downloading. Availability and implementation: Web server and source code are freely available at http://gsds.cbi.pku.edu.cn. Contact: gaog@mail.cbi.pku.edu.cn or gsds@mail.cbi.pku.edu.cn Supplementary information: Supplementary data are available at Bioinformatics online.",poster,cp22
p1313,9cda412e12e58769ea5fcf5ff450cfa2391d7f68,j248,WHO chronicle,International Agency for Research on Cancer.,Abstract content,fullPaper,jv248
p1314,76f96dadd80b19bde49e0e1f07bfa9fe8485eeec,c13,International Conference on Data Science and Advanced Analytics,"Learning with Kernels: support vector machines, regularization, optimization, and beyond","From the Publisher: 
In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs-kernels--for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics. 
Learning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.",poster,cp13
p1315,c74bcdf5393f8b14adda30dd178ab262ad944f48,j20,Proceedings of the National Academy of Sciences of the United States of America,A comprehensive two-hybrid analysis to explore the yeast protein interactome,"Protein–protein interactions play crucial roles in the execution of various biological functions. Accordingly, their comprehensive description would contribute considerably to the functional interpretation of fully sequenced genomes, which are flooded with novel genes of unpredictable functions. We previously developed a system to examine two-hybrid interactions in all possible combinations between the ≈6,000 proteins of the budding yeast Saccharomyces cerevisiae. Here we have completed the comprehensive analysis using this system to identify 4,549 two-hybrid interactions among 3,278 proteins. Unexpectedly, these data do not largely overlap with those obtained by the other project [Uetz, P., et al. (2000) Nature (London) 403, 623–627] and hence have substantially expanded our knowledge on the protein interaction space or interactome of the yeast. Cumulative connection of these binary interactions generates a single huge network linking the vast majority of the proteins. Bioinformatics-aided selection of biologically relevant interactions highlights various intriguing subnetworks. They include, for instance, the one that had successfully foreseen the involvement of a novel protein in spindle pole body function as well as the one that may uncover a hitherto unidentified multiprotein complex potentially participating in the process of vesicular transport. Our data would thus significantly expand and improve the protein interaction map for the exploration of genome functions that eventually leads to thorough understanding of the cell as a molecular system.",fullPaper,jv20
p1316,1c405ce3655515312cba508f6e8bb8c904d23d25,c92,Advances in Soft Computing,ExPASy: the proteomics server for in-depth protein knowledge and analysis,"The ExPASy (the Expert Protein Analysis System) World Wide Web server (http://www.expasy.org), is provided as a service to the life science community by a multidisciplinary team at the Swiss Institute of Bioinformatics (SIB). It provides access to a variety of databases and analytical tools dedicated to proteins and proteomics. ExPASy databases include SWISS-PROT and TrEMBL, SWISS-2DPAGE, PROSITE, ENZYME and the SWISS-MODEL repository. Analysis tools are available for specific tasks relevant to proteomics, similarity searches, pattern and profile searches, post-translational modification prediction, topology prediction, primary, secondary and tertiary structure analysis and sequence alignment. These databases and tools are tightly interlinked: a special emphasis is placed on integration of database entries with related resources developed at the SIB and elsewhere, and the proteomics tools have been designed to read the annotations in SWISS-PROT in order to enhance their predictions. ExPASy started to operate in 1993, as the first WWW server in the field of life sciences. In addition to the main site in Switzerland, seven mirror sites in different continents currently serve the user community.",poster,cp92
p1317,36c76c76e77237bd5da1d1dc4289641002dba7f5,j85,BMC Bioinformatics,An innovative approach for testing bioinformatics programs using metamorphic testing,Abstract content,fullPaper,jv85
p1318,8feb974bcf444da6723b73f5c482b09b1ddab4a5,j249,IEEE Transactions on Parallel and Distributed Systems,Cloud Technologies for Bioinformatics Applications,"Executing large number of independent jobs or jobs comprising of large number of tasks that perform minimal intertask communication is a common requirement in many domains. Various technologies ranging from classic job schedulers to the latest cloud technologies such as MapReduce can be used to execute these ""many-tasks” in parallel. In this paper, we present our experience in applying two cloud technologies Apache Hadoop and Microsoft DryadLINQ to two bioinformatics applications with the above characteristics. The applications are a pairwise Alu sequence alignment application and an Expressed Sequence Tag (EST) sequence assembly program. First, we compare the performance of these cloud technologies using the above applications and also compare them with traditional MPI implementation in one application. Next, we analyze the effect of inhomogeneous data on the scheduling mechanisms of the cloud technologies. Finally, we present a comparison of performance of the cloud technologies under virtual and nonvirtual hardware platforms.",fullPaper,jv249
p1319,bf6806dc1dfe057e907abfe786ee5037c4f7ea47,j153,bioRxiv,Nextstrain: real-time tracking of pathogen evolution,"Summary Understanding the spread and evolution of pathogens is important for effective public health measures and surveillance. Nextstrain consists of a database of viral genomes, a bioinformatics pipeline for phylodynamics analysis, and an interactive visualisation platform. Together these present a real-time view into the evolution and spread of a range of viral pathogens of high public health importance. The visualization integrates sequence data with other data types such as geographic information, serology, or host species. Nextstrain compiles our current understanding into a single accessible location, publicly available for use by health professionals, epidemiologists, virologists and the public alike. Availability and implementation All code (predominantly JavaScript and Python) is freely available from github.com/nextstrain and the web-application is available at nextstrain.org.",fullPaper,jv153
p1320,4ccd70db97b3670920b390bd28853307f296f5e0,j85,BMC Bioinformatics,BIGSdb: Scalable analysis of bacterial genome variation at the population level,Abstract content,fullPaper,jv85
p1321,b4445a19a9d7e404cf512833bc185811c88b8538,c63,IEEE International Software Metrics Symposium,phangorn: phylogenetic analysis in R,"Summary: phangorn is a package for phylogenetic reconstruction and analysis in the R language. Previously it was only possible to estimate phylogenetic trees with distance methods in R. phangorn, now offers the possibility of reconstructing phylogenies with distance based methods, maximum parsimony or maximum likelihood (ML) and performing Hadamard conjugation. Extending the general ML framework, this package provides the possibility of estimating mixture and partition models. Furthermore, phangorn offers several functions for comparing trees, phylogenetic models or splits, simulating character data and performing congruence analyses. Availability: phangorn can be obtained through the CRAN homepage http://cran.r-project.org/web/packages/phangorn/index.html. phangorn is licensed under GPL 2. Contact: klaus.kschliep@snv.jussieu.fr Supplementary information: Supplementary data are available at Bioinformatics online.",poster,cp63
p1322,963b6626180c10826dade32678fcf305a8f7bc15,c25,International Conference on Contemporary Computing,CD-HIT Suite: a web server for clustering and comparing biological sequences,"Summary: CD-HIT is a widely used program for clustering and comparing large biological sequence datasets. In order to further assist the CD-HIT users, we significantly improved this program with more functions and better accuracy, scalability and flexibility. Most importantly, we developed a new web server, CD-HIT Suite, for clustering a user-uploaded sequence dataset or comparing it to another dataset at different identity levels. Users can now interactively explore the clusters within web browsers. We also provide downloadable clusters for several public databases (NCBI NR, Swissprot and PDB) at different identity levels. Availability: Free access at http://cd-hit.org Contact: liwz@sdsc.edu Supplementary information: Supplementary data are available at Bioinformatics online.",poster,cp25
p1323,1baee4ae5e5eaf75e322b53afa3cbdea89dcc2d0,c102,International Conference on Biometrics,"Efficient and Robust Feature Selection via Joint ℓ2, 1-Norms Minimization","Feature selection is an important component of many machine learning applications. Especially in many bioinformatics tasks, efficient and robust feature selection methods are desired to extract meaningful features and eliminate noisy ones. In this paper, we propose a new robust feature selection method with emphasizing joint l2,1-norm minimization on both loss function and regularization. The l2,1-norm based loss function is robust to outliers in data points and the l2,1-norm regularization selects features across all data points with joint sparsity. An efficient algorithm is introduced with proved convergence. Our regression based objective makes the feature selection process more efficient. Our method has been applied into both genomic and proteomic biomarkers discovery. Extensive empirical studies are performed on six data sets to demonstrate the performance of our feature selection method.",poster,cp102
p1324,f09c52301704d1c89380f90b50b682dcc3f9d80f,c22,International Conference on Data Technologies and Applications,ConsensusClusterPlus: a class discovery tool with confidence assessments and item tracking,"Summary: Unsupervised class discovery is a highly useful technique in cancer research, where intrinsic groups sharing biological characteristics may exist but are unknown. The consensus clustering (CC) method provides quantitative and visual stability evidence for estimating the number of unsupervised classes in a dataset. ConsensusClusterPlus implements the CC method in R and extends it with new functionality and visualizations including item tracking, item-consensus and cluster-consensus plots. These new features provide users with detailed information that enable more specific decisions in unsupervised class discovery. Availability: ConsensusClusterPlus is open source software, written in R, under GPL-2, and available through the Bioconductor project (http://www.bioconductor.org/). Contact: mwilkers@med.unc.edu Supplementary Information: Supplementary data are available at Bioinformatics online.",poster,cp22
p1325,10374ecf564306f0b87761056213d36b2ada75a6,c113,International Conference on Image Analysis and Processing,ProtTest 3: fast selection of best-fit models of protein evolution,"UNLABELLED
We have implemented a high-performance computing (HPC) version of ProtTest that can be executed in parallel in multicore desktops and clusters. This version, called ProtTest 3, includes new features and extended capabilities.


AVAILABILITY
ProtTest 3 source code and binaries are freely available under GNU license for download from http://darwin.uvigo.es/software/prottest3, linked to a Mercurial repository at Bitbucket (https://bitbucket.org/).


CONTACT
dposada@uvigo.es


SUPPLEMENTARY INFORMATION
Supplementary data are available at Bioinformatics online.",poster,cp113
p1326,dd833d84dc4ee2e90477a8d6559235519555a06e,c68,International Workshop on Software Engineering for Large-Scale Multi-Agent Systems,Deriving the consequences of genomic variants with the Ensembl API and SNP Effect Predictor,"Summary: A tool to predict the effect that newly discovered genomic variants have on known transcripts is indispensible in prioritizing and categorizing such variants. In Ensembl, a web-based tool (the SNP Effect Predictor) and API interface can now functionally annotate variants in all Ensembl and Ensembl Genomes supported species. Availability: The Ensembl SNP Effect Predictor can be accessed via the Ensembl website at http://www.ensembl.org/. The Ensembl API (http://www.ensembl.org/info/docs/api/api_installation.html for installation instructions) is open source software. Contact: wm2@ebi.ac.uk; fiona@ebi.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.",poster,cp68
p1327,f8dc73630864b4903f8f57b5c8d7b098eb300e4f,c103,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,"Reactome: a database of reactions, pathways and biological processes","Reactome (http://www.reactome.org) is a collaboration among groups at the Ontario Institute for Cancer Research, Cold Spring Harbor Laboratory, New York University School of Medicine and The European Bioinformatics Institute, to develop an open source curated bioinformatics database of human pathways and reactions. Recently, we developed a new web site with improved tools for pathway browsing and data analysis. The Pathway Browser is an Systems Biology Graphical Notation (SBGN)-based visualization system that supports zooming, scrolling and event highlighting. It exploits PSIQUIC web services to overlay our curated pathways with molecular interaction data from the Reactome Functional Interaction Network and external interaction databases such as IntAct, BioGRID, ChEMBL, iRefIndex, MINT and STRING. Our Pathway and Expression Analysis tools enable ID mapping, pathway assignment and overrepresentation analysis of user-supplied data sets. To support pathway annotation and analysis in other species, we continue to make orthology-based inferences of pathways in non-human species, applying Ensembl Compara to identify orthologs of curated human proteins in each of 20 other species. The resulting inferred pathway sets can be browsed and analyzed with our Species Comparison tool. Collaborations are also underway to create manually curated data sets on the Reactome framework for chicken, Drosophila and rice.",poster,cp103
p1328,3aca912f21d54b3931fa1fdfac0c199c557374a4,c34,IEEE Working Conference on Mining Software Repositories,GTDB-Tk: a toolkit to classify genomes with the Genome Taxonomy Database,Abstract Summary The Genome Taxonomy Database Toolkit (GTDB-Tk) provides objective taxonomic assignments for bacterial and archaeal genomes based on the GTDB. GTDB-Tk is computationally efficient and able to classify thousands of draft genomes in parallel. Here we demonstrate the accuracy of the GTDB-Tk taxonomic assignments by evaluating its performance on a phylogenetically diverse set of 10 156 bacterial and archaeal metagenome-assembled genomes. Availability and implementation GTDB-Tk is implemented in Python and licenced under the GNU General Public Licence v3.0. Source code and documentation are available at: https://github.com/ecogenomics/gtdbtk. Supplementary information Supplementary data are available at Bioinformatics online.,poster,cp34
p1329,0c9de4aa510659b520cd4c944ee3fb7acda91558,c93,Human Language Technology - The Baltic Perspectiv,Using MetaboAnalyst 4.0 for Comprehensive and Integrative Metabolomics Data Analysis,"MetaboAnalyst (https://www.metaboanalyst.ca) is an easy‐to‐use web‐based tool suite for comprehensive metabolomic data analysis, interpretation, and integration with other omics data. Since its first release in 2009, MetaboAnalyst has evolved significantly to meet the ever‐expanding bioinformatics demands from the rapidly growing metabolomics community. In addition to providing a variety of data processing and normalization procedures, MetaboAnalyst supports a wide array of functions for statistical, functional, as well as data visualization tasks. Some of the most widely used approaches include PCA (principal component analysis), PLS‐DA (partial least squares discriminant analysis), clustering analysis and visualization, MSEA (metabolite set enrichment analysis), MetPA (metabolic pathway analysis), biomarker selection via ROC (receiver operating characteristic) curve analysis, as well as time series and power analysis. The current version of MetaboAnalyst (4.0) features a complete overhaul of the user interface and significantly expanded underlying knowledge bases (compound database, pathway libraries, and metabolite sets). Three new modules have been added to support pathway activity prediction directly from mass peaks, biomarker meta‐analysis, and network‐based multi‐omics data integration. To enable more transparent and reproducible analysis of metabolomic data, we have released a companion R package (MetaboAnalystR) to complement the web‐based application. This article provides an overview of the main functional modules and the general workflow of MetaboAnalyst 4.0, followed by 12 detailed protocols: © 2019 by John Wiley & Sons, Inc.",poster,cp93
p1330,28a7f8d518045a394b9f8be9bb68a71cc0ff9027,c103,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,GAPIT: genome association and prediction integrated tool,"SUMMARY
Software programs that conduct genome-wide association studies and genomic prediction and selection need to use methodologies that maximize statistical power, provide high prediction accuracy and run in a computationally efficient manner. We developed an R package called Genome Association and Prediction Integrated Tool (GAPIT) that implements advanced statistical methods including the compressed mixed linear model (CMLM) and CMLM-based genomic prediction and selection. The GAPIT package can handle large datasets in excess of 10 000 individuals and 1 million single-nucleotide polymorphisms with minimal computational time, while providing user-friendly access and concise tables and graphs to interpret results.


AVAILABILITY
http://www.maizegenetics.net/GAPIT.


CONTACT
zhiwu.zhang@cornell.edu


SUPPLEMENTARY INFORMATION
Supplementary data are available at Bioinformatics online.",poster,cp103
p1331,ecf9e1564767c19d37dccf1aae5515281e3e0ad8,j6,Nature Methods,ChromHMM: automating chromatin-state discovery and characterization,Abstract content,fullPaper,jv6
p1332,367aacce3d30a9f76cc5c5a54f91cd29df7d236b,c14,International Conference on Exploring Services Science,Gene Ontology Consortium: going forward,"The Gene Ontology (GO; http://www.geneontology.org) is a community-based bioinformatics resource that supplies information about gene product function using ontologies to represent biological knowledge. Here we describe improvements and expansions to several branches of the ontology, as well as updates that have allowed us to more efficiently disseminate the GO and capture feedback from the research community. The Gene Ontology Consortium (GOC) has expanded areas of the ontology such as cilia-related terms, cell-cycle terms and multicellular organism processes. We have also implemented new tools for generating ontology terms based on a set of logical rules making use of templates, and we have made efforts to increase our use of logical definitions. The GOC has a new and improved web site summarizing new developments and documentation, serving as a portal to GO data. Users can perform GO enrichment analysis, and search the GO for terms, annotations to gene products, and associated metadata across multiple species using the all-new AmiGO 2 browser. We encourage and welcome the input of the research community in all biological areas in our continued effort to improve the Gene Ontology.",poster,cp14
p1333,61d090d329434e8edfc8c08b34c61a0c7b378fb8,c23,International Conference on Open and Big Data,Scalable web services for the PSIPRED Protein Analysis Workbench,"Here, we present the new UCL Bioinformatics Group’s PSIPRED Protein Analysis Workbench. The Workbench unites all of our previously available analysis methods into a single web-based framework. The new web portal provides a greatly streamlined user interface with a number of new features to allow users to better explore their results. We offer a number of additional services to enable computationally scalable execution of our prediction methods; these include SOAP and XML-RPC web server access and new HADOOP packages. All software and services are available via the UCL Bioinformatics Group website at http://bioinf.cs.ucl.ac.uk/.",poster,cp23
p1334,11fa1e048a2e57cd0c042b3078f920c7f9f71c59,j250,Human Genetics,"The Human Gene Mutation Database: building a comprehensive mutation repository for clinical and molecular genetics, diagnostic testing and personalized genomic medicine",Abstract content,fullPaper,jv250
p1335,66340a5d9237efb858f895d6f8af77762d25bbad,j194,Current Topics in Medicinal Chemistry,Medicinal chemistry and bioinformatics--current trends in drugs discovery with networks topological indices.,"The numerical encoding of chemical structure with Topological Indices (TIs) is currently growing in importance in Medicinal Chemistry and Bioinformatics. This approach allows the rapid collection, annotation, retrieval, comparison and mining of chemical structures within large databases. TIs can subsequently be used to seek quantitative structure-activity relationships (QSAR), which are models connecting chemical structure with biological activity. In the early 1990's, there was an explosion in the introduction and definition of new TIs. The Handbook of Molecular Descriptors by Todeschini and Consonni lists more than 1500 of these indices. At the end of the last century, researchers produced a large number of TIs with essentially the same advantages and/or disadvantages. Consequently, many researchers abandoned the definition of TIs for a time. In our opinion, one of the problems associated with TIs is that researchers aimed their efforts only at the codification of chemical connectivity for small-sized drugs. As a consequence, recently it seems that we have arrived at ""Fukuyama's End of History in TIs definition"". In the work described here, we review and comment on the ""quo vadis"" and challenges in the definition of TIs as we enter the new century. Emphasis is placed on new chiral TIs (CTIs), flexible TIs for unifying QSAR models with multiple targets, topographic indices (TPGIs), TIs for DNA and protein sequences, TIs for 2D RNA structures, TPGIs and drug-protein or drug-RNA quantitative structure-binding relationship (QSBR) studies, TIs to encode protein surface information and TIs for protein interaction networks (PINs).",fullPaper,jv194
p1336,a093ba08980422d9d2d1f7d49aac350b8b7e1676,c67,Enterprise Application Integration,ShinyGO: a graphical gene-set enrichment tool for animals and plants,"MOTIVATION
Gene lists are routinely produced from various genome-wide studies. Enrichment analysis can link these gene lists with underlying molecular pathways and functional categories such as gene ontology (GO) and other databases.


RESULTS
To complement existing tools, we developed ShinyGO based on a large annotation database derived from Ensembl and STRING-db for 59 plant, 256 animal, 115 archaeal, and 1678 bacterial species. ShinyGO's novel features include graphical visualization of enrichment results and gene characteristics, and application program interface (API) access to KEGG and STRING for the retrieval of pathway diagrams and protein-protein interaction networks. ShinyGO is an intuitive, graphical web application that can help researchers gain actionable insights from gene lists.


AVAILABILITY
http://ge-lab.org/go/.


SUPPLEMENTARY INFORMATION
Supplementary data are available at Bioinformatics online.",poster,cp67
p1337,9079c4dfd4be155efc670ce1333cdcc3e5c6480f,c38,"IEEE International Conference on Software Analysis, Evolution, and Reengineering",The Austronesian Basic Vocabulary Database: From Bioinformatics to Lexomics,"Phylogenetic methods have revolutionised evolutionary biology and have recently been applied to studies of linguistic and cultural evolution. However, the basic comparative data on the languages of the world required for these analyses is often widely dispersed in hard to obtain sources. Here we outline how our Austronesian Basic Vocabulary Database (ABVD) helps remedy this situation by collating wordlists from over 500 languages into one web-accessible database. We describe the technology underlying the ABVD and discuss the benefits that an evolutionary bioinformatic approach can provide. These include facilitating computational comparative linguistic research, answering questions about human prehistory, enabling syntheses with genetic data, and safe-guarding fragile linguistic information.",poster,cp38
p1338,3ee4e2aa335d30eb093b15ee9c55de3c350c8ea0,c78,Neural Information Processing Systems,The European Bioinformatics Institute’s data resources,"The wide uptake of next-generation sequencing and other ultra-high throughput technologies by life scientists with a diverse range of interests, spanning fundamental biological research, medicine, agriculture and environmental science, has led to unprecedented growth in the amount of data generated. It has also put the need for unrestricted access to biological data at the centre of biology. The European Bioinformatics Institute (EMBL-EBI) is unique in Europe and is one of only two organisations worldwide providing access to a comprehensive, integrated set of these collections. Here, we describe how the EMBL-EBI’s biomolecular databases are evolving to cope with increasing levels of submission, a growing and diversifying user base, and the demand for new types of data. All of the resources described here can be accessed from the EMBL-EBI website: http://www.ebi.ac.uk",poster,cp78
p1339,780725e727d6cc7a8c32dd613540960457bf188a,j85,BMC Bioinformatics,VennDiagram: a package for the generation of highly-customizable Venn and Euler diagrams in R,Abstract content,fullPaper,jv85
p1340,6000d13f57b91b29061af94375bc25ccc960a3dd,j251,Gut,"Epidemiological, clinical and virological characteristics of 74 cases of coronavirus-infected disease 2019 (COVID-19) with gastrointestinal symptoms","Objective The SARS-CoV-2-infected disease (COVID-19) outbreak is a major threat to human beings. Previous studies mainly focused on Wuhan and typical symptoms. We analysed 74 confirmed COVID-19 cases with GI symptoms in the Zhejiang province to determine epidemiological, clinical and virological characteristics. Design COVID-19 hospital patients were admitted in the Zhejiang province from 17 January 2020 to 8 February 2020. Epidemiological, demographic, clinical, laboratory, management and outcome data of patients with GI symptoms were analysed using multivariate analysis for risk of severe/critical type. Bioinformatics were used to analyse features of SARS-CoV-2 from Zhejiang province. Results Among enrolled 651 patients, 74 (11.4%) presented with at least one GI symptom (nausea, vomiting or diarrhoea), average age of 46.14 years, 4-day incubation period and 10.8% had pre-existing liver disease. Of patients with COVID-19 with GI symptoms, 17 (22.97%) and 23 (31.08%) had severe/critical types and family clustering, respectively, significantly higher than those without GI symptoms, 47 (8.14%) and 118 (20.45%). Of patients with COVID-19 with GI symptoms, 29 (39.19%), 23 (31.08%), 8 (10.81%) and 16 (21.62%) had significantly higher rates of fever >38.5°C, fatigue, shortness of breath and headache, respectively. Low-dose glucocorticoids and antibiotics were administered to 14.86% and 41.89% of patients, respectively. Sputum production and increased lactate dehydrogenase/glucose levels were risk factors for severe/critical type. Bioinformatics showed sequence mutation of SARS-CoV-2 with m6A methylation and changed binding capacity with ACE2. Conclusion We report COVID-19 cases with GI symptoms with novel features outside Wuhan. Attention to patients with COVID-19 with non-classic symptoms should increase to protect health providers.",fullPaper,jv251
p1341,7c9c2f49c0733bc554ae18c587bde536c42aaa6f,c72,Intelligent Systems in Molecular Biology,TAMBIS: Transparent Access to Multiple Bioinformatics Information Sources,"UNLABELLED
TAMBIS (Transparent Access to Multiple Bioinformatics Information Sources) is an application that allows biologists to ask rich and complex questions over a range of bioinformatics resources. It is based on a model of the knowledge of the concepts and their relationships in molecular biology and bioinformatics.


AVAILABILITY
TAMBIS is available as an applet from http://img.cs.man.ac.uk/tambis SUPPLEMENTARY: A full manual, tutorial and videos can be found at http://img.cs.man.ac.uk/tambis.


CONTACT
tambis@cs.man.ac.uk",fullPaper,cp72
p1342,bb1461f8e65914f6755a924398c760bb1aec5f44,c74,IEEE International Conference on Tools with Artificial Intelligence,Ontology-based Knowledge Representation for Bioinformatics,"Much of biology works by applying prior knowledge ('what is known') to an unknown entity, rather than the application of a set of axioms that will elicit knowledge. In addition, the complex biological data stored in bioinformatics databases often require the addition of knowledge to specify and constrain the values held in that database. One way of capturing knowledge within bioinformatics applications and databases is the use of ontologies. An ontology is the concrete form of a conceptualisation of a community's knowledge of a domain. This paper aims to introduce the reader to the use of ontologies within bioinformatics. A description of the type of knowledge held in an ontology will be given.The paper will be illustrated throughout with examples taken from bioinformatics and molecular biology, and a survey of current biological ontologies will be presented. From this it will be seen that the use to which the ontology is put largely determines the content of the ontology. Finally, the paper will describe the process of building an ontology, introducing the reader to the techniques and methods currently in use and the open research questions in ontology development.",poster,cp74
p1343,46bf90cbffd3ec016b385ef7daf3aaf092639cd0,j252,Molecular Ecology Notes,bold: The Barcode of Life Data System (http://www.barcodinglife.org),"The Barcode of Life Data System (bold) is an informatics workbench aiding the acquisition, storage, analysis and publication of DNA barcode records. By assembling molecular, morphological and distributional data, it bridges a traditional bioinformatics chasm. bold is freely available to any researcher with interests in DNA barcoding. By providing specialized services, it aids the assembly of records that meet the standards needed to gain BARCODE designation in the global sequence databases. Because of its web-based delivery and flexible data security model, it is also well positioned to support projects that involve broad research alliances. This paper provides a brief introduction to the key elements of bold, discusses their functional capabilities, and concludes by examining computational resources and future prospects.",fullPaper,jv252
p1344,929c17b1b4f77d24829ed79faa0ed68c50228316,c45,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",Better bioinformatics through usability analysis,"MOTIVATION
Improving the usability of bioinformatics resources enables researchers to find, interact with, share, compare and manipulate important information more effectively and efficiently. It thus enables researchers to gain improved insights into biological processes with the potential, ultimately, of yielding new scientific results. Usability 'barriers' can pose significant obstacles to a satisfactory user experience and force researchers to spend unnecessary time and effort to complete their tasks. The number of online biological databases available is growing and there is an expanding community of diverse users. In this context there is an increasing need to ensure the highest standards of usability.


RESULTS
Using 'state-of-the-art' usability evaluation methods, we have identified and characterized a sample of usability issues potentially relevant to web bioinformatics resources, in general. These specifically concern the design of the navigation and search mechanisms available to the user. The usability issues we have discovered in our substantial case studies are undermining the ability of users to find the information they need in their daily research activities. In addition to characterizing these issues, specific recommendations for improvements are proposed leveraging proven practices from web and usability engineering. The methods and approach we exemplify can be readily adopted by the developers of bioinformatics resources.",poster,cp45
p1345,6f85730e4efa0f6472b16f0f3c330ef0dae150d0,c64,Experimental Software Engineering Network,Bioinformatics for DNA Sequence Analysis,Abstract content,poster,cp64
p1346,9a15f8d07ce8de2f47c52a0b661a73b8590ccb17,j253,Journal of Proteome Research,Identification and analysis of occludin phosphosites: a combined mass spectrometry and bioinformatics approach.,"The molecular function of occludin, an integral membrane component of tight junctions, remains unclear. VEGF-induced phosphorylation sites were mapped on occludin by combining MS data analysis with bioinformatics. In vivo phosphorylation of Ser490 was validated and protein interaction studies combined with crystal structure analysis suggest that Ser490 phosphorylation attenuates the interaction between occludin and ZO-1. This study demonstrates that combining MS data and bioinformatics can successfully identify novel phosphorylation sites from limiting samples.",fullPaper,jv253
p1347,5aea95e1ae78a66474051a330ded374e199b658c,c75,International Conference on Machine Learning,Representation Learning on Graphs with Jumping Knowledge Networks,"Recent deep learning approaches for representation learning on graphs follow a neighborhood aggregation procedure. We analyze some important properties of these models, and propose a strategy to overcome those. In particular, the range of ""neighboring"" nodes that a node's representation draws from strongly depends on the graph structure, analogous to the spread of a random walk. To adapt to local neighborhood properties and tasks, we explore an architecture -- jumping knowledge (JK) networks -- that flexibly leverages, for each node, different neighborhood ranges to enable better structure-aware representation. In a number of experiments on social, bioinformatics and citation networks, we demonstrate that our model achieves state-of-the-art performance. Furthermore, combining the JK framework with models like Graph Convolutional Networks, GraphSAGE and Graph Attention Networks consistently improves those models' performance.",fullPaper,cp75
p1348,df28aaf1dc26190cdb122966cb223250d3180d52,c14,International Conference on Exploring Services Science,DrugBank: a comprehensive resource for in silico drug discovery and exploration,"DrugBank is a unique bioinformatics/cheminformatics resource that combines detailed drug (i.e. chemical) data with comprehensive drug target (i.e. protein) information. The database contains >4100 drug entries including >800 FDA approved small molecule and biotech drugs as well as >3200 experimental drugs. Additionally, >14,000 protein or drug target sequences are linked to these drug entries. Each DrugCard entry contains >80 data fields with half of the information being devoted to drug/chemical data and the other half devoted to drug target or protein data. Many data fields are hyperlinked to other databases (KEGG, PubChem, ChEBI, PDB, Swiss-Prot and GenBank) and a variety of structure viewing applets. The database is fully searchable supporting extensive text, sequence, chemical structure and relational query searches. Potential applications of DrugBank include in silico drug target discovery, drug design, drug docking or screening, drug metabolism prediction, drug interaction prediction and general pharmaceutical education. DrugBank is available at http://redpoll.pharmacy.ualberta.ca/drugbank/.",poster,cp14
p1349,ea2d20d78f8b482c9e92e33a9d93ae788ece37d5,c99,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,Bioinformatics : applications in life and environmental sciences,Abstract content,poster,cp99
p1350,4c921e841bf85f2d27eaf7c25d2d7d1b8b81cd25,c20,Research Conference of the South African Institute of Computer Scientists and Information Technologists,A Quick Guide for Developing Effective Bioinformatics Programming Skills,"Bioinformatics programming skills are becoming a necessity across many facets of biology and medicine, owed in part to the continuing explosion of biological data aggregation and the complexity and scale of questions now being addressed through modern bioinformatics. Although many are now receiving formal training in bioinformatics through various university degree and certificate programs, this training is often focused strongly on bioinformatics methodology, leaving many important and practical aspects of bioinformatics to self-education and experience. The following set of guidelines distill several key principals of effective bioinformatics programming, which the authors learned through insights gained across many years of combined experience developing popular bioinformatics software applications and database systems in both academic and commercial settings [1]–[6]. Successful adoption of these principals will serve both beginner and experienced bioinformaticians alike in career development and pursuit of professional and scientific goals.",poster,cp20
p1351,d3a4e5a31fc9383089f8bd39010bf86a3bf62eb0,c92,Advances in Soft Computing,Bioinformatics analysis of microarray data.,Abstract content,poster,cp92
p1352,7d71be3ab1d6fb26c6bf72006ac7059dabde7863,j254,Annual Review of Plant Biology,Lignin biosynthesis.,"The lignin biosynthetic pathway has been studied for more than a century but has undergone major revisions over the past decade. Significant progress has been made in cloning new genes by genetic and combined bioinformatics and biochemistry approaches. In vitro enzymatic assays and detailed analyses of mutants and transgenic plants altered in the expression of lignin biosynthesis genes have provided a solid basis for redrawing the monolignol biosynthetic pathway, and structural analyses have shown that plant cell walls can tolerate large variations in lignin content and structure. In some cases, the potential value for agriculture of transgenic plants with modified lignin structure has been demonstrated. This review presents a current picture of monolignol biosynthesis, polymerization, and lignin structure.",fullPaper,jv254
p1353,fddc613a2a208f241b38bc41f62b4fc4db7217c7,c6,Americas Conference on Information Systems,Nonnegative Matrix and Tensor Factorizations - Applications to Exploratory Multi-way Data Analysis and Blind Source Separation,"This book provides a broad survey of models and efficient algorithms for Nonnegative Matrix Factorization (NMF). This includes NMFs various extensions and modifications, especially Nonnegative Tensor Factorizations (NTF) and Nonnegative Tucker Decompositions (NTD). NMF/NTF and their extensions are increasingly used as tools in signal and image processing, and data analysis, having garnered interest due to their capability to provide new insights and relevant information about the complex latent relationships in experimental data sets. It is suggested that NMF can provide meaningful components with physical interpretations; for example, in bioinformatics, NMF and its extensions have been successfully applied to gene expression, sequence analysis, the functional characterization of genes, clustering and text mining. As such, the authors focus on the algorithms that are most useful in practice, looking at the fastest, most robust, and suitable for large-scale models. Key features: Acts as a single source reference guide to NMF, collating information that is widely dispersed in current literature, including the authors own recently developed techniques in the subject area. Uses generalized cost functions such as Bregman, Alpha and Beta divergences, to present practical implementations of several types of robust algorithms, in particular Multiplicative, Alternating Least Squares, Projected Gradient and Quasi Newton algorithms. Provides a comparative analysis of the different methods in order to identify approximation error and complexity. Includes pseudo codes and optimized MATLAB source codes for almost all algorithms presented in the book. The increasing interest in nonnegative matrix and tensor factorizations, as well as decompositions and sparse representation of data, will ensure that this book is essential reading for engineers, scientists, researchers, industry practitioners and graduate students across signal and image processing; neuroscience; data mining and data analysis; computer science; bioinformatics; speech processing; biomedical engineering; and multimedia.",poster,cp6
p1354,2b8714e4daf025a2f928f20529ef4153c31182fd,j255,Journal of Experimental Botany,Nitric oxide-responsive genes and promoters in Arabidopsis thaliana: a bioinformatics approach.,"Due to its high reactivity and its ability to diffuse and permeate the cell membrane, nitric oxide (NO) and its exchangeable redox-activated species are unique biological messengers in animals and in plants. Although an increasing number of reports indicate that NO is an essential molecule in several physiological processes, there is not a clear picture of its method of action. Studies on the transcriptional changes induced by NO permitted identification of genes involved in different functional processes such as signal transduction, defence and cell death, transport, basic metabolism, and reactive oxygen species (ROS) production and degradation. The co-expression of these genes can be explained by the co-operation of a set of transcription factors that bind a common region in the promoter of the regulated genes. The present report describes the search for a common transcription factor-binding site (TFBS) in promoter regions of NO-regulated genes, based on microarray analyses. Using Genomatix Gene2Promotor and MatInspector, eight families of TFBSs were found to occur at least 15% more often in the promoter regions of the responsive genes in comparison with the promoter regions of 28,447 Arabidopsis control genes. Most of these TFBSs, such as ocs element-like sequences and WRKY, have already been reported to be involved in particular stress responses. Furthermore, the promoter regions of genes involved in jasmonic acid (JA) biosynthesis were analysed for a common TFBS module, since some genes responsible for JA biosynthesis are induced by NO, and an interaction between NO and JA signalling has already been described.",fullPaper,jv255
p1355,69e3442fb05c33b537471007345a79397e849e68,j20,Proceedings of the National Academy of Sciences of the United States of America,Bioinformatics construction of the human cell surfaceome,"Cell surface proteins are excellent targets for diagnostic and therapeutic interventions. By using bioinformatics tools, we generated a catalog of 3,702 transmembrane proteins located at the surface of human cells (human cell surfaceome). We explored the genetic diversity of the human cell surfaceome at different levels, including the distribution of polymorphisms, conservation among eukaryotic species, and patterns of gene expression. By integrating expression information from a variety of sources, we were able to identify surfaceome genes with a restricted expression in normal tissues and/or differential expression in tumors, important characteristics for putative tumor targets. A high-throughput and efficient quantitative real-time PCR approach was used to validate 593 surfaceome genes selected on the basis of their expression pattern in normal and tumor samples. A number of candidates were identified as potential diagnostic and therapeutic targets for colorectal tumors and glioblastoma. Several candidate genes were also identified as coding for cell surface cancer/testis antigens. The human cell surfaceome will serve as a reference for further studies aimed at characterizing tumor targets at the surface of human cells.",fullPaper,jv20
p1356,e41a4e92bf68aaa3170e22e7ae1d57fc495a93fc,c114,IEEE International Conference on Robotics and Automation,Biodoop: Bioinformatics on Hadoop,"Bioinformatics applications currently require both processing of huge amounts of data and heavy computation. Fulfilling these requirements calls for simple ways to implement parallel computing. MapReduce is a general-purpose parallelization model that seems particularly well-suited to this task and for which an open source implementation (Hadoop) is available. Here we report on its application to three relevant algorithms: BLAST, GSEA and GRAMMAR. The first is characterized by relatively low-weight computation on large data sets, while the second requires heavy processing of relatively small data sets. The third one can be considered as containing a mixture of these two computational flavors. Our results are encouraging and indicate that the framework could have a wide range of bioinformatics applications while maintaining good computational efficiency, scalability and ease of maintenance.",poster,cp114
p1357,50f174ad5d1de758ef371a178209a95c36e24f37,j85,BMC Bioinformatics,XMPP for cloud computing in bioinformatics supporting discovery and invocation of asynchronous web services,Abstract content,fullPaper,jv85
p1358,ba0e4fc4f8f9c630b3a215cbde79db594dab67aa,c77,Networks,PROVEAN web server: a tool to predict the functional effect of amino acid substitutions and indels,"UNLABELLED
We present a web server to predict the functional effect of single or multiple amino acid substitutions, insertions and deletions using the prediction tool PROVEAN. The server provides rapid analysis of protein variants from any organisms, and also supports high-throughput analysis for human and mouse variants at both the genomic and protein levels.


AVAILABILITY AND IMPLEMENTATION
The web server is freely available and open to all users with no login requirements at http://provean.jcvi.org.


CONTACT
achan@jcvi.org


SUPPLEMENTARY INFORMATION
Supplementary data are available at Bioinformatics online.",poster,cp77
p1359,4143db924596b5ef39fe1cc9ea96fb97a6adfd9d,j153,bioRxiv,Bandage: interactive visualization of de novo genome assemblies,"Summary While de novo assembly graphs contain assembled contigs (nodes), the connections between those contigs (edges) are difficult for users to access. Bandage (a Bioinformatics Application for Navigating De novo Assembly Graphs Easily) is a tool for visualising assembly graphs with connections. Users can zoom in to specific areas of the graph and interact with it by moving nodes, adding labels, changing colours and extracting sequences. BLAST searches can be performed within the Bandage GUI and the hits are displayed as highlights in the graph. By displaying connections between contigs, Bandage presents new possibilities for analysing de novo assemblies that are not possible through investigation of contigs alone. Availability and implementation Source code and binaries are freely available at https://github.com/rrwick/Bandage. Bandage is implemented in C++ and supported on Linux, OS X and Windows. Contact rrwick@gmail.com Supplementary information A full feature list and screenshots are available at Bioinformatics online and http://rrwick.github.io/Bandage.",fullPaper,jv153
p1360,d03820bb0ee8a4b936cc5fd13053588d659ff946,j153,bioRxiv,NanoPack: visualizing and processing long-read sequencing data,"Summary: Here we describe NanoPack, a set of tools developed for visualization and processing of long read sequencing data from Oxford Nanopore Technologies and Pacific Biosciences. Availability and Implementation: The NanoPack tools are written in Python3 and released under the GNU GPL3.0 Licence. The source code can be found at https://github.com/wdecoster/nanopack, together with links to separate scripts and their documentation. The scripts are compatible with Linux, Mac OS and the MS Windows 10 subsystem for linux and are available as a graphical user interface, a web service at http://nanoplot.bioinf.be and command line tools. Contact: wouter.decoster@molgen.vib-ua.be Supplementary information: Supplementary tables and figures are available at Bioinformatics online.",fullPaper,jv153
p1361,0c8dacf66498f7e73d73ceb508dd1d2af81b680e,c27,ACM-SIAM Symposium on Discrete Algorithms,Pathview: an R/Bioconductor package for pathway-based data integration and visualization,"Summary: Pathview is a novel tool set for pathway-based data integration and visualization. It maps and renders user data on relevant pathway graphs. Users only need to supply their data and specify the target pathway. Pathview automatically downloads the pathway graph data, parses the data file, maps and integrates user data onto the pathway and renders pathway graphs with the mapped data. Although built as a stand-alone program, Pathview may seamlessly integrate with pathway and functional analysis tools for large-scale and fully automated analysis pipelines. Availability: The package is freely available under the GPLv3 license through Bioconductor and R-Forge. It is available at http://bioconductor.org/packages/release/bioc/html/pathview.html and at http://Pathview.r-forge.r-project.org/. Contact: luo_weijun@yahoo.com Supplementary information: Supplementary data are available at Bioinformatics online.",poster,cp27
p1362,7dc089d3001586244d8d3700ad1ae25bc24deaa0,c17,International Conference on Enterprise Information Systems,AliView: a fast and lightweight alignment viewer and editor for large datasets,"Summary: AliView is an alignment viewer and editor designed to meet the requirements of next-generation sequencing era phylogenetic datasets. AliView handles alignments of unlimited size in the formats most commonly used, i.e. FASTA, Phylip, Nexus, Clustal and MSF. The intuitive graphical interface makes it easy to inspect, sort, delete, merge and realign sequences as part of the manual filtering process of large datasets. AliView also works as an easy-to-use alignment editor for small as well as large datasets. Availability and implementation: AliView is released as open-source software under the GNU General Public License, version 3.0 (GPLv3), and is available at GitHub (www.github.com/AliView). The program is cross-platform and extensively tested on Linux, Mac OS X and Windows systems. Downloads and help are available at http://ormbunkar.se/aliview Contact: anders.larsson@ebc.uu.se Supplementary information: Supplementary data are available at Bioinformatics online.",poster,cp17
p1363,df24af3ee6a5cb289cff353611ff05855d9b22c2,c93,Human Language Technology - The Baltic Perspectiv,Robust Principal Component Analysis: Exact Recovery of Corrupted Low-Rank Matrices via Convex Optimization,"Principal component analysis is a fundamental operation in computational data analysis, with myriad applications ranging from web search to bioinformatics to computer vision and image analysis. However, its performance and applicability in real scenarios are limited by a lack of robustness to outlying or corrupted observations. This paper considers the idealized ""robust principal component analysis"" problem of recovering a low rank matrix A from corrupted observations D = A + E. Here, the corrupted entries E are unknown and the errors can be arbitrarily large (modeling grossly corrupted observations common in visual and bioinformatic data), but are assumed to be sparse. We prove that most matrices A can be efficiently and exactly recovered from most error sign-and-support patterns by solving a simple convex program, for which we give a fast and provably convergent algorithm. Our result holds even when the rank of A grows nearly proportionally (up to a logarithmic factor) to the dimensionality of the observation space and the number of errors E grows in proportion to the total number of entries in the matrix. A by-product of our analysis is the first proportional growth results for the related problem of completing a low-rank matrix from a small fraction of its entries. Simulations and real-data examples corroborate the theoretical results, and suggest potential applications in computer vision.",poster,cp93
p1364,ec3c1f392bacdd47b47221082ecbd3aaf1bb33d3,j256,Frontiers in Immunology,Toll-Like Receptor Signaling Pathways,"Toll-like receptors (TLRs) play crucial roles in the innate immune system by recognizing pathogen-associated molecular patterns derived from various microbes. TLRs signal through the recruitment of specific adaptor molecules, leading to activation of the transcription factors NF-κB and IRFs, which dictate the outcome of innate immune responses. During the past decade, the precise mechanisms underlying TLR signaling have been clarified by various approaches involving genetic, biochemical, structural, cell biological, and bioinformatics studies. TLR signaling appears to be divergent and to play important roles in many aspects of the innate immune responses to given pathogens. In this review, we describe recent progress in our understanding of TLR signaling regulation and its contributions to host defense.",fullPaper,jv256
p1365,439ede62248e5f6202982afead02b33d3feffae7,j102,Nucleic Acids Research,TCGAbiolinks: an R/Bioconductor package for integrative analysis of TCGA data,"The Cancer Genome Atlas (TCGA) research network has made public a large collection of clinical and molecular phenotypes of more than 10 000 tumor patients across 33 different tumor types. Using this cohort, TCGA has published over 20 marker papers detailing the genomic and epigenomic alterations associated with these tumor types. Although many important discoveries have been made by TCGA's research network, opportunities still exist to implement novel methods, thereby elucidating new biological pathways and diagnostic markers. However, mining the TCGA data presents several bioinformatics challenges, such as data retrieval and integration with clinical data and other molecular data types (e.g. RNA and DNA methylation). We developed an R/Bioconductor package called TCGAbiolinks to address these challenges and offer bioinformatics solutions by using a guided workflow to allow users to query, download and perform integrative analyses of TCGA data. We combined methods from computer science and statistics into the pipeline and incorporated methodologies developed in previous TCGA marker studies and in our own group. Using four different TCGA tumor types (Kidney, Brain, Breast and Colon) as examples, we provide case studies to illustrate examples of reproducibility, integrative analysis and utilization of different Bioconductor packages to advance and accelerate novel discoveries.",fullPaper,jv102
p1366,73dcc27f06eeff886be506b5d12354dd8dae2c04,j130,Scientific Reports,RASTtk: A modular and extensible implementation of the RAST algorithm for building custom annotation pipelines and annotating batches of genomes,Abstract content,fullPaper,jv130
p1367,78cc062150352512c2de9a00e59f97477d8a5540,j75,Lecture Notes in Computer Science,Tools and Algorithms for the Construction and Analysis of Systems,Abstract content,fullPaper,jv75
p1368,8f4f844b9f7534169138dbc8239a3158d495bc9a,j179,Protein Science,Toward understanding the origin and evolution of cellular organisms,"In this era of high‐throughput biology, bioinformatics has become a major discipline for making sense out of large‐scale datasets. Bioinformatics is usually considered as a practical field developing databases and software tools for supporting other fields, rather than a fundamental scientific discipline for uncovering principles of biology. The KEGG resource that we have been developing is a reference knowledge base for biological interpretation of genome sequences and other high‐throughput data. It is now one of the most utilized biological databases because of its practical values. For me personally, KEGG is a step toward understanding the origin and evolution of cellular organisms.",fullPaper,jv179
p1369,5070e30df0923b5661c80365273d21a12c517b20,j85,BMC Bioinformatics,BIRI: a new approach for automatically discovering and indexing available public bioinformatics resources from the literature,Abstract content,fullPaper,jv85
p1370,6426ee8528fb9996b60351a02a646e70b31d5938,c106,Chinese Conference on Biometric Recognition,Soft Computing Methodologies in Bioinformatics,"Bioinformatics is a promising and innovative research field in 21st century. Despite of a high number of techniques specifically dedicated to bioinformatics problems as well as many successful applications, we are in the beginning of a process to massively integrate the aspects and experiences in the different core subjects such as biology, medicine, computer science, engineering, chemistry, physics, and mathematics. Recently the use of soft computing tools for solving bioinformatics problems have been gaining the attention of researchers because of their ability to handle imprecision, uncertainty in large and complex search spaces. The paper will focus on soft computing paradigm in bioinformatics with particular emphasis on integrative research.",poster,cp106
p1371,099452350873cd12d05832fa4174b92dc7fdc9fd,c17,International Conference on Enterprise Information Systems,BOLD : The Barcode of Life Data System,"The Barcode of Life Data System ( BOLD ) is an informatics workbench aiding the acquisition, storage, analysis and publication of DNA barcode records. By assembling molecular, morphological and distributional data, it bridges a traditional bioinformatics chasm. BOLD is freely available to any researcher with interests in DNA barcoding. By providing specialized services, it aids the assembly of records that meet the standards needed to gain BARCODE designation in the global sequence databases. Because of its web-based delivery and flexible data security model, it is also well positioned to support projects that involve broad research alliances. This paper provides a brief introduction to the key elements of BOLD , discusses their functional capabilities, and concludes by examining computational resources and future prospects.",poster,cp17
p1372,6480039b32988cf83bde873263b09897e14d551e,c65,Formal Concept Analysis,SNAP: a web-based tool for identification and annotation of proxy SNPs using HapMap,"SUMMARY
The interpretation of genome-wide association results is confounded by linkage disequilibrium between nearby alleles. We have developed a flexible bioinformatics query tool for single-nucleotide polymorphisms (SNPs) to identify and to annotate nearby SNPs in linkage disequilibrium (proxies) based on HapMap. By offering functionality to generate graphical plots for these data, the SNAP server will facilitate interpretation and comparison of genome-wide association study results, and the design of fine-mapping experiments (by delineating genomic regions harboring associated variants and their proxies).


AVAILABILITY
SNAP server is available at http://www.broad.mit.edu/mpg/snap/.",poster,cp65
p1373,a7dc3c92386011aece0daa61556bc30c0e168cc6,c33,International Conference on Agile Software Development,Evolution in bioinformatic resources: 2009 update on the Bioinformatics Links Directory,"All of the life science research web servers published in this and previous issues of Nucleic Acids Research, together with other useful tools, databases and resources for bioinformatics and molecular biology research are freely accessible online through the Bioinformatics Links Directory, http://bioinformatics.ca/links_directory/. Entirely dependent on user feedback and community input, the Bioinformatics Links Directory exemplifies an open access research tool and resource. With 112 websites featured in the July 2009 Web Server Issue of Nucleic Acids Research, the 2009 update brings the total number of servers listed in the Bioinformatics Links Directory close to an impressive 1400 links. A complete list of all links listed in this Nucleic Acids Research 2009 Web Server Issue can be accessed online at http://bioinfomatics.ca/links_directory/narweb2009/. The 2009 update of the Bioinformatics Links Directory, which includes the Web Server list and summaries, is also available online at the Nucleic Acids Research website, http://nar.oxfordjournals.org/.",poster,cp33
p1374,33b1c59f1db83adf63696d4ed9718b20e290a943,j85,BMC Bioinformatics,"Bias in random forest variable importance measures: Illustrations, sources and a solution",Abstract content,fullPaper,jv85
p1375,ad3f933d1fdb30d447d0cf543c3d882b341cc0e6,c28,International Conference on Collaboration Technologies and Systems,PRGdb: a bioinformatics platform for plant resistance gene analysis,"PRGdb is a web accessible open-source (http://www.prgdb.org) database that represents the first bioinformatic resource providing a comprehensive overview of resistance genes (R-genes) in plants. PRGdb holds more than 16 000 known and putative R-genes belonging to 192 plant species challenged by 115 different pathogens and linked with useful biological information. The complete database includes a set of 73 manually curated reference R-genes, 6308 putative R-genes collected from NCBI and 10463 computationally predicted putative R-genes. Thanks to a user-friendly interface, data can be examined using different query tools. A home-made prediction pipeline called Disease Resistance Analysis and Gene Orthology (DRAGO), based on reference R-gene sequence data, was developed to search for plant resistance genes in public datasets such as Unigene and Genbank. New putative R-gene classes containing unknown domain combinations were discovered and characterized. The development of the PRG platform represents an important starting point to conduct various experimental tasks. The inferred cross-link between genomic and phenotypic information allows access to a large body of information to find answers to several biological questions. The database structure also permits easy integration with other data types and opens up prospects for future implementations.",poster,cp28
p1376,f64dd54444a724574deb7710888091350eebb2b9,c50,International Conference on Automated Software Engineering,"BBMap: A Fast, Accurate, Splice-Aware Aligner","Alignment of reads is one of the primary computational tasks in bioinformatics. Of paramount importance to resequencing, alignment is also crucial to other areas - quality control, scaffolding, string-graph assembly, homology detection, assembly evaluation, error-correction, expression quantification, and even as a tool to evaluate other tools. An optimal aligner would greatly improve virtually any sequencing process, but optimal alignment is prohibitively expensive for gigabases of data. Here, we will present BBMap [1], a fast splice-aware aligner for short and long reads. We will demonstrate that BBMap has superior speed, sensitivity, and specificity to alternative high-throughput aligners bowtie2 [2], bwa [3], smalt, [4] GSNAP [5], and BLASR [6].",poster,cp50
p1377,aee9709704f84defcf97d622b490eb662593d026,c110,IEEE International Conference on Automatic Face & Gesture Recognition,Modeling and Simulation of Genetic Regulatory Systems: A Literature Review,"In order to understand the functioning of organisms on the molecular level, we need to know which genes are expressed, when and where in the organism, and to which extent. The regulation of gene expression is achieved through genetic regulatory systems structured by networks of interactions between DNA, RNA, proteins, and small molecules. As most genetic regulatory networks of interest involve many components connected through interlocking positive and negative feedback loops, an intuitive understanding of their dynamics is hard to obtain. As a consequence, formal methods and computer tools for the modeling and simulation of genetic regulatory networks will be indispensable. This paper reviews formalisms that have been employed in mathematical biology and bioinformatics to describe genetic regulatory systems, in particular directed graphs, Bayesian networks, Boolean networks and their generalizations, ordinary and partial differential equations, qualitative differential equations, stochastic equations, an...",poster,cp110
p1378,97cd8334bff9b6325a3513d1557109b5e54299d8,c53,International Conference on Software Engineering and Knowledge Engineering,Web Services at the European Bioinformatics Institute,"We present a new version of the European Bioinformatics Institute Web Services, a complete suite of SOAP-based web tools for structural and functional analysis, with new and improved applications. New functionality has been added to most of the services already available, and an improved version of the underlying framework has allowed us to include more applications. Information on the EBI Web Services, tutorials and clients can be found at http://www.ebi.ac.uk/Tools/webservices.",poster,cp53
p1379,4b3b74aec92b46812ea87011058326794f59db10,c60,IEEE International Conference on Software Engineering and Formal Methods,Qualimap 2: advanced multi-sample quality control for high-throughput sequencing data,"Motivation: Detection of random errors and systematic biases is a crucial step of a robust pipeline for processing high-throughput sequencing (HTS) data. Bioinformatics software tools capable of performing this task are available, either for general analysis of HTS data or targeted to a specific sequencing technology. However, most of the existing QC instruments only allow processing of one sample at a time. Results: Qualimap 2 represents a next step in the QC analysis of HTS data. Along with comprehensive single-sample analysis of alignment data, it includes new modes that allow simultaneous processing and comparison of multiple samples. As with the first version, the new features are available via both graphical and command line interface. Additionally, it includes a large number of improvements proposed by the user community. Availability and implementation: The implementation of the software along with documentation is freely available at http://www.qualimap.org. Contact: meyer@mpiib-berlin.mpg.de Supplementary information: Supplementary data are available at Bioinformatics online.",poster,cp60
p1380,81b74385488ab8704ce7e04a332de4971d00a499,j257,International Journal of Plant Genomics,Blast2GO: A Comprehensive Suite for Functional Analysis in Plant Genomics,"Functional annotation of novel sequence data is a primary requirement for the utilization of functional genomics approaches in plant research. In this paper, we describe the Blast2GO suite as a comprehensive bioinformatics tool for functional annotation of sequences and data mining on the resulting annotations, primarily based on the gene ontology (GO) vocabulary. Blast2GO optimizes function transfer from homologous sequences through an elaborate algorithm that considers similarity, the extension of the homology, the database of choice, the GO hierarchy, and the quality of the original annotations. The tool includes numerous functions for the visualization, management, and statistical analysis of annotation results, including gene set enrichment analysis. The application supports InterPro, enzyme codes, KEGG pathways, GO direct acyclic graphs (DAGs), and GOSlim. Blast2GO is a suitable tool for plant genomics research because of its versatility, easy installation, and friendly use.",fullPaper,jv257
p1381,1221de833f4909b7bc27790fa0cd080d756d8982,j102,Nucleic Acids Research,"The Sanger FASTQ file format for sequences with quality scores, and the Solexa/Illumina FASTQ variants","FASTQ has emerged as a common file format for sharing sequencing read data combining both the sequence and an associated per base quality score, despite lacking any formal definition to date, and existing in at least three incompatible variants. This article defines the FASTQ format, covering the original Sanger standard, the Solexa/Illumina variants and conversion between them, based on publicly available information such as the MAQ documentation and conventions recently agreed by the Open Bioinformatics Foundation projects Biopython, BioPerl, BioRuby, BioJava and EMBOSS. Being an open access publication, it is hoped that this description, with the example files provided as Supplementary Data, will serve in future as a reference for this important file format.",fullPaper,jv102
p1382,807873123e4230d8c61297f75c969f04fe4d9930,j20,Proceedings of the National Academy of Sciences of the United States of America,Cloning of a human parvovirus by molecular screening of respiratory tract samples.,"The identification of new virus species is a key issue for the study of infectious disease but is technically very difficult. We developed a system for large-scale molecular virus screening of clinical samples based on host DNA depletion, random PCR amplification, large-scale sequencing, and bioinformatics. The technology was applied to pooled human respiratory tract samples. The first experiments detected seven human virus species without the use of any specific reagent. Among the detected viruses were one coronavirus and one parvovirus, both of which were at that time uncharacterized. The parvovirus, provisionally named human bocavirus, was in a retrospective clinical study detected in 17 additional patients and associated with lower respiratory tract infections in children. The molecular virus screening procedure provides a general culture-independent solution to the problem of detecting unknown virus species in single or pooled samples. We suggest that a systematic exploration of the viruses that infect humans, ""the human virome,"" can be initiated.",fullPaper,jv20
p1383,e8dba5bf84c7c3cdae3648a76d0af6ff1e393009,c72,Intelligent Systems in Molecular Biology,De novo identification of repeat families in large genomes,"MOTIVATION
De novo repeat family identification is a challenging algorithmic problem of great practical importance. As the number of genome sequencing projects increases, there is a pressing need to identify the repeat families present in large, newly sequenced genomes. We develop a new method for de novo identification of repeat families via extension of consensus seeds; our method enables a rigorous definition of repeat boundaries, a key issue in repeat analysis.


RESULTS
Our RepeatScout algorithm is more sensitive and is orders of magnitude faster than RECON, the dominant tool for de novo repeat family identification in newly sequenced genomes. Using RepeatScout, we estimate that approximately 2% of the human genome and 4% of mouse and rat genomes consist of previously unannotated repetitive sequence.


AVAILABILITY
Source code is available for download at http://www-cse.ucsd.edu/groups/bioinformatics/software.html",fullPaper,cp72
p1384,e049bd1ef18a0f1cdb45477d957393cf9ef41c6d,c78,Neural Information Processing Systems,The UCSC Genome Browser Database,"The University of California Santa Cruz (UCSC) Genome Browser Database is an up to date source for genome sequence data integrated with a large collection of related annotations. The database is optimized to support fast interactive performance with the web-based UCSC Genome Browser, a tool built on top of the database for rapid visualization and querying of the data at many levels. The annotations for a given genome are displayed in the browser as a series of tracks aligned with the genomic sequence. Sequence data and annotations may also be viewed in a text-based tabular format or downloaded as tab-delimited flat files. The Genome Browser Database, browsing tools and downloadable data files can all be found on the UCSC Genome Bioinformatics website (http://genome.ucsc.edu), which also contains links to documentation and related technical information.",poster,cp78
p1385,9e26c8be21f341102016fa5a67b06db362bf272a,j258,Statistical Applications in Genetics and Molecular Biology,A Shrinkage Approach to Large-Scale Covariance Matrix Estimation and Implications for Functional Genomics,"Inferring large-scale covariance matrices from sparse genomic data is an ubiquitous problem in bioinformatics. Clearly, the widely used standard covariance and correlation estimators are ill-suited for this purpose. As statistically efficient and computationally fast alternative we propose a novel shrinkage covariance estimator that exploits the Ledoit-Wolf (2003) lemma for analytic calculation of the optimal shrinkage intensity.Subsequently, we apply this improved covariance estimator (which has guaranteed minimum mean squared error, is well-conditioned, and is always positive definite even for small sample sizes) to the problem of inferring large-scale gene association networks. We show that it performs very favorably compared to competing approaches both in simulations as well as in application to real expression data.",fullPaper,jv258
p1386,7a14141492d305fd7a3d075623b92ac4a87e8165,j259,BMC Systems Biology,Bioinformatics strategies for lipidomics analysis: characterization of obesity related hepatic steatosis,Abstract content,fullPaper,jv259
p1387,fded6f9cefb06c1da7acddd5b9b89b1eabb08b7d,c83,International Conference on Computer Graphics and Interactive Techniques,The Ensembl genome database project,"The Ensembl (http://www.ensembl.org/) database project provides a bioinformatics framework to organise biology around the sequences of large genomes. It is a comprehensive source of stable automatic annotation of the human genome sequence, with confirmed gene predictions that have been integrated with external data sources, and is available as either an interactive web site or as flat files. It is also an open source software engineering project to develop a portable system able to handle very large genomes and associated requirements from sequence analysis to data storage and visualisation. The Ensembl site is one of the leading sources of human genome sequence annotation and provided much of the analysis for publication by the international human genome project of the draft genome. The Ensembl system is being installed around the world in both companies and academic sites on machines ranging from supercomputers to laptops.",poster,cp83
p1388,3822821ec99fc8788a640820eb2a1027ed41e3c3,j85,BMC Bioinformatics,Ontology-driven indexing of public datasets for translational bioinformatics,Abstract content,fullPaper,jv85
p1389,0ac836c1d35c5f5cd21bd249564da3711f17b375,j260,Bioinformatics,Bioinformatics for next generation sequencing.,Abstract content,fullPaper,jv260
p1390,3142f9c54220f78c8500c6149df1667332ea9d6c,j97,Science,An Extensive Class of Small RNAs in Caenorhabditis elegans,"The lin-4 and let-7 antisense RNAs are temporal regulators that control the timing of developmental events inCaenorhabditis elegans by inhibiting translation of target mRNAs. let-7 RNA is conserved among bilaterian animals, suggesting that this class of small RNAs [microRNAs (miRNAs)] is evolutionarily ancient. Using bioinformatics and cDNA cloning, we found 15 new miRNA genes in C. elegans. Several of these genes express small transcripts that vary in abundance during C. elegans larval development, and three of them have apparent homologs in mammals and/or insects. Small noncoding RNAs of the miRNA class appear to be numerous and diverse.",fullPaper,jv97
p1391,b86b57745a6f7bad1b1c7d09210e6f957d9d5ce3,j209,Human Mutation,MITOMASTER: a bioinformatics tool for the analysis of mitochondrial DNA sequences,"We have developed a computer system, MITOMASTER, to make analysis of human mitochondrial DNA (mtDNA) sequences efficient, accurate, and easily available. From imported sequences, the system identifies nucleotide variants, determines the haplogroup, rules out possible pseudogene contamination, identifies novel DNA sequence variants, and evaluates the potential biological significance of each variant. This system should be beneficial for mtDNA analyses of biomedical physicians and investigators, population biologists and forensic scientists. MITOMASTER can be accessed at http://mammag.web.uci.edu/twiki/bin/view/Mitomaster. Hum Mutat 0,1–6, 2008. © 2008 Wiley‐Liss, Inc.",fullPaper,jv209
p1392,9235d511dea04aa563a577ab236506b8eb8242ff,c76,International Conference on Artificial Neural Networks,A Survey on Deep Transfer Learning,Abstract content,fullPaper,cp76
p1393,e8da033f03c679e87b95a9e8cc1cfdd8c6ccb526,c88,Symposium on the Theory of Computing,Foundations of multidimensional and metric data structures,"Multidimensional data is data that exists and changes in more than one dimension, by time, or spatially, or both, sometimes dynamically. Think here of tracking hurricane data in order to project the storm's path, for just one example. As spatial and other multidimensional data structures become increasingly important for the applications in game programming, data mining, bioinformatics, and many other areas--including astronomy, geographic information systems, physics, etc., the need for a comprehensive book on the subject is paramount. This book is truly a life's work by the author who is clearly the best person for the job.",poster,cp88
p1394,0f9a9310d8ff5ab620f83dc8066b105d2f8a3df0,c41,Software Product Lines Conference,ThunderSTORM: a comprehensive ImageJ plug-in for PALM and STORM data analysis and super-resolution imaging,"Summary: ThunderSTORM is an open-source, interactive and modular plug-in for ImageJ designed for automated processing, analysis and visualization of data acquired by single-molecule localization microscopy methods such as photo-activated localization microscopy and stochastic optical reconstruction microscopy. ThunderSTORM offers an extensive collection of processing and post-processing methods so that users can easily adapt the process of analysis to their data. ThunderSTORM also offers a set of tools for creation of simulated data and quantitative performance evaluation of localization algorithms using Monte Carlo simulations. Availability and implementation: ThunderSTORM and the online documentation are both freely accessible at https://code.google.com/p/thunder-storm/ Contact: guy.hagen@lf1.cuni.cz Supplementary information: Supplementary data are available at Bioinformatics online.",poster,cp41
p1395,defc662852076dd9156d5435cbc69484bfbc536d,c17,International Conference on Enterprise Information Systems,BOLD : The Barcode of Life Data System (www.barcodinglife.org),"The Barcode of Life Data System ( BOLD ) is an informatics workbench aiding the acquisition, storage, analysis and publication of DNA barcode records. By assembling molecular, morphological and distributional data, it bridges a traditional bioinformatics chasm. BOLD is freely available to any researcher with interests in DNA barcoding. By providing specialized services, it aids the assembly of records that meet the standards needed to gain BARCODE designation in the global sequence databases. Because of its web-based delivery and flexible data security model, it is also well positioned to support projects that involve broad research alliances. This paper provides a brief introduction to the key elements of BOLD , discusses their functional capabilities, and concludes by examining computational resources and future prospects.",poster,cp17
p1396,7efd35eb509190bf1d006d4a28b1bf89b812241a,c107,British Machine Vision Conference,Activities at the Universal Protein Resource (UniProt),"The mission of the Universal Protein Resource (UniProt) (http://www.uniprot.org) is to provide the scientific community with a comprehensive, high-quality and freely accessible resource of protein sequences and functional annotation. It integrates, interprets and standardizes data from literature and numerous resources to achieve the most comprehensive catalog possible of protein information. The central activities are the biocuration of the UniProt Knowledgebase and the dissemination of these data through our Web site and web services. UniProt is produced by the UniProt Consortium, which consists of groups from the European Bioinformatics Institute (EBI), the SIB Swiss Institute of Bioinformatics (SIB) and the Protein Information Resource (PIR). UniProt is updated and distributed every 4 weeks and can be accessed online for searches or downloads.",poster,cp107
p1397,33780e4aba639a97f9fb7f7e773853f74dd494b7,j187,Genome Research,The Bioperl toolkit: Perl modules for the life sciences.,"The Bioperl project is an international open-source collaboration of biologists, bioinformaticians, and computer scientists that has evolved over the past 7 yr into the most comprehensive library of Perl modules available for managing and manipulating life-science information. Bioperl provides an easy-to-use, stable, and consistent programming interface for bioinformatics application programmers. The Bioperl modules have been successfully and repeatedly used to reduce otherwise complex tasks to only a few lines of code. The Bioperl object model has been proven to be flexible enough to support enterprise-level applications such as EnsEMBL, while maintaining an easy learning curve for novice Perl programmers. Bioperl is capable of executing analyses and processing results from programs such as BLAST, ClustalW, or the EMBOSS suite. Interoperation with modules written in Python and Java is supported through the evolving BioCORBA bridge. Bioperl provides access to data stores such as GenBank and SwissProt via a flexible series of sequence input/output modules, and to the emerging common sequence data storage format of the Open Bioinformatics Database Access project. This study describes the overall architecture of the toolkit, the problem domains that it addresses, and gives specific examples of how the toolkit can be used to solve common life-sciences problems. We conclude with a discussion of how the open-source nature of the project has contributed to the development effort.",fullPaper,jv187
p1398,3e24ff6f5f8eba407effc09a3e23ee583d4ee0e7,c63,IEEE International Software Metrics Symposium,"Oncomine 3.0: genes, pathways, and networks in a collection of 18,000 cancer gene expression profiles.","DNA microarrays have been widely applied to cancer transcriptome analysis; however, the majority of such data are not easily accessible or comparable. Furthermore, several important analytic approaches have been applied to microarray analysis; however, their application is often limited. To overcome these limitations, we have developed Oncomine, a bioinformatics initiative aimed at collecting, standardizing, analyzing, and delivering cancer transcriptome data to the biomedical research community. Our analysis has identified the genes, pathways, and networks deregulated across 18,000 cancer gene expression microarrays, spanning the majority of cancer types and subtypes. Here, we provide an update on the initiative, describe the database and analysis modules, and highlight several notable observations. Results from this comprehensive analysis are available at http://www.oncomine.org.",poster,cp63
p1399,cc9c8a81a5e2be0c8abdb39c905f5faaf621dd81,j5,Genome Biology,Toward almost closed genomes with GapFiller,Abstract content,fullPaper,jv5
p1400,3b3ad5eaddd5a970519b8c9b4097816fe374e8ec,c46,Brazilian Symposium on Software Engineering,The Proteomics Identifications (PRIDE) database and associated tools: status in 2013,"The PRoteomics IDEntifications (PRIDE, http://www.ebi.ac.uk/pride) database at the European Bioinformatics Institute is one of the most prominent data repositories of mass spectrometry (MS)-based proteomics data. Here, we summarize recent developments in the PRIDE database and related tools. First, we provide up-to-date statistics in data content, splitting the figures by groups of organisms and species, including peptide and protein identifications, and post-translational modifications. We then describe the tools that are part of the PRIDE submission pipeline, especially the recently developed PRIDE Converter 2 (new submission tool) and PRIDE Inspector (visualization and analysis tool). We also give an update about the integration of PRIDE with other MS proteomics resources in the context of the ProteomeXchange consortium. Finally, we briefly review the quality control efforts that are ongoing at present and outline our future plans.",poster,cp46
p1401,2501576dc02c6ca1504b045885eb1cec8d4d3f16,j175,Nature reviews genetics,Repetitive DNA and next-generation sequencing: computational challenges and solutions,Abstract content,fullPaper,jv175
p1402,597bf9470f0b8f43067945fcaf82972f6cac63cb,c86,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,The MPI Bioinformatics Toolkit for protein sequence analysis,"The MPI Bioinformatics Toolkit is an interactive web service which offers access to a great variety of public and in-house bioinformatics tools. They are grouped into different sections that support sequence searches, multiple alignment, secondary and tertiary structure prediction and classification. Several public tools are offered in customized versions that extend their functionality. For example, PSI-BLAST can be run against regularly updated standard databases, customized user databases or selectable sets of genomes. Another tool, Quick2D, integrates the results of various secondary structure, transmembrane and disorder prediction programs into one view. The Toolkit provides a friendly and intuitive user interface with an online help facility. As a key feature, various tools are interconnected so that the results of one tool can be forwarded to other tools. One could run PSI-BLAST, parse out a multiple alignment of selected hits and send the results to a cluster analysis tool. The Toolkit framework and the tools developed in-house will be packaged and freely available under the GNU Lesser General Public Licence (LGPL). The Toolkit can be accessed at .",poster,cp86
p1403,c7870b78c57e0bd2e9fb6907c0702e28eb87e239,c47,International Symposium on Empirical Software Engineering and Measurement,An Introduction to Conditional Random Fields,"Many tasks involve predicting a large number of variables that depend on each other as well as on other observed variables. Structured prediction methods are essentially a combination of classification and graphical modeling. They combine the ability of graphical models to compactly model multivariate data with the ability of classification methods to perform prediction using large sets of input features. This survey describes conditional random fields, a popular probabilistic method for structured prediction. CRFs have seen wide application in many areas, including natural language processing, computer vision, and bioinformatics. We describe methods for inference and parameter estimation for CRFs, including practical issues for implementing large-scale CRFs. We do not assume previous knowledge of graphical modeling, so this survey is intended to be useful to practitioners in a wide variety of fields.",poster,cp47
p1404,ea398413f2827444c455325dfe1c3846cef0c543,c104,IEEE International Conference on Multimedia and Expo,GenomeScope: fast reference‐free genome profiling from short reads,"Summary: GenomeScope is an open‐source web tool to rapidly estimate the overall characteristics of a genome, including genome size, heterozygosity rate and repeat content from unprocessed short reads. These features are essential for studying genome evolution, and help to choose parameters for downstream analysis. We demonstrate its accuracy on 324 simulated and 16 real datasets with a wide range in genome sizes, heterozygosity levels and error rates. Availability and Implementation: http://genomescope.org, https://github.com/schatzlab/genomescope.git. Contact: mschatz@jhu.edu Supplementary information: Supplementary data are available at Bioinformatics online.",poster,cp104
p1405,7cc9496750d82e5d839159e7489e63351b404ea6,j234,Nature Reviews Microbiology,Fungal secondary metabolism — from biochemistry to genomics,Abstract content,fullPaper,jv234
p1406,2304163f5af1516a5552ad55936254a62e2412c2,j261,Biologia plantarum,Understanding Bioinformatics,Abstract content,fullPaper,jv261
p1407,522c0bc79ced1526f30d0e089cfb7b52bca644d9,c6,Americas Conference on Information Systems,Bioinformatics and computational methods for lipidomics.,Abstract content,poster,cp6
p1408,2866d1586a6fef7d4c7c3446f1e716c7166bb61d,j262,"Tanpakushitsu kakusan koso. Protein, nucleic acid, enzyme",[Structural bioinformatics].,Abstract content,fullPaper,jv262
p1409,622fdccc84c6732553855d38482021cef9063684,j219,IEEE/ACM Transactions on Computational Biology & Bioinformatics,Parallel Clustering Algorithm for Large Data Sets with Applications in Bioinformatics,"Large sets of bioinformatical data provide a challenge in time consumption while solving the cluster identification problem, and that is why a parallel algorithm is so needed for identifying dense clusters in a noisy background. Our algorithm works on a graph representation of the data set to be analyzed. It identifies clusters through the identification of densely intraconnected subgraphs. We have employed a minimum spanning tree (MST) representation of the graph and solve the cluster identification problem using this representation. The computational bottleneck of our algorithm is the construction of an MST of a graph, for which a parallel algorithm is employed. Our high-level strategy for the parallel MST construction algorithm is to first partition the graph, then construct MSTs for the partitioned subgraphs and auxiliary bipartite graphs based on the subgraphs, and finally merge these MSTs to derive an MST of the original graph. The computational results indicate that when running on 150 CPUs, our algorithm can solve a cluster identification problem on a data set with 1,000,000 data points almost 100 times faster than on single CPU, indicating that this program is capable of handling very large data clustering problems in an efficient manner. We have implemented the clustering algorithm as the software CLUMP.",fullPaper,jv219
p1410,f4cc83f0ca55c503d74a36e7c0e276738ad07142,c0,International Conference on Human Factors in Computing Systems,From Protein Structure to Function with Bioinformatics,Abstract content,poster,cp0
p1411,b2ea282f7ca557c7fc228a90305d9a32d2aa5276,c96,USENIX Symposium on Operating Systems Design and Implementation,Algorithms in Bioinformatics: A Practical Introduction,Abstract content,poster,cp96
p1412,eecdd9bf1598632e3d233e19054707912e57ac8a,c77,Networks,ChEBI: An Open Bioinformatics and Cheminformatics Resource,"Chemical Entities of Biological Interest (ChEBI) is a freely available dictionary of molecular entities focused on “small” chemical compounds. This unit provides a detailed guide to browsing, searching, downloading, and programmatic access to the ChEBI database. Curr. Protoc. Bioinform. 26:14.9.1‐14.9.20. © 2009 by John Wiley & Sons, Inc.",poster,cp77
p1413,c6480aecd32cff4ebbd9d12e0c351249cd76a760,c28,International Conference on Collaboration Technologies and Systems,GEOquery: a bridge between the Gene Expression Omnibus (GEO) and BioConductor,"UNLABELLED
Microarray technology has become a standard molecular biology tool. Experimental data have been generated on a huge number of organisms, tissue types, treatment conditions and disease states. The Gene Expression Omnibus (Barrett et al., 2005), developed by the National Center for Bioinformatics (NCBI) at the National Institutes of Health is a repository of nearly 140,000 gene expression experiments. The BioConductor project (Gentleman et al., 2004) is an open-source and open-development software project built in the R statistical programming environment (R Development core Team, 2005) for the analysis and comprehension of genomic data. The tools contained in the BioConductor project represent many state-of-the-art methods for the analysis of microarray and genomics data. We have developed a software tool that allows access to the wealth of information within GEO directly from BioConductor, eliminating many the formatting and parsing problems that have made such analyses labor-intensive in the past. The software, called GEOquery, effectively establishes a bridge between GEO and BioConductor. Easy access to GEO data from BioConductor will likely lead to new analyses of GEO data using novel and rigorous statistical and bioinformatic tools. Facilitating analyses and meta-analyses of microarray data will increase the efficiency with which biologically important conclusions can be drawn from published genomic data.


AVAILABILITY
GEOquery is available as part of the BioConductor project.",poster,cp28
p1414,4b537a6953d4726ddd6ced4fe1ef4ca96296b597,j97,Science,The Obesity-Associated FTO Gene Encodes a 2-Oxoglutarate-Dependent Nucleic Acid Demethylase,"Variants in the FTO (fat mass and obesity associated) gene are associated with increased body mass index in humans. Here, we show by bioinformatics analysis that FTO shares sequence motifs with Fe(II)- and 2-oxoglutarate–dependent oxygenases. We find that recombinant murine Fto catalyzes the Fe(II)- and 2OG-dependent demethylation of 3-methylthymine in single-stranded DNA, with concomitant production of succinate, formaldehyde, and carbon dioxide. Consistent with a potential role in nucleic acid demethylation, Fto localizes to the nucleus in transfected cells. Studies of wild-type mice indicate that Fto messenger RNA (mRNA) is most abundant in the brain, particularly in hypothalamic nuclei governing energy balance, and that Fto mRNA levels in the arcuate nucleus are regulated by feeding and fasting. Studies can now be directed toward determining the physiologically relevant FTO substrate and how nucleic acid methylation status is linked to increased fat mass.",fullPaper,jv97
p1415,e9cfe56b8eb7a84dab4237dced7e15a56d5eaebc,j263,Electrophoresis,Automated comparative protein structure modeling with SWISS‐MODEL and Swiss‐PdbViewer: A historical perspective,"SWISS‐MODEL pioneered the field of automated modeling as the first protein modeling service on the Internet. In combination with the visualization tool Swiss‐PdbViewer, the Internet‐based Workspace and the SWISS‐MODEL Repository, it provides a fully integrated sequence to structure analysis and modeling platform. This computational environment is made freely available to the scientific community with the aim to hide the computational complexity of structural bioinformatics and encourage bench scientists to make use of the ever‐increasing structural information available. Indeed, over the last decade, the availability of structural information has significantly increased for many organisms as a direct consequence of the complementary nature of comparative protein modeling and experimental structure determination. This has a very positive and enabling impact on many different applications in biomedical research as described in this paper.",fullPaper,jv263
p1416,2498fa6bb2acf63abe1d430fc0d7a60a8b9eecbf,j234,Nature Reviews Microbiology,The abundance and variety of carbohydrate-active enzymes in the human gut microbiota,Abstract content,fullPaper,jv234
p1417,c5be2470f09cc15e7bf8aaa23da9d0d41f7d2a52,j264,Briefings in Bioinformatics,Briefings in bioinformatics.,Abstract content,fullPaper,jv264
p1418,8b27e2fafbe24cf9ce24f308a7e746489ff0dfb8,c15,International Conference on Conceptual Structures,Trainable Weka Segmentation: a machine learning tool for microscopy pixel classification,"Summary: State‐of‐the‐art light and electron microscopes are capable of acquiring large image datasets, but quantitatively evaluating the data often involves manually annotating structures of interest. This process is time‐consuming and often a major bottleneck in the evaluation pipeline. To overcome this problem, we have introduced the Trainable Weka Segmentation (TWS), a machine learning tool that leverages a limited number of manual annotations in order to train a classifier and segment the remaining data automatically. In addition, TWS can provide unsupervised segmentation learning schemes (clustering) and can be customized to employ user‐designed image features or classifiers. Availability and Implementation: TWS is distributed as open‐source software as part of the Fiji image processing distribution of ImageJ at http://imagej.net/Trainable_Weka_Segmentation. Contact: ignacio.arganda@ehu.eus Supplementary information: Supplementary data are available at Bioinformatics online.",poster,cp15
p1419,2323e0b4b4e6776b339fa11bf30009db8c298aaf,c5,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,R Programming for Bioinformatics,"From the co-developer of R and lead founder of the Bioconductor Project Thanks to its data handling and modeling capabilities and its flexibility, R is becoming the most widely used software in bioinformatics. R Programming for Bioinformatics builds the programming skills needed to use R for solving bioinformatics and computational biology problems. Drawing on the authors experiences as an R expert, the book begins with coverage on the general properties of the R language, several unique programming aspects of R, and object-oriented programming in R. It presents methods for data input and output as well as database interactions. The author also examines different facets of string handling and manipulations, discusses the interfacing of R with other languages, and describes how to write software packages. He concludes with a discussion on the debugging and profiling of R code.",poster,cp5
p1420,889340191b278df22f5d034d9361d2f0dae7de9e,j85,BMC Bioinformatics,A comparison of common programming languages used in bioinformatics,Abstract content,fullPaper,jv85
p1421,0299e3eb5299eaf7eebfbba36ec15552771dbe89,c71,IEEE International Conference on Information Reuse and Integration,Bringing Web 2.0 to bioinformatics,"Enabling deft data integration from numerous, voluminous and heterogeneous data sources is a major bioinformatic challenge. Several approaches have been proposed to address this challenge, including data warehousing and federated databasing. Yet despite the rise of these approaches, integration of data from multiple sources remains problematic and toilsome. These two approaches follow a user-to-computer communication model for data exchange, and do not facilitate a broader concept of data sharing or collaboration among users. In this report, we discuss the potential of Web 2.0 technologies to transcend this model and enhance bioinformatics research. We propose a Web 2.0-based Scientific Social Community (SSC) model for the implementation of these technologies. By establishing a social, collective and collaborative platform for data creation, sharing and integration, we promote a web services-based pipeline featuring web services for computer-to-computer data exchange as users add value. This pipeline aims to simplify data integration and creation, to realize automatic analysis, and to facilitate reuse and sharing of data. SSC can foster collaboration and harness collective intelligence to create and discover new knowledge. In addition to its research potential, we also describe its potential role as an e-learning platform in education. We discuss lessons from information technology, predict the next generation of Web (Web 3.0), and describe its potential impact on the future of bioinformatics studies.",poster,cp71
p1422,8bfa96e777ee8239f5063c3a84df1f2338b0211e,c79,International Conference on Medical Image Computing and Computer-Assisted Intervention,Swarm Intelligence Algorithms in Bioinformatics,Abstract content,poster,cp79
p1423,11d5bd7d92393695eb74a905006f4201088a22ff,j265,BioData Mining,A review of estimation of distribution algorithms in bioinformatics,Abstract content,fullPaper,jv265
p1424,b66561622170d97f4127f4131d485faefe3833f7,c94,Vision,"Improved structure, function and compatibility for CellProfiler: modular high-throughput image analysis software","UNLABELLED
There is a strong and growing need in the biology research community for accurate, automated image analysis. Here, we describe CellProfiler 2.0, which has been engineered to meet the needs of its growing user base. It is more robust and user friendly, with new algorithms and features to facilitate high-throughput work. ImageJ plugins can now be run within a CellProfiler pipeline.


AVAILABILITY AND IMPLEMENTATION
CellProfiler 2.0 is free and open source, available at http://www.cellprofiler.org under the GPL v. 2 license. It is available as a packaged application for Macintosh OS X and Microsoft Windows and can be compiled for Linux.


CONTACT
anne@broadinstitute.org


SUPPLEMENTARY INFORMATION
Supplementary data are available at Bioinformatics online.",poster,cp94
p1425,bf19714930d10b4263b8d0b6966702a4785f5fd0,j85,BMC Bioinformatics,A flexible R package for nonnegative matrix factorization,Abstract content,fullPaper,jv85
p1426,926ce1ba608a936f1b9d26e4f185ed8eae6a33d4,c28,International Conference on Collaboration Technologies and Systems,Viewpoint Paper: Translational Bioinformatics: Coming of Age,"The American Medical Informatics Association (AMIA) recently augmented the scope of its activities to encompass translational bioinformatics as a third major domain of informatics. The AMIA has defined translational bioinformatics as ""... the development of storage, analytic, and interpretive methods to optimize the transformation of increasingly voluminous biomedical data into proactive, predictive, preventative, and participatory health."" In this perspective, I will list eight reasons why this is an excellent time to be studying translational bioinformatics, including the significant increase in funding opportunities available for informatics from the United States National Institutes of Health, and the explosion of publicly-available data sets of molecular measurements. I end with the significant challenges we face in building a community of future investigators in Translational Bioinformatics.",poster,cp28
p1427,0d6c7c2fa5c75b330dc6ef4eb0fa79ace9b0e80f,j104,Scientometrics,Comparison and evaluation of Chinese research performance in the field of bioinformatics,Abstract content,fullPaper,jv104
p1428,e96384772807f391a7a62001631b3ec8730491e8,c112,Very Large Data Bases Conference,"RCSB Protein Data Bank: biological macromolecular structures enabling research and education in fundamental biology, biomedicine, biotechnology and energy","Abstract The Research Collaboratory for Structural Bioinformatics Protein Data Bank (RCSB PDB, rcsb.org), the US data center for the global PDB archive, serves thousands of Data Depositors in the Americas and Oceania and makes 3D macromolecular structure data available at no charge and without usage restrictions to more than 1 million rcsb.org Users worldwide and 600 000 pdb101.rcsb.org education-focused Users around the globe. PDB Data Depositors include structural biologists using macromolecular crystallography, nuclear magnetic resonance spectroscopy and 3D electron microscopy. PDB Data Consumers include researchers, educators and students studying Fundamental Biology, Biomedicine, Biotechnology and Energy. Recent reorganization of RCSB PDB activities into four integrated, interdependent services is described in detail, together with tools and resources added over the past 2 years to RCSB PDB web portals in support of a ‘Structural View of Biology.’",poster,cp112
p1429,405f85213f8df4a6a90eec9c7ac39985ea96ffaf,c48,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",Bioinformatics Research and Applications,Abstract content,poster,cp48
p1430,cd49acefc8d51e324aa562e5337e1c2aff067053,c62,International Conference on Software Reuse,An Overview of Multi-task Learning,"As a promising area in machine learning, multi-task learning (MTL) aims to improve the performance of multiple related learning tasks by leveraging useful information among them. In this paper, we give an overview of MTL by first giving a definition of MTL. Then several different settings of MTL are introduced, including multi-task supervised learning, multi-task unsupervised learning, multi-task semi-supervised learning, multi-task active learning, multi-task reinforcement learning, multi-task online learning and multi-task multi-view learning. For each setting, representative MTL models are presented. In order to speed up the learning process, parallel and distributed MTL models are introduced. Many areas, including computer vision, bioinformatics, health informatics, speech, natural language processing, web applications and ubiquitous computing, use MTL to improve the performance of the applications involved and some representative works are reviewed. Finally, recent theoretical analyses for MTL are presented.",poster,cp62
p1431,31c5beeafccb48877530db023610e8604b1136ea,j5,Genome Biology,GoMiner: a resource for biological interpretation of genomic and proteomic data,Abstract content,fullPaper,jv5
p1432,c9f3c3ec1dfe2b4152cc95ef70e42ff7c309d773,j85,BMC Bioinformatics,"Bio-jETI: a service integration, design, and provisioning platform for orchestrated bioinformatics processes",Abstract content,fullPaper,jv85
p1433,4268936344a3b518dac023328af908c644665232,c37,Asia-Pacific Software Engineering Conference,Bioinformatics Algorithms: Techniques and Applications,"This book introduces algorithmic techniques in bioinformatics, emphasizing their application to solving novel problems in post-genomic molecular biology. Beginning with a thought-provoking discussion on the role of algorithms in twenty-first-century bioinformatics education, the book covers: general algorithmic techniques, algorithms and tools for genome and sequence analysis, microarray design and analysis, algorithmic issues arising in the analysis of genetic variation across human population, and algorithmic approaches in structural and systems biology.",poster,cp37
p1434,fdc20dd8ff20ef9ad955fcf10ba6c0cb9e684633,c66,Annual Conference on Innovation and Technology in Computer Science Education,An Introduction to Bioinformatics for Glycomics Research,"Carbohydrates are considered the thirdclass of information-encoding biologicalmacromolecules. ‘‘Glycomics,’’ the scientificattempt to characterize and study carbohy-drates, is a rapidly emerging branch ofscience, for which informatics is just begin-ning. Glycomics requires sophisticated algo-rithmic approaches. Several algorithms andmodels have been developed for glycobiol-ogy research in the past several years. Thistutorial will provide a brief introduction tothe field of glycome informatics, which willinclude a primer on glycobiology as well asdescriptions of the algorithms and modelsthat have been developed in this field.The four essential molecular buildingblocks of cells are nucleic acids, proteins,lipids, and carbohydrates, often referred toas glycans. Nucleotide and protein sequenc-es are at the heart of nearly all bioinfor-matics applications and research, whereasglycan and lipid structures have been widelyneglected in bioinformatics. However, gly-cans are the most abundant and structurallydiverse biopolymers formed in nature.Bound to proteins, as glycoproteins, theyare known to affectthefunctions of proteins.More than half of all protein sequencesdeposited in the SWISS-PROT databankinclude potential glycosylation sites and thusmay be glycoproteins. Based on an analysisof well-annotated and characterized glyco-proteins inSWISS-PROT,itwas concludedthat more than half of all proteins areglycosylated [1].The development and use of informaticstools and databases for glycobiology andglycomics research has increased consider-ably in recent years. However, the generaldevelopment in this field can still beconsidered as being in its infancy whencompared to the genomics and proteomicsareas. In terms of bioinformatics in glyco-biology, there are several paths of researchthat are currently in progress. The develop-ment of algorithms to reliably support thecharacterization of glycan structures forhigh-throughput applications is the mostimmediate demand of the glycomics com-munity. Additionally, several major glyco-related projects (Consortium for FunctionalGlycomics [2], KEGG Glycan [3], GLY-COSCIENCES.de [4]) are maturing andprovide well-structured glyco-related datathat are awaiting data mining and analysis.With the exciting new developments incarbohydrate arrays and automated MSannotation, the analysis of the glycome hasreached a new level of sophistication, whichrequires broader informatics support. Thistutorial aims to give an overview of thecurrent status of carbohydrate databases, thenewest analytical techniques, as well as theinformatics needed for rapid progress inglycomics research.",poster,cp66
p1435,57c4f921e6ac7bf27a51fbbcb5ee1d160cf9bb98,j266,Biopolymers,Protein secondary structure analyses from circular dichroism spectroscopy: methods and reference databases.,"Circular dichroism (CD) spectroscopy has been a valuable method for the analysis of protein secondary structures for many years. With the advent of synchrotron radiation circular dichroism (SRCD) and improvements in instrumentation for conventional CD, lower wavelength data are obtainable and the information content of the spectra increased. In addition, new computation and bioinformatics methods have been developed and new reference databases have been created, which greatly improve and facilitate the analyses of CD spectra. This article discusses recent developments in the analysis of protein secondary structures, including features of the DICHROWEB analysis webserver.",fullPaper,jv266
p1436,908501ab3725ec461d04cf5bf7334aa992322f7c,j267,Current Opinion in Biotechnology,Bioinformatics applications for pathway analysis of microarray data.,Abstract content,fullPaper,jv267
p1437,d4ed9b961fe3ac06d125cbfb2cc533584d41f6e8,c54,International Workshop on Agent-Oriented Software Engineering,Computational Intelligence in Bioinformatics,Abstract content,poster,cp54
p1438,c4769f807b0700d077f37edc9a48de5486e1e638,c2,International Symposium on Intelligent Data Analysis,Applications of Fuzzy Logic in Bioinformatics,Introduction to Bioinformatics Introduction to Fuzzy Set Theory and Fuzzy Logic Fuzzy Similarities in Ontologies Fuzzy Logic in Structural Bioinformatics Microarray Data Analysis Other Applications Summary Fundamental Biological Concepts Online Resources.,poster,cp2
p1439,63b0c8c5c98acab0d8866af8518470306363bc64,c15,International Conference on Conceptual Structures,A multivariate von mises distribution with applications to bioinformatics,"Motivated by problems of modelling torsional angles in molecules, Singh, Hnizdo & Demchuk (2002) proposed a bivariate circular model which is a natural torus analogue of the bivariate normal distribution and a natural extension of the univariate von Mises distribution to the bivariate case. The authors present here a multivariate extension of the bivariate model of Singh, Hnizdo & Demchuk (2002). They study the conditional distributions and investigate the shapes of marginal distributions for a special case. The methods of moments and pseudo‐likelihood are considered for the estimation of parameters of the new distribution. The authors investigate the efficiency of the pseudo‐likelihood approach in three dimensions. They illustrate their methods with protein data of conformational angles",poster,cp15
p1440,4c45d3bc03cbbeeeee9578847284ce5c1acffe1c,j223,Physiological Genomics,Data analysis and bioinformatics tools for tandem mass spectrometry in proteomics.,"Data processing is a central and critical component of a successful proteomics experiment, and is often the most time-consuming step. There have been considerable advances in the field of proteomics informatics in the past 5 years, spurred mainly by free and open-source software tools. Along with the gains afforded by new software, the benefits of making raw data and processed results freely available to the community in data repositories are finally in evidence. In this review, we provide an overview of the general analysis approaches, software tools, and repositories that are enabling successful proteomics research via tandem mass spectrometry.",fullPaper,jv223
p1441,c83033edb5e5b6d3e1a67374083e00118ff5375b,c109,International Conference on Mobile Data Management,VARNA: Interactive drawing and editing of the RNA secondary structure,"Description: VARNA is a tool for the automated drawing, visualization and annotation of the secondary structure of RNA, designed as a companion software for web servers and databases. Features: VARNA implements four drawing algorithms, supports input/output using the classic formats dbn, ct, bpseq and RNAML and exports the drawing as five picture formats, either pixel-based (JPEG, PNG) or vector-based (SVG, EPS and XFIG). It also allows manual modification and structural annotation of the resulting drawing using either an interactive point and click approach, within a web server or through command-line arguments. Availability: VARNA is a free software, released under the terms of the GPLv3.0 license and available at http://varna.lri.fr Contact: ponty@lri.fr Supplementary information: Supplementary data are available at Bioinformatics online.",poster,cp109
p1442,5e9f8540a473f0ab3038b5825fc79751d978f12b,j85,BMC Bioinformatics,FastGroupII: A web-based bioinformatics platform for analyses of large 16S rDNA libraries,Abstract content,fullPaper,jv85
p1443,e715c5cb0f0b89bb1936440fa7b50e8f015a0fca,c40,IEEE International Conference on Software Maintenance and Evolution,Bayesian methods in bioinformatics and computational systems biology,"Bayesian methods are valuable, inter alia, whenever there is a need to extract information from data that are uncertain or subject to any kind of error or noise (including measurement error and experimental error, as well as noise or random variation intrinsic to the process of interest). Bayesian methods offer a number of advantages over more conventional statistical techniques that make them particularly appropriate for complex data. It is therefore no surprise that Bayesian methods are becoming more widely used in the fields of genetics, genomics, bioinformatics and computational systems biology, where making sense of complex noisy data is the norm. This review provides an introduction to the growing literature in this area, with particular emphasis on recent developments in Bayesian bioinformatics relevant to computational systems biology.",poster,cp40
p1444,a8379a2045b261bda529b70eca23827120760ed0,j268,Biometrics,Protein Bioinformatics and Mixtures of Bivariate von Mises Distributions for Angular Data,"Summary A fundamental problem in bioinformatics is to characterize the secondary structure of a protein, which has traditionally been carried out by examining a scatterplot (Ramachandran plot) of the conformational angles. We examine two natural bivariate von Mises distributions—referred to as Sine and Cosine models—which have five parameters and, for concentrated data, tend to a bivariate normal distribution. These are analyzed and their main properties derived. Conditions on the parameters are established which result in bimodal behavior for the joint density and the marginal distribution, and we note an interesting situation in which the joint density is bimodal but the marginal distributions are unimodal. We carry out comparisons of the two models, and it is seen that the Cosine model may be preferred. Mixture distributions of the Cosine model are fitted to two representative protein datasets using the expectation maximization algorithm, which results in an objective partition of the scatterplot into a number of components. Our results are consistent with empirical observations; new insights are discussed.",fullPaper,jv268
p1445,c49fd8fc2e1eb9e2c4dfb729e269e6e746dd01a9,c29,International Conference on Software Engineering,Hidden Markov Models in Bioinformatics,"Hidden Markov Models (HMMs) became recently important and popular among bioinformatics researchers, and many software tools are based on them. In this survey, we first consider in some detail the mathematical foundations of HMMs, we describe the most important algorithms, and provide useful comparisons, pointing out advantages and drawbacks. We then consider the major bioinformatics applications, such as alignment, labeling, and profiling of sequences, protein structure prediction, and pattern recognition. We finally provide a critical appraisal of the use and perspectives of HMMs in bioinformatics.",poster,cp29
p1446,adea14ce6f45a439d3480bf2f4c030b77ea89102,j20,Proceedings of the National Academy of Sciences of the United States of America,Assigning protein functions by comparative genome analysis: protein phylogenetic profiles.,"Determining protein functions from genomic sequences is a central goal of bioinformatics. We present a method based on the assumption that proteins that function together in a pathway or structural complex are likely to evolve in a correlated fashion. During evolution, all such functionally linked proteins tend to be either preserved or eliminated in a new species. We describe this property of correlated evolution by characterizing each protein by its phylogenetic profile, a string that encodes the presence or absence of a protein in every known genome. We show that proteins having matching or similar profiles strongly tend to be functionally linked. This method of phylogenetic profiling allows us to predict the function of uncharacterized proteins.",fullPaper,jv20
p1447,a8e1bec4e48ea4e022ee15004eddca5e1c440d02,j175,Nature reviews genetics,DNA secondary structures: stability and function of G-quadruplex structures,Abstract content,fullPaper,jv175
p1448,a0b789d1f3afe9cf6fadfaa8f120479181f0c4e5,c56,European Conference on Software Process Improvement,"The Taverna workflow suite: designing and executing workflows of Web Services on the desktop, web or in the cloud","The Taverna workflow tool suite (http://www.taverna.org.uk) is designed to combine distributed Web Services and/or local tools into complex analysis pipelines. These pipelines can be executed on local desktop machines or through larger infrastructure (such as supercomputers, Grids or cloud environments), using the Taverna Server. In bioinformatics, Taverna workflows are typically used in the areas of high-throughput omics analyses (for example, proteomics or transcriptomics), or for evidence gathering methods involving text mining or data mining. Through Taverna, scientists have access to several thousand different tools and resources that are freely available from a large range of life science institutions. Once constructed, the workflows are reusable, executable bioinformatics protocols that can be shared, reused and repurposed. A repository of public workflows is available at http://www.myexperiment.org. This article provides an update to the Taverna tool suite, highlighting new features and developments in the workbench and the Taverna Server.",poster,cp56
p1449,4054e6682de81b8f524ae519ca14a50f9a58fa3a,j85,BMC Bioinformatics,"A Semantic Web for bioinformatics: goals, tools, systems, applications",Abstract content,fullPaper,jv85
p1450,ca34071945a3983f2cdbbeab0b9ea19555c3c3be,c10,Big Data,Introduction to bioinformatics,"This textbook is a solid introduction to the science of Bioinformatics, an integration of computing skills and biological methods.",poster,cp10
p1451,a996e4dd541f36005960b1d084eea33dbb14666c,j269,Applied Bioinformatics,Support vector machine applications in bioinformatics.,"The support vector machine (SVM) approach represents a data-driven method for solving classification tasks. It has been shown to produce lower prediction error compared to classifiers based on other methods like artificial neural networks, especially when large numbers of features are considered for sample description. In this review, the theory and main principles of the SVM approach are outlined, and successful applications in traditional areas of bioinformatics research are described. Current developments in techniques related to the SVM approach are reviewed which might become relevant for future functional genomics and chemogenomics projects. In a comparative study, we developed neural network and SVM models to identify small organic molecules that potentially modulate the function of G-protein coupled receptors. The SVM system was able to correctly classify approximately 90% of the compounds in a cross-validation study yielding a Matthews correlation coefficient of 0.78. This classifier can be used for fast filtering of compound libraries in virtual screening applications.",fullPaper,jv269
p1452,f5f4a0933cf097fb14ab84ca295b1bdfe01f97e0,c26,PS,PlasmoDB: a functional genomic database for malaria parasites,"PlasmoDB (http://PlasmoDB.org) is a functional genomic database for Plasmodium spp. that provides a resource for data analysis and visualization in a gene-by-gene or genome-wide scale. PlasmoDB belongs to a family of genomic resources that are housed under the EuPathDB (http://EuPathDB.org) Bioinformatics Resource Center (BRC) umbrella. The latest release, PlasmoDB 5.5, contains numerous new data types from several broad categories—annotated genomes, evidence of transcription, proteomics evidence, protein function evidence, population biology and evolution. Data in PlasmoDB can be queried by selecting the data of interest from a query grid or drop down menus. Various results can then be combined with each other on the query history page. Search results can be downloaded with associated functional data and registered users can store their query history for future retrieval or analysis.",poster,cp26
p1453,e4fe2b079f96f67de6155353d6e68a512046bc15,j85,BMC Bioinformatics,Bioclipse: an open source workbench for chemo- and bioinformatics,Abstract content,fullPaper,jv85
p1454,6fad89f42679456392454cc1b0629ec063418611,c7,European Conference on Modelling and Simulation,"Computational Intelligence in Solving Bioinformatics Problems: Reviews, Perspectives, and Challenges",Abstract content,poster,cp7
p1455,5033fa9b716c8935a919c60213db6de1a1ab1c2f,j97,Science,Recurrent Fusion of TMPRSS2 and ETS Transcription Factor Genes in Prostate Cancer,"Recurrent chromosomal rearrangements have not been well characterized in common carcinomas. We used a bioinformatics approach to discover candidate oncogenic chromosomal aberrations on the basis of outlier gene expression. Two ETS transcription factors, ERG and ETV1, were identified as outliers in prostate cancer. We identified recurrent gene fusions of the 5′ untranslated region of TMPRSS2 to ERG or ETV1 in prostate cancer tissues with outlier expression. By using fluorescence in situ hybridization, we demonstrated that 23 of 29 prostate cancer samples harbor rearrangements in ERG or ETV1. Cell line experiments suggest that the androgen-responsive promoter elements of TMPRSS2 mediate the overexpression of ETS family members in prostate cancer. These results have implications in the development of carcinomas and the molecular diagnosis and treatment of prostate cancer.",fullPaper,jv97
p1456,3fca86d94b9ea87d7dcf59fade8f2b9f7f5f269e,c89,Conference on Uncertainty in Artificial Intelligence,The New Bioinformatics: Integrating Ecological Data from the Gene to the Biosphere,"Bioinformatics, the application of computational tools to the management and analysis of biological data, has stimulated rapid research advances in genomics through the development of data archives such as GenBank, and similar progress is just beginning within ecology. One reason for the belated adoption of informatics approaches in ecology is the breadth of ecologically pertinent data (from genes to the biosphere) and its highly heterogeneous nature. The variety of formats, logical structures, and sampling methods in ecology create significant challenges. Cultural barriers further impede progress, especially for the creation and adoption of data standards. Here we describe informatics frameworks for ecology, from subject-specific data warehouses, to generic data collections that use detailed metadata descriptions and formal ontologies to catalog and cross-reference information. Combining these approaches with automated data integration techniques and scientific workflow systems will maximize the value of data and open new frontiers for research in ecology.",poster,cp89
p1457,f63041d1ef283b7fe438380f0d481b801cf9d5e2,c34,IEEE Working Conference on Mining Software Repositories,"Computational Intelligence in Biomedicine and Bioinformatics, Current Trends and Applications",Abstract content,poster,cp34
p1458,1c03fe2109a009899b8110a375269c34afcc3f76,c91,Workshop on Algorithms and Models for the Web-Graph,Bioinformatics for Geneticists: A Bioinformatics Primer for the Analysis of Genetic Data,"Michael R. Barnes, ed 554 pages. England, UK: John Wiley & Sons; 2007. 2nd ed. $90.00. Paperback. ISBN 978-0-470-02620-5

This book, now in its second edition for more than a year, is positioned at the intersection of disciplines including genetics, bioinformatics (the melding of computer science and biology), biomedical research, and molecular biology. Over 19 chapters, the authors cover an impressive terrain. The focus is mainly on human genetics and genomics, with research in other species also presented, particularly where it supports and advances our understanding of human genetics. Although a thoughtful discussion of the relevant literature and techniques is found in each chapter, the book is not overly technical and does not present advanced mathematical, statistical, or genetic concepts in great depth. Instead, the focus is on practical applications, available tools, software, and databases, and the presentation of supporting real world research examples. The end result is one of the best available and most accessible texts on bioinformatics and genetics in the postgenome age.

This book is …",poster,cp91
p1459,c6514083b0da5da8bbb4fcf35380b24045d21752,j270,Journal of Proteomics & Bioinformatics,Integrated Bioinformatics for Radiation-Induced Pathway Analysis from Proteomics and Microarray Data.,"Functional analysis and interpretation of large-scale proteomics and gene expression data require effective use of bioinformatics tools and public knowledge resources coupled with expert-guided examination. An integrated bioinformatics approach was used to analyze cellular pathways in response to ionizing radiation. ATM, or ataxia-telangiectasia mutated , a serine-threonine protein kinase, plays critical roles in radiation responses, including cell cycle arrest and DNA repair. We analyzed radiation responsive pathways based on 2D-gel/MS proteomics and microarray gene expression data from fibroblasts expressing wild type or mutant ATM gene. The analysis showed that metabolism was significantly affected by radiation in an ATM dependent manner. In particular, purine metabolic pathways were differentially changed in the two cell lines. The expression of ribonucleoside-diphosphate reductase subunit M2 (RRM2) was increased in ATM-wild type cells at both mRNA and protein levels, but no changes were detected in ATM-mutated cells. Increased expression of p53 was observed 30min after irradiation of the ATM-wild type cells. These results suggest that RRM2 is a downstream target of the ATM-p53 pathway that mediates radiation-induced DNA repair. We demonstrated that the integrated bioinformatics approach facilitated pathway analysis, hypothesis generation and target gene/protein identification.",fullPaper,jv270
p1460,b7e4d6a502e1cf13a304ca9b96bc3a7f96820b9b,j271,New Phytologist,Fungal community analysis by high-throughput sequencing of amplified markers – a user's guide,"Novel high-throughput sequencing methods outperform earlier approaches in terms of resolution and magnitude. They enable identification and relative quantification of community members and offer new insights into fungal community ecology. These methods are currently taking over as the primary tool to assess fungal communities of plant-associated endophytes, pathogens, and mycorrhizal symbionts, as well as free-living saprotrophs. Taking advantage of the collective experience of six research groups, we here review the different stages involved in fungal community analysis, from field sampling via laboratory procedures to bioinformatics and data interpretation. We discuss potential pitfalls, alternatives, and solutions. Highlighted topics are challenges involved in: obtaining representative DNA/RNA samples and replicates that encompass the targeted variation in community composition, selection of marker regions and primers, options for amplification and multiplexing, handling of sequencing errors, and taxonomic identification. Without awareness of methodological biases, limitations of markers, and bioinformatics challenges, large-scale sequencing projects risk yielding artificial results and misleading conclusions.",fullPaper,jv271
p1461,67ae074faadc90b9a4f7b439cda42c29a9e87bd6,j85,BMC Bioinformatics,Accelerating String Set Matching in FPGA Hardware for Bioinformatics Research,Abstract content,fullPaper,jv85
p1462,8788d1c95b3d2b6a8653a35a4fcd4988c73f8769,c107,British Machine Vision Conference,Bioinformatics for Vaccinology,"CONTENTS Preface Acknowledgements Exordium: Vaccines: a Very, Very Short Introduction 1 Vaccines: Their Place in History Smallpox in History Variolation Variolation in History Variolation Comes to Britain Lady Mary Wortley Montagu Variolation and the Sublime Porte The Royal Experiment The Boston Connection Variolation Takes Hold The Suttonian Method Variolation in Europe The Coming of Vaccination Edward Jenner Cowpox Vaccination Vindicated Louis Pasteur Vaccination Becomes a Science Meister, Pasteur, and Rabies A Vaccine for Every Disease In the Time of Cholera Haffkine and Cholera Bubonic Plague The Changing Face of Disease Almroth Wright and Typhoid Tuberculosis, Koch, and Calmette Vaccine BCG Poliomyelitis Salk and Sabin Diptheria Whooping Cough Many Diseases, Many Vaccines Smallpox: Endgame Further Reading 2 Vaccines: Need and Opportunity Eradication and Reservoirs The Ongoing Burden of Disease Lifespans The Evolving Nature of Disease Economics, Climate, and Disease Three Threats Tuberculosis in the 21st Century HIV and AIDS Malaria: Then and Now Influenza Bioterrorism Vaccines as Medicines Vaccines and the Pharmaceutical Industry Making Vaccines The Coming of the Vaccine Industry 3 Vaccines: How They Work Challenging the Immune System The Threat from Bacteria: Robust, Diverse, and Endemic MiCrobes, Diversity, and Metagenomics The Intrinsic Complexity of the Bacterial Threat Microbes and Humankind The Nature of Vaccines Types of Vaccine Carbohydrate Vaccines Epitopic Vaccines Adjuvants and Vaccine Delivery Emerging Immunovaccinology The Immune System Innate Immunity Adaptive Immunity The Microbiome and Mucosal Immunity Cellular Components of Immunity Cellular Immunity The T Cell Repertoire Epitopes: The Immunological Quantum The Major Histocompatility Complex MHC Nomenclature Peptide Binding by the MHC The Structure of the MHC Antigen Presentation The Proteasome Transporter Associated with Antigen Processing Class II Processing Seek Simplicity and Then Distrust It Cross Presentation T Cell Receptor T Cell Activation Immunological Synapse Signal 1, Signal 2, Immunodominance Humoral Immunity Further Reading 4 Vaccines: Data and Databases Making Sense of Data Knowledge in a Box The Science of -Omes and -Omics The Proteome Systems Biology The Immunome Databases and Databanks The Relational Database The XML Database The Protein Universe Much Data, Many Databases What Proteins Do What Proteins Are The Amino Acid World The Chiral Nature of Amino Acids Naming the Amino Acids The Amino Acid Alphabet Defining Amino Acid Properties Size, Charge, and Hydrogen Bonding Hydrophobicity, Lipophilicity, and Partitioning Understanding Partitioning Charges, Ionization, and pKa Many Kinds of Property Mapping the World of Sequences Biological Sequence Databases Nucleic Acid Sequence Databases Protein Sequence Databases Annotating Databases Text Mining Ontologies Secondary Sequence Databases Other Databases Databases in Immunology Host Databases Pathogen Databases Functional Immunological Databases Composite, Integrated Databases Allergen Databases Further Reading Reference 5 Vaccines: Data Driven Prediction of Binders, Epitopes and Immunogenicity Towards Epitope-Based Vaccines T Cell Epitope Prediction Predicting MHC Binding Binding is Biology Quantifying Binding Entropy, Enthalpy, and Entropy-Enthalpy Compensation Experimental Measurement of Binding Modern Measurement Methods Isothermal Titration Calorimetry Long and Short of Peptide Binding The Class I Peptide Repertoire Practicalities of Binding Prediction Binding Becomes Recognition Immunoinformatics Lends a Hand Motif Based Prediction The Imperfect Motif Other Approaches to Binding Prediction Representing Sequences Computer Science Lends a Hand Artificial Neural Networks Hidden Markov Model Support Vector Machines Robust Multivariate Statistics Partial Least Squares Quantitative Structure Activity Relationships Other Techniques and Sequence Representations Amino Acid Properties Direct Epitope Prediction Predicting Antigen Presentation Predicting Class II MHC Binding Assessing Prediction Accuracy Roc Plots Quantitative Accuracy Prediction Assessment Protocols Comparing Predictions Prediction Versus Experiment Predicting B Cell Epitopes Peak Profiles and Smoothing Early Methods Imperfect B Cell Prediction References 6 Vaccines: Structural Approaches Structure and Function Structure and Function Types of Protein Structure Protein Folding Ramachandran Plots Local Structures Protein Families, Protein Folds Comparing Structures Experimental Structure Determination Structural Genomics Protein Structure Databases Other Databases Immunological Structural Databases Small Molecule Databases Protein Homology Modelling Using Homology Modelling Predicting MHC Supertypes Application to Alloreactivity 3D-QSAR Protein Docking Predicting B Cell Epitopes with Docking Virtual Screening Limitations to Virtual Screening Predicting Epitopes with Virtual Screening Virtual Screening and Adjuvant Discovery Adjuvants and Innate Immunity Small Molecule Adjuvants Molecular Dynamics and Immunology Molecular Dynamics Methodology Molecular Dynamics and Binding Immunological Applications Limitations of Molecular Dynamics Molecular Dynamics and High Performance Computing References 7 Vaccines: Computational Solutions Vaccines and the World Bioinformatics and the Challenge for Vaccinology Predicting Immunogenicity Computational Vaccinology The Threat Remains Beyond Empirical Vaccinology Designing New Vaccines The Perfect Vaccine Conventional Approaches Genome Sequences Size of a Genome Reverse Vaccinology Finding Antigens The Success of Reverse Vaccinology Tumour Vaccines Prediction and Personalised Medicine Imperfect Data Forecasting and the Future of Computational Vaccinology Index",poster,cp107
p1463,78d054b11a2f91623dffecc1f5a2d372ada0451f,c1,Technical Symposium on Computer Science Education,MorphoLibJ: integrated library and plugins for mathematical morphology with ImageJ,"MOTIVATION
Mathematical morphology (MM) provides many powerful operators for processing 2D and 3D images. However, most MM plugins currently implemented for the popular ImageJ/Fiji platform are limited to the processing of 2D images.


RESULTS
The MorphoLibJ library proposes a large collection of generic tools based on MM to process binary and grey-level 2D and 3D images, integrated into user-friendly plugins. We illustrate how MorphoLibJ can facilitate the exploitation of 3D images of plant tissues.


AVAILABILITY AND IMPLEMENTATION
MorphoLibJ is freely available at http://imagej.net/MorphoLibJ CONTACT: david.legland@nantes.inra.frSupplementary information: Supplementary data are available at Bioinformatics online.",poster,cp1
p1464,2bd0c5cd32ba4e954e863ad72cd5c00c90fc7ec0,c7,European Conference on Modelling and Simulation,GOSemSim: an R package for measuring semantic similarity among GO terms and gene products,"SUMMARY
The semantic comparisons of Gene Ontology (GO) annotations provide quantitative ways to compute similarities between genes and gene groups, and have became important basis for many bioinformatics analysis approaches. GOSemSim is an R package for semantic similarity computation among GO terms, sets of GO terms, gene products and gene clusters. Four information content (IC)- and a graph-based methods are implemented in the GOSemSim package, multiple species including human, rat, mouse, fly and yeast are also supported. The functions provided by the GOSemSim offer flexibility for applications, and can be easily integrated into high-throughput analysis pipelines.


AVAILABILITY
GOSemSim is released under the GNU General Public License within Bioconductor project, and freely available at http://bioconductor.org/packages/2.6/bioc/html/GOSemSim.html.",poster,cp7
p1465,ad8aea7f99d6f009d865c4e29a8c0b895c53daca,j272,International Journal of Bioinformatics Research and Applications,The myGrid ontology: bioinformatics service discovery,"(my)Grid supports in silico experiments in the life sciences, enabling the design and enactment of workflows as well as providing components to assist service discovery, data and metadata management. The (my)Grid ontology is one component in a larger semantic discovery framework for the identification of the highly distributed and heterogeneous bioinformatics services in the public domain. From an initial model of formal OWL-DL semantics throughout, we now adopt a spectrum of expressivity and reasoning for different tasks in service annotation and discovery. Here, we discuss the development and use of the (my)Grid ontology and our experiences in semantic service discovery.",fullPaper,jv272
p1466,017e6f89bf1aaea7c3c4a81a7c2cb4402b219251,j273,Current molecular medicine,Bioinformatics approaches in the study of cancer.,"A revolution is underway in the approach to studying the genetic basis of cancer. Massive amounts of data are now being generated via high-throughput techniques such as DNA microarray technology and new computational algorithms have been developed to aid in analysis. At the same time, standards-based repositories, including the Stanford Microarray Database and the Gene Expression Omnibus have been developed to store and disseminate the results of microarray experiments. Bioinformatics, the convergence of biology, information science, and computation, has played a key role in these developments. Recently developed techniques include Module Maps, SLAMS (Stepwise Linkage Analysis of Microarray Signatures), and COPA (Cancer Outlier Profile Analysis). What these techniques have in common is the application of novel algorithms to find high-level gene expression patterns across heterogeneous microarray experiments. Large-scale initiatives are underway as well. The Cancer Genome Atlas (TCGA) project is a logical extension of the Human Genome Project and is meant to produce a comprehensive atlas of genetic changes associated with cancer. The Cancer Biomedical Informatics Grid (caBIG), led by the NCI, also represents a colossal initiative involving virtually all aspects of cancer research and may help to transform the way cancer research is conducted and data are shared.",fullPaper,jv273
p1467,25a9a20c789c29ebea67e74f15ab14d2ae62ac78,c106,Chinese Conference on Biometric Recognition,A Tutorial on Hierarchical Classification with Applications in Bioinformatics.,"In Machine Learning and Data Mining, most of the works in classification problems deal with flat classification, where each instance is classified in one of a set of possible classes and there is no hierarchical relationship between the classes. There are, however, more complex classification problems where the classes to be predicted are hierarchically related. This chapter presents a tutorial on the hierarchical classification techniques found in the literature. We also discuss how hierarchical classification techniques have been applied to the area of Bioinformatics (particularly the prediction of protein function), where hierarchical classification problems are often found. INTRODUCTION Classification is one of the most important problems in Machine Learning (ML) and Data Mining (DM). In general, a classification problem can be formally defined as: Given a set of training examples composed of pairs {xi, yi}, find a function f(x) that maps each xi to its associated class yi, i = 1, 2, ..., n, where n is the total number of training examples. After training, the predictive accuracy of the classification function induced is evaluated by using it to classify a set of unlabeled examples, unseen during training. This evaluation measures the generalization ability (predictive accuracy) of the classification function induced. The vast majority of classification problems addressed in the literature involves flat classification, where each example is assigned to a class out of a finite (and usually small) set of classes. By contrast, in hierarchical classification problems, the classes are disposed in a hierarchical structure, such as a tree or a Directed Acyclic Graph (DAG). In these structures, the nodes represent classes. Figure 1 illustrates the difference between flat and hierarchical classification problems. To keep the example simple, Figure 1(b) shows a tree-structured class hierarchy. The more complex case of DAG-structured class hierarchies will be discussed later. In Figure 1, each node – except the root nodes – is labeled with the number of a class. In Figure 1(b), class 1 is divided into two sub-classes, 1.1 and 1.2, and class 3 is divided into three sub-classes. The root nodes are labeled “any class”, to denote the case where the class of an example is unknown. Figure 1 clearly shows that flat classification problems are actually a particular case of hierarchical classification problems where there is a single level of classes, i.e., where no class is divided into sub-classes.",poster,cp106
p1468,bdb46dbd60cb2d4b830204e389910db17a4aabd6,j85,BMC Bioinformatics,Biowep: a workflow enactment portal for bioinformatics applications,Abstract content,fullPaper,jv85
p1469,d403e3b41674381603731f9caff65cc358d45435,c56,European Conference on Software Process Improvement,"Bioinformatics and the cell - modern computational approaches in genomics, proteomics and transcriptomics",Blast and Fasta.- Sequence alignment.- Contig assembly.- DNA replication and viral evolution.- Gene and motif prediction.- Hidden Markov Models.- Gibbs Sampler.- Bioinformatics and vertebrate mitochondria.- Characterizing translation efficiency.- Protein isoelectric point.- Bioinformatics and Two-Dimensional Protein Separation.- Self-Organizing Map and other clustering Algorithms.- Molecular Phylogenetics.- Fundamentals of Proteomics.,poster,cp56
p1470,d47cdbf13756f4596f4fdf3c9fd4cb264ca4fe8a,c58,Australian Software Engineering Conference,"Bioinformatics: Genes, Proteins and Computers",1. Molecular Evolution. 2. Gene Finding. 3. Sequence Comparison Methods. 4. Amino Acid Residue Conservation. 5. Function Prediction From Protein Sequence. 6. Protein Structure Comparison. 7. Protein Structure Classifications. 8. Comparative Modelling. 9. Protein Structure Prediction. 10. From Protein Structure to Function. 11. From Structure-Based Genome Annotation to Understanding Genes and Proteins. 12. Global Approaches for Studying Protein-Protein Interactions. 13. Predicting The Structure of Protein-Biomolecular Interactions. 14. Experimental Use of DNA Arrays. 15. Mining Gene Expression Data 16. Proteomics. 17. Data Management of Biological Information. 18. Internet Technologies for Bioinformatics.,poster,cp58
p1471,30c3b06741c94f287d07b79c15db73ea0c55d123,j85,BMC Bioinformatics,BioWMS: a web-based Workflow Management System for bioinformatics,Abstract content,fullPaper,jv85
p1472,ce9456dd3a73a5587577eb56d314f9a2cbb7e196,c100,ACM SIGMOD Conference,BioWeka - extending the Weka framework for bioinformatics,"UNLABELLED
Given the growing amount of biological data, data mining methods have become an integral part of bioinformatics research. Unfortunately, standard data mining tools are often not sufficiently equipped for handling raw data such as e.g. amino acid sequences. One popular and freely available framework that contains many well-known data mining algorithms is the Waikato Environment for Knowledge Analysis (Weka). In the BioWeka project, we introduce various input formats for bioinformatics data and bioinformatics methods like alignments to Weka. This allows users to easily combine them with Weka's classification, clustering, validation and visualization facilities on a single platform and therefore reduces the overhead of converting data between different data formats as well as the need to write custom evaluation procedures that can deal with many different programs. We encourage users to participate in this project by adding their own components and data formats to BioWeka.


AVAILABILITY
The software, documentation and tutorial are available at http://www.bioweka.org.",poster,cp100
p1473,1471ff1368275dd2be4499726e04b2dd8295dd6d,j216,Infection and Immunity,National Institute of Allergy and Infectious Diseases Bioinformatics Resource Centers: New Assets for Pathogen Informatics,The National Institute of Allergy and Infectious Diseases (NIAID) began a new bioinformatic venture in July 2004 intended to integrate the vast amount of genomic and other biological data that are both available and being produced by the rapid increase in biodefense research. Eight Bioinformatics,fullPaper,jv216
p1474,c321a3faadad3ad1144a52fbad39069e98f1bfa1,c24,Decision Support Systems,Biskit - A software platform for structural bioinformatics,"UNLABELLED
Biskit is a modular, object-oriented python library that provides intuitive classes for many typical tasks of structural bioinformatics research. It facilitates the manipulation and analysis of macromolecular structures, protein complexes and molecular dynamics trajectories. At the same time, Biskit offers a software platform for the rapid integration of external programs and new algorithms into complex structural bioinformatics workflows. Calculations are thus often delegated to established programs like Xplor, Amber, Hex, Prosa, Hmmer and Modeller; interfaces to further software can be easily added. Moreover, Biskit simplifies the parallelization of time consuming calculations via PVM (Parallel Virtual Machine).


AVAILABILITY
The latest snapshot of Biskit, documentation and examples are freely available under the GNU General Public License at http://biskit.sf.net (alternate url http://biskit.pasteur.fr).",poster,cp24
p1475,001713ab9aa0943ff2b1795f4000bb571be83c45,j274,Nature Genetics,"Multiancestry genome-wide association study of 520,000 subjects identifies 32 loci associated with stroke and stroke subtypes",Abstract content,fullPaper,jv274
p1476,befc8b8f3865cfbb35634ea4db4aeca20a339d71,j187,Genome Research,Development of human protein reference database as an initial platform for approaching systems biology in humans.,"Human Protein Reference Database (HPRD) is an object database that integrates a wealth of information relevant to the function of human proteins in health and disease. Data pertaining to thousands of protein-protein interactions, posttranslational modifications, enzyme/substrate relationships, disease associations, tissue expression, and subcellular localization were extracted from the literature for a nonredundant set of 2750 human proteins. Almost all the information was obtained manually by biologists who read and interpreted >300,000 published articles during the annotation process. This database, which has an intuitive query interface allowing easy access to all the features of proteins, was built by using open source technologies and will be freely available at http://www.hprd.org to the academic community. This unified bioinformatics platform will be useful in cataloging and mining the large number of proteomic interactions and alterations that will be discovered in the postgenomic era.",fullPaper,jv187
p1477,921f69b23fe2dbca0c8d166b5af5d971267f22a8,c25,International Conference on Contemporary Computing,Bioinformatics and functional genomics,"The bestselling introduction to bioinformatics and functional genomicsnow in an updated editionWidely received in its previous edition, Bioinformatics and Functional Genomics offers the most broad-based introduction to this explosive new discipline. Now in a thoroughly updated and expanded Second Edition, it continues to be the go-to source for students and professionals involved in biomedical research.This edition provides up-to-the-minute coverage of the fields of bioinformatics and genomics. Features new to this edition include:Several fundamentally important proteins, such as globins, histones, insulin, and albumins, are included to better show how to apply bioinformatics tools to basic biological questions.A completely updated companion web site, which will be updated as new information becomes available - visit www.wiley.com/go/pevsnerbioinformaticsDescriptions of genome sequencing projects spanning the tree of life.A stronger focus on how bioinformatics tools are used to understand human disease.The book is complemented by lavish illustrations and more than 500 figures and tablesfifty of which are entirely new to this edition. Each chapter includes a Problem Set, Pitfalls, Boxes explaining key techniques and mathematics/statistics principles, Summary, Recommended Reading, and a list of freely available software. Readers may visit a related Web page for supplemental information at www.wiley.com/go/pevsnerbioinformatics.Bioinformatics and Functional Genomics, Second Edition serves as an excellent single-source textbook for advanced undergraduate and beginning graduate-level courses in the biological sciences and computer sciences. It is also an indispensable resource for biologists in a broad variety of disciplines who use the tools of bioinformatics and genomics to study particular research problems; bioinformaticists and computer scientists who develop computer algorithms and databases; and medical researchers and clinicians who want to understand the genomic basis of viral, bacterial, parasitic, or other diseases.Praise for the first edition:""...ideal both for biologists who want to master the application of bioinformatics to real-world problems and for computer scientists who need to understand the biological questions that motivate algorithms."" Quarterly Review of Biology"" an excellent textbook for graduate students and upper level undergraduate students."" Annals of Biomedical Engineering""highly recommended for academic and medical libraries, and for researchers as an introduction and reference"" E-Streams",poster,cp25
p1478,de3bc42c5c9fa021dc9b937131a780751f66cfd2,j102,Nucleic Acids Research,"Correction to ‘The STRING database in 2021: customizable protein–protein networks, and functional characterization of user-uploaded gene/measurement sets’","1Department of Molecular Life Sciences and Swiss Institute of Bioinformatics, University of Zurich, 8057 Zurich, Switzerland, 2Novo Nordisk Foundation Center for Protein Research, University of Copenhagen, 2200 Copenhagen N, Denmark, 3TurkuNLP Group, Department of Future Technologies, University of Turku, 20014 Turun Yliopisto, Finland, 4Structural and Computational Biology Unit, European Molecular Biology Laboratory, 69117 Heidelberg, Germany, 5Molecular Medicine Partnership Unit, University of Heidelberg and European Molecular Biology Laboratory, 69117 Heidelberg, Germany, 6Max Delbrück Centre for Molecular Medicine, 13125 Berlin, Germany and 7Department of Bioinformatics, Biozentrum, University of Würzburg, 97074 Würzburg, Germany",fullPaper,jv102
p1479,cc8d8bb7164ca2edb0f1ebe62186660635a148bb,j275,Methods in Enzymology,Bioconductor: an open source framework for bioinformatics and computational biology.,Abstract content,fullPaper,jv275
p1480,12bc9fff964d40d1548b91fab1921ab12b253390,c100,ACM SIGMOD Conference,"UniProtKB/Swiss-Prot, the Manually Annotated Section of the UniProt KnowledgeBase: How to Use the Entry View.",Abstract content,poster,cp100
p1481,feae36b19d8f8566aefbb8b30e9fb55c1592f0a7,j274,Nature Genetics,A gene expression database for the molecular pharmacology of cancer,Abstract content,fullPaper,jv274
p1482,89929fb78d7aa9ac5f80c5b97088dc6dc1b37306,j276,Drug Discovery Today,Harnessing bioinformatics to discover new vaccines.,Abstract content,fullPaper,jv276
p1483,591c5b48ed3879f9b2f53bbb0a9e7483bb3b6b6d,j277,Journal of Immunology,Identification through bioinformatics of two new macrophage proinflammatory human chemokines: MIP-3alpha and MIP-3beta.,"An increasing number of proinflammatory peptides, known as chemokines, are constantly being described and characterized. Because of their proven biologic functions in allergy, AIDS and, in general, inflammatory processes, these proteins have recently gained more attention. In this study we report the identification through bioinformatics of two new human chemokines: MIP-3alpha and MIP-3beta. Both of them belong to the beta- or CC chemokine family. Expression studies indicate that MIP-3alpha is predominantly expressed in lymph nodes, appendix, PBL, fetal liver, fetal lung and several cell lines. However, MIP-3beta expression is restricted to lymph nodes, thymus and appendix. Interestingly enough, both chemokines manifested a pattern of expression strongly regulated by IL-10. In contrast with other CC chemokines, MIP-3beta maps to chromosome 9. Here we show the importance of bioinformatics to discover new molecules with possible therapeutic effects and regulatory functions.",fullPaper,jv277
p1484,346b3012b3428ad5a9ee3571fa6f9262ceaa7c7d,j247,IEEE Transactions on Neural Networks,"Data Mining. Multimedia, Soft Computing, and Bioinformatics",Preface. 1. Introduction to Data Mining. 2. Soft Computing. 3. Multimedia Data Compression. 4. String Matching. 5. Classification in Data Mining. 6. Clustering in Data Mining. 7. Association Rules. 8. Rule Mining with Soft Computing. 9. Multimedia Data Mining. 10. Bioinformatics: An Application. Index. About the Authors.,fullPaper,jv247
p1485,bc20c82a2b41cc52efe4b380a5d10dd00cc351a1,j85,BMC Bioinformatics,Detailed estimation of bioinformatics prediction reliability through the Fragmented Prediction Performance Plots,Abstract content,fullPaper,jv85
p1486,3af045503abf81705fc1833aadf5db93a9be854c,j85,BMC Bioinformatics,GSVA: gene set variation analysis for microarray and RNA-Seq data,Abstract content,fullPaper,jv85
p1487,6ce65f8c4f779fd7ca3e1de0ddec2355f4177f07,j85,BMC Bioinformatics,Structural and evolutionary bioinformatics of the SPOUT superfamily of methyltransferases,Abstract content,fullPaper,jv85
p1488,bb0e064812fdad9229173a1f5d5ffb2a6a3e0cc2,j278,Nature reviews. Immunology,Viruses and interferon: a fight for supremacy,Abstract content,fullPaper,jv278
p1489,4614f9cfea0ad834b2228029e954644dd1a1bd89,j62,Nature,Creating a bioinformatics nation,Abstract content,fullPaper,jv62
p1490,b742e1b5f7b913512e8862cc79286203f6a9091f,j274,Nature Genetics,A genomic view of alternative splicing,Abstract content,fullPaper,jv274
p1491,7a27877026d41260203e80cb2ba9a2389cc98bb0,c92,Advances in Soft Computing,FatiGO: a web tool for finding significant associations of Gene Ontology terms with groups of genes,"We present a simple but powerful procedure to extract Gene Ontology (GO) terms that are significantly over- or under-represented in sets of genes within the context of a genome-scale experiment (DNA microarray, proteomics, etc.). Said procedure has been implemented as a web application, FatiGO, allowing for easy and interactive querying. FatiGO, which takes the multiple-testing nature of statistical contrast into account, currently includes GO associations for diverse organisms (human, mouse, fly, worm and yeast) and the TrEMBL/Swissprot GOAnnotations@EBI correspondences from the European Bioinformatics Institute.",poster,cp92
p1492,d07690ec08672ebcd4bf00cb044af3a0afa95e94,c25,International Conference on Contemporary Computing,Spectral clustering and its use in bioinformatics,Abstract content,poster,cp25
p1493,266959a748dbcde39b8595ee339c081370673017,c1,Technical Symposium on Computer Science Education,BioManager: the use of a bioinformatics web application as a teaching tool in undergraduate bioinformatics training,"The completion of the human genome project, and other genome sequencing projects, has spearheaded the emergence of the field of bioinformatics. Using computer programs to analyse DNA and protein information has become an important area of life science research and development. While it is not necessary for most life science researchers to develop specialist bioinformatic skills (including software development), basic skills in the application of common bioinformatics software and the effective interpretation of results are increasingly required by all life science researchers. Training in bioinformatics is increasingly occurring within the university system as part of existing undergraduate science and specialist degrees. One difficulty in bioinformatics education is the sheer number of software programs required in order to provide a thorough grounding in the subject to the student. Teaching requires either a well-maintained internal server with all the required software, properly interfacing with student terminals, and with sufficient capacity to handle multiple simultaneous requests, or it requires the individual installation and maintenance of every piece of software on each computer. In both cases, there are difficult issues regarding site maintenance and accessibility. In this article, we discuss the use of BioManager, a web-based bioinformatics application integrating a variety of common bioinformatics tools, for teaching, including its role as the main bioinformatics training tool in some Australian and international universities. We discuss some of the issues with using a bioinformatics resource primarily created for research in an undergraduate teaching environment.",poster,cp1
p1494,c64e010dfbe3bbca174ea8ad6b4902f5f33cc82d,c8,The Compass,Bioinformatics for geneticists : a bioinformatics primer for the analysis of genetic data,"Foreword. Preface. Contributors. Glossary. SECTION I AN INTRODUCTION TO BIOINFORMATICS FOR THE GENETICIST. 1 Bioinformatics challenges for the geneticist (Michael R. Barnes). 1.1 Introduction. 1.2 The role of bioinformatics in genetics research. 1.3 Genetics in the post-genome era. 1.4 Conclusions. References. 2 Managing and manipulating genetic data (Karl W. Broman and Simon C. Heath). 2.1 Introduction. 2.2 Basic principles. 2.3 Data entry and storage. 2.4 Data manipulation. 2.5 Examples of code. 2.6 Resources. 2.7 Summary. References. SECTION II MASTERING GENES, GENOMES AND GENETIC VARIATION DATA. 3 The HapMap - A haplotype map of the human genome (Ellen M. Brown and Bryan J. Barratt). 3.1 Introduction. 3.2 Accessing the data. 3.3 Application of HapMap data in association studies. 3.4 Future Perspectives. References. 4 Assembling a view of the human genome (Colin A. M. Semple). 4.1 Introduction. 4.2 Genomic sequence assembly. 4.3 Annotation from a distance: the generalities. 4.4 Annotation up close and personal: the specifics. 4.5 Annotation: the next generation. References. 5 Finding, delineating and analysing genes (Christopher Southan and Michael R. Barnes). 5.1 Introduction. 5.2 Why learn to predict and analyse genes in the complete genome era? 5.3 The evidence cascade for gene products. 5.4 Dealing with the complexities of gene models. 5.5 Locating known genes in the human genome. 5.6 Genome portal inspection. 5.7 Analysing novel genes. 5.8 Conclusions and prospects. References. 6 Comparative genomics (Martin S. Taylor and Richard R. Copley). 6.1 Introduction. 6.2 The Genomic landscape. 6.3 Concepts. 6.4 Practicalities. 6.5 Technology. 6.6 Applications. 6.7 Challenges and future directions. 6.8 Conclusion. References. SECTION III BIOINFORMATICS FOR GENETIC STUDY DESIGN AND ANALYSIS. 7 Identifying mutations in single gene disorders (David P. Kelsell, Diana Blaydon and Charles A. Mein). 7.1 Introduction. 7.2 Clinical Ascertainment. 7.3 Genome-wide mapping of monogenic diseases. 7.4 The nature of mutation in monogenic diseases. 7.5 Considering epigenetic effects in mendelian traits. 7.6 Summary. References. 8 From Genome Scan Culprit Gene (Ian C. Gray). 8.1 Introduction. 8.2 Theoretical and practical considerations. 8.3 A stepwise approach to locus refinement and candidate gene identification. 8.4 Conclusion. 8.5 A list of the software tools and Web links mentioned in this chapter. References. 9 Integrating Genetics, Genomics and Epigenomics to Identify. Disease Genes (Michael R. Barnes). 9.1 Introduction. 9.2 Dealing with the (draft) human genome sequence. 9.3 Progressing loci of interest with genomic information. 9.4 In silico characterization of the IBD5 locus - a case study. 9.5 Drawing together biological rationale - hypothesis building. 9.6 Identification of potentially functional polymorphisms. 9.7 Conclusions. References. 10 Tools for statistical genetics (Aruna Bansal, Charlotte Vignal and Ralph McGinnis). 10.1 Introduction. 10.2 Linkage analysis. 10.3 Association analysis. 10.4 Linkage disequilibrium. 10.5 Quantitative trait locus (QTL) mapping in experimental crosses. 10.6 Closing remarks. References. SECTION IV MOVING FROM ASSOCIATED GENES TO DISEASE ALLELES. 11 Predictive functional analysis of polymorphisms: An overview (Mary Plumpton and Michael R. Barnes). 11.1 Introduction. 11.2 Principles of predictive functional analysis of polymorphisms. 11.3 The anatomy of promoter regions and regulatory elements. 11.4 The anatomy of genes. 11.5 Pseudogenes and regulatory mRNA. 11.6 Analysis of novel regulatory elements and motifs in. nucleotide sequences. 11.7 Functional analysis of non-synonymous coding polymorphisms. 11.8 Integrated tools for functional analysis of genetic variation. 11.9 A note of caution on the prioritization of in silico predictions for. further laboratory investigation. 11.10 Conclusions. References. 12 Functional in silico analysis of gene regulatory polymorphism (Chaolin Zhang, Xiaoyue Zhao, Michael Q. Zhang). 12.1 Introduction. 12.2 Predicting regulatory regions. 12.3 Modelling and predicting transcription factor-binding sites. 12.4 Predicting regulatory elements for splicing regulation. 12.5 Evaluating the functional importance of. regulatory polymorphisms. References. 13 Amino-acid properties and consequences of substitutions (Matthew J. Betts and Robert B. Russell). 13.1 Introduction. 13.2 Protein features relevant to amino-acid behaviour. 13.3 Amino-acid classifications. 13.4 Properties of the amino acids. 13.5 Amino-acid quick reference. 13.6 Studies of how mutations affect function. 13.7 A summary of the thought process. References. 14 Non-coding RNA bioinformatics (James Brown, Steve Deharo, Barry Dancis, Michael R. Barnes and Philippe Sanseau). 14.1 Introduction. 14.2 The non-coding (nc) RNA universe. 14.3 Computational analysis of ncRNA. 14.4 ncRNA variation in disease. 14.5 Assessing the impact of variation in ncRNA. 14.6 Data resources to support small ncRNA analysis. 14.7 Conclusions. References. SECTION V ANALYSIS AT THE GENETIC AND GENOMIC DATA INTERFACE. 15 What are microarrays? (Catherine A. Ball and Gavin Sherlock). 15.1 Introduction. 15.2 Principles of the application of microarray technology. 15.3 Complementary approaches to microarray analysis. 15.4 Differences between data repository and research database. 15.5 Descriptions of freely available research database packages. References. 16 Combining quantitative trait and gene-expression data (Elissa J. Chesler). 16.1 Introduction: the genetic regulation of endophenotypes. 16.2 Transcript abundance as a complex phenotype. 16.3 Scaling up genetic analysis and mapping models for microarrays. 16.4 Genetic correlation analysis. 16.5 Systems genetic analysis. 16.6 Using expression QTLs to identify candidate genes for the regulation of complex phenotypes. 16.7 Conclusions. References. 17 Bioinformatics and cancer genetics (Joel Greshock). 17.1 Introduction. 17.2 Cancer genomes. 17.3 Approaches to studying cancer genetics. 17.4 General resources for cancer genetics. 17.5 Cancer genes and mutations. 17.6 Copy number alterations in cancer. 17.7 Loss of heterozygosity in cancer. 17.8 Gene-expression data in cancer. 17.9 Multiplatform gene target identification. 17.10 The epigenetics of cancer. 17.11 Tumour modelling. 17.12 Conclusions. References. 18 Needle in a haystack? dealing with 500 SNP genome scans (Michael R. Barnes and Paul S. Derwent). 18.1 Introduction. 18.2 Genome scan analysis issues. 18.3 Ultra-high-density genome-scanning technologies. 18.4 Bioinformatics for genome scan analysis. 18.5 Conclusions. References. 19 A bioinformatics perspective on genetics in drug discovery and development (Christopher D. Southan, Magnus Ulvsb ack and Michael R. Barnes). 19.1 Introduction. 19.2 Target genetics. 19.3 Pharmacogenetics (PGx). 19.4 Conclusions: toward 'personalized medicine'. References. Appendix I. Appendix II. Index.",poster,cp8
p1495,44b711a04ee5d899a3ceef15ad33544c0c22fdaf,j279,IBM Systems Journal,Transparent access to multiple bioinformatics information sources,"This paper describes the Transparent Access to Multiple Bioinformatics Information Sources project, known as TAMBIS, in which a domain ontology for molecular biology and bioinformatics is used in a retrieval-based information integration system for biologists. The ontology, represented using a description logic and managed by a terminology server, is used both to drive a visual query interface and as a global schema against which complex intersource queries are expressed. These source-independent declarative queries are then rewritten into collections of ordered source-dependent queries for execution by a middleware layer. In bioinformatics, the majority of data sources are not databases but tools with limited accessible interfaces. The ontology helps manage the interoperation between these resources. The paper emphasizes the central role that is played by the ontology in the system. The project distinguishes itself from others in the following ways: the ontology, developed by a biologist, is substantial; the retrieval interface is sophisticated; the description logic is managed by a sophisticated terminology server. A full pilot application is available as a JavaTM applet integrating five sources concerned with proteins. This pilot is currently undergoing field trials with working biologists and is being used to answer real questions in biology, one of which is used as a case study throughout the paper.",fullPaper,jv279
p1496,e995078660f734127ba0e310c3e290ce29c554cb,c23,International Conference on Open and Big Data,Bioinformatics—an introduction for computer scientists,"The article aims to introduce computer scientists to the new field of bioinformatics. This area has arisen from the needs of biologists to utilize and help interpret the vast amounts of data that are constantly being gathered in genomic research---and its more recent counterparts, proteomics and functional genomics. The ultimate goal of bioinformatics is to develop in silico models that will complement in vitro and in vivo biological experiments. The article provides a bird's eye view of the basic concepts in molecular cell biology, outlines the nature of the existing data, and describes the kind of computer algorithms and techniques that are necessary to understand cell behavior. The underlying motivation for many of the bioinformatics approaches is the evolution of organisms and the complexity of working with incomplete and noisy data. The topics covered include: descriptions of the current software especially developed for biologists, computer and mathematical cell models, and areas of computer science that play an important role in bioinformatics.",poster,cp23
p1497,c3793f86f9d037c3681341fb9d48a34e49561d18,j280,Journal of Biological Chemistry,"Classical Nuclear Localization Signals: Definition, Function, and Interaction with Importin α*","The best understood system for the transport of macromolecules between the cytoplasm and the nucleus is the classical nuclear import pathway. In this pathway, a protein containing a classical basic nuclear localization signal (NLS) is imported by a heterodimeric import receptor consisting of the β-karyopherin importin β, which mediates interactions with the nuclear pore complex, and the adaptor protein importin α, which directly binds the classical NLS. Here we review recent studies that have advanced our understanding of this pathway and also take a bioinformatics approach to analyze the likely prevalence of this system in vivo. Finally, we describe how a predicted NLS within a protein of interest can be confirmed experimentally to be functionally important.",fullPaper,jv280
p1498,1020aeca3b6b9ba1367283c825d64c82e28a5bfc,j6,Nature Methods,Bioconda: sustainable and comprehensive software distribution for the life sciences,Abstract content,fullPaper,jv6
p1499,ceb13d0eaf68f7276ba726be0039df5c797c4c24,c62,International Conference on Software Reuse,Grid Computing for Bioinformatics and Computational Biology,"Preface. Chapter 1: Open computing Grid for molecular sciences (M. Romberg, E. Benfenati, and W. Dubitzky). Chapter 2: Designing high-performance concurrent strategies for biological sequence alignment problems on networked computing platforms (B. Veeravalli). Chapter 3: Optimized cluster-enabled HMMER searches (J. P. Walters, J. Landman, and V. Chaudhary). Chapter 4: Expanding the rich of Grid computing: combining Globus and BOINC based systems (D. S. Myers, A. L. Bazinet, and M. P. Cummings). Chapter 5: Hierarchical Grid computing for high performance bioinformatics (B. Schmidt, C.X. Chen and W. Liu). Chapter 6:Multiple sequence alignment and phylogenetic inference (D. Trystram, and J. Zola). Chapter 7: Data syndication techniques for bioinformatics applications (C. Wang, A. Y. Zomaya, and B. B. Zhou). Chapter 8: Conformational sampling and docking on Grids (A. Tantar, N. Melab, and E-G. Talbi). Chapter 9: Deployment of Grid life sciences applications (V. Breton, N. Jacq , V. Kasam, and J. Salzemann). Chapter 10: Grid-based interactive decision support in biomedicine (A. Tirado-Ramos, P. M. A. Sloot, and M. Bubak). Chapter 11: Database-driven grid computing and distributed web applications: a comparison (H. De Sterck, A.Papo, C. Zhang, M. Hamady, and R. Knight). Chapter 12: A semantic mediation architecture for a clinical Data Grid (K. Kumpf, A. Wohrer, S. Benkner, G. Engelbrecht, and Jochen Fingberg). Chapter 13: Bioinformatics applications in Grid computing environments (A. Boukerche, A. C. Magalhaes and Alves De Melo). Chapter 14: Recent advances in solving the protein threading problem (R. Andonov, G. Collet, J-F. Gibrat, A. Marin, V. Poirriez, and N. Yanev). Chapter 15: DNA fragment assembly using Grid systems (A. J. Nebro, G. Luque, and E. Alba). Chapter 16: Seeing is knowing: Visualization of parameter-parameter dependencies in biomedical network models (A. Konagaya, R. Azuma, R. Umetsu, S. Ohki, F. Konishi, K. Matsumura, and S. Yoshikawa).",poster,cp62
p1500,9de3f1ed6352262b24c63c3dad62e15a3ef5a653,c110,IEEE International Conference on Automatic Face & Gesture Recognition,Spectral graph theory,"With every graph (or digraph) one can associate several different matrices. We have already seen the vertex-edge incidence matrix, the Laplacian and the adjacency matrix of a graph. Here we shall concentrate mainly on the adjacency matrix of (undirected) graphs, and also discuss briefly the Laplacian. We shall show that spectral properies (the eigenvalues and eigenvectors) of these matrices provide useful information about the structure of the graph. It turns out that for regular graphs, the information one can deduce from one matrix representation (e.g., the adjacency matrix) is similar to the information one can deduce from other representations (such as the Laplacian). We remark that for nonregular graphs, this is not the case, and the choice of matrix representation may make a significant difference. We shall not elaborate on this issue further, as our main concern here will be either with regular or nearly regular graph. The adjacency matrix of a connected undirected graph is nonnegative, symmetric and irreducible (namely, it cannot be decomposed into two diagonal blocks and two off-diagonal blocks, one of which is all-0). As such, standard results n linear algebra, including the Perron-Frobenius theorem, imply that:",poster,cp110
p1501,1ee0abcb8f0afd74d602255d529d7c2a036a8f02,c64,Experimental Software Engineering Network,Graph theory,Abstract content,poster,cp64
p1502,b07c157e7d40e06a4f2d486b16d5180d8b24acb9,c94,Vision,Algebraic Graph Theory,Abstract content,poster,cp94
p1503,6bc77c4dc6075ee81c05f0f5f43e44b2a34a5876,c69,International Conference on Parallel Processing,Graph Theory with Applications,"When I first entered the world of Mathematics, I became aware of a strange and little-regarded sect of ""Graph Theorists"", inhabiting a shadowy borderland known to the rest of the community as the ""slums of Topology"". What changes there have been in a few short years! That shadowy borderland has become a thriving metropolis. International conferences on Graph Theory occur with almost embarrassing frequency. Journals on Graph Theory abound: I once counted the Editorial Offices of three of them in one of the mathematical departments of one of the Universities of one of the smaller cities of Canada. Any connection with Topology is likely to be firmly repudiated as soon as noted. I became aware of the burgeoning of Graph Theory when I studied the 1940 paper of Brooks, Smith, Stone and Tutte in the Duke Mathematical Journal, ostensibly on squared rectangles. They wrote of trees and Kirchhoffs Laws, of 3-connection and planarity, of duality and symmetry, of determinantal identities and coprime integers, ~ all in the Quest of the Perfect Square. I invariably recommend that paper to my students. ""Go to it"", I say, ""you will",poster,cp69
p1504,54006d6fb211a090c98fee9d8103479b022c0db2,c77,Networks,Introduction to graph theory,Abstract content,fullPaper,cp77
p1505,c2f21a6b917286c7e904e0f168b53bbaa2bda4ba,j281,Frontiers in Neuroscience,Application of Graph Theory for Identifying Connectivity Patterns in Human Brain Networks: A Systematic Review,"Background: Analysis of the human connectome using functional magnetic resonance imaging (fMRI) started in the mid-1990s and attracted increasing attention in attempts to discover the neural underpinnings of human cognition and neurological disorders. In general, brain connectivity patterns from fMRI data are classified as statistical dependencies (functional connectivity) or causal interactions (effective connectivity) among various neural units. Computational methods, especially graph theory-based methods, have recently played a significant role in understanding brain connectivity architecture. Objectives: Thanks to the emergence of graph theoretical analysis, the main purpose of the current paper is to systematically review how brain properties can emerge through the interactions of distinct neuronal units in various cognitive and neurological applications using fMRI. Moreover, this article provides an overview of the existing functional and effective connectivity methods used to construct the brain network, along with their advantages and pitfalls. Methods: In this systematic review, the databases Science Direct, Scopus, arXiv, Google Scholar, IEEE Xplore, PsycINFO, PubMed, and SpringerLink are employed for exploring the evolution of computational methods in human brain connectivity from 1990 to the present, focusing on graph theory. The Cochrane Collaboration's tool was used to assess the risk of bias in individual studies. Results: Our results show that graph theory and its implications in cognitive neuroscience have attracted the attention of researchers since 2009 (as the Human Connectome Project launched), because of their prominent capability in characterizing the behavior of complex brain systems. Although graph theoretical approach can be generally applied to either functional or effective connectivity patterns during rest or task performance, to date, most articles have focused on the resting-state functional connectivity. Conclusions: This review provides an insight into how to utilize graph theoretical measures to make neurobiological inferences regarding the mechanisms underlying human cognition and behavior as well as different brain disorders.",fullPaper,jv281
p1506,95d6ff6279fa0f92df6fae0e6bd4c259acfc8f09,c38,"IEEE International Conference on Software Analysis, Evolution, and Reengineering",Spectral Graph Theory,"Eigenvalues and the Laplacian of a graph Isoperimetric problems Diameters and eigenvalues Paths, flows, and routing Eigenvalues and quasi-randomness Expanders and explicit constructions Eigenvalues of symmetrical graphs Eigenvalues of subgraphs with boundary conditions Harnack inequalities Heat kernels Sobolev inequalities Advanced techniques for random walks on graphs Bibliography Index.",poster,cp38
p1507,1034712495a5d8f54d489674c3eac69250bf107e,c35,EUROMICRO Conference on Software Engineering and Advanced Applications,Graph Theory,"Gaph Teory Fourth Edition Th is standard textbook of modern graph theory, now in its fourth edition, combines the authority of a classic with the engaging freshness of style that is the hallmark of active mathematics. It covers the core material of the subject with concise yet reliably complete proofs, while offering glimpses of more advanced methods in each fi eld by one or two deeper results, again with proofs given in full detail.",poster,cp35
p1508,7624a21a8d962b4e0aaa90d2c9b7b70f98dd008f,c101,International Conference on Automatic Face and Gesture Recognition,Introduction to Graph Theory,"1. Fundamental Concepts. What Is a Graph? Paths, Cycles, and Trails. Vertex Degrees and Counting. Directed Graphs. 2. Trees and Distance. Basic Properties. Spanning Trees and Enumeration. Optimization and Trees. 3. Matchings and Factors. Matchings and Covers. Algorithms and Applications. Matchings in General Graphs. 4. Connectivity and Paths. Cuts and Connectivity. k-connected Graphs. Network Flow Problems. 5. Coloring of Graphs. Vertex Colorings and Upper Bounds. Structure of k-chromatic Graphs. Enumerative Aspects. 6. Planar Graphs. Embeddings and Euler's Formula. Characterization of Planar Graphs. Parameters of Planarity. 7. Edges and Cycles. Line Graphs and Edge-Coloring. Hamiltonian Cycles. Planarity, Coloring, and Cycles. 8. Additional Topics (Optional). Perfect Graphs. Matroids. Ramsey Theory. More Extremal Problems. Random Graphs. Eigenvalues of Graphs. Appendix A: Mathematical Background. Appendix B: Optimization and Complexity. Appendix C: Hints for Selected Exercises. Appendix D: Glossary of Terms. Appendix E: Supplemental Reading. Appendix F: References. Indices.",poster,cp101
p1509,2e75dd6fb569ee917b6571d89afadd980af499c0,j282,Frontiers in Bioengineering and Biotechnology,A Guide to Conquer the Biological Network Era Using Graph Theory,"Networks are one of the most common ways to represent biological systems as complex sets of binary interactions or relations between different bioentities. In this article, we discuss the basic graph theory concepts and the various graph types, as well as the available data structures for storing and reading graphs. In addition, we describe several network properties and we highlight some of the widely used network topological features. We briefly mention the network patterns, motifs and models, and we further comment on the types of biological and biomedical networks along with their corresponding computer- and human-readable file formats. Finally, we discuss a variety of algorithms and metrics for network analyses regarding graph drawing, clustering, visualization, link prediction, perturbation, and network alignment as well as the current state-of-the-art tools. We expect this review to reach a very broad spectrum of readers varying from experts to beginners while encouraging them to enhance the field further.",fullPaper,jv282
p1510,8851f2f404b3322170c3c660fb3cf30a2911d6ec,c8,The Compass,Graph Theory,Abstract content,poster,cp8
p1511,2e16a9743c14b8d1fc626c2c5d982c3b371db6c8,c30,IEEE Aerospace Conference,Graph Theory,Abstract content,poster,cp30
p1512,88648d2ae7c18ee6b79d1d2754cf92724787f8ce,j283,Dialogues in Clinical Neuroscience,Graph theory methods: applications in brain networks,"Network neuroscience is a thriving and rapidly expanding field. Empirical data on brain networks, from molecular to behavioral scales, are ever increasing in size and complexity. These developments lead to a strong demand for appropriate tools and methods that model and analyze brain network data, such as those provided by graph theory. This brief review surveys some of the most commonly used and neurobiologically insightful graph measures and techniques. Among these, the detection of network communities or modules, and the identification of central network elements that facilitate communication and signal transfer, are particularly salient. A number of emerging trends are the growing use of generative models, dynamic (time-varying) and multilayer networks, as well as the application of algebraic topology. Overall, graph theory methods are centrally important to understanding the architecture, development, and evolution of brain networks.",fullPaper,jv283
p1513,72744fb5963b168b4590e5eaa148c8d0e5eafe38,j168,Proceedings of the IEEE,"Electrical Networks and Algebraic Graph Theory: Models, Properties, and Applications","Algebraic graph theory is a cornerstone in the study of electrical networks ranging from miniature integrated circuits to continental-scale power systems. Conversely, many fundamental results of algebraic graph theory were laid out by early electrical circuit analysts. In this paper, we survey some fundamental and historic as well as recent results on how algebraic graph theory informs electrical network analysis, dynamics, and design. In particular, we review the algebraic and spectral properties of graph adjacency, Laplacian, incidence, and resistance matrices and how they relate to the analysis, network reduction, and dynamics of certain classes of electrical networks. We study these relations for models of increasing complexity ranging from static resistive direct current (dc) circuits, over dynamic resistor..inductor..capacitor (RLC) circuits, to nonlinear alternating current (ac) power flow. We conclude this paper by presenting a set of fundamental open questions at the intersection of algebraic graph theory and electrical networks.",fullPaper,jv168
p1514,81b8dd00d832cc46bc72f11298b0839483c48d52,c7,European Conference on Modelling and Simulation,Graph theory approaches to functional network organization in brain disorders: A critique for a brave new small-world,"Over the past two decades, resting-state functional connectivity (RSFC) methods have provided new insights into the network organization of the human brain. Studies of brain disorders such as Alzheimer’s disease or depression have adapted tools from graph theory to characterize differences between healthy and patient populations. Here, we conducted a review of clinical network neuroscience, summarizing methodological details from 106 RSFC studies. Although this approach is prevalent and promising, our review identified four challenges. First, the composition of networks varied remarkably in terms of region parcellation and edge definition, which are fundamental to graph analyses. Second, many studies equated the number of connections across graphs, but this is conceptually problematic in clinical populations and may induce spurious group differences. Third, few graph metrics were reported in common, precluding meta-analyses. Fourth, some studies tested hypotheses at one level of the graph without a clear neurobiological rationale or considering how findings at one level (e.g., global topology) are contextualized by another (e.g., modular structure). Based on these themes, we conducted network simulations to demonstrate the impact of specific methodological decisions on case-control comparisons. Finally, we offer suggestions for promoting convergence across clinical studies in order to facilitate progress in this important field.",poster,cp7
p1515,8b7a2b2f4665be8b177abcf9a4d3634ef20eaefc,c17,International Conference on Enterprise Information Systems,"Algebraic Graph Theory: Morphisms, Monoids and Matrices","This is a highly self-contained book about algebraic graph theory which iswritten with a view to keep the lively and unconventional atmosphere of a spoken text to communicate the enthusiasm the author feels about this subject. The focus is on homomorphisms and endomorphisms, matrices and eigenvalues. Graph models are extremely useful for almost all applications and applicators as they play an important role as structuring tools. They allow to model net structures -like roads, computers, telephones -instances of abstract data structures -likelists, stacks, trees -and functional or object oriented programming.",poster,cp17
p1516,ee4fd9cd27836870dd18eb2d81efac596a758fb1,j284,Journal of manufacturing science and engineering,Thermal Modeling in Metal Additive Manufacturing Using Graph Theory,"The goal of this work is to predict the effect of part geometry and process parameters on the instantaneous spatiotemporal distribution of temperature, also called the thermal field or temperature history, in metal parts as they are being built layer-by-layer using additive manufacturing (AM) processes. In pursuit of this goal, the objective of this work is to develop and verify a graph theory-based approach for predicting the temperature distribution in metal AM parts. This objective is consequential to overcome the current poor process consistency and part quality in AM. One of the main reasons for poor part quality in metal AM processes is ascribed to the nature of temperature distribution in the part. For instance, steep thermal gradients created in the part during printing leads to defects, such as warping and thermal stress-induced cracking. Existing nonproprietary approaches to predict the temperature distribution in AM parts predominantly use mesh-based finite element analyses that are computationally tortuous—the simulation of a few layers typically requires several hours, if not days. Hence, to alleviate these challenges in metal AM processes, there is a need for efficient computational models to predict the temperature distribution, and thereby guide part design and selection of process parameters instead of expensive empirical testing. Compared with finite element analyses techniques, the proposed mesh-free graph theory-based approach facilitates prediction of the temperature distribution within a few minutes on a desktop computer. To explore these assertions, we conducted the following two studies: (1) comparing the heat diffusion trends predicted using the graph theory approach with finite element analysis, and analytical heat transfer calculations based on Green’s functions for an elementary cuboid geometry which is subjected to an impulse heat input in a certain part of its volume and (2) simulating the laser powder bed fusion metal AM of three-part geometries with (a) Goldak’s moving heat source finite element method, (b) the proposed graph theory approach, and (c) further comparing the thermal trends predicted from the last two approaches with a commercial solution. From the first study, we report that the thermal trends approximated by the graph theory approach are found to be accurate within 5% of the Green’s functions-based analytical solution (in terms of the symmetric mean absolute percentage error). Results from the second study show that the thermal trends predicted for the AM parts using graph theory approach agree with finite element analyses, and the computational time for predicting the temperature distribution was significantly reduced with graph theory. For instance, for one of the AM part geometries studied, the temperature trends were predicted in less than 18 min within 10% error using the graph theory approach compared with over 180 min with finite element analyses. Although this paper is restricted to theoretical development and verification of the graph theory approach, our forthcoming research will focus on experimental validation through in-process thermal measurements.",fullPaper,jv284
p1517,16b8b3a7840e9b61756db16097e2a8b1d18b8bf6,c1,Technical Symposium on Computer Science Education,Graph Theory,"Mathematics acts an important and essential need in different fields. One of the significant roles in mathematics is played by graph theory that is used in structural models and innovative methods, models in various disciplines for better strategic decisions. In mathematics, graph theory is the study through graphs by which the structural relationship studied with a pair wise relationship between different objects. The different types of network theory or models or model of the network are called graphs. These graphs do not form a part of analytical geometry, but they are called graph theory, which is points connected by lines. The various concepts of graph theory have varied applications in diverse fields. The chapter will deal with graph theory and its application in various financial market decisions. The topological properties of the network of stocks will provide a deeper understanding and a good conclusion to the market structure and connectivity. The chapter is very useful for academicians, market researchers, financial analysts, and economists.",poster,cp1
p1518,968ea337e15004a2ed3e1442d7f632a1214e2268,j285,Engineering computations,Novel reliable routing method for engineering of internet of vehicles based on graph theory,"
Purpose
The communication link in the engineering of Internet of Vehicle (IOV) is more frequent than the communication link in the Mobile ad hoc Network (MANET). Therefore, the highly dynamic network routing reliability problem is a research hotspot to be solved.


Design/methodology/approach
The graph theory is used to model the MANET communication diagram on the highway and propose a new reliable routing method for internet of vehicles based on graph theory.


Findings
The expanded graph theory can help capture the evolution characteristics of the network topology and predetermine the reliable route to promote quality of service (QoS) in the routing process. The program can find the most reliable route from source to the destination from the MANET graph theory.


Originality/value
The good performance of the proposed method is verified and compared with the related algorithms of the literature.
",fullPaper,jv285
p1519,56a13467a3cfb7a9ec00b7f3ed5e953324225233,c13,International Conference on Data Science and Advanced Analytics,Graph Theory with Applications,Abstract content,poster,cp13
p1520,aec03b62900277709b10793a168058c5d05fd34f,j286,Bulletin of the Australian Mathematical Society,A GENERAL POSITION PROBLEM IN GRAPH THEORY,"The paper introduces a graph theory variation of the general position problem: given a graph $G$ , determine a largest set $S$ of vertices of $G$ such that no three vertices of $S$ lie on a common geodesic. Such a set is a max-gp-set of $G$ and its size is the gp-number $\text{gp}(G)$ of $G$ . Upper bounds on $\text{gp}(G)$ in terms of different isometric covers are given and used to determine the gp-number of several classes of graphs. Connections between general position sets and packings are investigated and used to give lower bounds on the gp-number. It is also proved that the general position problem is NP-complete.",fullPaper,jv286
p1521,ecaa9d581d26cb23a759ca1310b0a5d8ce27b7b2,c93,Human Language Technology - The Baltic Perspectiv,Topics in graph theory,"A graph is a system G = (V, E) consisting of a set V of vertices and a set E (disjoint from V ) of edges, together with an incidence function End : E → M2(V ), where M2(V ) is set of all 2-element sub-multisets of V . We usually write V = V (G), E = E(G), and End = EndG. For each edge e ∈ E with End(e) = {u, v}, we called u, v the end-vertices of e, and say that the edge e is incident with the vertices u, v, or the vertices u, v are incident with the edge e, or the vertices u, v are adjacent by the edge e. Sometimes it is more convenient to just write the incidence relation as e = uv. If u = v, the edge e is called a loop; if u 6= v, the edge is called a link. Two edges are said to be parallel if their end vertices are the same. Parallel edges are also referred to multiple edges. A simple graph is a graph without loops and multiple edges. When we emphasize that a graph may have loops and multiple edges, we refer the graph as a multigraph. A graph is said to be (i) finite if it has finite number of vertices and edges; (ii) null if it has no vertices, and consequently has no edges; (iii) trivial if it has only one vertex with possible loops; (iv) empty if its has no edges; and (v) nontrivial if it is not trivial. A complete graph is a simple graph that every pair of vertices are adjacent. A complete graph with n vertices is denoted by Kn. A graph G is said to be bipartite if its vertex set V (G) can be partitioned into two disjoint nonempty parts X,Y such that every edge has one end-vertex in X and the other in Y ; such a partition {X,Y } is called a bipartition of G, and such a bipartite graph is denoted by G[X,Y ]. A bipartite graph G[X,Y ] is called a complete bipartite graph if each vertex in X is joined to every vertex in Y ; we abbreviate G[X,Y ] to Km,n if |X| = m and |Y | = n. Let G be a graph. Two vertices of G are called neighbors each other if they are adjacent. For each vertex v ∈ V (G), the set of neighbors of v in G is denoted by Nv(G), the number of edges incident with v (loops counted twice) is called the degree of v in G, denoted deg (v) or deg G(v). A vertex of degree 0 is called an isolated vertex; a vertex of degree 1 is called a leaf. A graph is said to be regular if its every vertex has the same degree. A graph is said to be k-regular if its every vertex has degree k. We always have",poster,cp93
p1522,bdba4cf0aa5c831301fd75e0d100f1fb14158bc6,j287,Oberwolfach Reports,Graph Theory,Abstract content,fullPaper,jv287
p1523,756bd8b609f9754fee6ae9e9056f5face4386726,j153,bioRxiv,BRAPH: A graph theory software for the analysis of brain connectivity,"The brain is a large-scale complex network whose workings rely on the interaction between its various regions. In the past few years, the organization of the human brain network has been studied extensively using concepts from graph theory, where the brain is represented as a set of nodes connected by edges. This representation of the brain as a connectome can be used to assess important measures that reflect its topological architecture. We have developed a freeware MatLab-based software (BRAPH – BRain Analysis using graPH theory) for connectivity analysis of brain networks derived from structural magnetic resonance imaging (MRI), functional MRI (fMRI), positron emission tomography (PET) and electroencephalogram (EEG) data. BRAPH allows building connectivity matrices, calculating global and local network measures, performing non-parametric permutations for group comparisons, assessing the modules in the network, and comparing the results to random networks. By contrast to other toolboxes, it allows performing longitudinal comparisons of the same patients across different points in time. Furthermore, even though a user-friendly interface is provided, the architecture of the program is modular (object-oriented) so that it can be easily expanded and customized. To demonstrate the abilities of BRAPH, we performed structural and functional graph theory analyses in two separate studies. In the first study, using MRI data, we assessed the differences in global and nodal network topology in healthy controls, patients with amnestic mild cognitive impairment, and patients with Alzheimer’s disease. In the second study, using resting-state fMRI data, we compared healthy controls and Parkinson’s patients with mild cognitive impairment.",fullPaper,jv153
p1524,fc9051185b4879606ed006d00055940aee4da5e1,c48,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",Modern Graph Theory,Abstract content,poster,cp48
p1525,e1274867d404fd1dbf73a654c217ea5c0c32852c,j288,The Neuroscientist,Graph Theory and Brain Connectivity in Alzheimer’s Disease,This article presents a review of recent advances in neuroscience research in the specific area of brain connectivity as a potential biomarker of Alzheimer’s disease with a focus on the application of graph theory. The review will begin with a brief overview of connectivity and graph theory. Then resent advances in connectivity as a biomarker for Alzheimer’s disease will be presented and analyzed.,fullPaper,jv288
p1526,0e63d3bea6a258017c43bb6c91f923839d260c8f,c63,IEEE International Software Metrics Symposium,Graph Theory,Abstract content,poster,cp63
p1527,7d08932d11d5cf03ac2725250e83ec68f5a0f878,c97,Interspeech,Graph Theory 1736 1936,"Thank you very much for downloading graph theory 1736 1936. Maybe you have knowledge that, people have search hundreds times for their favorite readings like this graph theory 1736 1936, but end up in malicious downloads. Rather than enjoying a good book with a cup of coffee in the afternoon, instead they cope with some malicious bugs inside their laptop. graph theory 1736 1936 is available in our digital library an online access to it is set as public so you can get it instantly. Our books collection spans in multiple countries, allowing you to get the most less latency time to download any of our books like this one. Kindly say, the graph theory 1736 1936 is universally compatible with any devices to read.",poster,cp97
p1528,b9d26450520aa98e6ccb1b2e11a48fec29f273b9,c48,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",Cytoscape.js: a graph theory library for visualisation and analysis,"Summary: Cytoscape.js is an open-source JavaScript-based graph library. Its most common use case is as a visualization software component, so it can be used to render interactive graphs in a web browser. It also can be used in a headless manner, useful for graph operations on a server, such as Node.js. Availability and implementation: Cytoscape.js is implemented in JavaScript. Documentation, downloads and source code are available at http://js.cytoscape.org. Contact: gary.bader@utoronto.ca",poster,cp48
p1529,f8dab78b4ddd64a83bbc62cb153161a033dc06d1,j289,Saudi Journal of Biological Sciences,Study of biological networks using graph theory,Abstract content,fullPaper,jv289
p1530,12748e904f5025ca1757ce49d72cad3878e1be8f,c41,Software Product Lines Conference,Graph Theory-Based Pinning Synchronization of Stochastic Complex Dynamical Networks,"This paper is concerned with the adaptive pinning synchronization problem of stochastic complex dynamical networks (CDNs). Based on algebraic graph theory and Lyapunov theory, pinning controller design conditions are derived, and the rigorous convergence analysis of synchronization errors in the probability sense is also conducted. Compared with the existing results, the topology structures of stochastic CDN are allowed to be unknown due to the use of graph theory. In particular, it is shown that the selection of nodes for pinning depends on the unknown lower bounds of coupling strengths. Finally, an example on a Chua’s circuit network is given to validate the effectiveness of the theoretical results.",poster,cp41
p1531,6192ad6db166477e001d853ff9ca217a5bf8f7a3,j290,Journal of Neuroscience Research,"Network science and the human brain: Using graph theory to understand the brain and one of its hubs, the amygdala, in health and disease","Over the past 15 years, the emerging field of network science has revealed the key features of brain networks, which include small‐world topology, the presence of highly connected hubs, and hierarchical modularity. The value of network studies of the brain is underscored by the range of network alterations that have been identified in neurological and psychiatric disorders, including epilepsy, depression, Alzheimer's disease, schizophrenia, and many others. Here we briefly summarize the concepts of graph theory that are used to quantify network properties and describe common experimental approaches for analysis of brain networks of structural and functional connectivity. These range from tract tracing to functional magnetic resonance imaging, diffusion tensor imaging, electroencephalography, and magnetoencephalography. We then summarize the major findings from the application of graph theory to nervous systems ranging from Caenorhabditis elegans to more complex primate brains, including man. Focusing, then, on studies involving the amygdala, a brain region that has attracted intense interest as a center for emotional processing, fear, and motivation, we discuss the features of the amygdala in brain networks for fear conditioning and emotional perception. Finally, to highlight the utility of graph theory for studying dysfunction of the amygdala in mental illness, we review data with regard to changes in the hub properties of the amygdala in brain networks of patients with depression. We suggest that network studies of the human brain may serve to focus attention on regions and connections that act as principal drivers and controllers of brain function in health and disease.†Published 2016",fullPaper,jv290
p1532,74eb4d6abf1d0236be338c1bd5ee59a498b961b1,c35,EUROMICRO Conference on Software Engineering and Advanced Applications,Band connectivity for topological quantum chemistry: Band structures as a graph theory problem,"The conventional theory of solids is well suited to describing band structures locally near isolated points in momentum space, but struggles to capture the full, global picture necessary for understanding topological phenomena. In part of a recent paper [B. Bradlyn et al., Nature 547, 298 (2017)], we have introduced the way to overcome this difficulty by formulating the problem of sewing together many disconnected local ""k-dot-p"" band structures across the Brillouin zone in terms of graph theory. In the current manuscript we give the details of our full theoretical construction. We show that crystal symmetries strongly constrain the allowed connectivities of energy bands, and we employ graph-theoretic techniques such as graph connectivity to enumerate all the solutions to these constraints. The tools of graph theory allow us to identify disconnected groups of bands in these solutions, and so identify topologically distinct insulating phases.",poster,cp35
p1533,abb780f85a1a27919e461efdcc2a77cf1ab46c8b,c73,Workshop on Algorithms in Bioinformatics,Applying Graph Theory in Ecological Research,"Graph theory can be applied to ecological questions in many ways, and more insights can be gained by expanding the range of graph theoretical concepts applied to a specific system. But how do you know which methods might be used? And what do you do with the graph once it has been obtained? This book provides a broad introduction to the application of graph theory in different ecological systems, providing practical guidance for researchers in ecology and related fields. Readers are guided through the creation of an appropriate graph for the system being studied, including the application of spatial, spatio-temporal, and more abstract structural process graphs. Simple figures accompany the explanations to add clarity, and a broad range of ecological phenomena from many ecological systems are covered. This is the ideal book for graduate students and researchers looking to apply graph theoretical methods in their work.",poster,cp73
p1534,f6c5a2a6cccae1db5f0a753946a2e89d7d22aa5d,c70,International Conference on Intelligent Robotics and Applications,"Handbook of graph theory, combinatorial optimization, and algorithms","Basic Concepts and Algorithms Basic Concepts in Graph Theory and Algorithms Subramanian Arumugam and Krishnaiyan ""KT"" Thulasiraman Basic Graph Algorithms Krishnaiyan ""KT"" Thulasiraman Depth-First Search and Applications Krishnaiyan ""KT"" Thulasiraman Flows in Networks Maximum Flow Problem F. Zeynep Sargut, Ravindra K. Ahuja, James B. Orlin, and Thomas L. Magnanti Minimum Cost Flow Problem Balachandran Vaidyanathan, Ravindra K. Ahuja, James B. Orlin, and Thomas L. Magnanti Multi-Commodity Flows Balachandran Vaidyanathan, Ravindra K. Ahuja, James B. Orlin, and Thomas L. Magnanti Algebraic Graph Theory Graphs and Vector Spaces Krishnaiyan ""KT"" Thulasiraman and M.N.S. Swamy Incidence, Cut, and Circuit Matrices of a Graph Krishnaiyan ""KT"" Thulasiraman and M.N.S. Swamy Adjacency Matrix and Signal Flow Graphs Krishnaiyan ""KT"" Thulasiraman and M.N.S. Swamy Adjacency Spectrum and the Laplacian Spectrum of a Graph R. Balakrishnan Resistance Networks, Random Walks, and Network Theorems Krishnaiyan ""KT"" Thulasiraman and Mamta Yadav Structural Graph Theory Connectivity Subramanian Arumugam and Karam Ebadi Connectivity Algorithms Krishnaiyan ""KT"" Thulasiraman Graph Connectivity Augmentation Andras Frank and Tibor Jordan Matchings Michael D. Plummer Matching Algorithms Krishnaiyan ""KT"" Thulasiraman Stable Marriage Problem Shuichi Miyazaki Domination in Graphs Subramanian Arumugam and M. Sundarakannan Graph Colorings Subramanian Arumugam and K. Raja Chandrasekar Planar Graphs Planarity and Duality Krishnaiyan ""KT"" Thulasiraman and M.N.S. Swamy Edge Addition Planarity Testing Algorithm John M. Boyer Planarity Testing Based on PC-Trees Wen-Lian Hsu Graph Drawing Md. Saidur Rahman and Takao Nishizeki Interconnection Networks Introduction to Interconnection Networks S.A. Choudum, Lavanya Sivakumar, and V. Sunitha Cayley Graphs S. Lakshmivarahan, Lavanya Sivakumar, and S.K. Dhall Graph Embedding and Interconnection Networks S.A. Choudum, Lavanya Sivakumar, and V. Sunitha Special Graphs Program Graphs Krishnaiyan ""KT"" Thulasiraman Perfect Graphs Chinh T. Hoang and R. Sritharan Tree-Structured Graphs Andreas Brandstadt and Feodor F. Dragan Partitioning Graph and Hypergraph Partitioning Sachin B. Patkar and H. Narayanan Matroids Matroids H. Narayanan and Sachin B. Patkar Hybrid Analysis and Combinatorial Optimization H. Narayanan Probabilistic Methods, Random Graph Models, and Randomized Algorithms Probabilistic Arguments in Combinatorics C.R. Subramanian Random Models and Analyses for Chemical Graphs Daniel Pascua, Tina M. Kouri, and Dinesh P. Mehta Randomized Graph Algorithms: Techniques and Analysis Surender Baswana and Sandeep Sen Coping with NP-Completeness General Techniques for Combinatorial Approximation Sartaj Sahni epsilon-Approximation Schemes for the Constrained Shortest Path Problem Krishnaiyan ""KT"" Thulasiraman Constrained Shortest Path Problem: Lagrangian Relaxation-Based Algorithmic Approaches Ying Xiao and Krishnaiyan ""KT"" Thulasiraman Algorithms for Finding Disjoint Paths with QoS Constraints Alex Sprintson and Ariel Orda Set-Cover Approximation Neal E. Young Approximation Schemes for Fractional Multicommodity Flow Problems George Karakostas Approximation Algorithms for Connectivity Problems Ramakrishna Thurimella Rectilinear Steiner Minimum Trees Tao Huang and Evangeline F.Y. Young Fixed-Parameter Algorithms and Complexity Venkatesh Raman and Saket Saurabh",poster,cp70
p1535,37c0809e246e593d524fa8a918eaee661124c21f,c8,The Compass,An Introduction to Bipolar Single Valued Neutrosophic Graph Theory,"In this paper, we first define the concept of bipolar single neutrosophic graphs as the generalization of bipolar fuzzy graphs, N-graphs, intuitionistic fuzzy graph, single valued neutrosophic graphs and bipolar intuitionistic fuzzy graphs.",poster,cp8
p1536,8e8152d46c8ff1070805096c214df7f389c57b80,c10,Big Data,Wavelets on Graphs via Spectral Graph Theory,Abstract content,poster,cp10
p1537,36e7356a12554d8af87094a9ca245b49b4f4a5c4,c26,PS,Basic Graph Theory,Abstract content,poster,cp26
p1538,cfc104f686b2190f64db03c8933d8f2a7fff5a5d,c14,International Conference on Exploring Services Science,Molecular Orbital Calculations Using Chemical Graph Theory,"molecular orbital calculations using chemical graph theory is available in our digital library an online access to it is set as public so you can get it instantly. Our book servers hosts in multiple countries, allowing you to get the most less latency time to download any of our books like this one. Kindly say, the molecular orbital calculations using chemical graph theory is universally compatible with any devices to read.",poster,cp14
p1539,d66c93134b8345d76c6d4d25cab52abb160fee67,c50,International Conference on Automated Software Engineering,A Brief Introduction to Spectral Graph Theory,"Spectral graph theory starts by associating matrices to graphs, notably, the adjacency matrix and the laplacian matrix. The general theme is then, firstly, to compute or estimate the eigenvalues of such matrices, and secondly, to relate the eigenvalues to structural properties of graphs. As it turns out, the spectral perspective is a powerful tool. Some of its loveliest applications concern facts that are, in principle, purely graph-theoretic or combinatorial. To give just one example, spectral ideas are a key ingredient in the proof of the so-called Friendship Theorem: if, in a group of people, any two persons have exactly one common friend, then there is a person who is everybody’s friend. This text is an introduction to spectral graph theory, but it could also be seen as an invitation to algebraic graph theory. On the one hand, there is, of course, the linear algebra that underlies the spectral ideas in graph theory. On the other hand, most of our examples are graphs of algebraic origin. The two recurring sources are Cayley graphs of groups, and graphs built out of finite fields. In the study of such graphs, some further algebraic ingredients (e.g., characters) naturally come up. The table of contents gives, as it should, a good glimpse of where is this text going. Very broadly, the first half is devoted to graphs, finite fields, and how they come together. This part is meant as an appealing and meaningful motivation. It provides a context that frames and fuels much of the second, spectral, half. Most sections have one or two exercises. Their position within the text is a hint. The exercises are optional, in the sense that virtually nothing in the main body depends on them. But the exercises are often of the non-trivial variety, and they should enhance the text in an interesting way. The hope is that the reader will enjoy them. We assume a basic familiarity with linear algebra, finite fields, and groups, but not necessarily with graph theory. This, again, betrays our algebraic perspective. This text is based on a course I taught in Göttingen, in the Fall of 2015. I would like to thank Jerome Baum for his help with some of the drawings. The present version is preliminary, and comments are welcome (email: bogdan.nica@gmail.com).",poster,cp50
p1540,a94b761fc2c9c299f5dc4c10d44e6d2bdd9a8267,j291,IEEE Transactions on Information Theory,Quantum Zero-Error Source-Channel Coding and Non-Commutative Graph Theory,"Alice and Bob receive a bipartite state (possibly entangled) from some finite collection or from some subspace. Alice sends a message to Bob through a noisy quantum channel such that Bob may determine the initial state, with zero chance of error. This framework encompasses, for example, teleportation, dense coding, entanglement assisted quantum channel capacity, and one-way communication complexity of function evaluation. With classical sources and channels, this problem can be analyzed using graph homomorphisms. We show this quantum version can be analyzed using homomorphisms on non-commutative graphs (an operator space generalization of graphs). Previously the Lovász ϑ number has been generalized to non-commutative graphs; we show this to be a homomorphism monotone, thus providing bounds on quantum source-channel coding. We generalize the Schrijver and Szegedy numbers, and show these to be monotones as well. As an application, we construct a quantum channel whose entanglement assisted zero-error one-shot capacity can only be unlocked using a non-maximally entangled state. These homomorphisms allow definition of a chromatic number for non-commutative graphs. Many open questions are presented regarding the possibility of a more fully developed theory.",fullPaper,jv291
p1541,788eab78c4c6e998fc6812b7d7af7ce6505af2e4,c86,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,Graph Theory And Sparse Matrix Computation,"Thank you for downloading graph theory and sparse matrix computation. As you may know, people have look numerous times for their chosen readings like this graph theory and sparse matrix computation, but end up in infectious downloads. Rather than reading a good book with a cup of coffee in the afternoon, instead they juggled with some infectious virus inside their laptop. graph theory and sparse matrix computation is available in our book collection an online access to it is set as public so you can get it instantly. Our books collection hosts in multiple countries, allowing you to get the most less latency time to download any of our books like this one. Merely said, the graph theory and sparse matrix computation is universally compatible with any devices to read.",poster,cp86
p1542,088e404fb84c0dc1a5c77724705e04b7872b1e01,c66,Annual Conference on Innovation and Technology in Computer Science Education,Graph theory,Abstract content,poster,cp66
p1543,d53aa6487575762c1a14addf273ea271fef24d29,c91,Workshop on Algorithms and Models for the Web-Graph,Algorithmic graph theory and perfect graphs,Abstract content,poster,cp91
p1544,9893595f6950ca1340055da05fd2fae2b7b4dfd3,c45,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",Three conjectures in extremal spectral graph theory,Abstract content,poster,cp45
p1545,e9b90c75c5c99353a43cbf5e083334a59e50cca8,c73,Workshop on Algorithms in Bioinformatics,Rational exponents in extremal graph theory,"Given a family of graphs $\mathcal{H}$, the extremal number $\textrm{ex}(n, \mathcal{H})$ is the largest $m$ for which there exists a graph with $n$ vertices and $m$ edges containing no graph from the family $\mathcal{H}$ as a subgraph. We show that for every rational number $r$ between $1$ and $2$, there is a family of graphs $\mathcal{H}_r$ such that $\textrm{ex}(n, \mathcal{H}_r) = \Theta(n^r)$. This solves a longstanding problem in the area of extremal graph theory.",poster,cp73
p1546,d2ae0082986d965469ece8c0ebcf4885c84b9ccc,j292,Epilepsia,Presurgery resting‐state local graph‐theory measures predict neurocognitive outcomes after brain surgery in temporal lobe epilepsy,This study determined the ability of resting‐state functional connectivity (rsFC) graph‐theory measures to predict neurocognitive status postsurgery in patients with temporal lobe epilepsy (TLE) who underwent anterior temporal lobectomy (ATL).,fullPaper,jv292
p1547,9699f9186a38107ff5dfaae9d44dd1796cbe7ac7,c7,European Conference on Modelling and Simulation,Water Network Sectorization Based on Graph Theory and Energy Performance Indices,"AbstractThis paper proposes a new methodology for the optimal design of water network sectorization, which is an essential technique for improving the management and security of multiple-source water supply systems. In particular, the network sectorization problem under consideration concerns the definition of isolated district meter areas, each of which is supplied by its own source (or sources) and is completely disconnected from the rest of the water system through boundary valves or permanent pipe sectioning. The proposed methodology uses graph theory principles and a heuristic procedure based on minimizing the amount of dissipated power in the water network. The procedure has been tested on two existing water distribution networks (WDNs) (in Parete, Italy and San Luis Rio Colorado, Mexico) using different performance indices. The simulation results, which confirmed the effectiveness of the proposed methodology, surpass empirical trial-and-error approaches and offer water utilities a tool for the desi...",poster,cp7
p1548,bba08b50d33448a3b65b4baf7c0df3c1eca8b10d,j293,Proceedings of the Royal Society A,Evolutionary graph theory revisited: when is an evolutionary process equivalent to the Moran process?,"Evolution in finite populations is often modelled using the classical Moran process. Over the last 10 years, this methodology has been extended to structured populations using evolutionary graph theory. An important question in any such population is whether a rare mutant has a higher or lower chance of fixating (the fixation probability) than the Moran probability, i.e. that from the original Moran model, which represents an unstructured population. As evolutionary graph theory has developed, different ways of considering the interactions between individuals through a graph and an associated matrix of weights have been considered, as have a number of important dynamics. In this paper, we revisit the original paper on evolutionary graph theory in light of these extensions to consider these developments in an integrated way. In particular, we find general criteria for when an evolutionary graph with general weights satisfies the Moran probability for the set of six common evolutionary dynamics.",fullPaper,jv293
p1549,e3acbca01b107b43b04f73499fbce2eeadc5970a,c45,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",Neutrosophic Graphs: A New Dimension to Graph Theory,"In this book authors for the first time have made a through study of neutrosophic graphs. This study reveals that these neutrosophic graphs give a new dimension to graph theory. The important feature of this book is it contains over 200 neutrosophic graphs to provide better understanding of this concepts. Further these graphs happen to behave in a unique way inmost cases, for even the edge colouring problem is different from the classical one. Several directions and dimensions in graph theory are obtained from this study.",poster,cp45
p1550,b255694c69768b03bf854a72f3b0520befb0c509,c67,Enterprise Application Integration,The Fascinating World of Graph Theory,"The fascinating world of graph theory goes back several centuries and revolves around the study of graphsmathematical structures showing relations between objects. With applications in biology, computer science, transportation science, and other areas, graph theory encompasses some of the most beautiful formulas in mathematicsand some of its most famous problems. For example, what is the shortest route for a traveling salesman seeking to visit a number of cities in one trip? What is the least number of colors needed to fill in any map so that neighboring regions are always colored differently? Requiring readers to have a math background only up to high school algebra, this book explores the questions and puzzles that have been studied, and often solved, through graph theory. In doing so, the book looks at graph theorys development and the vibrant individuals responsible for the fields growth. Introducing graph theorys fundamental concepts, the authors explore a diverse plethora of classic problems such as the Lights Out Puzzle, the Minimum Spanning Tree Problem, the Knigsberg Bridge Problem, the Chinese Postman Problem, a Knights Tour, and the Road Coloring Problem. They present every type of graph imaginable, such as bipartite graphs, Eulerian graphs, the Petersen graph, and trees. Each chapter contains math exercises and problems for readers to savor. An eye-opening journey into the world of graphs, this book offers exciting problem-solving possibilities for mathematics and beyond.",poster,cp67
p1551,5b2d23a567b117ffe32dad3907a473e328127ff2,c88,Symposium on the Theory of Computing,Modern Graph Theory,Abstract content,poster,cp88
p1552,89ce4cf020e64bb4c7820b550ed9895525d31872,j294,Neurobiology of Aging,Functional connectivity and graph theory in preclinical Alzheimer's disease,Abstract content,fullPaper,jv294
p1553,0c2cab97a9ea7cc7f31801d21a19aec82fc9a28c,c48,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",An Introduction To The Theory Of Graph Spectra,"an introduction to the theory of graph spectra is available in our book collection an online access to it is set as public so you can download it instantly. Our digital library hosts in multiple countries, allowing you to get the most less latency time to download any of our books like this one. Merely said, the an introduction to the theory of graph spectra is universally compatible with any devices to read.",poster,cp48
p1554,b19fd39320634d3421191e0614b32f809d8b0866,c55,Annual Workshop of the Psychology of Programming Interest Group,A Survey on some Applications of Graph Theory in Cryptography,"Abstract Graph theory is rapidly moving into the main stream of research because of its applications in diverse fields such as biochemistry (genomics), coding theory, communication networks and their security etc. In particular researchers are exploring the concepts of graph theory that can be used in different areas of Cryptography. In this paper a review of the works carried out in the field of Cryptography which use the concepts of Graph Theory, is given. Some of the Cryptographic Algorithms based on general graph theory concepts, Extremal Graph Theory and Expander Graphs are analyzed.",poster,cp55
p1555,73be8ad32db73d4f4f50c5dd96d71bdfb02ea9bb,c56,European Conference on Software Process Improvement,Algorithmic graph theory and perfect graphs,Abstract content,poster,cp56
p1556,ca73d88c497fb354a3a1f59fd2a3ca2bfbb7358c,c24,Decision Support Systems,Extremal Graph Theory,3 Third Lecture 11 3.1 Applications of the Zarankiewicz Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3.2 The Turán Problem for Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 3.3 The Girth Problem and Moore’s Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 3.4 Application of Moore’s Bound to Graph Spanners . . . . . . . . . . . . . . . . . . . . . . . 14,poster,cp24
p1557,3278d34e0eb3d58d36cc2300a4396664a6c950e5,c100,ACM SIGMOD Conference,Chemical Graph Theory,This chapter on chemical graph theory forms part of the natural science and processes section of the handbook,poster,cp100
p1558,9455977ce159fdc496cda7128ec35e03615baf43,c17,International Conference on Enterprise Information Systems,Graph theory in the geosciences,Abstract content,poster,cp17
p1559,09cc16e292040eac6a2ad5b37028414a5ffa2240,c108,International Conference on Information Integration and Web-based Applications & Services,Graph Theory,Abstract content,poster,cp108
p1560,3bf89f38b238dd2d4ca4870e6b1fb28dbd136c84,c27,ACM-SIAM Symposium on Discrete Algorithms,"Handbook of Graph Theory, Second Edition","In the ten years since the publication of the best-selling first edition, more than 1,000 graph theory papers have been published each year. Reflecting these advances, Handbook of Graph Theory, Second Edition provides comprehensive coverage of the main topics in pure and applied graph theory. This second editionover 400 pages longer than its predecessorincorporates 14 new sections. Each chapter includes lists of essential definitions and facts, accompanied by examples, tables, remarks, and, in some cases, conjectures and open problems. A bibliography at the end of each chapter provides an extensive guide to the research literature and pointers to monographs. In addition, a glossary is included in each chapter as well as at the end of each section. This edition also contains notes regarding terminology and notation. With 34 new contributors, this handbook is the most comprehensive single-source guide to graph theory. It emphasizes quick accessibility to topics for non-experts and enables easy cross-referencing among chapters.",poster,cp27
p1561,9318f9c688debeee0d671faae0f2cc384a673499,j295,Biomedical Optics Express,Automatic cone photoreceptor segmentation using graph theory and dynamic programming,"Geometrical analysis of the photoreceptor mosaic can reveal subclinical ocular pathologies. In this paper, we describe a fully automatic algorithm to identify and segment photoreceptors in adaptive optics ophthalmoscope images of the photoreceptor mosaic. This method is an extension of our previously described closed contour segmentation framework based on graph theory and dynamic programming (GTDP). We validated the performance of the proposed algorithm by comparing it to the state-of-the-art technique on a large data set consisting of over 200,000 cones and posted the results online. We found that the GTDP method achieved a higher detection rate, decreasing the cone miss rate by over a factor of five.",fullPaper,jv295
p1562,b131ac160af2c3ef91aff47f6578067183ca4c4b,c77,Networks,"Descriptive Complexity, Canonisation, and Definable Graph Structure Theory","Descriptive complexity theory establishes a connection between the computational complexity of algorithmic problems (the computational resources required to solve the problems) and their descriptive complexity (the language resources required to describe the problems). This ground-breaking book approaches descriptive complexity from the angle of modern structural graph theory, specifically graph minor theory. It develops a ‘definable structure theory’ concerned with the logical definability of graph-theoretic concepts such as tree decompositions and embeddings. The first part starts with an introduction to the background, from logic, complexity, and graph theory, and develops the theory up to first applications in descriptive complexity theory and graph isomorphism testing. It may serve as the basis for a graduate-level course. The second part is more advanced and mainly devoted to the proof of a single, previously unpublished theorem: properties of graphs with excluded minors are decidable in polynomial time if, and only if, they are definable in fixed-point logic with counting.",poster,cp77
p1563,c6309b2aa8706f69b70830ef18147c79be36ba50,j296,Nature Reviews Neuroscience,Complex brain networks: graph theoretical analysis of structural and functional systems,Abstract content,fullPaper,jv296
p1564,a23407b19100acba66fcfc2803d251a3c829e9e3,c90,Computer Vision and Pattern Recognition,Applying Graph theory to the Internet of Things,"In the Internet of Things (IoT), we all are ``things''. Graph theory, a branch of discrete mathematics, has been proven to be useful and powerful in understanding complex networks in history. By means of graph theory, we define new concepts and terminology, and explore the definition of IoT, and then show that IoT is the union of a topological network, a data-functional network and a domi-functional network.",poster,cp90
p1565,03f69f3cd279b1c0293ee27fc0f9e99ea5efcb5b,c16,Knowledge Discovery and Data Mining,Algorithm Design Using Spectral Graph Theory,"Spectral graph theory is the interplay between linear algebra and combinatorial graph theory. Laplace’s equation and its discrete form, the Laplacian matrix, appear ubiquitously in mathematical physics. Due to the recent discovery of very fast solvers for these equations, they are also becoming increasingly useful in combinatorial optimization, computer vision, computer graphics, and machine learning. In this thesis, we develop highly efficient and parallelizable algorithms for solving linear systems involving graph Laplacian matrices. These solvers can also be extended to symmetric diagonally dominant matrices and M -matrices, both of which are closely related to graph Laplacians. Our algorithms build upon two decades of progress on combinatorial preconditioning, which connects numerical and combinatorial algorithms through spectral graph theory. They in turn rely on tools from numerical analysis, metric embeddings, and random matrix theory. We give two solver algorithms that take diametrically opposite approaches. The first is motivated by combinatorial algorithms, and aims to gradually break the problem into several smaller ones. It represents major simplifications over previous solver constructions, and has theoretical running time comparable to sorting. The second is motivated by numerical analysis, and aims to rapidly improve the algebraic connectivity of the graph. It is the first highly efficient solver for Laplacian linear systems that parallelizes almost completely. Our results improve the performances of applications of fast linear system solvers ranging from scientific computing to algorithmic graph theory. We also show that these solvers can be used to address broad classes of image processing tasks, and give some preliminary experimental results.",poster,cp16
p1566,bfacd964ba59bcc6b87b251fa3b30df90736c315,c105,Biometrics and Identity Management,Fuzzy Graph Theory: A Survey,"A fuzzy graph (f-graph) is a pair G : ( σ, �) where σ is a fuzzy subset of a set S andis a fuzzy relation on σ. A fuzzy graph H : ( τ, υ) is called a partial fuzzy subgraph of G : ( σ, �) if τ (u) ≤ σ(u) for every u and υ (u, v) ≤ �(u, v) for every u and v . In particular we call a partial fuzzy subgraph H : ( τ, υ) a fuzzy subgraph of G : ( σ, � ) if τ (u) = σ(u) for every u in τ * and υ (u, v) = �(u, v) for every arc (u, v) in υ*. A connected f-graph G : ( σ, �) is a fuzzy tree(f-tree) if it has a fuzzy spannin g subgraph F : (σ, υ), which is a tree, where for all arcs (x, y) not i n F there exists a path from x to y in F whose strength is more than �(x, y). A path P of length n is a sequence of disti nct nodes u0, u 1, ..., u n such that �(u i1 , u i) > 0, i = 1, 2, ..., n and the degree of membershi p of a weakest arc is defined as its strength. If u 0 = u n and n ≥ 3, then P is called a cycle and a cycle P is called a fuzzy cycle(f-cycle) if it cont ains more than one weakest arc . The strength of connectedness between two nodes x and y is defined as the maximum of the strengths of all paths between x and y and is denot ed by CONN G(x, y). An x y path P is called a strongest x y",poster,cp105
p1567,6b5d3dbfe9ed31517b657ea9072064ec1a024b4b,c82,Workshop on Interdisciplinary Software Engineering Research,Introduction to Graph Theory and Algebraic Graph Theory,Abstract content,poster,cp82
p1568,9a5a7a1b13f51af9369ef4ddeb75bb8b5ff87e70,c79,International Conference on Medical Image Computing and Computer-Assisted Intervention,Graph Theory: An Introductory Course,"From the reviews: ""Bla Bollob's introductory course on graph theory deserves to be considered as a watershed in the development of this theory as a serious academic subject...The book has chapters on electrical networks, flows, connectivity and matchings, extremal problems, colouring, Ramsey theory, random graphs, and graphs and groups. Each chapter starts at a measured and gentle pace. Classical results are proved and new insight is provided, with the examples at the end of each chapter fully supplementing the text...Even so this allows an introduction not only to some of the deeper results but, more vitally, provides outlines of, and firm insights into, their proofs. Thus in an elementary text book, we gain an overall understanding of well-known standard results, and yet at the same time constant hints of, and guidelines into, the higher levels of the subject. It is this aspect of the book which should guarantee it a permanent place in the literature.""",poster,cp79
p1569,ab9accbaf61438bf518de0af0946f4bf12b7e158,j297,Research Synthesis Methods,"Network meta‐analysis, electrical networks and graph theory","Network meta‐analysis is an active field of research in clinical biostatistics. It aims to combine information from all randomized comparisons among a set of treatments for a given medical condition. We show how graph‐theoretical methods can be applied to network meta‐analysis. A meta‐analytic graph consists of vertices (treatments) and edges (randomized comparisons). We illustrate the correspondence between meta‐analytic networks and electrical networks, where variance corresponds to resistance, treatment effects to voltage, and weighted treatment effects to current flows. Based thereon, we then show that graph‐theoretical methods that have been routinely applied to electrical networks also work well in network meta‐analysis. In more detail, the resulting consistent treatment effects induced in the edges can be estimated via the Moore–Penrose pseudoinverse of the Laplacian matrix. Moreover, the variances of the treatment effects are estimated in analogy to electrical effective resistances. It is shown that this method, being computationally simple, leads to the usual fixed effect model estimate when applied to pairwise meta‐analysis and is consistent with published results when applied to network meta‐analysis examples from the literature. Moreover, problems of heterogeneity and inconsistency, random effects modeling and including multi‐armed trials are addressed. Copyright © 2012 John Wiley & Sons, Ltd.",fullPaper,jv297
p1570,fbe0aeb6d1aaca06c84c985ce63b7c061569dd99,j108,PLoS ONE,Comparing Brain Networks of Different Size and Connectivity Density Using Graph Theory,"Graph theory is a valuable framework to study the organization of functional and anatomical connections in the brain. Its use for comparing network topologies, however, is not without difficulties. Graph measures may be influenced by the number of nodes (N) and the average degree (k) of the network. The explicit form of that influence depends on the type of network topology, which is usually unknown for experimental data. Direct comparisons of graph measures between empirical networks with different N and/or k can therefore yield spurious results. We list benefits and pitfalls of various approaches that intend to overcome these difficulties. We discuss the initial graph definition of unweighted graphs via fixed thresholds, average degrees or edge densities, and the use of weighted graphs. For instance, choosing a threshold to fix N and k does eliminate size and density effects but may lead to modifications of the network by enforcing (ignoring) non-significant (significant) connections. Opposed to fixing N and k, graph measures are often normalized via random surrogates but, in fact, this may even increase the sensitivity to differences in N and k for the commonly used clustering coefficient and small-world index. To avoid such a bias we tried to estimate the N,k-dependence for empirical networks, which can serve to correct for size effects, if successful. We also add a number of methods used in social sciences that build on statistics of local network structures including exponential random graph models and motif counting. We show that none of the here-investigated methods allows for a reliable and fully unbiased comparison, but some perform better than others.",fullPaper,jv108
p1571,29af614088e9a2b387c2dc81539cf8420483c28a,c102,International Conference on Biometrics,A First Course in Graph Theory,"Because of its inherent simplicity, graph theory has a wide range of applications in engineering, and in physical sciences. It has of course uses in social sciences, in linguistics and in numerous other areas. In fact, a graph can be used to represent almost any physical situation involving discrete objects and the relationship among them. Now with the solutions to engineering and other problems becoming so complex leading to larger graphs, it is virtually difficult to analyze without the use of computers. This book is recommended in IIT Kharagpur, West Bengal for B.Tech Computer Science, NIT Arunachal Pradesh, NIT Nagaland, NIT Agartala, NIT Silchar, Gauhati University, Dibrugarh University, North Eastern Regional Institute of Management, Assam Engineering College, West Bengal Univerity of Technology (WBUT) for B.Tech, M.Tech Computer Science, University of Burdwan, West Bengal for B.Tech. Computer Science, Jadavpur University, West Bengal for M.Sc. Computer Science, Kalyani College of Engineering, West Bengal for B.Tech. Computer Science. Key Features: This book provides a rigorous yet informal treatment of graph theory with an emphasis on computational aspects of graph theory and graph-theoretic algorithms. Numerous applications to actual engineering problems are incorpo-rated with software design and optimization topics.",poster,cp102
p1572,e4715a13f6364b1c81e64f247651c3d9e80b6808,c78,Neural Information Processing Systems,Link Prediction Based on Graph Neural Networks,"Link prediction is a key problem for network-structured data. Link prediction heuristics use some score functions, such as common neighbors and Katz index, to measure the likelihood of links. They have obtained wide practical uses due to their simplicity, interpretability, and for some of them, scalability. However, every heuristic has a strong assumption on when two nodes are likely to link, which limits their effectiveness on networks where these assumptions fail. In this regard, a more reasonable way should be learning a suitable heuristic from a given network instead of using predefined ones. By extracting a local subgraph around each target link, we aim to learn a function mapping the subgraph patterns to link existence, thus automatically learning a `heuristic' that suits the current network. In this paper, we study this heuristic learning paradigm for link prediction. First, we develop a novel $\gamma$-decaying heuristic theory. The theory unifies a wide range of heuristics in a single framework, and proves that all these heuristics can be well approximated from local subgraphs. Our results show that local subgraphs reserve rich information related to link existence. Second, based on the $\gamma$-decaying theory, we propose a new algorithm to learn heuristics from local subgraphs using a graph neural network (GNN). Its experimental results show unprecedented performance, working consistently well on a wide range of problems.",fullPaper,cp78
p1573,5b18ce706c138393a127658d0a062d24f5f145bb,c13,International Conference on Data Science and Advanced Analytics,"Exponential Random Graph Models for Social Networks: Theory, Methods, and Applications","result of the social structure of American society.’’ This is true of the particular form or version of organized crime which he describes— it did not emanate from a transplanted Sicilian Mafia. That explanation, however, does not account for the many faces of organized crime in Australia, China, Russia, Japan, and many other places. Nor does it account for the growing phenomenon of transnational organized crime. Apart from any points of disagreement, this is a serious work of scholarship on a subject that has too often gotten short shrift in that respect. Organized Crime in Chicago is a good addition to the organized crime literature.",poster,cp13
p1574,5656c718164f6ab8950f13d78ccf5d3b9942df7f,c79,International Conference on Medical Image Computing and Computer-Assisted Intervention,Test-Retest Reliability of Graph Theory Measures of Structural Brain Connectivity,Abstract content,fullPaper,cp79
p1575,cfa6e9e2331fca45f5c948a3477164fd14c956c9,c103,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,"Graph Theory, 4th Edition",Abstract content,poster,cp103
p1576,446cd7da78f629cfdcef88b91ffbd24022a0b67b,c80,International Conference on Learning Representations,Graph Neural Networks Exponentially Lose Expressive Power for Node Classification,"Graph Neural Networks (graph NNs) are a promising deep learning approach for analyzing graph-structured data. However, it is known that they do not improve (or sometimes worsen) their predictive performance as we pile up many layers and add non-lineality. To tackle this problem, we investigate the expressive power of graph NNs via their asymptotic behaviors as the layer size tends to infinity. Our strategy is to generalize the forward propagation of a Graph Convolutional Network (GCN), which is a popular graph NN variant, as a specific dynamical system. In the case of a GCN, we show that when its weights satisfy the conditions determined by the spectra of the (augmented) normalized Laplacian, its output exponentially approaches the set of signals that carry information of the connected components and node degrees only for distinguishing nodes. Our theory enables us to relate the expressive power of GCNs with the topological information of the underlying graphs inherent in the graph spectra. To demonstrate this, we characterize the asymptotic behavior of GCNs on the Erdős -- Renyi graph. We show that when the Erdős -- Renyi graph is sufficiently dense and large, a broad range of GCNs on it suffers from the ""information loss"" in the limit of infinite layers with high probability. Based on the theory, we provide a principled guideline for weight normalization of graph NNs. We experimentally confirm that the proposed weight scaling enhances the predictive performance of GCNs in real data. Code is available at this https URL.",fullPaper,cp80
p1577,d089f5e5d0548df6066ff281f3918ed67ae742a0,c56,European Conference on Software Process Improvement,Recent developments in graph Ramsey theory,"Given a graph $H$, the Ramsey number $r(H)$ is the smallest natural number $N$ such that any two-colouring of the edges of $K_N$ contains a monochromatic copy of $H$. The existence of these numbers has been known since 1930 but their quantitative behaviour is still not well understood. Even so, there has been a great deal of recent progress on the study of Ramsey numbers and their variants, spurred on by the many advances across extremal combinatorics. In this survey, we will describe some of this progress.",poster,cp56
p1578,fc472f5017c6200e79b7540be07bc6b4798b4ef8,c43,ACM Symposium on Applied Computing,A review of evolutionary graph theory with applications to game theory,Abstract content,poster,cp43
p1579,a51a630f1088baff85282993ebf14fb910ccf126,j265,BioData Mining,Using graph theory to analyze biological networks,Abstract content,fullPaper,jv265
p1580,f4080a231a943b848d4d8e6c597c647caf4390c1,c66,Annual Conference on Innovation and Technology in Computer Science Education,Spectral Graph Theory,Abstract content,poster,cp66
p1581,4d89d0da3bb1c8bc2ef36c7641abd41149def20b,c76,International Conference on Artificial Neural Networks,Graph theory and molecular orbitals. Total φ-electron energy of alternant hydrocarbons,Abstract content,poster,cp76
p1582,8df5d1e909de14932e42b347adf35070f60dc9ba,j298,Transactions of the American Mathematical Society,"An $L^p$ theory of sparse graph convergence I: Limits, sparse random graph models, and power law distributions","We introduce and develop a theory of limits for sequences of sparse graphs based on $L^p$ graphons, which generalizes both the existing $L^\infty$ theory of dense graph limits and its extension by Bollob\'as and Riordan to sparse graphs without dense spots. In doing so, we replace the no dense spots hypothesis with weaker assumptions, which allow us to analyze graphs with power law degree distributions. This gives the first broadly applicable limit theory for sparse graphs with unbounded average degrees. In this paper, we lay the foundations of the $L^p$ theory of graphons, characterize convergence, and develop corresponding random graph models, while we prove the equivalence of several alternative metrics in a companion paper.",fullPaper,jv298
p1583,74b6089b46e3e12f74da7d5d981d787aa7fb8459,c16,Knowledge Discovery and Data Mining,Active semi-supervised learning using sampling theory for graph signals,"We consider the problem of offline, pool-based active semi-supervised learning on graphs. This problem is important when the labeled data is scarce and expensive whereas unlabeled data is easily available. The data points are represented by the vertices of an undirected graph with the similarity between them captured by the edge weights. Given a target number of nodes to label, the goal is to choose those nodes that are most informative and then predict the unknown labels. We propose a novel framework for this problem based on our recent results on sampling theory for graph signals. A graph signal is a real-valued function defined on each node of the graph. A notion of frequency for such signals can be defined using the spectrum of the graph Laplacian matrix. The sampling theory for graph signals aims to extend the traditional Nyquist-Shannon sampling theory by allowing us to identify the class of graph signals that can be reconstructed from their values on a subset of vertices. This approach allows us to define a criterion for active learning based on sampling set selection which aims at maximizing the frequency of the signals that can be reconstructed from their samples on the set. Experiments show the effectiveness of our method.",fullPaper,cp16
p1584,36e3cf186ebc510800fb6e217cc8edae93fbf4f9,c47,International Symposium on Empirical Software Engineering and Measurement,"An $L^{p}$ theory of sparse graph convergence II: LD convergence, quotients and right convergence","We extend the LpLp theory of sparse graph limits, which was introduced in a companion paper, by analyzing different notions of convergence. Under suitable restrictions on node weights, we prove the equivalence of metric convergence, quotient convergence, microcanonical ground state energy convergence, microcanonical free energy convergence and large deviation convergence. Our theorems extend the broad applicability of dense graph convergence to all sparse graphs with unbounded average degree, while the proofs require new techniques based on uniform upper regularity. Examples to which our theory applies include stochastic block models, power law graphs and sparse versions of WW-random graphs.",poster,cp47
p1585,d133cb102ad0f81e3fd17a7db090b28afc124c4a,j91,Physical Review Letters,Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties.,"The use of machine learning methods for accelerating the design of crystalline materials usually requires manually constructed feature vectors or complex transformation of atom coordinates to input the crystal structure, which either constrains the model to certain crystal types or makes it difficult to provide chemical insights. Here, we develop a crystal graph convolutional neural networks framework to directly learn material properties from the connection of atoms in the crystal, providing a universal and interpretable representation of crystalline materials. Our method provides a highly accurate prediction of density functional theory calculated properties for eight different properties of crystals with various structure types and compositions after being trained with 10^{4} data points. Further, our framework is interpretable because one can extract the contributions from local chemical environments to global properties. Using an example of perovskites, we show how this information can be utilized to discover empirical rules for materials design.",fullPaper,jv91
p1586,39afec82e60d44f003cbd15e85d892b6fc5016e9,c9,Pacific Symposium on Biocomputing,"Algorithms, Graph Theory, and Linear Equations in Laplacian Matrices","The Laplacian matrices of graphs are fundamental. In addition to facilitating the application of linear algebra to graph theory, they arise in many practical problems. In this talk we survey recent progress on the design of provably fast algorithms for solving linear equations in the Laplacian matrices of graphs. These algorithms motivate and rely upon fascinating primitives in graph theory, including low-stretch spanning trees, graph sparsifiers, ultra-sparsifiers, and local graph clustering. These are all connected by a definition of what it means for one graph to approximate another. While this definition is dictated by Numerical Linear Algebra, it proves useful and natural from a graph theoretic perspective. Mathematics Subject Classification (2010). Primary 68Q25; Secondary 65F08.",poster,cp9
p1587,f2a34394994089ee965dd9fa7a4bbc2446432770,c0,International Conference on Human Factors in Computing Systems,Applications of Graph Theory and Network Science to Transit Network Design,"Abstract While the network nature of public transportation systems is well known, the study of their design from a topological/geometric perspective remains relatively limited. From the work of Euler in the 1750s to the discovery of scale-free networks in the late 1990s, the goal of this paper is to review the topical literature that applied concepts of graph theory and network science. After briefly introducing the origins of graph theory, we review early indicators developed to study transport networks, which notably includes the works of Garrison and Marble, and Kansky. Afterwards, we examine network indicators and characteristics developed to study transit systems specifically, in particular by reviewing the works of Vuchic and Musso. Subsequently, we introduce the concepts of small-worlds and scale-free networks from the emerging field network science, and review early applications to transit networks. Finally, we identify three challenges that will need to be addressed in the future. As transit systems are likely to grow in the world, the study of their network feature could be of substantial help to planners so as to better design the transit systems of tomorrow, but much work lies ahead.",poster,cp0
p1588,cc2101c1368c528a7def9bf9ae711d3725933ac8,c16,Knowledge Discovery and Data Mining,Local Higher-Order Graph Clustering,"Local graph clustering methods aim to find a cluster of nodes by exploring a small region of the graph. These methods are attractive because they enable targeted clustering around a given seed node and are faster than traditional global graph clustering methods because their runtime does not depend on the size of the input graph. However, current local graph partitioning methods are not designed to account for the higher-order structures crucial to the network, nor can they effectively handle directed networks. Here we introduce a new class of local graph clustering methods that address these issues by incorporating higher-order network information captured by small subgraphs, also called network motifs. We develop the Motif-based Approximate Personalized PageRank (MAPPR) algorithm that finds clusters containing a seed node with minimal \emph{motif conductance}, a generalization of the conductance metric for network motifs. We generalize existing theory to prove the fast running time (independent of the size of the graph) and obtain theoretical guarantees on the cluster quality (in terms of motif conductance). We also develop a theory of node neighborhoods for finding sets that have small motif conductance, and apply these results to the case of finding good seed nodes to use as input to the MAPPR algorithm. Experimental validation on community detection tasks in both synthetic and real-world networks, shows that our new framework MAPPR outperforms the current edge-based personalized PageRank methodology.",fullPaper,cp16
p1589,2ced07a295025f46ae51efa0a61679815c0866c5,c53,International Conference on Software Engineering and Knowledge Engineering,Some new results in extremal graph theory,"In recent years several classical results in extremal graph theory have been improved in a uniform way and their proofs have been simplified and streamlined. These results include a new Erd\H{o}s-Stone-Bollob\'as theorem, several stability theorems, several saturation results and bounds for the number of graphs with large forbidden subgraphs. Another recent trend is the expansion of spectral extremal graph theory, in which extremal properties of graphs are studied by means of eigenvalues of various matrices. One particular achievement in this area is the casting of the central results above in spectral terms, often with additional enhancement. In addition, new, specific spectral results were found that have no conventional analogs. All of the above material is scattered throughout various journals, and since it may be of some interest, the purpose of this survey is to present the best of these results in a uniform, structured setting, together with some discussions of the underpinning ideas.",poster,cp53
p1590,8bd4a8eb93ede5c55c09bed842be671c8cb3413f,j295,Biomedical Optics Express,Robust automatic segmentation of corneal layer boundaries in SDOCT images using graph theory and dynamic programming,"Segmentation of anatomical structures in corneal images is crucial for the diagnosis and study of anterior segment diseases. However, manual segmentation is a time-consuming and subjective process. This paper presents an automatic approach for segmenting corneal layer boundaries in Spectral Domain Optical Coherence Tomography images using graph theory and dynamic programming. Our approach is robust to the low-SNR and different artifact types that can appear in clinical corneal images. We show that our method segments three corneal layer boundaries in normal adult eyes more accurately compared to an expert grader than a second grader—even in the presence of significant imaging outliers.",fullPaper,jv295
p1591,1a6b1bac33da9cdff5fc311e9b9e4bd065bd7a9f,c61,Jahrestagung der Gesellschaft für Informatik,Systemic Risk in Energy Derivative Markets: A Graph-Theory Analysis,"This article uses graph theory to provide novel evidence regarding market integration, a favorable condition for systemic risk to appear in. Relying on daily futures returns covering a 12-year period, we examine cross- and inter-market linkages, both within the commodity complex and between commodities and other financial assets. In such a high dimensional analysis, graph theory enables us to understand the dynamic behavior of our price system. We show that energy markets - as a whole - stand at the heart of this system. We also establish that crude oil is itself at the center of the energy complex. Further, we provide evidence that commodity markets have become more integrated over time.",poster,cp61
p1592,cc2c33aea50ef4e2aec9413f83ccd7d35dace3c6,c80,International Conference on Learning Representations,Applications of Graph Theory in Computer Science,"Graphs are among the most ubiquitous models of both natural and human-made structures. They can be used to model many types of relations and process dynamics in computer science, physical, biological and social systems. Many problems of practical interest can be represented by graphs. In general graphs theory has a wide range of applications in diverse fields. This paper explores different elements involved in graph theory including graph representations using computer systems and graph-theoretic data structures such as list structure and matrix structure. The emphasis of this paper is on graph applications in computer science. To demonstrate the importance of graph theory in computer science, this article addresses most common applications for graph theory in computer science. These applications are presented especially to project the idea of graph theory and to demonstrate its importance in computer science.",poster,cp80
p1593,a44b5f0314a12aca034d52cf047445cdf7c884bf,c52,Workshop on Learning from Authoritative Security Experiment Results,"Graph Theory, Combinatorics and Algorithms: Interdisciplinary Applications","Graph Theory, Combinatorics and Algorithms: Interdisciplinary Applications focuses on discrete mathematics and combinatorial algorithms interacting with real world problems in computer science, operations research, applied mathematics and engineering.The book containseleven chapters written by experts in their respective fields, and covers a wide spectrum of high-interest problems across these discipline domains. Among the contributing authors are Richard Karp of UC Berkeley and Robert Tarjan of Princeton; both are at the pinnacle of research scholarship in Graph Theory and Combinatorics. The chapters from the contributing authors focus on ""real world"" applications, all of which will be of considerable interest across the areas of Operations Research, Computer Science, Applied Mathematics, and Engineering. These problems include Internet congestion control, high-speed communication networks, multi-object auctions, resource allocation, software testing, data structures, etc. In sum, this is a book focused on major, contemporary problems, written by the top research scholars in the field, using cutting-edge mathematical and computational techniques.",poster,cp52
p1594,7f5ea861a57e14796f033fd0f5580dbc34ff88f2,c75,International Conference on Machine Learning,Relational Pooling for Graph Representations,"This work generalizes graph neural networks (GNNs) beyond those based on the Weisfeiler-Lehman (WL) algorithm, graph Laplacians, and diffusions. Our approach, denoted Relational Pooling (RP), draws from the theory of finite partial exchangeability to provide a framework with maximal representation power for graphs. RP can work with existing graph representation models and, somewhat counterintuitively, can make them even more powerful than the original WL isomorphism test. Additionally, RP allows architectures like Recurrent Neural Networks and Convolutional Neural Networks to be used in a theoretically sound approach for graph classification. We demonstrate improved performance of RP-based graph representations over state-of-the-art methods on a number of tasks.",fullPaper,cp75
p1595,69381b5efd97e7c55f51c2730caccab3d632d4d2,j299,IEEE Transactions on Pattern Analysis and Machine Intelligence,Graph Embedding and Extensions: A General Framework for Dimensionality Reduction,"A large family of algorithms - supervised or unsupervised; stemming from statistics or geometry theory - has been designed to provide different solutions to the problem of dimensionality reduction. Despite the different motivations of these algorithms, we present in this paper a general formulation known as graph embedding to unify them within a common framework. In graph embedding, each algorithm can be considered as the direct graph embedding or its linear/kernel/tensor extension of a specific intrinsic graph that describes certain desired statistical or geometric properties of a data set, with constraints from scale normalization or a penalty graph that characterizes a statistical or geometric property that should be avoided. Furthermore, the graph embedding framework can be used as a general platform for developing new dimensionality reduction algorithms. By utilizing this framework as a tool, we propose a new supervised dimensionality reduction algorithm called marginal Fisher analysis in which the intrinsic graph characterizes the intraclass compactness and connects each data point with its neighboring points of the same class, while the penalty graph connects the marginal points and characterizes the interclass separability. We show that MFA effectively overcomes the limitations of the traditional linear discriminant analysis algorithm due to data distribution assumptions and available projection directions. Real face recognition experiments show the superiority of our proposed MFA in comparison to LDA, also for corresponding kernel and tensor extensions",fullPaper,jv299
p1596,287650fad5a478581f0040ad064e6856c7c13af2,c87,European Conference on Computer Vision,Topological Graph Theory,Introduction Voltage Graphs and Covering Spaces Surfaces and Graph Imbeddings Imbedded Voltage Graphs and Current Graphs Map Colorings The Genus of A Group References.,poster,cp87
p1597,1f400c9732c13eae95be34a181c2b8042a774a39,c32,International Conference on Software Technology: Methods and Tools,Handbook of graph theory,"Introduction to Graphs Fundamentals of Graph Theory, Jonathan L. Gross and Jay Yellen Families of Graphs and Digraphs, Lowell W. Beineke History of Graph Theory, Robin J. Wilson Graph Representation Computer Representation of Graphs, Alfred V. Aho Graph Isomorphism, Brendan D. McKay The Reconstruction Problem, Josef Lauri Recursively Constructed Graphs, Richard B. Borie, R. Gary Parker, and Craig A. Tovey Structural Graph Theory, Maria Chudnovsky Directed Graphs Basic Digraph Models and Properties, Jay Yellen Directed Acyclic Graphs, Stephen B. Maurer Tournaments, K.B. Reid Connectivity and Traversability Connectivity Properties and Structure, Camino Balbuena, Josep Fabrega, and Miguel Angel Fiol Eulerian Graphs, Herbert Fleischner Chinese Postman Problems, R. Gary Parker and Richard B. Borie DeBruijn Graphs and Sequences, A.K. Dewdney Hamiltonian Graphs, Ronald J. Gould Traveling Salesman Problems, Gregory Gutin Further Topics in Connectivity, Josep Fabrega and Miguel Angel Fiol Colorings and Related Topics Graph Coloring, Zsolt Tuza Further Topics in Graph Coloring, Zsolt Tuza Independence and Cliques, Gregory Gutin Factors and Factorization, Michael Plummer Applications to Timetabling, Edmund Burke, Dominique de Werra, and Jeffrey Kingston Graceful Labelings, Joseph A. Gallian Algebraic Graph Theory Automorphisms, Mark E. Watkins Cayley Graphs, Brian Alspach Enumeration, Paul K. Stockmeyer Graphs and Vector Spaces, Krishnaiyan ""KT"" Thulasiraman Spectral Graph Theory, Michael Doob Matroidal Methods in Graph Theory, James Oxley Topological Graph Theory Graphs on Surfaces, Tomaz Pisanski and Primoz Potocnik Minimum Genus and Maximum Genus, Jianer Chen Genus Distributions, Jonathan L. Gross Voltage Graphs, Jonathan L. Gross The Genus of a Group, Thomas W. Tucker Maps, Roman Nedela and Martin Skoviera Representativity, Dan Archdeacon Triangulations, Seiya Negami Graphs and Finite Geometries, Arthur T. White Crossing Numbers, R. Bruce Richter and Gelasio Salazar Analytic Graph Theory Extremal Graph Theory, Bela Bollobas and Vladimir Nikiforov Random Graphs, Nicholas Wormald Ramsey Graph Theory, Ralph J. Faudree The Probabilistic Method, Alan Frieze and Po-Shen Loh Graph Limits, Bojan Mohar Graphical Measurement Distance in Graphs, Gary Chartrand and Ping Zhang Domination in Graphs, Teresa W. Haynes and Michael A. Henning Tolerance Graphs, Martin Charles Golumbic Bandwidth, Robert C. Brigham Pursuit-Evasion Problems, Richard B. Borie, Sven Koenig, and Craig A. Tovey Graphs in Computer Science Searching, Harold N. Gabow Dynamic Graph Algorithms, Camil Demetrescu, Irene Finocchi, and Giuseppe F. Italiano Drawings of Graphs, Emilio Di Giacomo, Giuseppe Liotta, and Roberto Tamassia Algorithms on Recursively Constructed Graphs, Richard B. Borie, R. Gary Parker, and Craig A. Tovey Fuzzy Graphs, John N. Mordeson and D.S. Malik Expander Graphs, Mike Krebs and Anthony Shaheen Visibility Graphs, Alice M. Dean and Joan P. Hutchinson Networks and Flows Maximum Flows, Clifford Stein Minimum Cost Flows, Lisa Fleischer Matchings and Assignments, Jay Sethuraman and Douglas R. Shier Communication Networks Complex Networks, Anthony Bonato and Fan Chung Broadcasting and Gossiping, Hovhannes A. Harutyunyan, Arthur L. Liestman, Joseph G. Peters, and Dana Richards Communication Network Design Models, Prakash Mirchandani and David Simchi-Levi Network Science for Graph Theorists, David C. Arney and Steven B. Horton Natural Science and Processes Chemical Graph Theory, Ernesto Estrada and Danail Bonchev Ties between Graph Theory and Biology, Jacek Blazewicz, Marta Kasprzak, and Nikos Vlassis Index A Glossary appears at the end of each chapter.",poster,cp32
p1598,ef2704b8e44692c7f6430451434af64df913d575,c68,International Workshop on Software Engineering for Large-Scale Multi-Agent Systems,Introduction to Graph and Hypergraph Theory,Preface Basic Definitions and Concepts Trees and Bipartite Graphs Chordal Graphs Planar Graphs Graph Coloring Traversals and Flows Basic Hypergraph Concepts Hypertrees and Chordal Hypergraphs Some Other Remarkable Hypergraph Classes Hypergraph Coloring Modeling with Hypergraphs Appendix Index.,poster,cp68
p1599,a59b5da1de93e35bbb26d335ca2247ea710a9315,j179,Protein Science,A graph‐theory algorithm for rapid protein side‐chain prediction,"Fast and accurate side‐chain conformation prediction is important for homology modeling, ab initio protein structure prediction, and protein design applications. Many methods have been presented, although only a few computer programs are publicly available. The SCWRL program is one such method and is widely used because of its speed, accuracy, and ease of use. A new algorithm for SCWRL is presented that uses results from graph theory to solve the combinatorial problem encountered in the side‐chain prediction problem. In this method, side chains are represented as vertices in an undirected graph. Any two residues that have rotamers with nonzero interaction energies are considered to have an edge in the graph. The resulting graph can be partitioned into connected subgraphs with no edges between them. These subgraphs can in turn be broken into biconnected components, which are graphs that cannot be disconnected by removal of a single vertex. The combinatorial problem is reduced to finding the minimum energy of these small biconnected components and combining the results to identify the global minimum energy conformation. This algorithm is able to complete predictions on a set of 180 proteins with 34,342 side chains in <7 min of computer time. The total χ1 and χ1 + 2 dihedral angle accuracies are 82.6% and 73.7% using a simple energy function based on the backbone‐dependent rotamer library and a linear repulsive steric energy. The new algorithm will allow for use of SCWRL in more demanding applications such as sequence design and ab initio structure prediction, as well addition of a more complex energy function and conformational flexibility, leading to increased accuracy.",fullPaper,jv179
p1600,a59b5da1de93e35bbb26d335ca2247ea710a9315,j179,Protein Science,A graph‐theory algorithm for rapid protein side‐chain prediction,"Fast and accurate side‐chain conformation prediction is important for homology modeling, ab initio protein structure prediction, and protein design applications. Many methods have been presented, although only a few computer programs are publicly available. The SCWRL program is one such method and is widely used because of its speed, accuracy, and ease of use. A new algorithm for SCWRL is presented that uses results from graph theory to solve the combinatorial problem encountered in the side‐chain prediction problem. In this method, side chains are represented as vertices in an undirected graph. Any two residues that have rotamers with nonzero interaction energies are considered to have an edge in the graph. The resulting graph can be partitioned into connected subgraphs with no edges between them. These subgraphs can in turn be broken into biconnected components, which are graphs that cannot be disconnected by removal of a single vertex. The combinatorial problem is reduced to finding the minimum energy of these small biconnected components and combining the results to identify the global minimum energy conformation. This algorithm is able to complete predictions on a set of 180 proteins with 34,342 side chains in <7 min of computer time. The total χ1 and χ1 + 2 dihedral angle accuracies are 82.6% and 73.7% using a simple energy function based on the backbone‐dependent rotamer library and a linear repulsive steric energy. The new algorithm will allow for use of SCWRL in more demanding applications such as sequence design and ab initio structure prediction, as well addition of a more complex energy function and conformational flexibility, leading to increased accuracy.",fullPaper,jv179
p1601,7dbdb4209626fd92d2436a058663206216036e68,c89,Conference on Uncertainty in Artificial Intelligence,Elements of Information Theory,"Preface to the Second Edition. Preface to the First Edition. Acknowledgments for the Second Edition. Acknowledgments for the First Edition. 1. Introduction and Preview. 1.1 Preview of the Book. 2. Entropy, Relative Entropy, and Mutual Information. 2.1 Entropy. 2.2 Joint Entropy and Conditional Entropy. 2.3 Relative Entropy and Mutual Information. 2.4 Relationship Between Entropy and Mutual Information. 2.5 Chain Rules for Entropy, Relative Entropy, and Mutual Information. 2.6 Jensen's Inequality and Its Consequences. 2.7 Log Sum Inequality and Its Applications. 2.8 Data-Processing Inequality. 2.9 Sufficient Statistics. 2.10 Fano's Inequality. Summary. Problems. Historical Notes. 3. Asymptotic Equipartition Property. 3.1 Asymptotic Equipartition Property Theorem. 3.2 Consequences of the AEP: Data Compression. 3.3 High-Probability Sets and the Typical Set. Summary. Problems. Historical Notes. 4. Entropy Rates of a Stochastic Process. 4.1 Markov Chains. 4.2 Entropy Rate. 4.3 Example: Entropy Rate of a Random Walk on a Weighted Graph. 4.4 Second Law of Thermodynamics. 4.5 Functions of Markov Chains. Summary. Problems. Historical Notes. 5. Data Compression. 5.1 Examples of Codes. 5.2 Kraft Inequality. 5.3 Optimal Codes. 5.4 Bounds on the Optimal Code Length. 5.5 Kraft Inequality for Uniquely Decodable Codes. 5.6 Huffman Codes. 5.7 Some Comments on Huffman Codes. 5.8 Optimality of Huffman Codes. 5.9 Shannon-Fano-Elias Coding. 5.10 Competitive Optimality of the Shannon Code. 5.11 Generation of Discrete Distributions from Fair Coins. Summary. Problems. Historical Notes. 6. Gambling and Data Compression. 6.1 The Horse Race. 6.2 Gambling and Side Information. 6.3 Dependent Horse Races and Entropy Rate. 6.4 The Entropy of English. 6.5 Data Compression and Gambling. 6.6 Gambling Estimate of the Entropy of English. Summary. Problems. Historical Notes. 7. Channel Capacity. 7.1 Examples of Channel Capacity. 7.2 Symmetric Channels. 7.3 Properties of Channel Capacity. 7.4 Preview of the Channel Coding Theorem. 7.5 Definitions. 7.6 Jointly Typical Sequences. 7.7 Channel Coding Theorem. 7.8 Zero-Error Codes. 7.9 Fano's Inequality and the Converse to the Coding Theorem. 7.10 Equality in the Converse to the Channel Coding Theorem. 7.11 Hamming Codes. 7.12 Feedback Capacity. 7.13 Source-Channel Separation Theorem. Summary. Problems. Historical Notes. 8. Differential Entropy. 8.1 Definitions. 8.2 AEP for Continuous Random Variables. 8.3 Relation of Differential Entropy to Discrete Entropy. 8.4 Joint and Conditional Differential Entropy. 8.5 Relative Entropy and Mutual Information. 8.6 Properties of Differential Entropy, Relative Entropy, and Mutual Information. Summary. Problems. Historical Notes. 9. Gaussian Channel. 9.1 Gaussian Channel: Definitions. 9.2 Converse to the Coding Theorem for Gaussian Channels. 9.3 Bandlimited Channels. 9.4 Parallel Gaussian Channels. 9.5 Channels with Colored Gaussian Noise. 9.6 Gaussian Channels with Feedback. Summary. Problems. Historical Notes. 10. Rate Distortion Theory. 10.1 Quantization. 10.2 Definitions. 10.3 Calculation of the Rate Distortion Function. 10.4 Converse to the Rate Distortion Theorem. 10.5 Achievability of the Rate Distortion Function. 10.6 Strongly Typical Sequences and Rate Distortion. 10.7 Characterization of the Rate Distortion Function. 10.8 Computation of Channel Capacity and the Rate Distortion Function. Summary. Problems. Historical Notes. 11. Information Theory and Statistics. 11.1 Method of Types. 11.2 Law of Large Numbers. 11.3 Universal Source Coding. 11.4 Large Deviation Theory. 11.5 Examples of Sanov's Theorem. 11.6 Conditional Limit Theorem. 11.7 Hypothesis Testing. 11.8 Chernoff-Stein Lemma. 11.9 Chernoff Information. 11.10 Fisher Information and the Cram-er-Rao Inequality. Summary. Problems. Historical Notes. 12. Maximum Entropy. 12.1 Maximum Entropy Distributions. 12.2 Examples. 12.3 Anomalous Maximum Entropy Problem. 12.4 Spectrum Estimation. 12.5 Entropy Rates of a Gaussian Process. 12.6 Burg's Maximum Entropy Theorem. Summary. Problems. Historical Notes. 13. Universal Source Coding. 13.1 Universal Codes and Channel Capacity. 13.2 Universal Coding for Binary Sequences. 13.3 Arithmetic Coding. 13.4 Lempel-Ziv Coding. 13.5 Optimality of Lempel-Ziv Algorithms. Compression. Summary. Problems. Historical Notes. 14. Kolmogorov Complexity. 14.1 Models of Computation. 14.2 Kolmogorov Complexity: Definitions and Examples. 14.3 Kolmogorov Complexity and Entropy. 14.4 Kolmogorov Complexity of Integers. 14.5 Algorithmically Random and Incompressible Sequences. 14.6 Universal Probability. 14.7 Kolmogorov complexity. 14.9 Universal Gambling. 14.10 Occam's Razor. 14.11 Kolmogorov Complexity and Universal Probability. 14.12 Kolmogorov Sufficient Statistic. 14.13 Minimum Description Length Principle. Summary. Problems. Historical Notes. 15. Network Information Theory. 15.1 Gaussian Multiple-User Channels. 15.2 Jointly Typical Sequences. 15.3 Multiple-Access Channel. 15.4 Encoding of Correlated Sources. 15.5 Duality Between Slepian-Wolf Encoding and Multiple-Access Channels. 15.6 Broadcast Channel. 15.7 Relay Channel. 15.8 Source Coding with Side Information. 15.9 Rate Distortion with Side Information. 15.10 General Multiterminal Networks. Summary. Problems. Historical Notes. 16. Information Theory and Portfolio Theory. 16.1 The Stock Market: Some Definitions. 16.2 Kuhn-Tucker Characterization of the Log-Optimal Portfolio. 16.3 Asymptotic Optimality of the Log-Optimal Portfolio. 16.4 Side Information and the Growth Rate. 16.5 Investment in Stationary Markets. 16.6 Competitive Optimality of the Log-Optimal Portfolio. 16.7 Universal Portfolios. 16.8 Shannon-McMillan-Breiman Theorem (General AEP). Summary. Problems. Historical Notes. 17. Inequalities in Information Theory. 17.1 Basic Inequalities of Information Theory. 17.2 Differential Entropy. 17.3 Bounds on Entropy and Relative Entropy. 17.4 Inequalities for Types. 17.5 Combinatorial Bounds on Entropy. 17.6 Entropy Rates of Subsets. 17.7 Entropy and Fisher Information. 17.8 Entropy Power Inequality and Brunn-Minkowski Inequality. 17.9 Inequalities for Determinants. 17.10 Inequalities for Ratios of Determinants. Summary. Problems. Historical Notes. Bibliography. List of Symbols. Index.",poster,cp89
p1602,3f2e5b88c4b631c931f6db805f4ae3c0b531a8e8,c94,Vision,APPLICATIONS OF GRAPH THEORY IN COMPUTER SCIENCE AN OVERVIEW,"The field of mathematics plays vital role in various fields. One of the important areas in mathematics is graph theory which is used in structural models. This structural arrangements of various objects or technologies lead to new inventions and modifications in the existing environment for enhancement in those fields. The field graph theory started its journey from the problem of Koinsberg bridge in 1735. This paper gives an overview of the applications of graph theory in heterogeneous fields to some extent but mainly focuses on the computer science applications that uses graph theoretical concepts. Various papers based on graph theory have been studied related to scheduling concepts, computer science applications and an overview has been presented here.",poster,cp94
p1603,6362675dc7f0a41061ceb6c10852e84e6141e509,c27,ACM-SIAM Symposium on Discrete Algorithms,Graph Theory in the Information Age,"I n the past decade, graph theory has gone through a remarkable shift and a profound transformation. The change is in large part due to the humongous amount of information that we are confronted with. A main way to sort through massive data sets is to build and examine the network formed by interrelations. For example, Google’s successful Web search algorithms are based on the WWW graph, which contains all Web pages as vertices and hyperlinks as edges. There are all sorts of information networks, such as biological networks built from biological databases and social networks formed by email, phone calls, instant messaging, etc., as well as various types of physical networks. Of particular interest to mathematicians is the collaboration graph, which is based on the data from Mathematical Reviews. In the collaboration graph, every mathematician is a vertex, and two mathematicians who wrote a joint paper are connected by an edge. Figure 1 illustrates a portion of the collaboration graph consisting of about 5,000 vertices, representing mathematicians with Erdős number 2 (i.e., mathematicians who wrote a paper with a coauthor of Paul Erdős). Graph theory has two hundred years of history studying the basic mathematical structures called graphs. A graph G consists of a collection V of vertices and a collection E of edges that connect pairs of vertices. In the past, graph theory has",poster,cp27
p1604,3bc2b3028064f5b580ee8d5dcc17b6bb346015fb,c71,IEEE International Conference on Information Reuse and Integration,Graph Theory and Complex Networks: An Introduction,"Chapter Description 01: Introduction History, background 02: Foundations Basic terminology and properties of graphs 03: Extensions Directed & weighted graphs, colorings 04: Network traversal Walking through graphs (cf. traveling) 05: Trees Graphs without cycles; routing algorithms 06: Network analysis Basic metrics for analyzing large graphs 07: Random networks Introduction modeling real-world networks 08: Computer networks The Internet & WWW seen as a huge graph 09: Social networks Communities seen as graphs",poster,cp71
p1605,c50328abeea3d9dea2146af86d1ca90996de067d,c83,International Conference on Computer Graphics and Interactive Techniques,Assessing the vulnerability of supply chains using graph theory,Abstract content,poster,cp83
p1606,9e1610daa286703564f145d21448ece479f26b76,c6,Americas Conference on Information Systems,Extremal Graph Theory,Abstract content,poster,cp6
p1607,2d832d0d1baaaba224a21ca926a10c0529da3628,j300,BMC Neuroscience,Functional neural network analysis in frontotemporal dementia and Alzheimer's disease using EEG and graph theory,Abstract content,fullPaper,jv300
p1608,cca8df8c82a189e4af17919859d56e40acc7c3c9,c77,Networks,Graph Theory with Applications to Engineering and Computer Science,"Graph Theory with Applications to Engineering and Computer ... This outstanding introductory treatment of graph theory and its applications has had a long life in the instruction of advanced undergraduates and graduate students in all areas that require knowledge of this subject. The first nine chapters constitute an excellent overall introduction, requiring only some knowledge of set theory and matrix algebra.",fullPaper,cp77
p1609,e39ba4c989874285e9473b4532dc275c12ed78c0,c27,ACM-SIAM Symposium on Discrete Algorithms,Computing the shortest path: A search meets graph theory,"We propose shortest path algorithms that use A* search in combination with a new graph-theoretic lower-bounding technique based on landmarks and the triangle inequality. Our algorithms compute optimal shortest paths and work on any directed graph. We give experimental results showing that the most efficient of our new algorithms outperforms previous algorithms, in particular A* search with Euclidean bounds, by a wide margin on road networks and on some synthetic problem families.",fullPaper,cp27
p1610,54155120a8801ed3afcc3bd3d4c6eb0fb4d028c7,j99,IEEE Transactions on Geoscience and Remote Sensing,Urban-Area and Building Detection Using SIFT Keypoints and Graph Theory,"Very high resolution satellite images provide valuable information to researchers. Among these, urban-area boundaries and building locations play crucial roles. For a human expert, manually extracting this valuable information is tedious. One possible solution to extract this information is using automated techniques. Unfortunately, the solution is not straightforward if standard image processing and pattern recognition techniques are used. Therefore, to detect the urban area and buildings in satellite images, we propose the use of scale invariant feature transform (SIFT) and graph theoretical tools. SIFT keypoints are powerful in detecting objects under various imaging conditions. However, SIFT is not sufficient for detecting urban areas and buildings alone. Therefore, we formalize the problem in terms of graph theory. In forming the graph, we represent each keypoint as a vertex of the graph. The unary and binary relationships between these vertices (such as spatial distance and intensity values) lead to the edges of the graph. Based on this formalism, we extract the urban area using a novel multiple subgraph matching method. Then, we extract separate buildings in the urban area using a novel graph cut method. We form a diverse and representative test set using panchromatic 1-m-resolution Ikonos imagery. By extensive testings, we report very promising results on automatically detecting urban areas and buildings.",fullPaper,jv99
p1611,db9967871249ffd77adc0c0cc16154ccc37d09ad,c108,International Conference on Information Integration and Web-based Applications & Services,A property of eigenvectors of nonnegative symmetric matrices and its application to graph theory,Abstract content,poster,cp108
p1612,3375dc753685773f050392d1b5dfd9a4e3d20b96,c67,Enterprise Application Integration,Chemical Graph Theory,"INTRODUCTION. ELEMENTS OF GRAPH THEORY. The Definition of a Graph. Isomorphic Graphs and Graph Automorphism. Walks, Trails, Paths, Distances and Valencies in Graphs. Subgraphs. Regular Graphs. Trees. Planar Graphs. The Story of the Koenigsberg Bridge Problem and Eulerian Graphs. Hamiltonian Graphs. Line Graphs. Vertex Coloring of a Graph. CHEMICAL GRAPHS. The Concept of a Chemical Graph. Molecular Topology. Huckel Graphs. Polyhexes and Benzenoid Graphs. Weighted Graphs. GRAPH-THEORETICAL MATRICES. The Adjacency Matrix. The Distance Matrix. THE CHARACTERISTIC POLYNOMIAL OF A GRAPH. The Definition of the Characteristic Polynomial. The Method of Sachs for Computing the Characteristic Polynomial. The Characteristic Polynomials of Some Classes of Simple Graphs. The Le Verrier-Faddeev-Frame Method for Computing the Characteristic Polynomial. TOPOLOGICAL ASPECTS OF HUECKEL THEORY. Elements of Huckel Theory. Isomorphism of Huckel Theory and Graph Spectral Theory. The Huckel Spectrum. Charge Densities and Bond Orders in Conjugated Systems. The Two-Color Problem in Huckel Theory. Eigenvalues of Linear Polyenes. Eigenvalues of Annulenes. Eigenvalues of Moebius Annulenes. A Classification Scheme for Monocyclic Systems. Total p-Electron Energy. TOPOLOGICAL RESONANCE ENERGY. Huckel Resonance Energy. Dewar Resonance Energy. The Concept of Topological Resonance Energy. Computation of the Acyclic Polynomial. Applications of the TRE Model. ENUMERATION OF KEKULE VALENCE STRUCTURES. The Role of Kekule Valence Structures in Chemistry. The Identification of Kekule Systems. Methods for the Enumeration of Kekule Structures. The Concept of Parity of Kekule Structures. THE CONJUGATED-CIRCUIT MODEL. The Concept of Conjugated Circuits. The p-Resonance Energy Expression. Selection of the Parameters. Computational Procedure. Applications of the Conjugated-Circuit Model. Parity of Conjugated Circuits. TOPOLOGICAL INDICES. Definitions of Topological Indices. The Three-Dimensional Wiener Number. ISOMER ENUMERATION. The Cayley Generation Functions. The Henze-Blair Approach. The Polya Enumeration Method. The Enumeration Method Based on the N-Tuple Code.",poster,cp67
p1613,250748b4494cec56abd55ae049bdd38f4d42e5c8,j299,IEEE Transactions on Pattern Analysis and Machine Intelligence,An Optimal Graph Theoretic Approach to Data Clustering: Theory and Its Application to Image Segmentation,"A novel graph theoretic approach for data clustering is presented and its application to the image segmentation problem is demonstrated. The data to be clustered are represented by an undirected adjacency graph G with arc capacities assigned to reflect the similarity between the linked vertices. Clustering is achieved by removing arcs of G to form mutually exclusive subgraphs such that the largest inter-subgraph maximum flow is minimized. For graphs of moderate size ( approximately 2000 vertices), the optimal solution is obtained through partitioning a flow and cut equivalent tree of G, which can be efficiently constructed using the Gomory-Hu algorithm (1961). However for larger graphs this approach is impractical. New theorems for subgraph condensation are derived and are then used to develop a fast algorithm which hierarchically constructs and partitions a partially equivalent tree of much reduced size. This algorithm results in an optimal solution equivalent to that obtained by partitioning the complete equivalent tree and is able to handle very large graphs with several hundred thousand vertices. The new clustering algorithm is applied to the image segmentation problem. The segmentation is achieved by effectively searching for closed contours of edge elements (equivalent to minimum cuts in G), which consist mostly of strong edges, while rejecting contours containing isolated strong edges. This method is able to accurately locate region boundaries and at the same time guarantees the formation of closed edge contours. >",fullPaper,jv299
p1614,54cf44503876c4a0c4875aa6f0dff837f0e17f39,c0,International Conference on Human Factors in Computing Systems,Network Analysis of World Subway Systems Using Updated Graph Theory,"This paper demonstrates that network topologies play a key role in attracting people to use public transit; ridership is not solely determined by cultural characteristics (North American versus European versus Asian) or city design (transit oriented versus automobile oriented). The analysis considers 19 subway systems worldwide: those in Toronto, Ontario, Canada; Montreal, Quebec, Canada; Chicago, Illinois; New York City; Washington, D.C.; San Francisco, California; Mexico City, Mexico; London; Paris; Lyon, France; Madrid, Spain; Berlin; Athens, Greece; Stockholm, Sweden; Moscow; Tokyo; Osaka, Japan; Seoul, South Korea; and Singapore. The relationship between ridership and network design was studied by using updated graph theory concepts. Ridership was computed as the annual number of boardings per capita. Network design was measured according to three major indicators. The first is a measure of transit coverage and is based on the total number of stations and land area. The second relates to the maximum number of transfers necessary to go from one station to another and is called directness. The third attempts to get an overall view of transfer possibilities to travel in the network to appreciate a sense of mobility; it is termed connectivity. Multiple-regression analysis showed a strong relationship between these three indicators and ridership, achieving a goodness of fit (adjusted R2 value) of .725. The importance of network design is significant and should be considered in future public transportation projects.",poster,cp0
p1615,cb03d03377bba14607b0a5603bc90aa4c0650b78,j301,Genetic Epidemiology,Discovering genetic ancestry using spectral graph theory,"As one approach to uncovering the genetic underpinnings of complex disease, individuals are measured at a large number of genetic variants (usually SNPs) across the genome and these SNP genotypes are assessed for association with disease status. We propose a new statistical method called Spectral‐GEM for the analysis of genome‐wide association studies; the goal of Spectral‐GEM is to quantify the ancestry of the sample from such genotypic data. Ignoring structure due to differential ancestry can lead to an excess of spurious findings and reduce power. Ancestry is commonly estimated using the eigenvectors derived from principal component analysis (PCA). To develop an alternative to PCA we draw on connections between multidimensional scaling and spectral graph theory. Our approach, based on a spectral embedding derived from the normalized Laplacian of a graph, can produce more meaningful delineation of ancestry than by using PCA. Often the results from Spectral‐GEM are straightforward to interpret and therefore useful in association analysis. We illustrate the new algorithm with an analysis of the POPRES data [Nelson et al., 2008]. Genet. Epidemiol. 34:51–59, 2010. © 2009 Wiley‐Liss, Inc.",fullPaper,jv301
p1616,f443eec6541ca6fe3bcb0ceb9a39c181dd37febc,j280,Journal of Biological Chemistry,Insights into the Organization of Biochemical Regulatory Networks Using Graph Theory Analyses*,"Graph theory has been a valuable mathematical modeling tool to gain insights into the topological organization of biochemical networks. There are two types of insights that may be obtained by graph theory analyses. The first provides an overview of the global organization of biochemical networks; the second uses prior knowledge to place results from multivariate experiments, such as microarray data sets, in the context of known pathways and networks to infer regulation. Using graph analyses, biochemical networks are found to be scale-free and small-world, indicating that these networks contain hubs, which are proteins that interact with many other molecules. These hubs may interact with many different types of proteins at the same time and location or at different times and locations, resulting in diverse biological responses. Groups of components in networks are organized in recurring patterns termed network motifs such as feedback and feed-forward loops. Graph analysis revealed that negative feedback loops are less common and are present mostly in proximity to the membrane, whereas positive feedback loops are highly nested in an architecture that promotes dynamical stability. Cell signaling networks have multiple pathways from some input receptors and few from others. Such topology is reminiscent of a classification system. Signaling networks display a bow-tie structure indicative of funneling information from extracellular signals and then dispatching information from a few specific central intracellular signaling nexuses. These insights show that graph theory is a valuable tool for gaining an understanding of global regulatory features of biochemical networks.",fullPaper,jv280
p1617,6211974939c9e83dd332278af5645228a32ccf57,c38,"IEEE International Conference on Software Analysis, Evolution, and Reengineering",Discrete Mathematics and Graph Theory,"This comprehensive and self-contained text provides a thorough understanding of the concepts and applications of discrete mathematics and graph theory. It is written in such a manner that beginners can develop an interest in the subject. Besides providing the essentials, it also provides problem-solving techniques and develops the skill of how to think logically. Organized into two parts. The first part on discrete mathematics covers a wide range of topics such as predicate logic, recurrences, generating function, combinatorics, partially-ordered sets, lattices, Boolean algebra, finite state machines, finite fields, elementary number theory and discrete probability. The second part on graph theory covers planarity, colouring and partitioning, directed and algebraic graphs.",poster,cp38
p1618,3fdef60d2c7738739bd7b74f5a3f7c076826ed30,c72,Intelligent Systems in Molecular Biology,Spectral Graph Theory and Network Dependability,"The paper introduces methods of graph theory for ranking substations of an electric power grid. In particular, spectral graph theory is used and several ranking algorithms are described. The procedure is illustrated on a practical numerical example",poster,cp72
p1619,01e75d802876a71b89efbbe5f61dfac6575b12a9,c104,IEEE International Conference on Multimedia and Expo,Computational Discrete Mathematics: Combinatorics and Graph Theory with Mathematica ®,"With examples of all 450 functions in action plus tutorial text on the mathematics, this book is the definitive guide to Experimenting with Combinatorica, a widely used software package for teaching and research in discrete mathematics. Three interesting classes of exercises are provided--theorem/proof, programming exercises, and experimental explorations--ensuring great flexibility in teaching and learning the material. The Combinatorica user community ranges from students to engineers, researchers in mathematics, computer science, physics, economics, and the humanities. Recipient of the EDUCOM Higher Education Software Award, Combinatorica is included with every copy of the popular computer algebra system Mathematica.",poster,cp104
p1620,f81a943a22abc31009f5bafc8c6c966189623593,j168,Proceedings of the IEEE,Graph theory with applications to engineering and computer science,"Introductory Graph Theory with ApplicationsGraph Theory with ApplicationsResearch Topics in Graph Theory and Its ApplicationsChemical Graph TheoryMathematical Foundations and Applications of Graph EntropyGraph Theory with Applications to Engineering and Computer ScienceGraphs Theory and ApplicationsQuantitative Graph TheoryApplied Graph TheoryChemical Graph TheoryA First Course in Graph TheoryGraph TheoryGraph Theory with ApplicationsGraph Theory with ApplicationsSpectra of GraphsFuzzy Graph Theory with Applications to Human TraffickingApplications of Graph TheoryChemical Applications of Graph TheoryRecent Advancements in Graph TheoryA Textbook of Graph TheoryGraph Theory and Its Engineering ApplicationsGraph Theory, Combinatorics, and ApplicationsAdvanced Graph Theory and CombinatoricsTopics in Intersection Graph TheoryGraph Theory with Applications to Engineering and Computer ScienceGraph Theory and Its Applications, Second EditionHandbook of Research on Advanced Applications of Graph Theory in Modern SocietyGraph Theory with Applications to Algorithms and Computer ScienceGraph TheoryGraph Theory with Algorithms and its ApplicationsGraph TheoryGraph Theory with ApplicationsGraph Theory ApplicationsHandbook of Graph TheoryGraph Theory and Its Applications to Problems of SocietyBasic Graph Theory with ApplicationsTen",fullPaper,jv168
p1621,d52a75da5a3785753437bd0e74935692764cb157,c43,ACM Symposium on Applied Computing,Recent Results in the Theory of Graph Spectra,Abstract content,poster,cp43
p1622,08b24601e1a1db623f6cebf740c557b23809b509,c72,Intelligent Systems in Molecular Biology,Application of the graph theory and matrix methods to contractor ranking,Abstract content,poster,cp72
p1623,321fede71792a304d44d8143399309c5d88c73a1,c58,Australian Software Engineering Conference,Graph Theory and Its Applications,"INTRODUCTION TO GRAPH MODELS Graphs and Digraphs Common Families of Graphs Graph Modeling Applications Walks and Distance Paths, Cycles, and Trees Vertex and Edge Attributes: More Applications STRUCTURE AND REPRESENTATION Graph Isomorphism Revised! Automorphisms and Symmetry Moved and revised! Subgraphs Some Graph Operations Tests for Non-Isomorphism Matrix Representation More Graph Operations TREES Reorganized and revised! Characterizations and Properties of Trees Rooted Trees, Ordered Trees, and Binary Trees Binary-Tree Traversals Binary-Search Trees Huffman Trees and Optimal Prefix Codes Priority Trees Counting Labeled Trees: Prufer Encoding Counting Binary Trees: Catalan Recursion SPANNING TREES Reorganized and revised! Tree-Growing Depth-First and Breadth-First Search Minimum Spanning Trees and Shortest Paths Applications of Depth-First Search Cycles, Edge Cuts, and Spanning Trees Graphs and Vector Spaces Matroids and the Greedy Algorithm CONNECTIVITY Revised! Vertex- and Edge-Connectivity Constructing Reliable Networks Max-Min Duality and Menger's Theorems Block Decompositions OPTIMAL GRAPH TRAVERSALS Eulerian Trails and Tours DeBruijn Sequences and Postman Problems Hamiltonian Paths and Cycles Gray Codes and Traveling Salesman Problems PLANARITY AND KURATOWSKI'S THEOREM Reorganized and revised! Planar Drawings and Some Basic Surfaces Subdivision and Homeomorphism Extending Planar Drawings Kuratowski's Theorem Algebraic Tests for Planarity Planarity Algorithm Crossing Numbers and Thickness DRAWING GRAPHS AND MAPS Reorganized and revised! The Topology of Low Dimensions Higher-Order Surfaces Mathematical Model for Drawing Graphs Regular Maps on a Sphere Imbeddings on Higher-Order Surfaces Geometric Drawings of Graphs New! GRAPH COLORINGS Vertex-Colorings Map-Colorings Edge-Colorings Factorization New! MEASUREMENT AND MAPPINGS New Chapter! Distance in Graphs New! Domination in Graphs New! Bandwidth New! Intersection Graphs New! Linear Graph Mappings Moved and revised! Modeling Network Emulation Moved and revised! ANALYTIC GRAPH THEORY New Chapter! Ramsey Graph Theory New! Extremal Graph Theory New! Random Graphs New! SPECIAL DIGRAPH MODELS Reorganized and revised! Directed Paths and Mutual Reachability Digraphs as Models for Relations Tournaments Project Scheduling and Critical Paths Finding the Strong Components of a Digraph NETWORK FLOWS AND APPLICATIONS Flows and Cuts in Networks Solving the Maximum-Flow Problem Flows and Connectivity Matchings, Transversals, and Vertex Covers GRAPHICAL ENUMERATION Reorganized and revised! Automorphisms of Simple Graphs Graph Colorings and Symmetry Burnside's Lemma Cycle-Index Polynomial of a Permutation Group More Counting, Including Simple Graphs Polya-Burnside Enumeration ALGEBRAIC SPECIFICATION OF GRAPHS Cyclic Voltages Cayley Graphs and Regular Voltages Permutation Voltages Symmetric Graphs and Parallel Architectures Interconnection-Network Performance NON-PLANAR LAYOUTS Reorganized and revised! Representing Imbeddings by Rotations Genus Distribution of a Graph Voltage-Graph Specification of Graph Layouts Non KVL Imbedded Voltage Graphs Heawood Map-Coloring Problem APPENDIX Logic Fundamentals Relations and Functions Some Basic Combinatorics Algebraic Structures Algorithmic Complexity Supplementary Reading BIBLIOGRAPHY General Reading References SOLUTIONS AND HINTS New! INDEXES Index of Applications Index of Algorithms Index of Notations General Index",poster,cp58
p1624,75264a58faee7b29b72ff329951d7fd353649da8,c112,Very Large Data Bases Conference,On an extremal problem in graph theory,"G ( n;l ) will denote a graph of n vertices and l edges. Let f 0 ( n, k ) be the smallest integer such that there is a G ( n;f 0 (n, k )) in which for every set of k vertices there is a vertex joined to each of these. Thus for example f o = 3 since in a triangle each pair of vertices is joined to a third. It can readily be checked that f o = 5 (the extremal graph consists of a complete 4-gon with one edge removed). In general we will prove: Let n > k , and then f 0 ( n, k ) = f(n, k) .",poster,cp112
p1625,0124235f4d41c320bac26486ca929da69dc5de67,j13,IEEE Transactions on Signal Processing,Efficient Sampling Set Selection for Bandlimited Graph Signals Using Graph Spectral Proxies,"We study the problem of selecting the best sampling set for bandlimited reconstruction of signals on graphs. A frequency domain representation for graph signals can be defined using the eigenvectors and eigenvalues of variation operators that take into account the underlying graph connectivity. Smoothly varying signals defined on the nodes are of particular interest in various applications, and tend to be approximately bandlimited in the frequency basis. Sampling theory for graph signals deals with the problem of choosing the best subset of nodes for reconstructing a bandlimited signal from its samples. Most approaches to this problem require a computation of the frequency basis (i.e., the eigenvectors of the variation operator), followed by a search procedure using the basis elements. This can be impractical, in terms of storage and time complexity, for real datasets involving very large graphs. We circumvent this issue in our formulation by introducing quantities called graph spectral proxies, defined using the powers of the variation operator, in order to approximate the spectral content of graph signals. This allows us to formulate a direct sampling set selection approach that does not require the computation and storage of the basis elements. We show that our approach also provides stable reconstruction when the samples are noisy or when the original signal is only approximately bandlimited. Furthermore, the proposed approach is valid for any choice of the variation operator, thereby covering a wide range of graphs and applications. We demonstrate its effectiveness through various numerical experiments.",fullPaper,jv13
p1626,a02c8182213c3501962bc3214cf236beb85b1684,j103,Conservation Biology,A Graph‐Theory Framework for Evaluating Landscape Connectivity and Conservation Planning,"Abstract:  Connectivity of habitat patches is thought to be important for movement of genes, individuals, populations, and species over multiple temporal and spatial scales. We used graph theory to characterize multiple aspects of landscape connectivity in a habitat network in the North Carolina Piedmont (U.S.A).. We compared this landscape with simulated networks with known topology, resistance to disturbance, and rate of movement. We introduced graph measures such as compartmentalization and clustering, which can be used to identify locations on the landscape that may be especially resilient to human development or areas that may be most suitable for conservation. Our analyses indicated that for songbirds the Piedmont habitat network was well connected. Furthermore, the habitat network had commonalities with planar networks, which exhibit slow movement, and scale‐free networks, which are resistant to random disturbances. These results suggest that connectivity in the habitat network was high enough to prevent the negative consequences of isolation but not so high as to allow rapid spread of disease. Our graph‐theory framework provided insight into regional and emergent global network properties in an intuitive and visual way and allowed us to make inferences about rates and paths of species movements and vulnerability to disturbance. This approach can be applied easily to assessing habitat connectivity in any fragmented or patchy landscape.",fullPaper,jv103
p1627,c29287c9f51cb5adc2db7d62439d1a54a295debb,c15,International Conference on Conceptual Structures,Chromatic Graph Theory,"Beginning with the origin of the four color problem in 1852, the field of graph colorings has developed into one of the most popular areas of graph theory. Introducing graph theory with a coloring theme, Chromatic Graph Theory explores connections between major topics in graph theory and graph colorings as well as emerging topics. This self-contained book first presents various fundamentals of graph theory that lie outside of graph colorings, including basic terminology and results, trees and connectivity, Eulerian and Hamiltonian graphs, matchings and factorizations, and graph embeddings. The remainder of the text deals exclusively with graph colorings. It covers vertex colorings and bounds for the chromatic number, vertex colorings of graphs embedded on surfaces, and a variety of restricted vertex colorings. The authors also describe edge colorings, monochromatic and rainbow edge colorings, complete vertex colorings, several distinguishing vertex and edge colorings, and many distance-related vertex colorings. With historical, applied, and algorithmic discussions, this text offers a solid introduction to one of the most popular areas of graph theory.",poster,cp15
p1628,2b10bdea7bfe8061a47185d7525c998b8e446832,c55,Annual Workshop of the Psychology of Programming Interest Group,Graph Theory. An Algorithmic Approach,Abstract content,poster,cp55
p1629,d0363de26c5e11cd739e10617960ac88161f6916,c111,International Society for Music Information Retrieval Conference,Fundamentals of Algebraic Graph Transformation,Abstract content,poster,cp111
p1630,ca50b8cb4e34aeca0df18b060911ba28dd79ef90,c103,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,Graph theory and molecular orbitals,Abstract content,poster,cp103
p1631,f96289e50a9486e8d423e9d207e89777d8eed5ef,j302,"Journal of Speech, Language and Hearing Research",What can graph theory tell us about word learning and lexical retrieval?,"PURPOSE
Graph theory and the new science of networks provide a mathematically rigorous approach to examine the development and organization of complex systems. These tools were applied to the mental lexicon to examine the organization of words in the lexicon and to explore how that structure might influence the acquisition and retrieval of phonological word-forms.


METHOD
Pajek, a program for large network analysis and visualization (V. Batagelj & A. Mvrar, 1998), was used to examine several characteristics of a network derived from a computerized database of the adult lexicon. Nodes in the network represented words, and a link connected two nodes if the words were phonological neighbors.


RESULTS
The average path length and clustering coefficient suggest that the phonological network exhibits small-world characteristics. The degree distribution was fit better by an exponential rather than a power-law function. Finally, the network exhibited assortative mixing by degree. Some of these structural characteristics were also found in graphs that were formed by 2 simple stochastic processes suggesting that similar processes might influence the development of the lexicon.


CONCLUSIONS
The graph theoretic perspective may provide novel insights about the mental lexicon and lead to future studies that help us better understand language development and processing.",fullPaper,jv302
p1632,36d3fc0411bec5aadfba8087a33022f99658ccf4,c49,International Symposium on Search Based Software Engineering,Graph Theory and Interconnection Networks,"The advancement of large scale integrated circuit technology has enabled the construction of complex interconnection networks. Graph theory provides a fundamental tool for designing and analyzing such networks. Graph Theory and Interconnection Networks provides a thorough understanding of these interrelated topics. After a brief introduction to graph terminology, this book presents well-known interconnection networks as examples of graphs, followed by in-depth coverage of Hamiltonian graphs. Different types of problems illustrate the wide range of available methods for solving such problems. The text also explores recent progress on the diagnosability of graphs under various models.",poster,cp49
p1633,a7cc41a3ed77583b0cbf44ab3d175ca1e27ceff2,c81,IEEE Annual Symposium on Foundations of Computer Science,Spectral Graph Theory and its Applications,"Spectral graph theory is the study of the eigenvalues and eigenvectors of matrices associated with graphs. In this tutorial, we will try to provide some intuition as to why these eigenvectors and eigenvalues have combinatorial significance, and will sitn'ey some of their applications.",fullPaper,cp81
p1634,a83bfe8999ba5f6c4baceb03e7a212a9d3b18e00,c7,European Conference on Modelling and Simulation,Topics in Graph Theory: Graphs and Their Cartesian Product,"From specialists in the field, learn about interesting connections and recent developments in the field of graph theory by looking in particular at Cartesian products arguably the most important of the four standard graph products. Many new results in this area appear for the first time in print in this book. Written in an accessible way, this book can be used for personal study in advanced applications of graph theory or for an advanced graph theory course.",poster,cp7
p1635,fa1f95d61ab2ac44c9bfdf9b53a80d6617545b95,j303,Current protein and peptide science,Proteins as networks: usefulness of graph theory in protein science.,"The network paradigm is based on the derivation of emerging properties of studied systems by their representation as oriented graphs: any system is traced back to a set of nodes (its constituent elements) linked by edges (arcs) correspondent to the relations existing between the nodes. This allows for a straightforward quantitative formalization of systems by means of the computation of mathematical descriptors of such graphs (graph theory). The network paradigm is particularly useful when it is clear which elements of the modelled system must play the role of nodes and arcs respectively, and when topological constraints have a major role with respect to kinetic ones. In this review we demonstrate how nodes and arcs of protein topology are characterized at different levels of definition: 1. Recurrence matrix of hydrophobicity patterns along the sequence 2. Contact matrix of alpha carbons of 3D structures 3. Correlation matrix of motions of different portion of the molecule in molecular dynamics. These three conditions represent different but potentially correlated reticular systems that can be profitably analysed by means of network analysis tools.",fullPaper,jv303
p1636,7233d8f8d9a0aaaa98b395b998b6e82f2f44ca28,c77,Networks,GRAPH THEORY AND COMPLEX NETWORKS,"In the past ten years,the fast development of complex network theory has provided a good support for the study of complexity and complex systems,since they describe clearly the important characteristics of complex systems,and show bright prospects in theory and applications.This paper presents mainly the application of graph theory to complex networks,especially to the synchronization problem of complex networks First,its application to the estimations of smallest nonzero,largest eigenvalues and synchronizability index of certain graphs are commented,followed by the effects of subgraph and graph eigenvector in the estimation of synchronizability index.Furthermore,the complexity between the relationships of synchronizability and network structural parameters are discussed via two simple graphs,and the effects of complementary graph, edge-addition and graph operation on the synchronization of complex networks are elaborated.Finally,some possible development directions in complex networks are predicted from the viewpoint of graph and control theory.",poster,cp77
p1637,e71dc6f83f18f654824c00f66361d346aa896049,j304,Evolutionary Applications,Applications of graph theory to landscape genetics,"We investigated the relationships among landscape quality, gene flow, and population genetic structure of fishers (Martes pennanti) in ON, Canada. We used graph theory as an analytical framework considering each landscape as a network node. The 34 nodes were connected by 93 edges. Network structure was characterized by a higher level of clustering than expected by chance, a short mean path length connecting all pairs of nodes, and a resiliency to the loss of highly connected nodes. This suggests that alleles can be efficiently spread through the system and that extirpations and conservative harvest are not likely to affect their spread. Two measures of node centrality were negatively related to both the proportion of immigrants in a node and node snow depth. This suggests that central nodes are producers of emigrants, contain high‐quality habitat (i.e., deep snow can make locomotion energetically costly) and that fishers were migrating from high to low quality habitat. A method of community detection on networks delineated five genetic clusters of nodes suggesting cryptic population structure. Our analyses showed that network models can provide system‐level insight into the process of gene flow with implications for understanding how landscape alterations might affect population fitness and evolutionary potential.",fullPaper,jv304
p1638,17c4b0fe4eaf5d96053952a0ffb5ac0b1fbfbe3d,c1,Technical Symposium on Computer Science Education,A BIRD'S EYE VIEW OF THE CUT METHOD AND A SURVEY OF ITS APPLICATIONS IN CHEMICAL GRAPH THEORY,"A general description of the cut method is presented and an overview of its applications in chemical graph theory is given. Applications include the Wiener index, the Szeged index, the hyper-Wiener index, the PI index, the weighted Wiener index, Wiener-type indices, and classes of chemical graphs such as trees, benzenoid graphs and phenylenes. A computation of the Wiener index of an arbitrary connected graph using its canonical metric representation is described. Algorithmic issues are also briefly mentioned as well as are the recently introduced CI index and related polynomials.",poster,cp1
p1639,20980d595d9dbdd3f3ef47e5b072b65799f0525a,c99,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,From Graph Theory to Models of Economic Networks. A Tutorial.,Abstract content,poster,cp99
p1640,4f1e6ec98650287ead42b1f231831234385c5e6b,c47,International Symposium on Empirical Software Engineering and Measurement,Applying Graph Theory to Interaction Design,Abstract content,poster,cp47
p1641,de299e7d659f8ac9bebd348888ea57e845d2ce76,c50,International Conference on Automated Software Engineering,Graph Theory Based Algorithms for Water Distribution Network Sectorization Projects,"Water distribution network sectorization projects, in process in Mexican cities in the last two decades, consist in dividing the large interconnected city distribution network in smaller networks with one (or two in exceptional cases) supply points. Known water distribution network models are routinely applied to revise any proposed sectorization. For large network sectorization projects, however, additional algorithmic capabilities are needed, such as connectivity and source contribution analysis. This paper presents algorithms of that type, based on graph theory, for obtaining the number of independent sectors in a network layout, the set of nodes belonging to each sector, the set of disconnected nodes, and the source to node contribution. The algorithms are implemented in a computer AutoCAD-based system. Real sectorization project in two cities, where the proposed algorithms are applied, are then commented.",poster,cp50
p1642,bf187d5d6bf7b18d8ab19b01da796665d622c60c,j10,Chemical Reviews,Some new trends in chemical graph theory.,"Unidad de Investigación de Diseño de Farmacos y Conectividad Molecular, Departamento de Quı́mica Fisica, Facultad de Farmacı́a, Universitat de València, 46100 Burjassot, València, Spain, Instituto de Tecnologia Quimica, CSIC-Universidad Politecnica de Valencia, Av. de los Naranjos s/n, 46022 València, Spain, and Dipartimento di Chimica, Università della Calabria, via P. Bucci 14/C, 87036 Rende (CS), Italy",fullPaper,jv10
p1643,389409b9389b0c5b3ec6806b0419fe2bac6e33f5,c21,Grid Computing Environments,Algebraic Graph Theory,1. Introduction to algebraic graph theory Part I. Linear Algebra in Graphic Thoery: 2. The spectrum of a graph 3. Regular graphs and line graphs 4. Cycles and cuts 5. Spanning trees and associated structures 6. The tree-number 7. Determinant expansions 8. Vertex-partitions and the spectrum Part II. Colouring Problems: 9. The chromatic polynomial 10. Subgraph expansions 11. The multiplicative expansion 12. The induced subgraph expansion 13. The Tutte polynomial 14. Chromatic polynomials and spanning trees Part III. Symmetry and Regularity: 15. Automorphisms of graphs 16. Vertex-transitive graphs 17. Symmetric graphs 18. Symmetric graphs of degree three 19. The covering graph construction 20. Distance-transitive graphs 21. Feasibility of intersection arrays 22. Imprimitivity 23. Minimal regular graphs with given girth References Index.,poster,cp21
p1644,6b7fe2878d6c42079562f8adb57635dd2f2a2772,c1,Technical Symposium on Computer Science Education,Graph Theory: A Problem Oriented Approach,Preface A. Basic Concepts B. Isomorphic graphs C. Bipartite graphs D. Trees and forests E. Spanning tree algorithms F. Euler paths G. Hamilton paths and cycles H. Planar graphs I. Independence and covering J. Connections and obstructions K. Vertex coloring L. Edge coloring M. Matching theory for bipartite graphs N. Applications of matching theory O. Cycle-Free digraphs Answers to selected problems.,poster,cp1
p1645,7fc5859635b8779519698c33168256fd99ac3741,c16,Knowledge Discovery and Data Mining,"Algorithmic Graph Theory and Perfect Graphs (Annals of Discrete Mathematics, Vol 57)",Abstract content,poster,cp16
p1646,603c893660f04899f6b220795348f236d6896cf3,j305,Ecological Applications,Graph theory as a proxy for spatially explicit population models in conservation planning.,"Spatially explicit population models (SEPMs) are often considered the best way to predict and manage species distributions in spatially heterogeneous landscapes. However, they are computationally intensive and require extensive knowledge of species' biology and behavior, limiting their application in many cases. An alternative to SEPMs is graph theory, which has minimal data requirements and efficient algorithms. Although only recently introduced to landscape ecology, graph theory is well suited to ecological applications concerned with connectivity or movement. This paper compares the performance of graph theory to a SEPM in selecting important habitat patches for Wood Thrush (Hylocichla mustelina) conservation. We use both models to identify habitat patches that act as population sources and persistent patches and also use graph theory to identify patches that act as stepping stones for dispersal. Correlations of patch rankings were very high between the two models. In addition, graph theory offers the ability to identify patches that are very important to habitat connectivity and thus long-term population persistence across the landscape. We show that graph theory makes very similar predictions in most cases and in other cases offers insight not available from the SEPM, and we conclude that graph theory is a suitable and possibly preferable alternative to SEPMs for species conservation in heterogeneous landscapes.",fullPaper,jv305
p1647,d869bcd9c4c2eb29fb4d01906841d1d79d5c9cb6,c107,British Machine Vision Conference,AN EXTREMAL PROBLEM IN GRAPH THEORY,"G(?z; I) will denote a graph of n vertices and 1 edges. Let fO(lz, K) be the smallest integer such that there is a G (n; f,, (n, k)) in which for every set of K vertices there is a vertex joined to each of these. Thus for example fO(3, 2) = 3 since in a triangle each pair of vertices is joined to a third. It can readily be checked that f,(4, 2) = 5 (the extremal graph consists of a complete 4-gon with one edge removed). In general we will prove: Let n > k, and",poster,cp107
p1648,48ebab32e739e314cfcc6a8de9dd145b934aa5e6,j306,"Proteins: Structure, Function, and Bioinformatics",Protein flexibility predictions using graph theory,"Techniques from graph theory are applied to analyze the bond networks in proteins and identify the flexible and rigid regions. The bond network consists of distance constraints defined by the covalent and hydrogen bonds and salt bridges in the protein, identified by geometric and energetic criteria. We use an algorithm that counts the degrees of freedom within this constraint network and that identifies all the rigid and flexible substructures in the protein, including overconstrained regions (with more crosslinking bonds than are needed to rigidify the region) and underconstrained or flexible regions, in which dihedral bond rotations can occur. The number of extra constraints or remaining degrees of bond‐rotational freedom within a substructure quantifies its relative rigidity/flexibility and provides a flexibility index for each bond in the structure. This novel computational procedure, first used in the analysis of glassy materials, is approximately a million times faster than molecular dynamics simulations and captures the essential conformational flexibility of the protein main and side‐chains from analysis of a single, static three‐dimensional structure. This approach is demonstrated by comparison with experimental measures of flexibility for three proteins in which hinge and loop motion are essential for biological function: HIV protease, adenylate kinase, and dihydrofolate reductase. Proteins 2001;44:150–165. © 2001 Wiley‐Liss, Inc.",fullPaper,jv306
p1649,e764da56e8b4ba6bc21a80ec70b34a8a7b69de59,j307,IET Systems Biology,Graph theory and networks in Biology.,"A survey of the use of graph theoretical techniques in Biology is presented. In particular, recent work on identifying and modelling the structure of bio-molecular networks is discussed, as well as the application of centrality measures to interaction networks and research on the hierarchical structure of such networks and network motifs. Work on the link between structural network properties and dynamics is also described, with emphasis on synchronisation and disease propagation.",fullPaper,jv307
p1650,fd6432fa2b032dd5f246d26460bf3353c43c257a,j13,IEEE Transactions on Signal Processing,Discrete Signal Processing on Graphs: Sampling Theory,"We propose a sampling theory for signals that are supported on either directed or undirected graphs. The theory follows the same paradigm as classical sampling theory. We show that perfect recovery is possible for graph signals bandlimited under the graph Fourier transform. The sampled signal coefficients form a new graph signal, whose corresponding graph structure preserves the first-order difference of the original graph signal. For general graphs, an optimal sampling operator based on experimentally designed sampling is proposed to guarantee perfect recovery and robustness to noise; for graphs whose graph Fourier transforms are frames with maximal robustness to erasures as well as for Erdös-Rényi graphs, random sampling leads to perfect recovery with high probability. We further establish the connection to the sampling theory of finite discrete-time signal processing and previous work on signal recovery on graphs. To handle full-band graph signals, we propose a graph filter bank based on sampling theory on graphs. Finally, we apply the proposed sampling theory to semi-supervised classification of online blogs and digit images, where we achieve similar or better performance with fewer labeled samples compared to previous work.",fullPaper,jv13
p1651,f253627bfa7485c0e6e5e8f53a682d7149f0f790,j308,NeuroImage,Characterizing brain anatomical connections using diffusion weighted MRI and graph theory,Abstract content,fullPaper,jv308
p1652,995f4c79933bf2219c26a634207449f255ae7b6e,c72,Intelligent Systems in Molecular Biology,Bondary-connectivity via graph theory,"We generalize theorems of Kesten and Deuschel-Pisztora about the connectedness of the exterior boundary of a connected subset of $\Z^d$, where ""connectedness"" and ""boundary"" are understood with respect to various graphs on the vertices of $\Z^d$. We provide simple and elementary proofs of their results. It turns out that the proper way of viewing these questions is graph theory, instead of topology.",poster,cp72
p1653,125bb9e2f465f3fec6646ef286edc6d41074ca50,c26,PS,On Graph Theory and Its Application,"Graph theory has around 300 years of history,but many problems haven't been solved.With the development of computer science,graph theory becomes hot point again.In this paper,the application of graph theory is discussed.",poster,cp26
p1654,e24f2851d5cf4282239b4fd6a74a5d3bff7f8897,c39,International Conference on Global Software Engineering,Applied Graph Theory in Computer Vision and Pattern Recognition,Abstract content,poster,cp39
p1655,47199ebc578a0c80ecdafe496bd05b44140d6cdc,j309,Electronic Journal of Combinatorics,Extremal Graph Theory for Metric Dimension and Diameter,"A set of vertices $S$ resolves a connected graph $G$ if every vertex is uniquely determined by its vector of distances to the vertices in $S$. The metric dimension of $G$ is the minimum cardinality of a resolving set of $G$. Let ${\cal G}_{\beta,D}$ be the set of graphs with metric dimension $\beta$ and diameter $D$. It is well-known that the minimum order of a graph in ${\cal G}_{\beta,D}$ is exactly $\beta+D$. The first contribution of this paper is to characterise the graphs in ${\cal G}_{\beta,D}$ with order $\beta+D$ for all values of $\beta$ and $D$. Such a characterisation was previously only known for $D\leq2$ or $\beta\leq1$. The second contribution is to determine the maximum order of a graph in ${\cal G}_{\beta,D}$ for all values of $D$ and $\beta$. Only a weak upper bound was previously known.",fullPaper,jv309
p1656,d374ed80afc163f54b5fc4c63c445d120d44614f,c107,British Machine Vision Conference,Applications of graph theory,"Graph theory is rapidly moving into the mainstream of mathematics mainly because of its applications in diverse fields. In this paper, we discuss certain ways of applying graph theoretical techniques to solve various problems and present the review of some of the applications. (© 2008 WILEY‐VCH Verlag GmbH & Co. KGaA, Weinheim)",poster,cp107
p1657,448dc7527c4031086bcdf3d117e73bc163b64c5e,c21,Grid Computing Environments,The History of Degenerate (Bipartite) Extremal Graph Problems,Abstract content,poster,cp21
p1658,04eb71ccc65edf1c2b2f3b43bdb89a8e90fe8a8b,c55,Annual Workshop of the Psychology of Programming Interest Group,PEARLS in GRAPH THEORY,Abstract content,poster,cp55
p1659,04ff6d8d8708aef4ebc45ebee132fcc6f055bbd4,c13,International Conference on Data Science and Advanced Analytics,Modern temporal network theory: a colloquium,Abstract content,poster,cp13
p1660,5076d4a29581a986ec08e5c48cac0d6f3b2a1f3c,c91,Workshop on Algorithms and Models for the Web-Graph,Landscape connectivity: A conservation application of graph theory,"We use focal-species analysis to apply a graph-theoretic approach to landscape connectivity in the Coastal Plain of North Carolina. In doing so we demonstrate the utility of a mathematical graph as an ecological construct with respect to habitat connectivity. Graph theory is a well established mainstay of information technology and is concerned with highly efficient network flow. It employs fast algorithms and compact data structures that are easily adapted to landscape-level focal species analysis. American mink (Mustela vison) and prothonotary warblers (Protonotaria citrea) share the same habitat but have different dispersal capabilities, and therefore provide interesting comparisons on connections in the landscape. We built graphs using GIS coverages to define habitat patches and determined the functional distance between the patches with least-cost path modeling. Using graph operations concerned with edge and node removal we found that the landscape is fundamentally connected for mink and fundamentally unconnected for prothonotary warblers. The advantage of a graph-theoretic approach over other modeling techniques is that it is a heuristic framework which can be applied with very little data and improved from the initial results. We demonstrate the use of graph theory in a metapopulation context, and suggest that graph theory as applied to conservation biology can provide leverage on applications concerned with landscape connectivity.",poster,cp91
p1661,0b8a18c78e7493fe145382d98fc46f6b427a093f,c81,IEEE Annual Symposium on Foundations of Computer Science,"Graph Theory: Modeling, Applications, and Algorithms","Preface 1 Introduction to Graph Theory 2 Basic Concepts in Graph Theory 3 TreesandForests 4 Spanning Trees 5 Fundamental Properties of Graphs and Digraphs 6 Connectivity and Flow 7 Planar Graphs 8 Graph Coloring 9 Coloring Enumerations and Chordal Graphs 10 Independence,Dominance, and Matchings 11 Cover Parameters and MatchingPolynomials 12 GraphCounting 13 Graph Algorithms APPENDICES A Greek Alphabet B Notation C Top Ten Online References Index ix",poster,cp81
p1662,f6433be2ca69c760d51e9e68f7b2859e50422985,c45,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",A Walk Through Combinatorics: An Introduction to Enumeration and Graph Theory,"Basic Methods: Seven Is More Than Six. The Pigeon-Hole Principle One Step at a Time. The Method of Mathematical Induction Enumerative Combinatorics: There Are a Lot of Them. Elementary Counting Problems No Matter How You Slice It. The Binomial Theorem and Related Identities Divide and Conquer. Partitions Not So Vicious Cycles. Cycles in Permutations You Shall Not Overcount. The Sieve A Function is Worth Many Numbers. Generating Functions Graph Theory: Dots and Lines. The Origins of Graph Theory Staying Connected. Trees Finding a Good Match. Coloring and Matching Do Not Cross. Planar Graphs Horizons: Does It Clique? Ramsey Theory So Hard to Avoid. Subsequence Conditions on Permutations Who Knows What It Looks Like, but It Exists. The Probabilistic Method At Least Some Order. Partial Orders and Lattices The Sooner The Better. Combinatorial Algorithms Does Many Mean More Than One? Computational Complexity.",poster,cp45
p1663,fea1735f231f9c3cf27cc115b737a32614ff7376,c9,Pacific Symposium on Biocomputing,Combinatorics and Graph Theory,Abstract content,poster,cp9
p1664,0a99f890d1190443b0c8c02062363780d64dd907,c68,International Workshop on Software Engineering for Large-Scale Multi-Agent Systems,Complex Networks: from Graph Theory to Biology,Abstract content,poster,cp68
p1665,7d0d69eec235daf374e5a72a0831346e27fd1cae,c82,Workshop on Interdisciplinary Software Engineering Research,Application of graph theory to OO software engineering,"Graph Theory, which studies the properties of graphs, has been widely accepted as a core subject in the knowledge of computer scientists. So is Object-Oriented (OO) software engineering, which deals with the analysis, design and implementation of systems employing classes as modules. The latter field can greatly benefit from the application of Graph Theory, since the main mode of representation, namely the class diagram, is essentially a directed graph. The study of graph properties can be valuable in many ways for understanding the characteristics of the underlying software systems. Representative examples for the usefulness of graph theory on OO systems based on recent research results are presented in this paper.",fullPaper,cp82
p1666,9876dd16796ad425723a56d77146eeae0ccf9c51,c83,International Conference on Computer Graphics and Interactive Techniques,Graph theory,"rHYtHMs | slow art Art & Design Galleries 95 This animation was a happy accident. I was interested in seeing what kinds of shapes could be generated by simple L-systems, so I coded a Python/PyQt application to iterate the replacement rule and draw the resulting string using the typical turtle geometry interpretation. I quickly saw that changing the angle parameter about a degree could produce very different shapes, and that haphazard exploration of the angle parameter might miss interesting shapes. So I added an animation feature to vary the angle systematically and save out the resulting frames. By trial and error, I settled on .01 degree as an increment that produced a small but noticeable change in the picture. I rendered the 18,000 frames into a movie for review and was surprised to find that at 30 frames per second the animation was interesting and fun to watch for 10 minutes, and that the eye could easily pick out single-frame shapes that differed substantially from the neighboring frames. FFDavid Gladstein",fullPaper,cp83
p1667,7f7227a16c26b6f90693a2ec8ae2270e272eb548,c63,IEEE International Software Metrics Symposium,Metric graph theory and geometry: a survey,"The article surveys structural characterizations of several graph classes defined by distance properties, which have in part a general algebraic flavor and can be interpreted as subdirect decomposition. The graphs we feature in the first place are the median graphs and their various kinds of generalizations, e.g., weakly modular graphs, or fiber-complemented graphs, or l1-graphs. Several kinds of l1-graphs admit natural geometric realizations as polyhedral complexes. Particular instances of these graphs also occur in other geometric contexts, for example, as dual polar graphs, basis graphs of (even ∆-)matroids, tope graphs, lopsided sets, or plane graphs with vertex degrees and face sizes bounded from below. Several other classes of graphs, e.g., Helly graphs (as injective objects), or bridged graphs (generalizing chordal graphs), or tree-like graphs such as distance-hereditary graphs occur in the investigation of graphs satisfying some basic properties of the distance function, such as the Helly property for balls, or the convexity of balls or of the neighborhoods of convex sets, etc. Operators between graphs or complexes relate some of the graph classes reported in this survey.",poster,cp63
p1668,616e3dce90c8c261dc51c2f8db6306e0c634f518,c40,IEEE International Conference on Software Maintenance and Evolution,ON A PROBLEM OF GRAPH THEORY,"Let G "" be a non-directed graph having n vertices, without parallel edges and slings. Let the vertices of Gn be denoted by F 1 ,. . ., Pn. Let v(P j) denote the valency of the point P i and put (0. 1) V(G,) = max v(Pj). 1ninn Let E(G.) denote the number of edges of Gn. Let H d (n, k) denote the set of all graphs Gn for which V (G n) = k and the diameter D (Gn) of which is-d, In the present paper we shall investigate the quantity (0 .2) Thus we want to determine the minimal number N such that there exists a graph having n vertices, N edges and diameter-d and the maximum of the valencies of the vertices of the graph is equal to k. To help the understanding of the problem let us consider the following interpretation. Let be given in a country n airports ; suppose we want to plan a network of direct flights between these airports so that the maximal number of airports to which a given airport can be connected by a direct flight should be equal to k (i .e. the maximum of the capacities of the airports is prescribed), further it should be possible to fly from every airport to any other by changing the plane at most d-1 times ; what is the minimal number of flights by which such a plan can be realized? For instance, if n = 7, k = 3, d= 2 we have F2 (7, 3) = 9 and the extremal graph is shown by Fig. 1. The problem of determining Fd (n, k) has been proposed and discussed recently by two of the authors (see [1]). In § 1 we give a short summary of the results of the paper [1], while in § 2 and 3 we give some new results which go beyond those of [1]. Incidentally we solve a long-standing problem about the maximal number of edges of a graph not containing a cycle of length 4. In § 4 we mention some unsolved problems. Let us mention that our problem can be formulated also in terms of 0-1 matrices as follows : Let M=(a il) be a symmetrical n by n zero-one matrix such 2",poster,cp40
p1669,fd3388199757c081f45ec4c02b25cdab84076935,c111,International Society for Music Information Retrieval Conference,Graph Theory for Rule-Based Modeling of Biochemical Networks,Abstract content,poster,cp111
p1670,0f6d06e5e321698682c31ee07e25d76d03be6e52,c46,Brazilian Symposium on Software Engineering,A material selection model using graph theory and matrix approach,Abstract content,poster,cp46
p1671,cdb27b16d90d6fd01095f76988b48e26f621fc3b,c78,Neural Information Processing Systems,Algorithmic graph theory,"Introduction to graph theory algorithmic techniques shortest paths trees and acyclic diagraphs depth first search connectivity and routing graph colouring covers, domination, independent sets, matchings and factors, parallel algorithms computational complexity.",poster,cp78
p1672,cf87a3e897af7910f23b8cdf1b89110b88302526,c25,International Conference on Contemporary Computing,Introduction to Graph Theory,"Introduction * Definitions and examples* Paths and cycles* Trees* Planarity* Colouring graphs* Matching, marriage and Menger's theorem* Matroids Appendix 1: Algorithms Appendix 2: Table of numbers List of symbols Bibliography Solutions to selected exercises Index",poster,cp25
p1673,ced040290fa21c51c43a5be0eb2bbd5f2e59d54b,c112,Very Large Data Bases Conference,Implementing discrete mathematics - combinatorics and graph theory with Mathematica,"Permutations and Combinations Permutations Permutation Groups Inversions and Inversion Vectors Special Classes of Permutations Combinations Exercises and Research Problems * Partitions, Compositions, and Young Tableaux Partitions Compositions Young Tableaux Exercises and Research Problems * Representing Graphs Data Structures for Graphs Elementary Graph Operations Graph Embeddings Storage Formats Exercises and Research Problems * Generating Graphs Regular Structures Trees Random Graphs Relations and Functional Graphs Exercises and Research Problems * Properties of Graphs Connectivity Graph Isomorphism Cycles in Graphs Partial Orders Graph Coloring Cliques, Vertex Covers, and Independent Sets Exercises and Research Problems * Algorithmic Graph Theory Shortest Paths Minimum Spanning Trees Network Flow Matching Planar Graphs Exercises and Research Problems",poster,cp112
p1674,6d04cd7f8d326cb76c5ad15a4f1d006c171d42bf,c12,International Conference on Statistical and Scientific Database Management,Graph theory and its applications,"In this paper we will discuss how problems like Page ranking and finding the shortest paths can be solved by using Graph Theory. At its core, graph theory is the study of graphs as mathematical structures. In our paper, we will first cover Graph Theory as a broad topic. Then we will move on to Linear Algebra. Linear Algebra is the study of matrices. We will apply the skills discussed in these two sections to Dijkstra Algorithms which cover how to find the shortest paths in graphs. Finally, we will present PageRank where we will demonstrate how to rank pages based on their importance.",poster,cp12
p1675,37c1011ac59b0b4685b3dc8a9dda4d8f6e68f6c2,c58,Australian Software Engineering Conference,A textbook of graph theory,Basic Results.- Directed Graphs.- Connectivity.- Trees.- Independent Sets and Matchings.- Eulerian and Hamiltonian Graphs.- Graph Colourings.- Planarity.- Triangulated Graphs.- Applications.,poster,cp58
p1676,afd944ef42b760a62103bf35a86d24a8fe67c6ad,c58,Australian Software Engineering Conference,Szemeredi''s Regularity Lemma and its applications in graph theory,"Szemer\''edi''s Regularity Lemma is an important tool in discrete mathematics. It says that, in some sense, all graphs can be approximated by random-looking graphs. Therefore the lemma helps in proving theorems for arbitrary graphs whenever the corresponding result is easy for random graphs. Recently quite a few new results were obtained by using the Regularity Lemma, and also some new variants and generalizations appeared. In this survey we describe some typical applications and some generalizations.",poster,cp58
p1677,32b8b8033af70a2e52b6fe680780a712c24a31ee,c56,European Conference on Software Process Improvement,Introduction to Graph Theory,"Written by one of the leading authors in the field, this text provides a student-friendly approach to graph theory for undergraduates. Much care has been given to present the material at the most effective level for students taking a first course in graph theory. Gary Chartrand and Ping Zhang's lively and engaging style, historical emphasis, unique examples and clearly-written proof techniques make it a sound yet accessible text that stimulates interest in an evolving subject and exploration in its many applications. This text is part of the Walter Rudin Student Series in Advanced Mathematics.",poster,cp56
p1678,d141ad437d2b932d647cf16b81b8531645a4cd6c,c76,International Conference on Artificial Neural Networks,Applied and algorithmic graph theory,"Designed as the bridge to cross the widening gap between mathematics and computer science, and planned as the mathematical base for computer science students, this maths text is written for upper-level college students who have had previous coursework involving proofs and proof techniques. The close tie between the theoretical and algorithmic aspects of graph theory, and graphs that lend themselves naturally as models in computer science, results in a need for efficient algorithims to solve any large scale problems. Each algorithm in the text includes explanatory statements that clarify individual steps, a worst-case complexity analysis, and algorithmic correctness proofs. As a result, the student will develop an understanding of the concept of an efficient algorithm.",poster,cp76
p1679,afb562a7910b7c7be22fc0638d2d4951a5ff7b36,c22,International Conference on Data Technologies and Applications,Topics in algebraic graph theory,Foreword Peter J. Cameron Introduction 1. Eigenvalues of graphs Michael Doob 2. Graphs and matrices Richard A. Brualdi and Bryan L. Shader 3. Spectral graph theory Dragos Cvetkovic and Peter Rowlinson 4. Graph Laplacians Bojan Mohar 5. Automorphism groups Peter J. Cameron 6. Cayley graphs Brian Alspach 7. Finite symmetric graphs Cheryle E. Praeger 8. Strongly regular graphs Peter J. Cameron 9. Distance-transitive graphs Arjeh M. Cohen 10. Computing with graphs and groups Leonard H. Soicher.,poster,cp22
p1680,b7bf5e535ea38a781268b4580265a96cfc7da7c2,c50,International Conference on Automated Software Engineering,A LIMIT THEOREM IN GRAPH THEORY,"In this paper G(n ; I) will denote a graph of n vertices and l edges, K„ will denote the complete graph of p vertices G (p ; (PA and K,(p i , . . ., p,) will denote the rchromatic graph with p i vertices of the i-th colour, in which every two vertices of different colour are adjacent . 7r(G) will denote the number of vertices of G and v(G) denotes the number of edges of G . G(n :1) denotes the complementary graph of G(n : l) i . e. G(n ; 1) is the G (ii : (211) -/) which has the samevertices as G(n ; 1)",poster,cp50
p1681,84ba27f5ed15a9751aaac63d0d5068034351da0a,c35,EUROMICRO Conference on Software Engineering and Advanced Applications,SPECTRAL GRAPH THEORY AND THE INVERSE EIGENVALUE PROBLEM OF A GRAPH,"Spectral Graph Theory is the study of the spectra of certain matrices defined from a given graph, including the adjacency matrix, the Laplacian matrix and other related matrices. Graph spectra have been studied extensively for more than fifty years. In the last fifteen years, interest has developed in the study of generalized Laplacian matrices of a graph, that is, real symmetric matrices with negative off-diagonal entries in the positions described by the edges of the graph (and zero in every other off-diagonal position). The set of all real symmetric matrices having nonzero off-diagonal entries exactly where the graph G has edges is denoted by S(G). Given a graph G, the problem of characterizing the possible spectra of B, such that B ∈S (G), has been referred to as the Inverse Eigenvalue Problem of a Graph .I n the last fifteen years a number of papers on this problem have appeared, primarily concerning trees. The adjacency matrix and Laplacian matrix of G and their normalized forms are all in S(G). Recent work on generalized Laplacians and Colin de Verdiere matrices is bringing the two areas closer together. This paper surveys results in Spectral Graph Theory and the Inverse Eigenvalue Problem of a Graph, examines the connections between these problems, and presents some new results on construction of a matrix of minimum rank for a given graph having a special form such as a 0,1-matrix or a generalized Laplacian.",poster,cp35
p1682,5dde7192c840ae6f348008bd0fa0a652306da145,c111,International Society for Music Information Retrieval Conference,GRAPH THEORY AND PROBABILITY,"A well-known theorem of Ramsay (8; 9) states that to every n there exists a smallest integer g(n) so that every graph of g(n) vertices contains either a set of n independent points or a complete graph of order n, but there exists a graph of g(n) 1 vertices which does not contain a complete subgraph of n vertices and also does not contain a set of n independent points. (A graph is called complete if every two of its vertices are connected by an edge; a set of points is called independent if no two of its points are connected by an edge.) The determination of g(n) seems a very difficult problem; the best inequalities for g(lz) are (3)",poster,cp111
p1683,bfb523f12a2d3b7b3cc4fa11effd4de0a7ff998f,c77,Networks,Graph Theory Methods for the Analysis of Neural Connectivity Patterns,Abstract content,poster,cp77
p1684,c8a9622e6cde873c680d1888e957837553672e89,j214,Journal of chemical information and computer sciences,Applications of graph theory in chemistry,"Graph theoretical (GT) applications in chemistry underwent a dramatic revival lately. Constitutional (molecular) graphs have points (vertices) representing atoms and lines (edges) symbolizing malent bonds. This review deals with definition. enumeration. and systematic coding or nomenclature of constitutional or steric isomers, valence isomers (especially of annulenes). and condensed polycyclic aromatic hydrocarbons. A few key applications of graph theory in theoretical chemistry are pointed out. The complete set of all poasible monocyclic aromatic and heteroaromatic compounds may be explored by a mmbination of Pauli's principle, P6lya's theorem. and electronegativities. Topological indica and some of their applications are reviewed. Reaction graphs and synthon graphs differ from constitutional graphs i n their meaning of vertices and edges and find other kinds of chemical applications. This paper ends with a review of the use of GT applications for chemical nomenclature (nodal nomenclature and related areas), coding. and information processing/storage/retrieval",fullPaper,jv214
p1685,67fbb295f46f4d630fd76f6bee13f0b985216bce,c69,International Conference on Parallel Processing,PROTEIN STRUCTURE: INSIGHTS FROM GRAPH THEORY,"The sequence and structure of a large body of proteins are becoming increasingly available. It is desirable to explore mathematical tools for ecient extraction of information from such sources. The principles of graph theory, which was earlier applied in elds such as electrical engineering and computer networks are now being adopted to investigate protein structure, folding, stability, function and dynamics. This review deals with a brief account of relevant graphs and graph theoretic concepts. The concepts of protein graph construction are discussed. The manner in which graphs are analyzed and parameters relevant to protein structure are extracted, are explained. The structural and biological information derived from protein structures using these methods is presented.",poster,cp69
p1686,d326d75238945047fcaa7401eeb87cb1b400ecd0,c51,Conference of the Centre for Advanced Studies on Collaborative Research,Power transfer allocation for open access using graph theory-fundamentals and applications in systems without loopflow,"In this paper, graph theory is used to calculate the contributions of individual generators and loads to line flows and the real power transfer between individual generators and loads that are significant to transmission open access. Related lemmas are proved which present necessary conditions required by the method. Based on AC load flow solution a novel method is suggested which can decide downstream and upstream power flow tracing paths very fast and can calculate the contribution factors of generations and loads to the line flows efficiently. The power transfer between generators and loads can also be determined. The suggested method is suitable for both active and reactive power tracings of real power systems.",poster,cp51
p1687,c091d3b62283cecb205f5e4af63e677166bf281f,c89,Conference on Uncertainty in Artificial Intelligence,Dynamic Modelling of Mechatronic Multibody Systems With Symbolic Computing and Linear Graph Theory,"The application of linear graph theory to the modelling of flexible multibody systems is described. When combined with symbolic computing methods, linear graph theory leads to efficient dynamic models that facilitate real-time simulation of systems of rigid bodies and flexible beams. The natural extension of linear graphs to the modelling of mechatronic multibody systems is presented, along with a recently-developed theory for building complex system models from models of individual subsystems.",poster,cp89
p1688,92a46efd598cd478bccddacd8703a254bfc885b0,c32,International Conference on Software Technology: Methods and Tools,Model structure analysis through graph theory: partition heuristics and feedback structure decomposition,"The argument of this article is that it is possible to focus on the structural complexity of system dynamics models to design a partition strategy that maximizes the test points between the model and the real world, and a calibration sequence that permits an incremental development of model confidence. It further argues that graph theory could be used as a basis for making sense of the structural complexity of system dynamics models, and that this structure could be used as a basis for more formal analysis of dynamic complexity. After reviewing the graph representation of system structure, the article presents the rationale and algorithms for model partitions based on data availability and structural characteristics. Special attention is given to the decomposition of cycle partitions that contain all the model’s feedback loops, and a unique and granular representation of feedback complexity is derived. The article concludes by identifying future research avenues in this arena. Copyright © 2004 John Wiley & Sons, Ltd.",poster,cp32
p1689,6a4e2850ac32e258fcf23c2d7d8cf1b42fccae15,c49,International Symposium on Search Based Software Engineering,SOME UNSOLVED PROBLEMS IN GRAPH THEORY,CONTENTSIntroduction § 1. Fundamental concepts § 2. Isomorphism problems § 3. Metric questions § 4. Thickness and genus of graph § 5. Colouring problems § 6. Parts with given propertiesReferences,poster,cp49
p1690,a0894388f6dfc841c68d2458a390d924a9c0db0f,c20,Research Conference of the South African Institute of Computer Scientists and Information Technologists,A graph theory interpretation of nodal regions,Abstract content,poster,cp20
p1691,ef83866e2aa9e807bb4d3f07456fd9ec33b79f8b,c82,Workshop on Interdisciplinary Software Engineering Research,Graph Theory 1736-1936,1. Oaths 2. Circuits 3. Trees 4. Chemical graphs 5. Euler's polyhedral formula 6. The four-colour problem - early history 7. Colouring maps on surfaces 8. Ideas from algebra and topology 9. The four-colour problem - to 1936 10. The factorization of graphs Appendix 1: Graph theory since 1936 Appendix 2: Bibliographical notes Appendix 3: Bibliography: 1736-1936,poster,cp82
p1692,abdf8623c1e5d8d2fd234a255a1ca6c32c8d6be7,c36,Conference on Software Engineering Education and Training,Algorithmic Graph Theory,"Preface 1. Introducing graphs and algorithmic complexity 2. Spanning-trees, branchings and connectivity 3. Planar graphs 4. Networks and flows 5. Matchings 6. Eulerian and Hamiltonian tours 7. Colouring graphs 8. Graph problems and intractability Appendix Author Index Subject Index.",poster,cp36
p1693,4a6d22c23c3afef6acfb0fc28fae030004514791,j214,Journal of chemical information and computer sciences,Application of Chemical Graph Theory for Automated Mechanism Generation,"We present an application of the chemical graph theory approach for generating elementary reactions of complex systems. Molecular species are naturally represented by graphs, which are identified by their vertices and edges where vertices are atom types and edges are bonds. The mechanism is generated using a set of reaction patterns (sub-graphs). These subgraphs are the internal representations for a given class of reaction thus allowing for the possibility of eliminating unimportant product species a priori. Furthermore, each molecule is canonically represented by a set of topological indices (Connectivity Index, Balaban Index, Schulz TI Index, WID Index, etc.) and thus eliminates the probability for regenerating the same species twice. Theoretical background and test cases on combustion of hydrocarbons are presented.",fullPaper,jv214
p1694,1b62915056c5b768db22b47684eea15bce0aa450,c40,IEEE International Conference on Software Maintenance and Evolution,"Topology optimization of trusses using genetic algorithm, force method and graph theory","In this article size/topology optimization of trusses is performed using a genetic algorithm (GA), the force method and some concepts of graph theory. One of the main difficulties with optimization with a GA is that the parameters involved are not completely known and the number of operations needed is often quite high. Application of some concepts of the force method, together with theory of graphs, make the generation of a suitable initial population well‐matched with critical paths for the transformation of internal forces feasible. In the process of optimization generated topologically unstable trusses are identified without any matrix manipulation and highly penalized. Identifying a suitable range for the cross‐section of each member for the ground structure in the list of profiles, the length of the substrings representing the cross‐sectional design variables are reduced. Using a contraction algorithm, the length of the strings is further reduced and a GA is performed in a smaller domain of design space. The above process is accompanied by efficient methods for selection, and by using a suitable penalty function in order to reduce the number of numerical operations and to increase the speed of the optimization toward a global optimum. The efficiency of the present method is illustrated using some examples, and compared to those of previous studies. Copyright © 2003 John Wiley & Sons, Ltd.",poster,cp40
p1695,19b108c083db1e78d2d7f44ededa55b44e360a2b,j299,IEEE Transactions on Pattern Analysis and Machine Intelligence,Pattern Vectors from Algebraic Graph Theory,"Graph structures have proven computationally cumbersome for pattern analysis. The reason for this is that, before graphs can be converted to pattern vectors, correspondences must be established between the nodes of structures which are potentially of different size. To overcome this problem, in this paper, we turn to the spectral decomposition of the Laplacian matrix. We show how the elements of the spectral matrix for the Laplacian can be used to construct symmetric polynomials that are permutation invariants. The coefficients of these polynomials can be used as graph features which can be encoded in a vectorial manner. We extend this representation to graphs in which there are unary attributes on the nodes and binary attributes on the edges by using the spectral decomposition of a Hermitian property matrix that can be viewed as a complex analogue of the Laplacian. To embed the graphs in a pattern space, we explore whether the vectors of invariants can be embedded in a low-dimensional space using a number of alternative strategies, including principal components analysis (PCA), multidimensional scaling (MDS), and locality preserving projection (LPP). Experimentally, we demonstrate that the embeddings result in well-defined graph clusters. Our experiments with the spectral representation involve both synthetic and real-world data. The experiments with synthetic data demonstrate that the distances between spectral feature vectors can be used to discriminate between graphs on the basis of their structure. The real-world experiments show that the method can be used to locate clusters of graphs.",fullPaper,jv299
p1696,da4ab623477d041c1944a66808cf38740e913a3f,c83,International Conference on Computer Graphics and Interactive Techniques,Combinatorics and graph theory,"Graph Theory: Introductory Concepts.- Trees.- Planarity.- Colorings.- Matchings.- Ramsey Theory.- References Combinatorics: Three Basic Problems.- Binomial Coefficients.- The Principle of Inclusion and Exclusion.- Generating Functions.- Polya's Theory of Counting.- More Numbers.- Stable Marriage.- References Infinite Combinatorics and Graph Theory: Pigeons and Trees.- Ramsey Revisited.- ZFC.- The Return of der Koenig.- Ordinals, Cardinals, and Many Pigeons.- Incompleteness and Coardinals.- Weakly Compact Cardinals.- Finite Combinatorics with Infinite Consequences.- Points of Departure.- References.",poster,cp83
p1697,8a18f6b55267c844060652ed38ffe49bfef9be88,c13,International Conference on Data Science and Advanced Analytics,The Foundations of Topological Graph Theory,Abstract content,poster,cp13
p1698,eb44563de6a8fcc1ba53a913d83dc0a5938ce627,c14,International Conference on Exploring Services Science,Chemical Graph Theory: Introduction and Fundamentals,The Origins of Chemical Graph Theory Elements of Graph Theory for Chemists Nomenclature of Chemical Compounds Polynomials in Graph Theory Enumeration of Isomers Graph Theory and Molecular Orbitals,poster,cp14
p1699,d0eb57dc62688054d62f60117394288b5b9ef11f,c31,International Conference on Evaluation & Assessment in Software Engineering,Application of graph theory to the synchronization in an array of coupled nonlinear oscillators,"In this letter, we show how algebraic graph theory can be used to derive sufficient conditions for an array of resistively coupled nonlinear oscillators to synchronize. These conditions are derived from the connectivity graph, which describes how the oscillators are connected. In particular, we show how such a sufficient condition is dependent on the algebraic connectivity of the connectivity graph. Intuition tells us that if the oscillators are more ""closely connected"" to each other, then they are more likely to synchronize. We discuss how to quantify connectedness in graph-theoretical terms and its relation to algebraic connectivity and show that our results are in accordance with this intuition. We also give an upper bound on the coupling conductance required for synchronization for arbitrary graphs, which is in the order of n/sup 2/, where n is the number of oscillators. >",poster,cp31
p1700,8ea2e1fc1fe8367627d868980a9f6bc4ca88d57a,c64,Experimental Software Engineering Network,On the use of linear graph theory in multibody system dynamics,Abstract content,poster,cp64
p1701,da4ab623477d041c1944a66808cf38740e913a3f,c84,The Web Conference,Combinatorics and graph theory,"Graph Theory: Introductory Concepts.- Trees.- Planarity.- Colorings.- Matchings.- Ramsey Theory.- References Combinatorics: Three Basic Problems.- Binomial Coefficients.- The Principle of Inclusion and Exclusion.- Generating Functions.- Polya's Theory of Counting.- More Numbers.- Stable Marriage.- References Infinite Combinatorics and Graph Theory: Pigeons and Trees.- Ramsey Revisited.- ZFC.- The Return of der Koenig.- Ordinals, Cardinals, and Many Pigeons.- Incompleteness and Coardinals.- Weakly Compact Cardinals.- Finite Combinatorics with Infinite Consequences.- Points of Departure.- References.",poster,cp84
p1702,d515c6e8488a7adbea32727bc436f723f63a2d82,c26,PS,Introduction to Graph Theory,"In graph theory, the term graph refers to a set of vertices and a set of edges. A vertex can be used to represent any object. Graphs may contain undirected or directed edges. An undirected edge is a set of two vertices. A directed edge is an ordered pair of two vertices where the edge goes from the first vertex to the second vertex. Graphs that contain directed edges are called directed graphs or digraphs.",poster,cp26
p1703,188c8889b26451ef6e0262e939373a946965ef62,c104,IEEE International Conference on Multimedia and Expo,Application of Graph Theory: Relationship of Eccentric Connectivity Index and Wiener's Index with Anti-inflammatory Activity,"Abstract Graph theory was successfully applied in developing a relationship between chemical structure and biological activity. The relationship of two graph invariants—the eccentric connectivity index and the Wiener's index was investigated with regard to anti-inflammatory activity, for a data set consisting of 76 pyrazole carboxylic acid hydrazide analogues. The values of the eccentric connectivity index and the Wiener's index of each analogue in the data set were computed and active ranges were identified. Subsequently, each analogue was assigned a biological activity that was compared with the anti-inflammatory activity reported as percent reduction in paw swelling. Prediction with an accuracy of ∼90% was obtained using the eccentric connectivity index as compared to 84% in the case of Wiener's index.",poster,cp104
p1704,73799d9f998feb583c9e2a0ea42f419952a41f04,c22,International Conference on Data Technologies and Applications,On the ground states of the frustration model of a spin glass by a matching method of graph theory,"The ground states of a quenched random Ising spin system with variable concentration of mixed nearest-neighbour exchange couplings +or-J on a square lattice (frustration model) are studied by a new method of graph theory. The search for ground states is mapped into the problem of perfect matching of minimum weight in the graph of frustrated plaquettes, a problem which can be solved by the algorithm of Edmonds. A pedestrian presentation of this elaborated algorithm is given with a discussion of the condition of validity.",poster,cp22
p1705,c6fe0d808c37848ce661400a6cba5adee07304ac,c30,IEEE Aerospace Conference,Dynamics of Flexible Multibody Systems Using Virtual Work and Linear Graph Theory,Abstract content,poster,cp30
p1706,d0eb57dc62688054d62f60117394288b5b9ef11f,c21,Grid Computing Environments,Application of graph theory to the synchronization in an array of coupled nonlinear oscillators,"In this letter, we show how algebraic graph theory can be used to derive sufficient conditions for an array of resistively coupled nonlinear oscillators to synchronize. These conditions are derived from the connectivity graph, which describes how the oscillators are connected. In particular, we show how such a sufficient condition is dependent on the algebraic connectivity of the connectivity graph. Intuition tells us that if the oscillators are more ""closely connected"" to each other, then they are more likely to synchronize. We discuss how to quantify connectedness in graph-theoretical terms and its relation to algebraic connectivity and show that our results are in accordance with this intuition. We also give an upper bound on the coupling conductance required for synchronization for arbitrary graphs, which is in the order of n/sup 2/, where n is the number of oscillators. >",poster,cp21
p1707,3f558ea2e9a0aab2e247b3a67e3e6e343398056a,j310,Journal of Graph Theory,Reflections on graph theory,"At the occasion of the 250th anniversary of graph theory, we recall some of the basic results and unsolved problems, some of the attractive and surprising methods and results, and some possible future directions in graph theory.",fullPaper,jv310
p1708,048494dac2872f0be62c15aa85b8a1da017f21a2,c20,Research Conference of the South African Institute of Computer Scientists and Information Technologists,Graph theory,"In this unit, you will investigate some interesting applications of polygons and circles. You will first determine the area of any given regular polygon and explore what happens to the shape of a polygon as the number of sides increase. You will also examine the area formula and how it is derived from an infinite number of sides in a regular polygon. You will then look at geometric probability based on length and area. The final part of this unit is investigating graph theory which is a study of networks that connect nodes with straight or curved paths.",poster,cp20
p1709,c637b74814bd5049323bb6bbf077875a3282e02b,c10,Big Data,Graph Theory and Social Networks: A Technical Comment on Connectedness and Connectivity,"Concepts taken from graph theory and other branches of topology have been used by many sociologists and social psychologists, in particular Kurt Lewin and J. L. Moreno. Similar ideas have been used to construct statistical models of nervous systems, and these have been applied by J. S. Coleman and others to the spread of information and other social phenomena. The study of social networks by anthropologists has been based, knowingly or unknowingly, on the basic notions of graph theory, as has the identification and analysis of social cliques. There is little consensus among mathematicians about terminology, and social scientists have drawn fortuitously on various mathematical vocabularies as well as inventing their own technical terms. Applied to social networks, the words `connectedness' and `connectivity' may refer to properties of the distance between persons, the number of paths between them, whether there is a path at all, or the proportion of possible paths actually in existence. These different usages are contrasted by restating them all in the terminology set out in Structural models (1965) by Harary, Norman and Cartwright.",poster,cp10
p1710,8a18f6b55267c844060652ed38ffe49bfef9be88,c56,European Conference on Software Process Improvement,The Foundations of Topological Graph Theory,Abstract content,poster,cp56
p1711,f2e614adc8a6139cc1f5c725365bd132df4ed0a7,c114,IEEE International Conference on Robotics and Automation,SOME RECENT RESULTS ON EXTREMAL PROBLEMS IN GRAPH THEORY (Results),"Three years ago I gave a talk on extremal problems in graph theory at Smolenice [2]. I will refer to this paper as I. I will only discuss results which have been found since the publication of I. ~(9) will denote the number of vertices, V(g) the number of edges of 9. s(a ; I) will denote a graph of n vertices and I edges. The vertices of 3 will be denoted by letters x, y, . . . 9(x,, . . . . xk) will denote the subgraph of 9 spanned by the vertices x1, . . . . xA. u(x), the valence of X, denotes the number of edges incident to x. x(S) will denote the",poster,cp114
p1712,7d6094c924f5e0d8a10bfa78ff43b97d286e2b2b,j311,Landscape Ecology,Landscape graphs: Ecological modeling with graph theory to detect configurations common to diverse landscapes,Abstract content,fullPaper,jv311
p1713,278fde8c3c7d0ee96f0e562eba1c9cbb15428532,c41,Software Product Lines Conference,Graph theory and molecular orbitals. XII. Acyclic polyenes,"A graph‐theoretical study of acyclic polyenes is carried out with an emphasis on the influence of branching on several molecular properties. A definition of branching is given and several branching indices are analyzed. The case of polyenes without a Kekule structure is discussed briefly. The main conclusions are: (a) thermodynamic stability of conjugated polyenes decreases with branching, but (b) reactivity, in general, increases with branching.",poster,cp41
p1714,90fc44ddfc0a4e43e317699e16c5123429b58cf6,c52,Workshop on Learning from Authoritative Security Experiment Results,Algorithmic graph theory and perfect graphs,Abstract content,poster,cp52
p1715,0ddc015d3ff0a4247eca90a97cae64970275a19b,j312,Discrete Mathematics,A method in graph theory,Abstract content,fullPaper,jv312
p1716,9be30e1d9f4301afbe46a975c67e8a1481e3d56f,c91,Workshop on Algorithms and Models for the Web-Graph,Problems in combinatorics and graph theory,"Three hundred and sixty-nine problems with fully worked solutions for courses in computer science, combinatorics, and graph theory, designed to provide graded practice to students with as little as a high school algebra background. Originally used to prepare Rumanian candidates for participation in the International Mathematical Olympiads, this book includes both simple problems and complex ones, arranged according to subject. It provides various levels of problems, some of which had been previously available only in research journals. All details of the proofs are given in the solutions.",poster,cp91
p1717,b879e3124a79513ac0cf86979fc1a33fbbeb4875,c39,International Conference on Global Software Engineering,Hybrid Graph Theory and Network Analysis,"This book combines traditional graph theory with the matroid view of graphs in order to throw light on the mathematical approach to network analysis. The authors examine in detail two dual structures associated with a graph, namely circuits and cutsets. These are strongly dependent on one another and together constitute a third, hybrid, vertex-independent structure called a graphoid, whose study is here termed hybrid graph theory. This approach has particular relevance for network analysis. The first account of the subject in book form, the text includes many new results as well as the synthesizing and reworking of much research done over the past thirty years (historically, the study of hybrid aspects of graphs owes much to the foundational work of Japanese researchers). This work will be regarded as the definitive account of the subject, suitable for all working in theoretical network analysis: mathematicians, computer scientists or electrical engineers.",poster,cp39
p1718,13ec4520f33b8724d6dc2b66da1302ffeb4231b5,c63,IEEE International Software Metrics Symposium,Graph Theory Techniques in Model-Based Testing,"Models are a method of representing software behavior. Graph theory is an area of mathematics that can help us use this model information to test applications in many different ways. This paper describes several graph theory techniques, where they came from, and how they can be used to improve software testing.",poster,cp63
p1719,342a3f86045d99b1d83aec36b3c68611d945c227,c73,Workshop on Algorithms in Bioinformatics,Graph Connections: Relationships between Graph Theory and Other Areas of Mathematics,"The purpose of this book is to inform mathematicians about the applicability of graph theory to other areas of mathematics, from number theory, to linear algebra, knots, neural networks, and finance. This is achieved through a series of expository chapters, each devoted to a different field and written by an expert in that field. The book, however, is more than a collection of essays. Each chapter has been carefully edited to ensure a common level of exposition, with terminology and notation standarised as far as possible.",poster,cp73
p1720,a3c7bcf8e2090de45281cf160e5893766af27100,j310,Journal of Graph Theory,Examples and Counterexamples in Graph Theory,"It sounds good when knowing the examples and counterexamples in graph theory in this website. This is one of the books that many people looking for. In the past, many people ask about this book as their favourite book to read and collect. And now, we present hat you need quickly. It seems to be so happy to offer you this famous book. It will not become a unity of the way for you to get amazing benefits at all. But, it will serve something that will let you get the best time and moment to spend for reading the book.",fullPaper,jv310
p1721,682c67d11c3bc884ca6d551b546c8b1c6414c9b3,c60,IEEE International Conference on Software Engineering and Formal Methods,On domination related concepts in Graph Theory,Abstract content,poster,cp60
p1722,42eaf1aea33b9da962f32bf5e1a81fc4c60f0e40,c57,IEEE International Conference on Engineering of Complex Computer Systems,A SEMINAR ON GRAPH THEORY,"Abstract : The opening six chapters present a coherent body of graph theoretic concepts. The remaining eight chapters report lectures presented by various seminar participants. Topics presented include: Complete Bipartite Graphs, Extremal Problems in Graph Theory, Applications of Probabilistic Methods to Graph Theory, The Minimal Regular Graph Containing a Given Graph, Various Proofs of Cayley's Formula for Counting Trees, On Well-Quasi-Ordering Trees, Universal Graphs and Graphs and Composite Games. (Author)",poster,cp57
p1723,b13a8bd57def8a98f4e59fced4cce361875aa719,c77,Networks,On constructing a block layout by graph theory,"This paper examines the problem of developing block layouts using graph theory. It is shown that there are several limitations associated with such layouts, particularly when facility relationships are represented quantitatively by a from-to chart. A modification to conventional construction-type layout procedures is presented which allows a graph theoretic block layout to be developed, regardless of the type of facility relationships used, and without performing all the steps required in the graph theoretic approach. This new method helps to avoid the limitations of the approach and alleviate its computational burden.",poster,cp77
p1724,6f18387ee1b3533126d5d1e0dccc1fa1c81c8e17,c17,International Conference on Enterprise Information Systems,Lattice Constant Systems and Graph Theory,"The two principal systems of lattice constants that have arisen in the study of cooperative phenomena and related problems on crystal lattices are the strong (low‐temperature) and the weak (high‐temperature) systems. The two systems are defined in terms of the concepts of graph theory, and a general theorem relevant to cluster expansions is stated. The interrelation of the two systems is studied and exploited to derive configurational data for the face‐centered cubic lattice. All star graphs with up to seven points (vertices) or nine lines (edges) that are embeddable on the face‐centered cubic are described. A general classification of stars with cyclomatic index 3 is given.",poster,cp17
p1725,1098c6bad469b388be83566bfe0445b47ba4256f,j310,Journal of Graph Theory,Open problems of Paul Erdös in graph theory,"The main treasure that Paul Erdős has left us is his collection of problems, most of which are still open today. These problems are seeds that Paul sowed and watered by giving numerous talks at meetings big and small, near and far. In the past, his problems have spawned many areas in graph theory and beyond (e.g., in number theory, probability, geometry, algorithms and complexity theory). Solutions or partial solutions to Erdős problems usually lead to further questions, often in new directions. These problems provide inspiration and serve as a common focus for all graph theorists. Through the problems, the legacy of Paul Erdős continues (particularly if solving one of these problems results in creating three new problems, for example.) There is a huge literature of almost 1500 papers written by Erdős and his (more than 460) collaborators. Paul wrote many problem papers, some of which appeared in various (really hard-to-find) proceedings. Here is an attempt to collect and organize these problems in the area of graph theory. The list here is by no means complete or exhaustive. Our goal is to state the problems, locate the sources, and provide the references related to these problems. We will include the earliest and latest known references without covering the entire history of the problems because of space limitations (The most up-to-date list of Erdős' papers can be found in [65]; an electronic file is maintained by Jerry Grossman at grossman@oakland.edu.) There are many survey papers on the impact of Paul's work, e.g., see those in the books: A Tribute to Paul Erdős [84], Combinatorics, Paul Erdős is Eighty, Volumes 1 and 2 [83], and The Mathematics of Paul Erdős, Volumes I and II [81]. To honestly follow the unique style of Paul Erdős, we will mention the fact that Erdős often offered monetary awards for solutions to a number of his favorite problems. In November 1996, a committee of Erdős' friends decided no more such awards will be given in Erdős' name. However, the author, with the help of Ron Graham, will honor future claims on the problems in this paper, provided the requirements previously set by Paul are satisfied (e.g., proofs have been verified and published in recognized journals). Throughout this paper, the constants c, c′, c1, c2, · · · and extremal functions f(n), f(n, k), f(n, k, r, t), g(n), · · · are used extensively, although within the context of each problem, the",fullPaper,jv310
p1726,2777556053aed8a2e97055dd82f2ac026319b32c,c93,Human Language Technology - The Baltic Perspectiv,Applications of combinatorics and graph theory to the biological and social sciences,Abstract content,poster,cp93
p1727,19ccc67b2442c08baefdb30a772463134ec21d37,c46,Brazilian Symposium on Software Engineering,Computational Graph Theory,Abstract content,poster,cp46
p1728,9aa530783226dc392f570207e90f32cee8b20b96,c17,International Conference on Enterprise Information Systems,AN APPLICATION OF GRAPH THEORY TO ALGEBRA,"[Al, * * * , Ak ] 54-0. The original proof of the theorem [1] was elementary but very complicated. In attempting to simplify this proof, I found a more transparent proof based on the use of graph theory.3 One advantage of this approach is that complicated algebraic definitions can be replaced by much simpler geometric definitions merely by drawing a picture of the appropriate graph. Before stating the graph theoretic theorem which implies Theorem 1, I will give some elementary definitions and lemmas from graph theory.",poster,cp17
p1729,98db9d44877f40ea346844a7708c8d7cb64de3dd,c73,Workshop on Algorithms in Bioinformatics,Chemical signed graph theory,"Chemical signed graph theory is presented. Each topological orbital of an N-vertex molecular graph is represented by a vertex-signed graph (VSG) that is generated by assigning a sign, either plus or minus, to the vertices without solving the secular matrix equation. The bonding capacity of each VSG is represented by its corresponding edge-signed graph (ESG) and is quantified by the net sign of the ESG. The resulting 2N–1 configurations of VSGs can be divided into several groups according to the net signs of the corresponding ESGs. Summation of an appropriate set of degenerate VSGs is found to yield the conventional, canonical molecular orbitals. The distribution of the number of VSGs with respect to the net sign is found to be binomial, which can be related to bond percolation in statistical physics. © 1994 John Wiley & Sons, Inc.",poster,cp73
p1730,2c494605fe60583c0b8f20facc463ec49cb06a0c,c3,Frontiers in Education Conference,Potts model and graph theory,Abstract content,poster,cp3
p1731,056c50de715e9a3c2aff98d6f90caf877e9acf71,c56,European Conference on Software Process Improvement,Some recent results in topological graph theory,Abstract content,poster,cp56
p1732,2880478c793e500c42f4a0cf06d5036f0061c0bf,c65,Formal Concept Analysis,Conference on Graph Theory and Topology in Chemistry.,"Abstract : Prof. R. B. King and Dr. D. H. Rouvray organized an International Conference on Graph Theory and Topology in Chemistry which was held at the University of Georgia, Athens, Georgia, during the period March 15-20, 1987. A volume containing the papers presented at this conference is being published by Elsevier Scientific Publishing Company, Amsterdam, and will appear around the end of 1987. The following items are attached: (1) The program of the conference. (2) The short abstracts of the paper presented at the conference. (3) The contents of the conference volume. (4) The preface of the conference volume. Knots, Marcromolecules and Chemical Dynamics Topological Stereochemistry: Knot Theory of Molecular Graphs Extrinsic Topological Chirality Indices of Molecular Graphs A Topological Approach to the Stereochemistry of Nonrigid Molecules Chirality of Non-Standardly Embedded Mobius Ladders .",poster,cp65
p1733,7d0e37b3f932ac8c0dcec37c43e71bbbb0ce2811,c62,International Conference on Software Reuse,Graph theory and molecular orbitals. VII. The role of resonance structures,"The relations between the simplest variants of MO and VB theory are discussed. It is shown that there is a unique principle causing all the cases of congruity between these two theories‐Kekule structures being related to the permutations contained in the molecular graph [Eqs. (6) and (7)]. The class of benzenoid hydrocarbons where both theories are substantially equivalent is rigorously defined using graph theory. A number of topological regularities for these hydrocarbons are proved. Thus, the Dewar‐Longuet‐Higgins equation, the proof of the Ruedenberg's and Pauling's bond orders, the relation between the VB and MO spin and charge density, and Heilbronner's formula are obtained. The limits of validity for all these results are strictly determined.",poster,cp62
p1734,8850c6b96200bc8b92935d16a7685e621214d51c,c82,Workshop on Interdisciplinary Software Engineering Research,Facilities Planning with Graph Theory,Basic concepts of Graph Theory are discussed which are relevant to solving problems of locating economic activities within a service or manufacturing facility. The location problem is formulated in terms of Graph Theory knowledge and a solution procedure proposed. An example is provided and finally boundary conditions are elaborated.,poster,cp82
p1735,cd570ac39865e4f297491c8321df914eeaa2ac7d,c84,The Web Conference,On some solved and unsolved problems of chemical graph theory,"The development of several novel graph theoretical concepts and their applications in different branches of chemistry are reviewed. After a few introductory remarks we follow with an outline of selected important graph theoretical invariants, introducing some new results and indicating some open problems. We continue with discussing the problem of graph characterization and construction of graphs of chemical interest, with a particular emphasis on large systems. Finally we consider various problems and difficulties associated with special subgraphs, including subgraphs representing Kekule valence structures. The paper ends with a brief review of structure-property and structure-activity correlations, the topic which is one of prime motivations for application of graph theory to chemistry.",poster,cp84
p1736,c77d1da4e4c8c9b9eddfcbf16f98b9d38663e34b,j20,Proceedings of the National Academy of Sciences of the United States of America,TWO THEOREMS IN GRAPH THEORY.,"Introduction. Given an unoriented graph (or 1-dimensional regular complex), let X be the set of all its vertices and U be the set of all its edges. When the graph is finite, the following problems arise: Problem 1: A set A c X is said to be internally stable if x e A, y e A implies (x, y) o U. The symbol A will denote the number of elements of A. Construct an internally stable set A such that A is maximum. Problem 2: A set B c X is said to be a cover if every edge of U is adjacent to at least one vertex in B. Construct a cover with the minimum number of elements. Problem 3: A set of edges V c U is said to be a matching if two edges of V have no vertex in common. Construct a matching with the maximum number of elements. A particular case of Problem 1 is the chess problem of Gauss: Put eight queens on the board such that no one can take any other. In n-person game theory, if the graph of domination is symmetrical, a maximum internally stable set turns out to be a maximum solution (in the von Neumann-Morgenstern sense1), and the more usual case can be solved by means of the Grundy functions.2 Problem 2 is the set theoretic dual of Problem 1, since the complement of an internally stable set is a cover, and conversely. Particular cases of Problem 3 are the problem of distinct representatives (P. Hall1) and the problem of Petersen (D. K6nig4). In the case where the graph is bipartite, Problem 3 has been solved by algebraic methods by 0. Ore,5 and an efficient algorithm has been given by H. Kuhn.6 Unfortunately, the linear programming duality used by H. Kuhn no longer subsists when the graph is not bipartite. (Note that Problem 2 is the linear program dual to Problem 3 in the bipartite case.) In view of solving the general case, this paper states two theorems: Theorem 1 gives a necessary and sufficient condition for recognizing whether a matching is maximum and provides an algorithm for Problem 3, while Theorem 2 yields an algorithm for Problems 1 and 2. The Theorems.-Consider a graph G = (X, U) with a matching V0; if u e Vo we shall say that edge u is strong, otherwise that u is weak. An alternating chain is a chain which does not use the same edge twice and is such that for any two adjacent edges one is strong and the other is weak. A vertex x which is not adjacent to a strong edge is said to be neutral, the set of all neutral points being N. We shall also consider a graph G constructed from G by adding a vertex a and connecting a to every neutral point with a strong edge. If there exists an alternating chain from a to a vertex x, we shall picture an arrow on the last edge (z, x), directed from z to x. A vertex x (a N) which is not adjacent to a directed edge is said to be inaccessible, the set of all inaccessible points being I. A vertex x (t N) adjacent to a weak edge directed to x and not to a strong edge directed to x is said to be weak, the set of all weak points being W. A vertex x (f N) adjacent to a strong edge directed to x and not to a weak edge directed to x is said to be strong,",fullPaper,jv20
p1737,bbbff2de136561c6cb4c7890d9f93b357a99e244,j313,Canadian Journal of Mathematics - Journal Canadien de Mathematiques,Graph Theory and Probability,"A well-known theorem of Ramsay (8; 9) states that to every n there exists a smallest integer g(n) so that every graph of g(n) vertices contains either a set of n independent points or a complete graph of order n, but there exists a graph of g(n) — 1 vertices which does not contain a complete subgraph of n vertices and also does not contain a set of n independent points. (A graph is called complete if every two of its vertices are connected by an edge; a set of points is called independent if no two of its points are connected by an edge.) The determination of g(n) seems a very difficult problem; the best inequalities for g(n) are (3) It is not even known that g(n)1/n tends to a limit. The lower bound in (1) has been obtained by combinatorial and probabilistic arguments without an explicit construction.",fullPaper,jv313
p1738,e47f58edd1af4ee6b236d771b1e863920382c45d,c33,International Conference on Agile Software Development,Fractional Graph Theory: A Rational Approach to the Theory of Graphs,General Theory: Hypergraphs. Fractional Matching. Fractional Coloring. Fractional Edge Coloring. Fractional Arboricity and Matroid Methods. Fractional Isomorphism. Fractional Odds and Ends. Appendix. Bibliography. Indexes.,poster,cp33
p1739,25c6565750d7d46c63d9bc073e6826b4d5898498,c94,Vision,Graph Theory and Q-Analysis,"Structures of graph theory are compared with those of Q-analysis and there are many similarities. The graph and simplicial complex defined by a relation are equivalent in terms of the information they represent, so that the choice between graph theory and Q-analysis depends on which gives the most natural and complete description of a system. The higher dimensional graphs are shown to be simplicial families or complexes. Although network theory is very successful in those physical science applications for which it was developed, it is argued that Q-analysis gives a better description of human network systems as patterns of traffic on a backcloth of simplicial complexes. The q-nearness graph represents the q-nearness of pairs of simplices for a given q-value. It is concluded that known results from graph theory could be applied to the q-nearness graph to assist in the investigation of q-connectivity, to introduce the notion of connection defined by graph cuts, and to assist in computation. The application of the q-nearness graph to q-transmission and shomotopy is investigated.",poster,cp94
p1740,24f1ff4e693c9b362a565047b12659189a3eadbf,c55,Annual Workshop of the Psychology of Programming Interest Group,Neural networks and graph theory,"The relationships between artificial neural networks and graph theory are considered in detail. The applications of artificial neural networks to many difficult problems of graph theory, especially NP-complete problems, and the applications of graph theory to artificial neural networks are discussed. For example graph theory is used to study the pattern classification problem on the discrete type feedforward neural networks, and the stability analysis of feedback artificial neural networks etc.",poster,cp55
p1741,8691bd56aad7be73d5a6342ee434bea2d4774e00,c37,Asia-Pacific Software Engineering Conference,"ON SOME PROBLEMS IN GRAPH THEORY , COMBINATORIAL ANALYSIS AND COMBINATORIAL NUMBER THEORY","1. G(n) is a graph of n vertices and G(n ; e) is a graph of n vertices and e edges. Is it true that if every induced subgraph of a G(10n) of 5n vertices has more than 2n 2 edges then our G(10n) contains a triangle? It is easy to show that if true this result is best possible . To see this let A i , i =1, 2, . . . , 5, be sets of 2n vertices, put A, = A 6 and join every vertex of A, to every vertex of A; + , . This G(10n ; 20n 2) has of course no triangle and every induced subgraph of 5n vertices contains at least 2n2 edges . Equality is of course possible : choose A,, A, and half the vertices of A, Simonovits pointed out to me that a graph of completely different structure also shows that the conjecture, if true, is best possible . Consider the Petersen graph, which is a G(10 ; 15) . Replace each vertex by a set of n vertices and replace every edge of the Petersen graph by the n 2 edges of a K(n, n) . This gives a G(10n ; 15n 2) and it is easy to see that every induced subgraph of 5n vertices has at least 2n2 edges . The fact that two graphs of different structure are extremal perhaps indicates that the conjecture is either false or difficult to prove . I certainly hope that the latter is the case . It is perhaps tempting to conjecture that my graph has the following extremal property . If a G(10n) has no triangle and every induced subgraph of 5n vertices has at least 2n2 edges, then our graph can have at most 20n2 edges. Perhaps the graph of Simonovits has the smallest number of edges among all extremal graphs; perhaps in fact these two graphs are the only extremal graphs . Many generalizations are possible ; the triangle could be replaced by other graphs . Is it true that every G((4h+2)n), every induced subgraph",poster,cp37
p1742,a38faf02b193d8d121fc9946d5e7c53b13bd9d57,j314,Acta Crystallographica Section B Structural Science,Graph-set analysis of hydrogen-bond patterns in organic crystals.,"A method is presented based on graph theory for categorizing hydrogen-bond motifs in such a way that complex hydrogen-bond patterns can be disentangled, or decoded, systematically and consistently. This method is based on viewing hydrogen-bond patterns topologically as if they were intertwined nets with molecules as the nodes and hydrogen bonds as the lines. Surprisingly, very few parameters are needed to define the hydrogen-bond motifs comprising these networks. The methods for making these assignments, and examples of their chemical utility are given.",fullPaper,jv314
p1743,199f55f80973ff42c0df77495cc639ff6e211fac,c107,British Machine Vision Conference,Topics in Intersection Graph Theory,"Preface 1. Intersection Graphs. Basic Concepts Intersection Classes Parsimonious Set Representations Clique Graphs Line Graphs Hypergraphs 2. Chordal Graphs. Chordal Graphs as Intersection Graphs Other Characterizations Tree Hypergraphs Some Applications of Chordal Graphs Split Graphs 3. Interval Graphs. Definitions and Characterizations Interval Hypergraphs Proper Interval Graphs Some Applications of Interval Graphs 4. Competition Graphs. Neighborhood Graphs Competition Graphs Interval Competition Graphs Upper Bound Graphs 5. Threshold Graphs. Definitions and Characterizations Threshold Graphs as Intersection Graphs Difference Graphs and Ferrers Digraphs Some Applications of Threshold Graphs 6. Other Kinds of Intersection. p-Intersection Graphs Intersection Multigraphs and Pseudographs Tolerance Intersection Graphs 7. Guide to Related Topics. Assorted Geometric Intersection Graphs Bipartite Intersection Graphs, Intersection Digraphs, and Catch (Di)Graphs Chordal Bipartite and Weakly Chordal Graphs Circle Graphs and Permutation Graphs Clique Graphs of Chordal Graphs and Clique-Helly Graphs Containment, Comparability, Cocomparability, and Asteroidal Triple-Free Graphs Infinite Intersection Graphs Miscellaneous Topics P4-Free Chordal Graphs and Cographs Powers of Intersection Graphs Sphere-of-Influence Graphs Strongly Chordal Graphs Bibliography Index.",poster,cp107
p1744,bccb739463ffd0189a5c6b2f5e6415a9f50fb313,c77,Networks,Selected Topics in Graph Theory 2,Abstract content,poster,cp77
p1745,175a476feb8cf13d7f2c739c347de3262f817063,c94,Vision,A ring in graph theory,"We call a point set in a complex K a 0-cell if it contains just one point of K, and a 1-cell if it is an open arc. A set L of 0-cells and 1-cells of K is called a linear graph on K if (i) no two members of L intersect, (ii) the union of all the members of L is K, (iii) each end-point of a 1-cell of L is a 0-cell of L and (iv) the number of 0-cells and 1-cells of L is finite and not 0.",poster,cp94
p1746,2c4f5a30c195ef810415b3272a2bfc4af845c96c,c79,International Conference on Medical Image Computing and Computer-Assisted Intervention,On some extremal problems in graph theory,"In this paper we are concerned with various graph invariants (girth, diameter, expansion constants, eigenvalues of the Laplacian, tree number) and their analogs for weighted graphs -- weighing the graph changes a combinatorial problem to one in analysis. We study both weighted and unweighted graphs which are extremal for these invariants. In the unweighted case we concentrate on finding extrema among all (usually) regular graphs with the same number of vertices; we also study the relationships between such graphs.",poster,cp79
p1747,18261bcc9557f697e795d3aa3a9bb74da54fe58c,c108,International Conference on Information Integration and Web-based Applications & Services,"Introduction to Chordal Graphs and Clique Trees, in Graph Theory and Sparse Matrix Computation","Kjjrull U., Triangulation of graph-algorithms giving small total state space. 19 in the number of minimal separators. This possible amendment will resolve a theoretical problem raised by KBMK93, KBMK94] and further addressed by PS95] but will hardly aaect the running time of our algorithm. 18 The entry Frag in Table 3 measures the number of fragments produced for the dynamic programming phase. The entries R and R k measure, respectively, the number of minimal separators and the number of minimal separators of size less than k where k = 7. Note again that R k is smaller than R; Many minimal separators are generated in the rst phase of QuickTree but are not needed for the dynamic programming phase. A second reason for the high values of T 1 is that the algorithm runs over almost all pairs of vertices and for each pair fa; bg produces all minimal a; b-separators. However, after a few pairs, the algorithm usually nds most of the minimal sepa-rators and the remaining run time is just used to verify that indeed all minimal separators have been generated. Table 4 shows the number of Good-Pairs|pairs that generated at least one new minimal separator. All-Pairs denote the number of pairs we used which guarantee that all minimal separators have been generated. For Table 4, 30% of the edges were dropped. Table 4 suggests that if there is no needed guarantee of optimal triangulation, then Phase 1 can be run on a fraction of the possible pairs of vertices and then the dynamic programming phase can be applied. For example, on 3 graphs with 75 nodes and treewidth 7, we selected the top 20% of pairs that had a maximal mutual distance and got close to optimal triangulations (in two instances we got the optimal treewidth 7 and once we got 9 instead of 7). The average running time was reduced from 675 to 373 seconds. 6 Discussion In many applications the treewidth of a triangulation is just an approximation to the real optimization problem. For example, for the updating problem in Bayesian networks, one needs to nd a triangulation that minimizes the sum P i 2 w(Ci) where w is a positive additive weight function on the vertices of G and C i are the cliques of the triangulation Kj90, BG96]. The triangulation algorithm presented herein can be modiied to accommodate such variants. Currently, the algorithm …",poster,cp108
p1748,0a431a6f6816931287bd3b14b48884aa63913b41,c18,Conference on Innovative Data Systems Research,Lectures on Spectral Graph Theory,Contents Chapter 1. Eigenvalues and the Laplacian of a graph 1 1.1. Introduction 1 1.2. The Laplacian and eigenvalues 2 1.3. Basic facts about the spectrum of a graph 6,poster,cp18
p1749,a372f7ee74933e324b63dc34747ec28201048cb3,c34,IEEE Working Conference on Mining Software Repositories,Graph theory with applications to algorithms and computer science,"Partial table of contents: Finite Figures Consisting Of Regular Polygons (J. Akiyama, et al.). Eigenvalues, Geometric Expanders and Sorting in Rounds (N. Alon). Long Path Enumeration Algorithms for Timing Verification on Large Digital Systems (T. Asano and S. Sato). On Upsets in Bipartite Tournaments (K. Bagga). Some Results on Binary Matrices Obtained via Bipartite Tournaments (K. Bagga and L. Beineke). Partitioning the Nodes of a Graph (E. Barnes). A Graph Theoretical Characterization of Minimal Deadlocks in Petri Nets (J. Bermond and G. Memmi). On Graceful Directed Graphs that Are Computational Models of Some Algebraic Systems (G. Bloom and D. Hsu). The Cut Frequency Vector (F. Boesch). Diameter Vulnerability in Networks (J. Bond and C. Peyrat). Generalized Colorings of Outerplanar and Planar Graphs (I. Broere and C. Mynhardt). The Ramsey Number for the Pair Complete Bipartite Graph-Graph of Limited Degree (S. Burr, et al.). Embedding Graphs in Books: A Layout Problem with Applications to VLSI Design (F. Chung). Hamilton Cycles and Quotients of Bipartite Graphs (I. Dejter). Problems and Results on Chromatic Numbers in Finite and Infinite Graphs (P. Erdos). Supraconvergence and Functions that Sum to Zero on Cycles (V. Faber and A. White, Jr.). Edge-Disjoint Hamiltonian Cycles (R. Faudree, et al.). Studies Related to the Ramsey Number r(K d5 u - e) (R. Faudree, et al). The Structural Complexity of Flowgraphs (N. Fenton). n-Domination in Graphs (J. Fink and M. Jacobson).",poster,cp34
p1750,d007e0675a45264aedeacbbe12c731ea78bba070,c94,Vision,Problems and Results in Graph Theory and Combinatorial Analysis,"I published several papers with similar titles. One of my latest ones [13] (also see [16] and the yearly meetings at Boca Raton or Baton Rouge) contains, in the introduction, many references to my previous papers . I discuss here as much as possible new problems, and present proofs in only one case. I use the same notation as used in my previous papers . G(' )(n ;1) denotes an r-graph (uniform hypergraph all of whose edges have size r) of n vertices and I edges . If r = 2 and there is no danger of confusion . I omit the upper index r = 2 . K ( r ) (n) denotes the complete hypergraph G ( ' ) (n ; (;)) . K(a, b) denotes the complete bipartite graph (r = 2) of a white and b black vertices . K (r )(t) denotes the hypergraph of It vertices x (i ' ) , I < i < t, 1 < j < 1, and whose (I)tr edges are {x,1t ) , . . . , x~ )} where all the i's and all the j's are distinct . e(G(in)) is the number of edges of G(m) (graph of m vertices), the girth is the length of a smallest circuit of the graph .",poster,cp94
p1751,0e83b4f43f32727d7aa5cefbd12db8a7ed3f1346,c113,International Conference on Image Analysis and Processing,Some Topics in Graph Theory,1. Basic terminology 2. Edge-colourings of graphs 3. Symmetries in graphs 4. Packing of graphs 5. Computational complexity of graph properties.,poster,cp113
p1752,7ab055436895b106e5a82e910693539b9877d884,c65,Formal Concept Analysis,Compactness results in extremal graph theory,Abstract content,poster,cp65
p1753,ed98016990bc4bd91c910070563d7c5ad5f5a38c,c112,Very Large Data Bases Conference,A Friendly Introduction to Graph Theory,1. Introductory Concepts. 2. Introduction to Graphs and their Uses. 3. Trees and Bipartite Graphs. 4. Distance and Connectivity. 5. Eularian and Hamiltonian Graphs. 6. Graph Coloring. 7. Matrices. 8. Graph Algorithms. 9. Planar Graphs. 10. Digraphs and Networks. 11. Special Topics. Answers/Solutions to Selected Exercises. Index.,poster,cp112
p1754,bc1b7919b0a78f792edcf2cb3abbd082518b45c2,c68,International Workshop on Software Engineering for Large-Scale Multi-Agent Systems,Use of Graph Theory to Support Map Generalization,"In the generalization of a concept, we seek to preserve the essential characteristics and behavior of objects. In map generalization, the appropriate selection and application of procedures (such as merging, exaggeration, and selection) require information at the geometric, attribute, and topological levels. This article highlights the potential of graph theoretic representations in providing the topological information necessary for the efficient and effective application of specific generalization procedures. Besides ease of algebraic manipulation, the principal benefit of a graph theoretic approach is the ability to detect and thus preserve topological characteristics of map objects such as isolation, adjacency, and connectivity. While it is true that topologically based systems have been developed for consistency checking and error detection during editing, this article emphasizes the benefits from a map-generalization perspective. Examples are given with respect to specific generalization procedures ...",poster,cp68
p1755,21fac6c5fcfdfb51fa60a7f361179d25661c0a46,c95,IEEE International Conference on Computer Vision,Graph theory for image analysis: an approach based on the shortest spanning tree,"The paper describes methods of image segmentation and edge detection based on graph-theoretic representations of images. The image is mapped onto a weighted graph and a spanning tree of this graph is used to describe regions or edges in the image. Edge detection is shown to be a dual problem to segmentation. A number of methods are developed, each providing a different segmentation or edge detection technique. The simplest of these uses the shortest spanning tree (SST), a notion that forms the basis of the other improved methods. These further methods make use of global pictorial information, removing many of the problems of the SST segmentation in its simple form and of other pixel linking algorithms. An important feature in all of the proposed methods is that regions may be described in a hierarchical way.",poster,cp95
p1756,2808f916c72757f891a101c39a9942fb27f22b78,j310,Journal of Graph Theory,Extremal problems in graph theory,The aim of this note is to give an account of some recent results and state a number of conjectures concerning extremal properties of graphs.,fullPaper,jv310
p1757,814cbd66f918e17cd97e94792b6f3ed7f7b73c8e,c4,Annual Conference on Genetic and Evolutionary Computation,"Graph Theory: Flows, Matrices","STRUCTURE OF THE GRAPH MODEL The abstract graph Geometrical realization of graphs Components Leaves Blocks The strongly connected components of directed graphs Problems OPTIMAL FLOWS Two basic problems Maximal set of independent paths The optimal assignment problem The Hungarian method Max flow-min cut Dynamic flow The mobilization problem The synthesis of flow problems Optical planning The role of the critical path Minimal cost transportation Minimal cost flows Problems GRAPHS AND MATRICES The adjacency matrix The incidence matrix The circuit matrix Interrelations between the matrices of graphs The spectrum of graphs, the complexity Linear electrical networks Further matrices associated with graphs Problems and solutions",poster,cp4
p1758,9aa27be1bbeb898e6daf8c106ccafae6e09856f9,j313,Canadian Journal of Mathematics - Journal Canadien de Mathematiques,Graph Theory and Probability. II,"Define f(k, l) as the least integer so t h a t every graph having f(k, l) vertices contains either a complete graph of order k or a set of l independent vertices (a complete graph of order k is a graph of k vertices every two of which are connected by an edge, a set of I vertices is called independent if no two are connected by an edge). Throughout this paper c1, c2, … will denote positive absolute constants. It is known (1, 2) that (1) and in a previous paper (3) I stated that I can prove that for every ∈ > 0 and l > l(∈), f (3, l) > l2-∈ . In the present paper I am going to prove that (2)",fullPaper,jv313
p1759,f3f26c7fd8bca7262b23210bf5318037a27f939d,c88,Symposium on the Theory of Computing,Parallel computations in graph theory,"In parallel computation two approaches are common; namely unbounded parallelism and bounded parallelism. In this paper both approaches will be considered. The problem of unbounded parallelism is studied in section II and some lower and upper bounds on different connectivity problems for directed and undirected graphs are presented. In section III we mention bounded parallelism and three different k-parallel graph search techniques, namely k-depth search, breadth depth search, and breadth-first search. Each algorithm is analyzed with respect to the optimal serial algorithm. It is shown that for sufficiently dense graphs the parallel breadth first search technique is very close to the optimal bound. Techniques for searching sparse graphs are also discussed.",poster,cp88
p1760,df8f64361e1bd6e976b533f81a50c52ba0e2ed02,j315,Journal of the Australian Mathematical Society,An extremal problem in graph theory,"G(n;l) will denote a graph of n vertices and l edges. Let f0(n, k) be the smallest integer such that there is a G (n;f0(n, k)) in which for every set of k vertices there is a vertex joined to each of these. Thus for example fo = 3 since in a triangle each pair of vertices is joined to a third. It can readily be checked that fo = 5 (the extremal graph consists of a complete 4-gon with one edge removed). In general we will prove: Let n > k, andthen f0(n, k) = f(n, k).",fullPaper,jv315
p1761,5c0e88953a818d257805ae5f1fd8ba5a16661ada,c76,International Conference on Artificial Neural Networks,Chemical applications of graph theory,Abstract content,poster,cp76
p1762,84b5314ed88bb9fad764b85833fe8038f7f0f57f,c0,International Conference on Human Factors in Computing Systems,Graph Theory and the Evolution of Autocatalytic Networks,"We give a self-contained introduction to the theory of directed graphs, leading up to the relationship between the Perron-Frobenius eigenvectors of a graph and its autocatalytic sets. Then we discuss a particular dynamical system on a fixed but arbitrary graph, that describes the population dynamics of species whose interactions are determined by the graph. The attractors of this dynamical system are described as a function of graph topology. Finally we consider a dynamical system in which the graph of interactions of the species coevolves with the populations of the species. We show that this system exhibits complex dynamics including self-organization of the network by autocatalytic sets, growth of complexity and structure, and collapse of the network followed by recoveries. We argue that a graph theoretic classification of perturbations of the network is helpful in predicting the future impact of a perturbation over short and medium time scales.",poster,cp0
p1763,5549a317454b8314f585a6c13db39664c76686d2,c21,Grid Computing Environments,Graph theory and its engineering applications,Basic theory foundations of electrical network theory directed-graph solutions of linear algebraic equations topological analysis of linear systems trees and their generation the realizability of directed graphs with prescribed degrees state equations of networks.,poster,cp21
p1764,f8048aa570e607428f8cfbfbcdc2bfa15218df75,c35,EUROMICRO Conference on Software Engineering and Advanced Applications,Matrices in Combinatorics and Graph Theory,Abstract content,poster,cp35
p1765,d15c7f4c9eff3f5f1583a4ef3ac1c4fd88caf972,c84,The Web Conference,A Beginner's Guide to Graph Theory,"Graphs.- Walks, Paths and Cycles.- Connectivity.- Trees.- Linear Spaces Associated with Graphs.- Factorizations.- Graph Colorings.- Planarity.- Labeling.- Ramsey Theory.- Digraphs.- Critical Paths.- Flows in Networks.- Computational Considerations.- Communications Networks and Small-Worlds.",poster,cp84
p1766,8d2e0a40e065555c75891132def53556d7c958b9,j316,Mathematical Systems Theory,On finite 0-simple semigroups and graph theory,Abstract content,fullPaper,jv316
p1767,7a05bef155bc7a29581cac1c9b2ee9e8ca3dca42,c95,IEEE International Conference on Computer Vision,Fundamentals of Graph Theory,Abstract content,poster,cp95
p1768,ad950d0d65329910e6251168315db07e1cf7a35f,j317,SIAM Journal on Discrete Mathematics,On-Line Coloring and Recursive Graph Theory,"An on-line vertex coloring algorithm receives the vertices ofa graph in some externally determined order, and, whenever a new vertex is presented, the algorithm also learns to which of the previously presented vertices the new vertex is adjacent. As each vertex is received, the algorithm must make an irrevocable choice of a color to assign the new vertex, and it makes this choice without knowledge of future vertices. A class of graphs $r$ is said to be on-line $\chi$-bounded if there exists an on-line algorithm $A$ and a function $f$ such that $A$ uses at most $f(\omega(G))$ colors to properly color any graph $G$ in \Gamma. If $H$ is a graph, let Forb($H$) denote the class of graphs that do not induce $H$. The goal of this paper is to establish that Forb($T$) is on-line $\chi$-bounded for every radius-2 tree $T$. As a corollary, the authors answer a question of Schmerl's; the authors show that every recursive cocomparability graph can be recursively colored with a number of colors that depends only on its clique number.",fullPaper,jv317
p1769,f736de70b906f5aff9fc89a098adf5045cfa8360,c5,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,Graph Theory,Abstract content,poster,cp5
p1770,33d54858878c14923035ffcbc018b7128170a1fb,c50,International Conference on Automated Software Engineering,A First Look at Graph Theory,"This book is intended to be an introductory text for mathematics and computer science students at the second and third year levels in universities. It gives an introduction to the subject with sufficient theory for students at those levels, with emphasis on algorithms and applications.",poster,cp50
p1771,a4df015d82386bb4783e7a8bdf430d8118b5155a,c17,International Conference on Enterprise Information Systems,Contentment in graph theory: Covering graphs with cliques,Abstract content,poster,cp17
p1772,a36451b1dc5615ec33eb4004a1b1b5f2401ab683,j214,Journal of chemical information and computer sciences,Topological organic chemistry. 1. Graph theory and topological indices of alkanes,"coding reaction transforms and can be made more or less REFERENCES AND NOTES discriminating by using an appropriate number of eigenvalues. However, the method is not reversible in that the index cannot (1) Wiswesser, W. J. J . Chem. Inf, Comput. Sci. 1982, 22, 8 8 . (2) Wiswesser, W. J. J . Chem. I f . Comput. Sci. 1985, 25, 258. (3) RandiE, M. J . Chem. In$ Compur. Sci. 1984, 24, 164. (4) Ash, J. E.; Chubb, P. A.; Ward, S. E.; Welford, S. M.; Willett, P. be used to derive the original structure. On the other hand, it can be used to derive a unique numbering for substructures by ordering the eigenvalues of the defined &nnectivity matrix. Another useful benefit is associated with the eigenvectors, which can be used to determine the attribution of each atom to substructures upon disconnection of the main structure into distinct fragments' This be when the disconnections of a main structure into reacting substructures Communication, Storage and Retrieval of Chemical Information; Ellis H o r w d Ltd.: Chichester, U.K., 1985. (5) Press, W. H.; Flannery, B. P.; Teukolsky, S. A.; Vetterling, W. T. Numerical Recipes: The Art of Scientific Computing; Cambridge University Press: Cambridge, U.K., 1986. (6) Henze, H. R.; Blair, C. M. J . Am. Chem. SOC. 1931, 53, 3077. (7) Lederberg, J.; Sutherland, G. L.; Buchanan, B. G.; Feigenbaum, E. A.; Robertson, A. V.; Duffield, A. M.; Djerassi, C. J. Am. Chem. Soc. 1969, are considered. 91. 2973.",fullPaper,jv214
p1773,3767ec228395d3f50aced3cc823d7f5f67c52973,c107,British Machine Vision Conference,Geometric Graph Theory,"Note: Professor Pach's number: [172]; 2nd edition Reference DCG-CHAPTER-2008-027 Record created on 2008-11-18, modified on 2017-05-12",poster,cp107
p1774,2a7c8231105a4cb615b2b9b57a59658eadba3751,c9,Pacific Symposium on Biocomputing,Applied Graph Theory,Abstract content,poster,cp9
p1775,a2b76b4529bc7da1696b3b5883ba5b9ab9564d20,c13,International Conference on Data Science and Advanced Analytics,Algebraic Graph Theory: COLOURING PROBLEMS,Abstract content,poster,cp13
p1776,0b9a20fd991a2dc33e6e27b66941b206cb8c638b,j318,Journal of combinatorial theory. Series B (Print),On a Hopf Algebra in Graph Theory,"We introduce and start the study of a bialgebra of graphs, which we call the 4-bialgebra, and of the dual bialgebra of 4-invariants. The 4-bialgebra is similar to the ring of graphs introduced by W. T. Tutte in 1946, but its structure is more complicated. The roots of the definition are in low dimensional topology, namely, in the recent theory of Vassiliev knot invariants. In particular, 4-invariants of graphs determine Vassiliev invariants of knots. The relation between the two notions is discussed.",fullPaper,jv318
p1777,bf69eddc4efacdaf33c135009d468df52ef77f1c,c38,"IEEE International Conference on Software Analysis, Evolution, and Reengineering",Graph Decompositions: A Study in Infinite Graph Theory,Note to the reader Introduction Fundamental facts and concepts Separating simplices and the existence of prime decompositions Simplicial minors and the existence of prime decompositions The uniqueness of prime decompositions Decompositions into small factors Applications of simplicial decompositions Appendix: Some notes on set theory References Subject index Index of symbols.,poster,cp38
p1778,034f6a2897bacfe8b86f123f4dfe5aa29af8ac1b,j102,Nucleic Acids Research,Exploring the repertoire of RNA secondary motifs using graph theory; implications for RNA design.,"Understanding the structural repertoire of RNA is crucial for RNA genomics research. Yet current methods for finding novel RNAs are limited to small or known RNA families. To expand known RNA structural motifs, we develop a two-dimensional graphical representation approach for describing and estimating the size of RNA's secondary structural repertoire, including naturally occurring and other possible RNA motifs. We employ tree graphs to describe RNA tree motifs and more general (dual) graphs to describe both RNA tree and pseudoknot motifs. Our estimates of RNA's structural space are vastly smaller than the nucleotide sequence space, suggesting a new avenue for finding novel RNAs. Specifically our survey shows that known RNA trees and pseudoknots represent only a small subset of all possible motifs, implying that some of the 'missing' motifs may represent novel RNAs. To help pinpoint RNA-like motifs, we show that the motifs of existing functional RNAs are clustered in a narrow range of topological characteristics. We also illustrate the applications of our approach to the design of novel RNAs and automated comparison of RNA structures; we report several occurrences of RNA motifs within larger RNAs. Thus, our graph theory approach to RNA structures has implications for RNA genomics, structure analysis and design.",fullPaper,jv102
p1779,07ae6518908df65f028106213877c1705ad6692d,c100,ACM SIGMOD Conference,GRAPH THEORY IN PRACTICE : PART I,"This reprint is provided for personal and noncommercial use. For any other use, please send a request to Permissions,",poster,cp100
p1780,92a868e1159f7694da3f06faa8a345f3d3a50250,j312,Discrete Mathematics,Problems and results in combinatorial analysis and graph theory,Abstract content,fullPaper,jv312
p1781,4f270d2b736a4ef269bdfe0e7a1478027e370085,c25,International Conference on Contemporary Computing,Some Applications of Graph Theory to the Structural Analysis of Mechanisms,Abstract content,poster,cp25
p1782,e8821c26f6cdd1a7d1f1ebb59dff63585de530f7,c24,Decision Support Systems,Graph theory,Abstract content,poster,cp24
p1783,851eb4b78f6deb8aba9d39529e462c5319940f51,c105,Biometrics and Identity Management,"Spectral Graph Theory, Regional Conference Series in Math.",Abstract content,poster,cp105
p1784,a0cfdfb8cfbe58c3820fb1729c9030cd9a05bc2d,c71,IEEE International Conference on Information Reuse and Integration,Graph theory and theoretical physics,Abstract content,poster,cp71
p1785,f73bd6b6fa6267b08fad47ccde5700ff1def71f0,c43,ACM Symposium on Applied Computing,The Regularity Lemma and Its Applications in Graph Theory,Abstract content,poster,cp43
p1786,71aa2175f1abc1745e4a8deb7356d0307a7ca64f,c30,IEEE Aerospace Conference,Graph Theory Applications,1: Basic Ideas. 2: Connectivity. 3: Trees. 4: Traversability. 4: Planarity. 6: Matrices. 7: Digraphs. 8: Coverings and Colourings. 9: Algorithms. 10: Matroids. 11: Miscellaneous Applications. 12: Operations Research. 13: Electrical Engineering. 14: Industrial Engineering. 15: Science. 16: Civil Engineering.,poster,cp30
p1787,627429c1f7c5f0b35bfe4e7b8f529000e4746304,c69,International Conference on Parallel Processing,Graph Theory and Feynman Integrals,Abstract content,poster,cp69
p1788,dc9515a0f39739fd0e009a90d0dc48afb6e3ac07,c51,Conference of the Centre for Advanced Studies on Collaborative Research,Introductory Graph Theory,Abstract content,poster,cp51
p1789,df6e19d90ae8be22e3c4f81d9193af9456a21253,c77,Networks,Applications of Combinatorics and Graph Theory to Spectroscopy and Quantum Chemistry,Abstract content,poster,cp77
p1790,dce135859cb1bc830946f745067104b8e34d135c,c107,British Machine Vision Conference,Graph theory and applications,Abstract content,poster,cp107
p1791,74e3c1ddf732f0970882ef3712465b3b62318d11,c85,International Conference on Graph Transformation,Graph theory and Feynman integrals,Abstract content,poster,cp85
p1792,8dff88f4f78ebaebe62d6a8adc6aad586fb5e9de,c112,Very Large Data Bases Conference,Introductory Graph Theory,Abstract content,poster,cp112
p1793,28c1795001dfd52abf46d5b88c064811f974add2,c26,PS,Graph theory and combinatorics,"WORKING PAPERS q C. Borgs, J. Chayes, N. Immorlica, A. Kalai, V. S. Mirrokni and C. Papadimitriou, The Myth of the Folk Theorem, submitted to STOC. q U. Feige, N. Immorlica, V.S.Mirrokni and H. Nazerzadeh, Functional Approximations: A new approach for analyzing Heuristics, submitted to STOC. q B. Awerbuch, Y. Azar, and A. Epstein, V. S. Mirrokni, A. Skopalik, Fast Convergence to Nearly Optimal Solutions in Potential Games, submitted to STOC. q J. Hartline, V. S. Mirrokni, and M. Sundararajan, Marketing Strategies over Social Networks, submitted to WWW. q U. Feige, N. Immorlica, V.S. Mirrokni and H. Nazerzadeh, Combinatorial Allocation Mechanisms with Penalties for Banner Advertisement, submitted to WWW. q R. Andersen, C. Borgs, J. Chayes, U. Feige, A. Flaxman, A. Kalai, V. S. Mirrokni, M. Tennenholtz, Trust-based Recommendation Systems: An axiomatic Approach, submitted to WWW. q V. S. Mirrokni, M. Schapira, J. Vondrak, Tight Information-Theoretic Lower Bounds for Maximizing Social-Welfare in Combinatorial Auctions, submitted to IPCO. q M. Goemans, N. Harvey, R. Kleinberg, V. S. Mirrokni, On Learning submodular functions, to be submitted to ICALP. q H. Ackermann, P. Goldberg, V. S. Mirrokni, H. Roeglin, and B. Voecking, Uncoordinated Twosided Markets, to be submitted to ACM EC. q M. Ghodsi, M. Mahini, V. S. Mirrokni, and M. ZadiMoghaddam Singleton Betting for Permutation Betting Markets. q R. Andersen, C. Borgs, J. Chayes, K. Jain, J. Hopcroft, V. S. Mirrokni and S. Teng, Locally Computable Link Spam Features. q V.S. Mirrokni, A. Skopalik, On the Complexity of Nash Dynamics and Sink Equilibria. q R. Andersen, V. S. Mirrokni, Overlapping Clustering for Distributed Computation.",poster,cp26
p1794,188b3b2f4afda1303b6fe4dc61daa30c33ee497d,j319,American Scientist,Graph Theory in Practice: Part II,"This reprint is provided for personal and noncommercial use. For any other use, please send a request to Permissions,",fullPaper,jv319
p1795,80aae60da893d9fa40ac4bda63f2ae1aa3a29b55,c97,Interspeech,Graph theory in network analysis,Abstract content,poster,cp97
p1796,fe01d24bd55ca63ceca04539c88ea477c3a50b07,c97,Interspeech,Algebraic Graph Theory: The multiplicative expansion,Abstract content,poster,cp97
p1797,b2aa0851bf11cb4c21ccc9af6eec4d84dbcdd49a,c57,IEEE International Conference on Engineering of Complex Computer Systems,An Application of Graph Theory to Additive Number Theory,Abstract content,poster,cp57
p1798,90152dddf7c83f0d8d9ba95c48635cfab1016cbe,c83,International Conference on Computer Graphics and Interactive Techniques,Fractional Graph Theory,Abstract content,poster,cp83
p1799,b819dcbd786315de85aa649b87c90a15206a5db0,c42,Joint Conference of International Workshop on Software Measurement and International Conference on Software Process and Product Measurement,Three short proofs in graph theory,Abstract content,poster,cp42
p1800,07ae6518908df65f028106213877c1705ad6692d,c31,International Conference on Evaluation & Assessment in Software Engineering,GRAPH THEORY IN PRACTICE : PART I,"This reprint is provided for personal and noncommercial use. For any other use, please send a request to Permissions,",poster,cp31
p1801,ab695b94256553dc06276fc44455acb559611282,j320,Adaptive Behavior,"Affordances. Motivations, and the World Graph Theory","O'Keefe and Nadel (1978) distinguish two paradigms for navigation, the ""locale system"" for map-based navigation and the ""taxon (behavioral orientation) system"" for route navigation. This article models the taxon system, the map-based system, and their interaction, and argues that the map-based system involves the interaction of hippocampus and other systems. We relate taxes to the notion of an affordance. Just as a rat may have basic taxes for approaching food or avoiding a bright light, so does it have a wider repertoire of affordances for possible actions associated with immediate sensing of its environment. We propose that affordances are extracted by the rat posterior parietal cortex, which guides action selection by the premotor cortex and is influenced also by hypothalamic drive information. The taxon-affordances model (TAM) for taxon-based determination of movement direction is based on models of frog detour behavior, with expectations of future reward implemented using reinforcement learning. The specification of the direction of movement is refined by current affordances and motivational information to yield an appropriate course of action. The world graph (WG) theory expands the idea of a map by developing the hypothesis that cognitive and motivational states interact. This article describes an implementation of this theory, the WG model. The integrated TAM-WG model then allows us to explain data on the behavior of rats with and without fornix lesions, which disconnect the hippocampus from other neural systems.",fullPaper,jv320
p1802,dce135859cb1bc830946f745067104b8e34d135c,c6,Americas Conference on Information Systems,Graph theory and applications,Abstract content,poster,cp6
p1803,b819dcbd786315de85aa649b87c90a15206a5db0,c95,IEEE International Conference on Computer Vision,Three short proofs in graph theory,Abstract content,poster,cp95
p1804,80aae60da893d9fa40ac4bda63f2ae1aa3a29b55,c55,Annual Workshop of the Psychology of Programming Interest Group,Graph theory in network analysis,Abstract content,poster,cp55
p1805,74e3c1ddf732f0970882ef3712465b3b62318d11,c58,Australian Software Engineering Conference,Graph theory and Feynman integrals,Abstract content,poster,cp58
p1806,4f270d2b736a4ef269bdfe0e7a1478027e370085,c16,Knowledge Discovery and Data Mining,Some Applications of Graph Theory to the Structural Analysis of Mechanisms,Abstract content,poster,cp16
p1807,a0c1483e0e34fb9bc1baf929ce892f111e329fa6,j20,Proceedings of the National Academy of Sciences of the United States of America,From time series to complex networks: The visibility graph,"In this work we present a simple and fast computational method, the visibility algorithm, that converts a time series into a graph. The constructed graph inherits several properties of the series in its structure. Thereby, periodic series convert into regular graphs, and random series do so into random graphs. Moreover, fractal series convert into scale-free networks, enhancing the fact that power law degree distributions are related to fractality, something highly discussed recently. Some remarkable examples and analytical tools are outlined to test the method's reliability. Many different measures, recently developed in the complex network theory, could by means of this new approach characterize time series from a new point of view.",fullPaper,jv20
p1808,527881a4a938ae06856994bc86d2b792d8dabf41,c92,Advances in Soft Computing,On a Connection of Number Theory with Graph Theory,Abstract content,poster,cp92
p1809,96f795889e761f6eb0b4601b5803ed48294d6930,c44,International Workshop on Green and Sustainable Software,Discrete Mathematics With Graph Theory,"From the Publisher: 
Adopting a user-friendly, conversationaland at times humorousstyle, these authors make the principles and practices of discrete mathematics as stimulating as possible while presenting comprehensive, rigorous coverage. Examples and exercises integrated throughout each chapter serve to pique reader interest and bring clarity to even the most complex concepts. Above all, the book is designed to engage today's readers in the interesting, applicable facets of modern mathematics. More than 200 worked examples and problems, as well as over 2500 exercises are included. Full solutions are provided in the back of the book. More than 150 Pausesshort questions inserted at strategic pointsare included. Full solutions to Pauses are included at the end of each section. For educators in area of discrete mathematics.",poster,cp44
p1810,ec5e9df34a4671c07d54d73e0e6b67be47cb71af,j321,Mathematical Gazette,On a Problem in Graph Theory,Suppose there are n towns every pair of which are connected by a single one-way road (roads meet only at towns). Is it possible to choose the direction of the traffic on all the roads so that if any two towns are named there is always a third from which the two named can be reached directly by road?,fullPaper,jv321
p1811,963cd79bb635416aaf887206dcc21e323258e12a,c43,ACM Symposium on Applied Computing,Graph theory,Abstract content,poster,cp43
p1812,025c03c7fd1c410ca3a38d94f5696ddee5c28811,c34,IEEE Working Conference on Mining Software Repositories,Some problems in graph theory,Abstract content,poster,cp34
p1813,d5151713a569f203e8cc93d3aee5f3a2168ebe8a,c22,International Conference on Data Technologies and Applications,The Many Facets of Graph Theory,Abstract content,poster,cp22
p1814,f7f505e4984a8a60f4d7c3aafd4ad1875913fe2d,j318,Journal of combinatorial theory. Series B (Print),Maximizing the total number of spanning trees in a graph: Two related problems in graph theory and optimum design theory,Abstract content,fullPaper,jv318
p1815,aa31f664fb8c6fa41291832376abe94804410d14,j322,Journal of combinatorial theory. Series A,An Extremal Problem for Sets with Applications to Graph Theory,Abstract content,fullPaper,jv322
p1816,78d86a61dcba9d0ccaabf5de1f7ae19a033e0121,c86,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,Graph Theory As A Mathematical Model In Social Science,Abstract content,poster,cp86
p1817,8df919ca5405ca5d1d8b192c14584b4354fa3dde,c23,International Conference on Open and Big Data,SOME ODD GRAPH THEORY,Abstract content,poster,cp23
p1818,dacf87fd2f31762facc1dc7d3190c3455aa78d80,j168,Proceedings of the IEEE,Applied graph theory: Graphs and electrical networks,Abstract content,fullPaper,jv168
p1819,21569e44f1daa0a10f635391e1fdfed43decc394,c78,Neural Information Processing Systems,Graph theory and Gaussian elimination.,Abstract content,poster,cp78
p1820,8e91e3c18bff916938fb9d5a742611b6b423bc49,c57,IEEE International Conference on Engineering of Complex Computer Systems,Applications of graph theory algorithms,Abstract content,poster,cp57
p1821,4213b7478d12581b144e67d6355313297c80f926,j75,Lecture Notes in Computer Science,Graph Theory and Algorithms,Abstract content,fullPaper,jv75
p1822,aed08399802fe0ce11ddc17ac98758205aa26ce4,j312,Discrete Mathematics,On a valence problem in extremal graph theory,Abstract content,fullPaper,jv312
p1823,2f6d6254a33015aec2bcc0a79561262eb9f473ed,c114,IEEE International Conference on Robotics and Automation,Extremal graph theory with emphasis on probabilistic methods,Subdivisions Contractions Small graphs of large girth Large graphs of small diameter Cycles in dense graphs The evolution of random graphs The size Ramsey number of a path Weakly saturated graphs List colourings.,poster,cp114
p1824,b134ce6220b0c39bc3d258c7388afb589016a6c4,c44,International Workshop on Green and Sustainable Software,Graph theory and related topics,Abstract content,poster,cp44
p1825,a658730fe7dd269186dca9b09cde32fed232c169,c72,Intelligent Systems in Molecular Biology,Computational chemical graph theory,Abstract content,poster,cp72
p1826,4e618c4e02ec9cfe1e2e07dfeb9155071dabe9fa,c76,International Conference on Artificial Neural Networks,Extremal problems in graph theory,Abstract content,poster,cp76
p1827,7b44986e28bddf8bc163ac34b439b67169c29b55,c102,International Conference on Biometrics,Graph theory and computing,Abstract content,poster,cp102
p1828,8d15baabbc36f2bcaae9d023b8b3cc974c64428a,c111,International Society for Music Information Retrieval Conference,Applications of Graph Theory to Group Structure.,Abstract content,poster,cp111
p1829,56c979574a36c9f5bc7915964adc3d22972415c5,c107,British Machine Vision Conference,"Graph theory, coding theory, and block designs","Introduction 1. A brief introduction to design theory 2. Strongly regular graphs 3, Quasi-symmetric designs 4. Strongly regular graphs with no triangles 5. Polarities of designs 6. Extension of graphs 7. Codes 8. Cyclic codes 9. Threshold decoding 10. Reed-Muller codes 11. Self-orthogonal codes and designs 12. Quadratic residue codes 13. Symmetry codes over GF(3) 14. Nearly perfect binary codes and uniformly packed codes 15. Association schemes References Index.",poster,cp107
p1830,2b694677da99739df582a93bb7e6ca9a92aaf0fb,c87,European Conference on Computer Vision,Applications of graph theory,Abstract content,poster,cp87
p1831,bcb089785e52cfaebdf9de029ec3a6a7586dbe8c,c12,International Conference on Statistical and Scientific Database Management,Proof Techniques in Graph Theory,Abstract content,poster,cp12
p1832,3e3d6d84f66c6f26db23711931c87db01baf9d81,c46,Brazilian Symposium on Software Engineering,Graph theory and molecular orbitals,Abstract content,poster,cp46
p1833,ffcd4cb12e23e13e38c734031cf45571b93e8fbb,c19,ACM Conference on Economics and Computation,Graph Theory and Topology in Chemistry,Abstract content,poster,cp19
p1834,024d1cffa9e112d9e81c76cc00fb44e3bd761011,c34,IEEE Working Conference on Mining Software Repositories,Progress in Graph Theory,Abstract content,poster,cp34
p1835,6a52ba688f0c813dcd1eab78ee159ef9c2f5a31f,c39,International Conference on Global Software Engineering,Algebraic methods in graph theory,Abstract content,poster,cp39
p1836,6fc1057ccc4d430051b69d456224ce12bca17dcc,c17,International Conference on Enterprise Information Systems,Graph theory in modern engineering,Abstract content,poster,cp17
p1837,07dc56d86321ff89a96fd32521fbaefe24ea7376,c14,International Conference on Exploring Services Science,Topics in graph theory,Abstract content,poster,cp14
p1838,137a07b135e1bf80cd4aaa1207f8081dba5764bd,j323,Applied Mathematical Sciences,Foundations of Chemical Reaction Network Theory,Abstract content,fullPaper,jv323
p1839,4dd69578aaa585b5ad3f755fd38fff1402a22dec,c79,International Conference on Medical Image Computing and Computer-Assisted Intervention,LANDSCAPE CONNECTIVITY: A GRAPH‐THEORETIC PERSPECTIVE,"Ecologists are familiar with two data structures commonly used to represent landscapes. Vector-based maps delineate land cover types as polygons, while raster lattices represent the landscape as a grid. Here we adopt a third lattice data structure, the graph. A graph represents a landscape as a set of nodes (e.g., habitat patches) connected to some degree by edges that join pairs of nodes functionally (e.g., via dispersal). Graph theory is well developed in other fields, including geography (transportation networks, routing ap- plications, siting problems) and computer science (circuitry and network optimization). We present an overview of basic elements of graph theory as it might be applied to issues of connectivity in heterogeneous landscapes, focusing especially on applications of metapo- pulation theory in conservation biology. We develop a general set of analyses using a hypothetical landscape mosaic of habitat patches in a nonhabitat matrix. Our results suggest that a simple graph construct, the minimum spanning tree, can serve as a powerful guide to decisions about the relative importance of individual patches to overall landscape con- nectivity. We then apply this approach to an actual conservation scenario involving the",poster,cp79
p1840,133baa61ce0d210e67f12cbab6fd773a9d79a165,c64,Experimental Software Engineering Network,Graph theory,3. Colorings of graphs 7 3.,poster,cp64
p1841,ad9fc8f6a00093138157f789e1c1d4cb503b53ae,c54,International Workshop on Agent-Oriented Software Engineering,An Introduction to the Theory of Graph Spectra: References,Preface 1. Introduction 2. Graph operations and modifications 3. Spectrum and structure 4. Characterizations by spectra 5. Structure and one eigenvalue 6. Spectral techniques 7. Laplacians 8. Additional topics 9. Applications Appendix Bibliography Index of symbols Index.,poster,cp54
p1842,2b9e05c6e282934f69c366f1856b637a921fed5f,c71,IEEE International Conference on Information Reuse and Integration,Graph Structure and Monadic Second-Order Logic - A Language-Theoretic Approach,"The study of graph structure has advanced in recent years with great strides: finite graphs can be described algebraically, enabling them to be constructed out of more basic elements. Separately the properties of graphs can be studied in a logical language called monadic second-order logic. In this book, these two features of graph structure are brought together for the first time in a presentation that unifies and synthesizes research over the last 25 years. The author not only provides a thorough description of the theory, but also details its applications, on the one hand to the construction of graph algorithms, and, on the other to the extension of formal language theory to finite graphs. Consequently the book will be of interest to graduate students and researchers in graph theory, finite model theory, formal language theory, and complexity theory.",poster,cp71
p1843,c7e6d7d5c6a9438e3ea15c2c4bd440e63847ffbf,j324,The VLDB journal,"The core decomposition of networks: theory, algorithms and applications",Abstract content,fullPaper,jv324
p1844,39cd2f4696bcc1019e218fb7724d1fa263fee9f7,j318,Journal of combinatorial theory. Series B (Print),Supereulerian graphs and the Petersen graph,"A graph G is supereulerian if G has a spanning eulerian subgraph. Boesch et al. [J. Graph Theory, 1, 79–84 (1977)] proposed the problem of characterizing supereulerian graphs. In this paper, we prove that any 3-edge-connected graph with at most 11 edge-cuts of size 3 is supereulerian if and only if it cannot be contractible to the Petersen graph. This extends a former result of Catlin and Lai [J. Combin. Theory, Ser. B, 66, 123–139 (1996)].",fullPaper,jv318
p1845,cc0b5f0ea6a81c2f9ee95e8a246733b630c11e63,j325,Ecosphere,Guidelines for a graph-theoretic implementation of structural equation modeling,"Structural equation modeling (SEM) is increasingly being chosen by researchers as a framework for gaining scientific insights from the quantitative analyses of data. New ideas and methods emerging from the study of causality, influences from the field of graphical modeling, and advances in statistics are expanding the rigor, capability, and even purpose of SEM. Guidelines for implementing the expanded capabilities of SEM are currently lacking. In this paper we describe new developments in SEM that we believe constitute a third-generation of the methodology. Most characteristic of this new approach is the generalization of the structural equation model as a causal graph. In this generalization, analyses are based on graph theoretic principles rather than analyses of matrices. Also, new devices such as metamodels and causal diagrams, as well as an increased emphasis on queries and probabilistic reasoning, are now included. Estimation under a graph theory framework permits the use of Bayesian or likelihood methods. The guidelines presented start from a declaration of the goals of the analysis. We then discuss how theory frames the modeling process, requirements for causal interpretation, model specification choices, selection of estimation method, model evaluation options, and use of queries, both to summarize retrospective results and for prospective analyses. 
 
The illustrative example presented involves monitoring data from wetlands on Mount Desert Island, home of Acadia National Park. Our presentation walks through the decision process involved in developing and evaluating models, as well as drawing inferences from the resulting prediction equations. In addition to evaluating hypotheses about the connections between human activities and biotic responses, we illustrate how the structural equation (SE) model can be queried to understand how interventions might take advantage of an environmental threshold to limit Typha invasions. 
 
The guidelines presented provide for an updated definition of the SEM process that subsumes the historical matrix approach under a graph-theory implementation. The implementation is also designed to permit complex specifications and to be compatible with various estimation methods. Finally, they are meant to foster the use of probabilistic reasoning in both retrospective and prospective considerations of the quantitative implications of the results.",fullPaper,jv325
p1846,f96f659ada20f35e9fce66306c8f8f6a471e592d,c62,International Conference on Software Reuse,Graph theoretical analysis of magnetoencephalographic functional connectivity in Alzheimer's disease.,"In this study we examined changes in the large-scale structure of resting-state brain networks in patients with Alzheimer's disease compared with non-demented controls, using concepts from graph theory. Magneto-encephalograms (MEG) were recorded in 18 Alzheimer's disease patients and 18 non-demented control subjects in a no-task, eyes-closed condition. For the main frequency bands, synchronization between all pairs of MEG channels was assessed using a phase lag index (PLI, a synchronization measure insensitive to volume conduction). PLI-weighted connectivity networks were calculated, and characterized by a mean clustering coefficient and path length. Alzheimer's disease patients showed a decrease of mean PLI in the lower alpha and beta band. In the lower alpha band, the clustering coefficient and path length were both decreased in Alzheimer's disease patients. Network changes in the lower alpha band were better explained by a 'Targeted Attack' model than by a 'Random Failure' model. Thus, Alzheimer's disease patients display a loss of resting-state functional connectivity in lower alpha and beta bands even when a measure insensitive to volume conduction effects is used. Moreover, the large-scale structure of lower alpha band functional networks in Alzheimer's disease is more random. The modelling results suggest that highly connected neural network 'hubs' may be especially at risk in Alzheimer's disease.",poster,cp62
p1847,d9429f3ae8d04b0e0f357ada8389e066d664c294,c35,EUROMICRO Conference on Software Engineering and Advanced Applications,Theory and Applications,"Infinitary rewriting generalises usual finitary rewriting by providing infinite reduction sequences with a notion of convergence. The idea of – at least conceptually – assigning a meaning to infinite derivations is well-known, for example, from lazy functional programming or from process calculi. Infinitary rewriting makes it possible to apply rewriting in order to obtain a formal model for such infinite derivations. The goal of this thesis is to comprehensively survey the field of infinitary term rewriting, to point out its shortcomings, and to try to overcome some of these shortcomings. The most significant problem that arises in infinitary rewriting is the inherent difficulty to finitely represent and, hence, to implement it. To this end, we consider term graph rewriting, which is able to finitely represent restricted forms of infinitary term rewriting. Moreover, we study different models that are used to formalise infinite reduction sequences: The well-established metric approach as well as an alternative approach using partial orders. Both methods together with the consequent infinitary versions of confluence and termination properties are analysed on an abstract level. Based on this, we argue that the partial order model has more advantageous properties and represents the intuition of convergence in a more natural way. This assessment is also backed up by the results that we obtain for infinitary term rewriting: Unlike the metric approach, the partial order approach admits to generalise some results known from finitary orthogonal term rewriting – most importantly, confluence. It is also shown that so-called Böhm trees, usually constructed rather intricately, naturally arise as normal forms in the partial order model. Finally, we devise a complete ultrametric and a complete semilattice on term graphs both of which are used to introduce infinitary term graph rewriting. This is supposed to serve as a tool in order to investigate the limitations of term graph rewriting for implementing infinitary term rewriting.",poster,cp35
p1848,b8af507417e61be3ab2ba21a9d8c6a8174bb1be6,c84,The Web Conference,Subgraph frequencies: mapping the empirical and extremal geography of large graph collections,"A growing set of on-line applications are generating data that can be viewed as very large collections of small, dense social graphs --- these range from sets of social groups, events, or collaboration projects to the vast collection of graph neighborhoods in large social networks. A natural question is how to usefully define a domain-independent 'coordinate system' for such a collection of graphs, so that the set of possible structures can be compactly represented and understood within a common space. In this work, we draw on the theory of graph homomorphisms to formulate and analyze such a representation, based on computing the frequencies of small induced subgraphs within each graph. We find that the space of subgraph frequencies is governed both by its combinatorial properties --- based on extremal results that constrain all graphs --- as well as by its empirical properties --- manifested in the way that real social graphs appear to lie near a simple one-dimensional curve through this space. We develop flexible frameworks for studying each of these aspects. For capturing empirical properties, we characterize a simple stochastic generative model, a single-parameter extension of Erdos-Renyi random graphs, whose stationary distribution over subgraphs closely tracks the one-dimensional concentration of the real social graph families. For the extremal properties, we develop a tractable linear program for bounding the feasible space of subgraph frequencies by harnessing a toolkit of known extremal graph theory. Together, these two complementary frameworks shed light on a fundamental question pertaining to social graphs: what properties of social graphs are 'social' properties and what properties are 'graph' properties? We conclude with a brief demonstration of how the coordinate system we examine can also be used to perform classification tasks, distinguishing between structures arising from different types of social graphs.",fullPaper,cp84
p1849,5b64f9fe601db2cc862023efbaf56d9b74f1eef4,c62,International Conference on Software Reuse,Large Networks and Graph Limits,"Recently, it became apparent that a large number of the most interesting structures and phenomena of the world can be described by networks. To develop a mathematical theory of very large networks is an important challenge. This book describes one recent approach to this theory, the limit theory of graphs, which has emerged over the last decade. The theory has rich connections with other approaches to the study of large networks, such as ""property testing"" in computer science and regularity partition in graph theory. It has several applications in extremal graph theory, including the exact formulations and partial answers to very general questions, such as which problems in extremal graph theory are decidable. It also has less obvious connections with other parts of mathematics (classical and non-classical, like probability theory, measure theory, tensor algebras, and semidefinite optimization). This book explains many of these connections, first at an informal level to emphasize the need to apply more advanced mathematical methods, and then gives an exact development of the theory of the algebraic theory of graph homomorphisms and of the analytic theory of graph limits. This is an amazing book: readable, deep, and lively. It sets out this emerging area, makes connections between old classical graph theory and graph limits, and charts the course of the future. --Persi Diaconis, Stanford University This book is a comprehensive study of the active topic of graph limits and an updated account of its present status. It is a beautiful volume written by an outstanding mathematician who is also a great expositor. --Noga Alon, Tel Aviv University, Israel Modern combinatorics is by no means an isolated subject in mathematics, but has many rich and interesting connections to almost every area of mathematics and computer science. The research presented in Lovasz's book exemplifies this phenomenon. This book presents a wonderful opportunity for a student in combinatorics to explore other fields of mathematics, or conversely for experts in other areas of mathematics to become acquainted with some aspects of graph theory. --Terence Tao, University of California, Los Angeles, CA Laszlo Lovasz has written an admirable treatise on the exciting new theory of graph limits and graph homomorphisms, an area of great importance in the study of large networks. It is an authoritative, masterful text that reflects Lovasz's position as the main architect of this rapidly developing theory. The book is a must for combinatorialists, network theorists, and theoretical computer scientists alike. --Bela Bollobas, Cambridge University, UK",poster,cp62
p1850,c80238af26563d109434d30286a526e32dce95e2,c85,International Conference on Graph Transformation,Graph Spectra for Complex Networks,"Analyzing the behavior of complex networks is an important element in the design of new man-made structures such as communication systems and biologically engineered molecules. Because any complex network can be represented by a graph, and therefore in turn by a matrix, graph theory has become a powerful tool in the investigation of network performance. This self-contained book provides a concise introduction to the theory of graph spectra and its applications to the study of complex networks. Covering a range of types of graphs and topics important to the analysis of complex systems, this guide provides the mathematical foundation needed to understand and apply spectral insight to real-world systems. In particular, the general properties of both the adjacency and Laplacian spectrum of graphs are derived and applied to complex networks. An ideal resource for researchers and students in communications networking as well as in physics and mathematics.",poster,cp85
p1851,92170231069fd144805ba9c356ee002db123381b,c49,International Symposium on Search Based Software Engineering,Riemann–Roch and Abel–Jacobi theory on a finite graph,Abstract content,poster,cp49
p1852,e1a50831ee71998ca4c577bb996e6353c5eb2d4a,c46,Brazilian Symposium on Software Engineering,Connectedness Index of uncertain Graph,"In practical applications of graph theory, non-deterministic factors are frequently encountered. This paper employs uncertainty theory to deal with non-deterministic factors in problems of graph connectivity. The concepts of uncertain graph and connectedness index of uncertain graph are proposed in this paper. It presents two algorithms to calculate connectedness index of an uncertain graph.",poster,cp46
p1853,9e0b0decf155c3f802017238d0096255f75a263c,c67,Enterprise Application Integration,Topological Graph Polynomials in Colored Group Field Theory,Abstract content,poster,cp67
p1854,a09f3adb2c533223a6780dd23dc9809c6c6bb016,c111,International Society for Music Information Retrieval Conference,"Data clustering - theory, algorithms, and applications","Preface Part I. Clustering, Data and Similarity Measures: 1. Data clustering 2. DataTypes 3. Scale conversion 4. Data standardization and transformation 5. Data visualization 6. Similarity and dissimilarity measures Part II. Clustering Algorithms: 7. Hierarchical clustering techniques 8. Fuzzy clustering algorithms 9. Center Based Clustering Algorithms 10. Search based clustering algorithms 11. Graph based clustering algorithms 12. Grid based clustering algorithms 13. Density based clustering algorithms 14. Model based clustering algorithms 15. Subspace clustering 16. Miscellaneous algorithms 17. Evaluation of clustering algorithms Part III. Applications of Clustering: 18. Clustering gene expression data Part IV. Matlab and C++ for Clustering: 19. Data clustering in Matlab 20. Clustering in C/C++ A. Some clustering algorithms B. Thekd-tree data structure C. Matlab Codes D. C++ Codes Subject index Author index.",poster,cp111
p1855,804b3bbc7e5b9a14d446ff9f92236652cf1b1c72,c114,IEEE International Conference on Robotics and Automation,Spectra of Graphs: Theory and Applications,Introduction. Basic Concepts of the Spectrum of a Graph. Operations on Graphs and the Resulting Spectra. Relations Between Spectral and Structural Properties of Graphs. The Divisor of a Graph. The Spectrum and the Group of Automorphisms. Characterization of Graphs by Means of Spectra. Spectra Techniques in Graph Theory and Combinatories. Applications in Chemistry an Physics. Some Additional Results. Appendix. Tables of Graph Spectra Biblgraphy. Index of Symbols. Index of Names. Subject Index.,poster,cp114
p1856,c0f83e632def1b85df9e4f316b673d9eac05b3f1,c28,International Conference on Collaboration Technologies and Systems,Theory of didactical situations in mathematics,"ion is not inevitable. In fact, shopkeepers have never abstracted the structures of modules which regulate their financial exchanges because they haven’t the motivation to do so. Schematization and formulation The schematization and the formulation of the structure follows the process of identification and of updating. Diénès does not plan any general situations specific to this stage (would something which is well conceived of spell itself out clearly?) but the representation by a graph is often envisaged as a simplified but natural and direct expression of the thought of a child. Representing objects by points and operators by arrows is learned by the use of imitation, like a language. Symbolization Symbolization is the transcription in a new language of properties represented in the preceding stage.",poster,cp28
p1857,d224c80ac2034d832a35ba646a0062bd773ee3c4,j91,Physical Review Letters,New Integrable 4D Quantum Field Theories from Strongly Deformed Planar N=4 Supersymmetric Yang-Mills Theory.,"We introduce a family of new integrable quantum field theories in four dimensions by considering the γ-deformed N=4 supersymmetric Yang-Mills (SYM) theory in the double scaling limit of large imaginary twists and small coupling. This limit discards the gauge fields and retains only certain Yukawa and scalar interactions with three arbitrary effective couplings. In the 't Hooft limit, these 4D theories are integrable, and contain a wealth of conformal correlators such that the whole arsenal of AdS/CFT integrability remains applicable. As a special case of these models, we obtain a quantum field theory of two complex scalars with a chiral, quartic interaction. The Berenstein-Maldacena-Nastase vacuum anomalous dimension is dominated in each loop order by a single ""wheel"" graph, whose bulk represents an integrable ""fishnet"" graph. This explicitly demonstrates the all-loop integrability of gamma-deformed planar N=4 SYM theory, at least in our limit. Using this feature and integrability results we provide an explicit conjecture for the periods of double-wheel graphs with an arbitrary number of spokes in terms of multiple zeta values of limited depth.",fullPaper,jv91
p1858,49768fb7280aec248809ec47594a4458888018a8,j326,Nonlinear Biomedical Physics,Graph theoretical analysis of complex networks in the brain,Abstract content,fullPaper,jv326
p1859,0fb7af235cac61c78c4d184f718b043f3a0e37a4,c97,Interspeech,Graph Algorithms in the Language of Linear Algebra,"The thesis presents usefulness of duality between graph and his adjacency matrix. The teoretical part provides the basis of graph theory and matrix algebra mainly focusing on sparse matrices and options of their presentation witch takes into account the number of nonzero elements in the matrix. The thesis includes presentation of possible operations on sparse matrices and algorithms that basically work on graphs, but with help of duality between graph and his adjacency matrix can be presented with sequence of operations on matrices. 
Practical part presents implementation of some algorithms that can work both with graphs or their adjacency matrices in programming language Java and testing algorithms that work with matrices. 
It focuses on comparison in efficiency of algorithm working with matrix written in standard mode and with matrix written in format for sparse matrices. It also studies witch presentation of matrices works beter for witch algorithm.",poster,cp97
p1860,89cc8969c288b3cf6b5c691baaafb7c8e5c82d1b,c49,International Symposium on Search Based Software Engineering,Open problems in the spectral theory of signed graphs,"Signed graphs are graphs whose edges get a sign $+1$ or $-1$ (the signature). Signed graphs can be studied by means of graph matrices extended to signed graphs in a natural way. Recently, the spectra of signed graphs have attracted much attention from graph spectra specialists. One motivation is that the spectral theory of signed graphs elegantly generalizes the spectral theories of unsigned graphs. On the other hand, unsigned graphs do not disappear completely, since their role can be taken by the special case of balanced signed graphs. 
Therefore, spectral problems defined and studied for unsigned graphs can be considered in terms of signed graphs, and sometimes such generalization shows nice properties which cannot be appreciated in terms of (unsigned) graphs. Here, we survey some general results on the adjacency spectra of signed graphs, and we consider some spectral problems which are inspired from the spectral theory of (unsigned) graphs.",poster,cp49
p1861,5874afa99a66458efe791bfa5e7196ec870348db,c8,The Compass,TOPICS IN GEOMETRIC GROUP THEORY,"We present a brief overview of methods and results in geometric group theory, with the goal of introducing the reader to both topological and metric perspectives. Prerequisites are kept to a minimum: we require only basic algebra, graph theory, and metric space topology.",poster,cp8
p1862,5006347265a80c6df383d9be87361b1c2a1b1b19,c89,Conference on Uncertainty in Artificial Intelligence,Introduction to Coding Theory,"Definition 1 (distance amplified code G(C)) Let G = (L,R,E) be a bipartite graph with L = [n], R = [m], which is D-left-regular and d-right-regular. Let C be a binary linear code of block length n = |L|. For c ∈ {0, 1}n, define G(c) ∈ ({0, 1}d)m by G(c)j = (cΓ1(j), cΓ2(j), · · · , cΓd(j)), for j ∈ [m], where Γi(j) ∈ L denotes the i-th neighbor of j ∈ R. Now define the code G(C) as G(C) = {G(c)|c ∈ C}. Since each bit of a codeword c ∈ C is repeated D times in the associated codeword G(c) ∈ G(C), we have",poster,cp89
p1863,1d0bf1d65b61383b0f89aef7b907716f7e411128,c99,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,Image Processing and Analysis With Graphs: theory and Practice,"Graph Theory Concepts and Definitions Used in Image Processing and Analysis, O. Lezoray and L. Grady Introduction Basic Graph Theory Graph Representation Paths, Trees, and Connectivity Graph Models in Image Processing and Analysis Graph Cuts-Combinatorial Optimization in Vision, H. Ishikawa Introduction Markov Random Field Basic Graph Cuts: Binary Labels Multi-Label Minimization Examples Higher-Order Models in Computer Vision, P. Kohli and C. Rother Introduction Higher-Order Random Fields Patch and Region-Based Potentials Relating Appearance Models and Region-Based Potentials Global Potentials Maximum a Posteriori Inference A Parametric Maximum Flow Approach for Discrete Total Variation Regularization, A. Chambolle and J. Darbon Introduction Idea of the approach Numerical Computations Applications Targeted Image Segmentation Using Graph Methods, L. Grady The Regularization of Targeted Image Segmentation Target Specification Conclusion A Short Tour of Mathematical Morphology on Edge and Vertex Weighted Graphs, L. Najman and F. Meyer Introduction Graphs and lattices Neighborhood Operations on Graphs Filters Connected Operators and Filtering with the Component Tree Watershed Cuts MSF Cut Hierarchy and Saliency Maps Optimization and the Power Watershed Partial Difference Equations on Graphs for Local and Nonlocal Image Processing, A. Elmoataz, O. Lezoray, V.-T. Ta, and S. Bougleux Introduction Difference Operators on Weighted Graphs Construction of Weighted Graphs p-Laplacian Regularization on Graphs Examples Image Denoising with Nonlocal Spectral Graph Wavelets, D.K. Hammond, L. Jacques, and P. Vandergheynst Introduction Spectral Graph Wavelet Transform Nonlocal Image Graph Hybrid Local/Nonlocal Image Graph Scaled Laplacian Model Applications to Image Denoising Conclusions Acknowledgments Image and Video Matting, J. Wang Introduction Graph Construction for Image Matting Solving Image Matting Graphs Data Set Video Matting Optimal Simultaneous Multisurface and Multiobject Image Segmentation, X. Wu, M.K. Garvin, and M. Sonka Introduction Motivation and Problem Description Methods for Graph-Based Image Segmentation Case Studies Conclusion Acknowledgments Hierarchical Graph Encodings, L. Brun and W. Kropatsch Introduction Regular Pyramids Irregular Pyramids Parallel construction schemes Irregular Pyramids and Image properties Graph-Based Dimensionality Reduction, J.A. Lee and M. Verleysen Summary Introduction Classical methods Nonlinearity through Graphs Graph-Based Distances Graph-Based Similarities Graph embedding Examples and comparisons Graph Edit Distance-Theory, Algorithms, and Applications, M. Ferrer and H. Bunke Introduction Definitions and Graph Matching Theoretical Aspects of GED GED Computation Applications of GED The Role of Graphs in Matching Shapes and in Categorization, B. Kimia Introduction Using Shock Graphs for Shape Matching Using Proximity Graphs for Categorization Conclusion Acknowledgment 3D Shape Registration Using Spectral Graph Embedding and Probabilistic Matching, A. Sharma, R. Horaud, and D. Mateus Introduction Graph Matrices Spectral Graph Isomorphism Graph Embedding and Dimensionality Reduction Spectral Shape Matching Experiments and Results Discussion Appendix: Permutation and Doubly- stochastic Matrices Appendix: The Frobenius Norm Appendix: Spectral Properties of the Normalized Laplacian Modeling Images with Undirected Graphical Models, M.F. Tappen Introduction Background Graphical Models for Modeling Image Patches Pixel-Based Graphical Models Inference in Graphical Models Learning in Undirected Graphical Models Tree-Walk Kernels for Computer Vision, Z. Harchaoui and F. Bach Introduction Tree-Walk Kernels as Graph Kernels The Region Adjacency Graph Kernel as a Tree-Walk Kernel The Point Cloud Kernel as a Tree-Walk Kernel Experimental Results Conclusion Acknowledgments",poster,cp99
p1864,c99ac1ab758d967c4c07e03ad650ecc73814feb6,c2,International Symposium on Intelligent Data Analysis,Nonstable K-theory for Graph Algebras,Abstract content,poster,cp2
p1865,e8266ceec6df4db568c8da78c4b9611b471dda46,c30,IEEE Aerospace Conference,Some Recent Progress and Applications in Graph Minor Theory,Abstract content,poster,cp30
p1866,cd3929c6e994a712739b907f06622eb814e18881,c94,Vision,A theory of graph comprehension.,Abstract content,poster,cp94
p1867,b6c8e5cb261b412656c79b34268f11643ff358a2,c32,International Conference on Software Technology: Methods and Tools,Introduction to the Algebraic Theory of Graph Grammars (A Survey),Abstract content,poster,cp32
p1868,c335cbe2404b1164001fdab1abd27019e864f900,j327,IEEE Signal Processing Magazine,Coalitional game theory for communication networks,"In this tutorial, we provided a comprehensive overview of coalitional game theory, and its usage in wireless and communication networks. For this purpose, we introduced a novel classification of coalitional games by grouping the sparse literature into three distinct classes of games: canonical coalitional games, coalition formation games, and coalitional graph games. For each class, we explained in details the fundamental properties, discussed the main solution concepts, and provided an in-depth analysis of the methodologies and approaches for using these games in both game theory and communication applications. The presented applications have been carefully selected from a broad range of areas spanning a diverse number of research problems. The tutorial also sheds light on future opportunities for using the strong analytical tool of coalitional games in a number of applications. In a nutshell, this article fills a void in existing communications literature, by providing a novel tutorial on applying coalitional game theory in communication networks through comprehensive theory and technical details as well as through practical examples drawn from both game theory and communication application.",fullPaper,jv327
p1869,1eae65773032fae65e7c003da3f67c47996c7f71,c81,IEEE Annual Symposium on Foundations of Computer Science,"Algorithmic graph minor theory: Decomposition, approximation, and coloring","At the core of the seminal graph minor theory of Robertson and Seymour is a powerful structural theorem capturing the structure of graphs excluding a fixed minor. This result is used throughout graph theory and graph algorithms, but is existential. We develop a polynomial-time algorithm using topological graph theory to decompose a graph into the structure guaranteed by the theorem: a clique-sum of pieces almost-embeddable into bounded-genus surfaces. This result has many applications. In particular we show applications to developing many approximation algorithms, including a 2-approximation to graph coloring, constant-factor approximations to treewidth and the largest grid minor, combinatorial polylogarithmic approximation to half-integral multicommodity flow, subexponential fixed-parameter algorithms, and PTASs for many minimization and maximization problems, on graphs excluding a fixed minor.",fullPaper,cp81
p1870,ffb03ba337560ccf1204e57e1d1bb831c55e21bf,j168,Proceedings of the IEEE,Graph-theoretic connectivity control of mobile robot networks,"In this paper, we provide a theoretical framework for controlling graph connectivity in mobile robot networks. We discuss proximity-based communication models composed of disk-based or uniformly-fading-signal-strength communication links. A graph-theoretic definition of connectivity is provided, as well as an equivalent definition based on algebraic graph theory, which employs the adjacency and Laplacian matrices of the graph and their spectral properties. Based on these results, we discuss centralized and distributed algorithms to maintain, increase, and control connectivity in mobile robot networks. The various approaches discussed in this paper range from convex optimization and subgradient-descent algorithms, for the maximization of the algebraic connectivity of the network, to potential fields and hybrid systems that maintain communication links or control the network topology in a least restrictive manner. Common to these approaches is the use of mobility to control the topology of the underlying communication network. We discuss applications of connectivity control to multirobot rendezvous, flocking and formation control, where so far, network connectivity has been considered an assumption.",fullPaper,jv168
p1871,b75b83c0d4a87b0133dd2cdc30e885092c3e13fd,c93,Human Language Technology - The Baltic Perspectiv,Graph Minor Theory,"A monumental project in graph theory was recently completed. The project, started by Robertson and Seymour, and later joined by Thomas, led to entirely new concepts and a new way of looking at graph theory. The motivating problem was Kuratowski’s characterization of planar graphs, and a far-reaching generalization of this, conjectured by Wagner: If a class of graphs is minor-closed (i.e., it is closed under deleting and contracting edges), then it can be characterized by a finite number of excluded minors. The proof of this conjecture is based on a very general theorem about the structure of large graphs: If a minor-closed class of graphs does not contain all graphs, then every graph in it is glued together in a tree-like fashion from graphs that can almost be embedded in a fixed surface. We describe the precise formulation of the main results and survey some of its applications to algorithmic and structural problems in graph theory.",poster,cp93
p1872,87c3e5caec704425c53b76c7f4497b9c45479554,c85,International Conference on Graph Transformation,Fundamental Theory for Typed Attributed Graph Transformation,Abstract content,fullPaper,cp85
p1873,09350843d21273d36caf22675cb588f88c518462,c9,Pacific Symposium on Biocomputing,Estimating and understanding exponential random graph models,"We introduce a method for the theoretical analysis of exponential random graph models. The method is based on a large-deviations approximation to the normalizing constant shown to be consistent using theory developed by Chatterjee and Varadhan [European J. Combin. 32 (2011) 1000-1017]. The theory explains a host of difficulties encountered by applied workers: many distinct models have essentially the same MLE, rendering the problems ``practically'' ill-posed. We give the first rigorous proofs of ``degeneracy'' observed in these models. Here, almost all graphs have essentially no edges or are essentially complete. We supplement recent work of Bhamidi, Bresler and Sly [2008 IEEE 49th Annual IEEE Symposium on Foundations of Computer Science (FOCS) (2008) 803-812 IEEE] showing that for many models, the extra sufficient statistics are useless: most realizations look like the results of a simple Erd\H{o}s-R\'{e}nyi model. We also find classes of models where the limiting graphs differ from Erd\H{o}s-R\'{e}nyi graphs. A limitation of our approach, inherited from the limitation of graph limit theory, is that it works only for dense graphs.",poster,cp9
p1874,ec2701b6b5c58e1ba851e28452a303f76cb4c7c2,c34,IEEE Working Conference on Mining Software Repositories,Term graph rewriting: theory and practice,"Partial table of contents: How to Get Confluence for Explicit Substitutions (T. Hardin) Graph Rewriting Systems for Efficient Compilation (Z. Ariola & Arvind) Abstract Reduction: Towards a Theory via Abstract Interpretation (M. van Eekelen, et al.) The Adequacy of Term Graph Rewriting for Simulating Term Rewriting (J. Kennaway, et al.) Hypergraph Rewriting: Critical Pairs and Undecidability of Confluence (D. Plump) MONSTR: Term Graph Rewriting for Parallel Machines (R. Banach) Parallel Execution of Concurrent Clean on ZAPP (R. Goldsmith, et al.) Implementing Logical Variables and Disjunctions in Graph Rewrite Systems (P. McBrien) Index.",poster,cp34
p1875,fa459de6552f5cd0cbe28539c0c7c65bc112a164,c77,Networks,"Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods",Abstract The subject of graphical methods for data analysis and for data presentation needs a scientific foundation. In this article we take a few steps in the direction of establishing such a foundation. Our approach is based on graphical perception—the visual decoding of information encoded on graphs—and it includes both theory and experimentation to test the theory. The theory deals with a small but important piece of the whole process of graphical perception. The first part is an identification of a set of elementary perceptual tasks that are carried out when people extract quantitative information from graphs. The second part is an ordering of the tasks on the basis of how accurately people perform them. Elements of the theory are tested by experimentation in which subjects record their judgments of the quantitative information on graphs. The experiments validate these elements but also suggest that the set of elementary tasks should be expanded. The theory provides a guideline for graph construction...,poster,cp77
p1876,62fa835ba18dfac1e5ee801da1e27634cc5370e7,c86,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,Graph-theoretic methods in database theory,"As in many areas of computer science and other disciplines, graph theoretic tools play an important role also in databases. Many concepts are best captured in terms of graphs or hypergraphs, and problems can then be formulated and solved using graph theoretic algorithms. There is a great number of such examples from schema design, dependency theory, transaction processing, query optimization, data distribution, and a host of other areas. We will not attempt to touch on the wide range of all these applications. Rather, we will concentrate on a particular, basic type of problems that has attracted a great deal of attention in the database literature over the last few years and has come to play a central role: techniques for searching graphs and computing transitive closure, and some of the applications and related problems in query processing. There is an extensive literature on these types of problems, which we cannot reasonably hope to cover in this space, but we shall give a flavour of the issues that arise in solving these problems in various frameworks.",fullPaper,cp86
p1877,60797ccfb655cc1678793cf16ec787f73816cbf4,c32,International Conference on Software Technology: Methods and Tools,An algebraic theory of graph reduction,"We show how membership in classes of graphs definable in monadic second order logic and of bounded treewidth can be decided by finite sets of terminating reduction rules. The method is constructive in the sense that we describe an algorithm which will produce, from a formula in monadic second order logic and an integer k such that the class defined by the formula is of treewidth ≤ k, a set of rewrite rules that reduces any member of the class to one of finitely many graphs, in a number of steps bounded by the size of the graph. This reduction system corresponds to an algorithm that runs in time linear in the size of the graph.",poster,cp32
p1878,c41eb895616e453dcba1a70c9b942c5063cc656c,c110,IEEE International Conference on Automatic Face & Gesture Recognition,Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering,"In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.",poster,cp110
p1879,ef401b7e4ea0beff79fd8b77f1447ce16880ee93,c77,Networks,The Structure of Complex Networks: Theory and Applications,"This book deals with the analysis of the structure of complex networks by combining results from graph theory, physics, and pattern recognition. The book is divided into two parts. 11 chapters are dedicated to the development of theoretical tools for the structural analysis of networks, and 7 chapters are illustrating, in a critical way, applications of these tools to real-world scenarios. The first chapters provide detailed coverage of adjacency and metric and topological properties of networks, followed by chapters devoted to the analysis of individual fragments and fragment-based global invariants in complex networks. Chapters that analyse the concepts of communicability, centrality, bipartivity, expansibility and communities in networks follow. The second part of this book is devoted to the analysis of genetic, protein residue, protein-protein interaction, intercellular, ecological and socio-economic networks, including important breakthroughs as well as examples of the misuse of structural concepts.",poster,cp77
p1880,9d061f8cc3cb4eb77579adcdbe99169b3e839b27,c87,European Conference on Computer Vision,Graph Degree Linkage: Agglomerative Clustering on a Directed Graph,Abstract content,fullPaper,cp87
p1881,19cd8122dc531dcdf70f1430331f1df9458ccda2,c13,International Conference on Data Science and Advanced Analytics,The Gewirtz Graph: An Exercise in the Theory of Graph Spectra,"We prove that there is a unique graph (on 56 vertices) with spectrum 101235(-4)20 and examine its structure. It turns out that, e.g., the Coxeter graph (on 28 vertices) and the Sylvester graph (on 36 vertices) are induced subgraphs. We give descriptions of this graph.",poster,cp13
p1882,0cac090d379836a82b7aae6abc222ab7822b8763,c24,Decision Support Systems,Graph- Theoretical Approaches to the Theory of Voting*,"In this article, language, concepts, and theorems from the theory of directed graphs are used to characterize and analyze the structure of majority preference. A number of results are then derived concerning ""sincere,"" ""sophisticated,"" and ""cooperative"" voting decisions under two common majority voting procedures. These results supplement the work of Black and Farquharson. Perhaps contrary to ""common-sense"" thinking, general strategic manipulation of voting processes has beneficial consequences. It is widely recognized-and not only by political scientists-that the decisions of a voting body may be affected not only by such obviously relevant matters as the preferences of its members and their participation in or absence from particular votes, but also by such ""technical"" matters as the nature of the voting procedure and the order in which proposals are voted on. It is also recognized that voting may have ""gamelike"" characteristics offering strategic opportunities both to voters as individuals and to voters in coalitions. Finally, most political scientists-though probably few politicians or citizens-are by now aware of the ""paradox of voting"" and may have some sense of its connection with these questions of decision, procedure, and strategy. Over the past decade or so a somewhat technical literature on the theory of voting has developed in the ""public choice"" area. The present article adds to this literature by presenting a number of new propositions concerning majority voting under two common voting procedures. These propositions pertain to the questions alluded to in the first paragraph. These new results, together with some more familiar ones, are obtained by employing language, concepts, and theorems from the mathematical theory of directed graphs. In these respects, the article will be of interest primarily to specialists in the area *This article is in part a combination and revision of two unpublished papers:",poster,cp24
p1883,4060c1e0491f3cc34f77bd623592799d04f5c78a,j328,Ecology Letters,Graph models of habitat mosaics.,"Graph theory is a body of mathematics dealing with problems of connectivity, flow, and routing in networks ranging from social groups to computer networks. Recently, network applications have erupted in many fields, and graph models are now being applied in landscape ecology and conservation biology, particularly for applications couched in metapopulation theory. In these applications, graph nodes represent habitat patches or local populations and links indicate functional connections among populations (i.e. via dispersal). Graphs are models of more complicated real systems, and so it is appropriate to review these applications from the perspective of modelling in general. Here we review recent applications of network theory to habitat patches in landscape mosaics. We consider (1) the conceptual model underlying these applications; (2) formalization and implementation of the graph model; (3) model parameterization; (4) model testing, insights, and predictions available through graph analyses; and (5) potential implications for conservation biology and related applications. In general, and for a variety of ecological systems, we find the graph model a remarkably robust framework for applications concerned with habitat connectivity. We close with suggestions for further work on the parameterization and validation of graph models, and point to some promising analytic insights.",fullPaper,jv328
p1884,fc45cd4563ce8ab02a5fe5facb22ea69864d9ef0,c12,International Conference on Statistical and Scientific Database Management,Information Theory and Network Coding,"This book contains a thorough discussion of the classical topics in information theory together with the first comprehensive treatment of network coding, a subject first emerged under information theory in the mid 1990's that has now diffused into coding theory, computer networks, wireless communications, complexity theory, cryptography, graph theory, etc. With a large number of examples, illustrations, and original problems, this book is excellent as a textbook or reference book for a senior or graduate level course on the subject, as well as a reference for researchers in related fields.",poster,cp12
p1885,a968620dcf399d7d7f89f98dc1d3da8c114dac8d,j0,Nature Biotechnology,"MaxQuant enables high peptide identification rates, individualized p.p.b.-range mass accuracies and proteome-wide protein quantification",Abstract content,fullPaper,jv0
p1886,31fa50edbabd71611da501f6246d92c2562c4b46,c113,International Conference on Image Analysis and Processing,Graph Ramsey theory and the polynomial hierarchy,"Summary form only given, as follows. In the Ramsey theory of graphs F/spl rarr/(G, H) means that for every way of coloring the edges of F red and blue F will contain either a red G or a blue H as a subgraph. The problem ARROWING of deciding whether F/spl rarr/(G, H) lies in /spl Pi//sub 2//sup P/=coNP/sup NP/ and it was shown to be coNP-hard by S.A. Burr (1990). We prove that ARROWING is actually /spl Pi//sub 2//sup P/-complete, simultaneously settling a conjecture of Burr and providing a natural example of a problem complete for a higher level of the polynomial hierarchy. We also consider several specific variants of ARROWING, where G and H are restricted to particular families of graphs. We have a general completeness result for this case under the assumption that certain graphs are constructible in polynomial time. Furthermore we show that STRONG ARROWING, the version of ARROWING for induced subgraphs, is /spl Pi//sub 2//sup P/-complete.",poster,cp113
p1887,e507a66243223b83c50ec8609c8e2db5a99277a7,j291,IEEE Transactions on Information Theory,A Spectral Graph Uncertainty Principle,"The spectral theory of graphs provides a bridge between classical signal processing and the nascent field of graph signal processing. In this paper, a spectral graph analogy to Heisenberg's celebrated uncertainty principle is developed. Just as the classical result provides a tradeoff between signal localization in time and frequency, this result provides a fundamental tradeoff between a signal's localization on a graph and in its spectral domain. Using the eigenvectors of the graph Laplacian as a surrogate Fourier basis, quantitative definitions of graph and spectral “spreads” are given, and a complete characterization of the feasibility region of these two quantities is developed. In particular, the lower boundary of the region, referred to as the uncertainty curve, is shown to be achieved by eigenvectors associated with the smallest eigenvalues of an affine family of matrices. The convexity of the uncertainty curve allows it to be found to within ε by a fast approximation algorithm requiring O(ε-1/2) typically sparse eigenvalue evaluations. Closed-form expressions for the uncertainty curves for some special classes of graphs are derived, and an accurate analytical approximation for the expected uncertainty curve of Erd-s-Rényi random graphs is developed. These theoretical results are validated by numerical experiments, which also reveal an intriguing connection between diffusion processes on graphs and the uncertainty bounds.",fullPaper,jv291
p1888,764afc61d329400886d3d027c9d50d97c431f7c8,c24,Decision Support Systems,Multibond graph elements in physical systems theory,Abstract content,poster,cp24
p1889,dce8146987557735a19771aefa1f027211a2c275,c23,International Conference on Open and Big Data,Statistical mechanics of complex networks,"The emergence of order in natural systems is a constant source of inspiration for both physical and biological sciences. While the spatial order characterizing for example the crystals has been the basis of many advances in contemporary physics, most complex systems in nature do not offer such high degree of order. Many of these systems form complex networks whose nodes are the elements of the system and edges represent the interactions between them. 
Traditionally complex networks have been described by the random graph theory founded in 1959 by Paul Erdohs and Alfred Renyi. One of the defining features of random graphs is that they are statistically homogeneous, and their degree distribution (characterizing the spread in the number of edges starting from a node) is a Poisson distribution. In contrast, recent empirical studies, including the work of our group, indicate that the topology of real networks is much richer than that of random graphs. In particular, the degree distribution of real networks is a power-law, indicating a heterogeneous topology in which the majority of the nodes have a small degree, but there is a significant fraction of highly connected nodes that play an important role in the connectivity of the network. 
The scale-free topology of real networks has very important consequences on their functioning. For example, we have discovered that scale-free networks are extremely resilient to the random disruption of their nodes. On the other hand, the selective removal of the nodes with highest degree induces a rapid breakdown of the network to isolated subparts that cannot communicate with each other. 
The non-trivial scaling of the degree distribution of real networks is also an indication of their assembly and evolution. Indeed, our modeling studies have shown us that there are general principles governing the evolution of networks. Most networks start from a small seed and grow by the addition of new nodes which attach to the nodes already in the system. This process obeys preferential attachment: the new nodes are more likely to connect to nodes with already high degree. We have proposed a simple model based on these two principles wich was able to reproduce the power-law degree distribution of real networks. Perhaps even more importantly, this model paved the way to a new paradigm of network modeling, trying to capture the evolution of networks, not just their static topology.",poster,cp23
p1890,2d03b2d4cbd2c483a46c0c39b5fecdf407319eec,c82,Workshop on Interdisciplinary Software Engineering Research,Stochastic blockmodel approximation of a graphon: Theory and consistent estimation,"Non-parametric approaches for analyzing network data based on exchangeable graph models (ExGM) have recently gained interest. The key object that defines an ExGM is often referred to as a graphon. This non-parametric perspective on network modeling poses challenging questions on how to make inference on the graphon underlying observed network data. In this paper, we propose a computationally efficient procedure to estimate a graphon from a set of observed networks generated from it. This procedure is based on a stochastic blockmodel approximation (SBA) of the graphon. We show that, by approximating the graphon with a stochastic block model, the graphon can be consistently estimated, that is, the estimation error vanishes as the size of the graph approaches infinity.",poster,cp82
p1891,aa6be519b394b44ab24c6ad964f8a2c6a9b23571,j168,Proceedings of the IEEE,Consensus and Cooperation in Networked Multi-Agent Systems,"This paper provides a theoretical framework for analysis of consensus algorithms for multi-agent networked systems with an emphasis on the role of directed information flow, robustness to changes in network topology due to link/node failures, time-delays, and performance guarantees. An overview of basic concepts of information consensus in networks and methods of convergence and performance analysis for the algorithms are provided. Our analysis framework is based on tools from matrix theory, algebraic graph theory, and control theory. We discuss the connections between consensus problems in networked dynamic systems and diverse applications including synchronization of coupled oscillators, flocking, formation control, fast consensus in small-world networks, Markov processes and gossip-based algorithms, load balancing in networks, rendezvous in space, distributed sensor fusion in sensor networks, and belief propagation. We establish direct connections between spectral and structural properties of complex networks and the speed of information diffusion of consensus algorithms. A brief introduction is provided on networked systems with nonlocal information flow that are considerably faster than distributed systems with lattice-type nearest neighbor interactions. Simulation results are presented that demonstrate the role of small-world effects on the speed of consensus algorithms and cooperative control of multivehicle formations",fullPaper,jv168
p1892,0e21202ae9085bff250fd60ce0fcf54438c95ab6,c110,IEEE International Conference on Automatic Face & Gesture Recognition,On the algebraic theory of graph colorings,Abstract content,poster,cp110
p1893,1e41ed1ac234cba0138329047e16a8a424389e77,j142,IEEE Transactions on Software Engineering,A Complexity Measure,"This paper describes a graph-theoretic complexity measure and illustrates how it can be used to manage and control program complexity. The paper first explains how the graph-theory concepts apply and gives an intuitive explanation of the graph concepts in programming terms. The control graphs of several actual Fortran programs are then presented to illustrate the correlation between intuitive complexity and the graph-theoretic complexity. Several properties of the graph-theoretic complexity are then proved which show, for example, that complexity is independent of physical size (adding or subtracting functional statements leaves complexity unchanged) and complexity depends only on the decision structure of a program.",fullPaper,jv142
p1894,638df1b831feb3647a9bf5496780b38890573d4d,c92,Advances in Soft Computing,Parallel and Distributed Computation: Numerical Methods,"gineering, computer science, operations research, and applied mathematics. It is essentially a self-contained work, with the development of the material occurring in the main body of the text and excellent appendices on linear algebra and analysis, graph theory, duality theory, and probability theory and Markov chains supporting it. The introduction discusses parallel and distributed architectures, complexity measures, and communication and synchronization issues, and it presents both Jacobi and Gauss-Seidel iterations, which serve as algorithms of reference for many of the computational approaches addressed later. After the introduction, the text is organized in two parts: synchronous algorithms and asynchronous algorithms. The discussion of synchronous algorithms comprises four chapters, with Chapter 2 presenting both direct methods (converging to the exact solution within a finite number of steps) and iterative methods for linear",poster,cp92
p1895,125842668eab7decac136db8a59d392dc5e4e395,c75,International Conference on Machine Learning,Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions,"An approach to semi-supervised learning is proposed that is based on a Gaussian random field model. Labeled and unlabeled data are represented as vertices in a weighted graph, with edge weights encoding the similarity between instances. The learning problem is then formulated in terms of a Gaussian random field on this graph, where the mean of the field is characterized in terms of harmonic functions, and is efficiently obtained using matrix methods or belief propagation. The resulting learning algorithms have intimate connections with random walks, electric networks, and spectral graph theory. We discuss methods to incorporate class priors and the predictions of classifiers obtained by supervised learning. We also propose a method of parameter learning by entropy minimization, and show the algorithm's ability to perform feature selection. Promising experimental results are presented for synthetic data, digit classification, and text classification tasks.",fullPaper,cp75
p1896,7b00a2ae0c927a3d3dd875dc35f9a777d74a8f3e,c104,IEEE International Conference on Multimedia and Expo,A homology theory for spanning tress of a graph,Abstract content,poster,cp104
p1897,4dc403313b0fa80139fe6ac21802f0e1d16b772e,c53,International Conference on Software Engineering and Knowledge Engineering,Spectral sparsification of graphs: theory and algorithms,"Graph sparsification is the approximation of an arbitrary graph by a sparse graph.
 We explain what it means for one graph to be a spectral approximation of another and review the development of algorithms for spectral sparsification. In addition to being an interesting concept, spectral sparsification has been an important tool in the design of nearly linear-time algorithms for solving systems of linear equations in symmetric, diagonally dominant matrices. The fast solution of these linear systems has already led to breakthrough results in combinatorial optimization, including a faster algorithm for finding approximate maximum flows and minimum cuts in an undirected network.",poster,cp53
p1898,3a3d508407cf04be784c76b482c448a4b60c15a4,c8,The Compass,Graph-based Natural Language Processing and Information Retrieval,"Graph theory and the fields of natural language processing and information retrieval are well-studied disciplines. Traditionally, these areas have been perceived as distinct, with different algorithms, different applications, and different potential end-users. However, recent research has shown that these disciplines are intimately connected, with a large variety of natural language processing and information retrieval applications finding efficient solutions within graph-theoretical frameworks. This book extensively covers the use of graph-based algorithms for natural language processing and information retrieval. It brings together topics as diverse as lexical semantics, text summarization, text mining, ontology construction, text classification, and information retrieval, which are connected by the common underlying theme of the use of graph-theoretical methods for text and information processing tasks. Readers will come away with a firm understanding of the major methods and applications in natural language processing and information retrieval that rely on graph-based representations and algorithms.",poster,cp8
p1899,cc127ca14e04bb68cefe4848c4c0b8c5219309fd,c40,IEEE International Conference on Software Maintenance and Evolution,The theory of the monetary circuit,"The present paper reviews the pre-history, the process of formation and the possible directions of future development of the theory of monetary circuit. The author reveals the main theoretical constructions of Graziany and the other leading representatives of the circuitist school. The principles of derivation of transaction and balance sheet matrices reflecting the main ideas of the theory are discussed. The dynamic variants of the theory as well as the connection between the circuitist approach and the input-output model are subject to examination. The paper studies the possibility the circuitist approach to be further broadened on the basis of the mathematical graphs theory. The author emphasizes that the theory of monetary circuit denies the neoclassical dichotomy and rejects the postulate of the neutrality of money. The opportunity is also offered to upgrade the monetary circuit theory by using the mathematical graph theory. The paper includes also a critical evaluation of the presented theory.",poster,cp40
p1900,88496bd36dd61ca42dbd5020d23e76ebeaa994a4,j329,IEEE Transactions on Automatic Control,Information flow and cooperative control of vehicle formations,"We consider the problem of cooperation among a collection of vehicles performing a shared task using intervehicle communication to coordinate their actions. Tools from algebraic graph theory prove useful in modeling the communication network and relating its topology to formation stability. We prove a Nyquist criterion that uses the eigenvalues of the graph Laplacian matrix to determine the effect of the communication topology on formation stability. We also propose a method for decentralized information exchange between vehicles. This approach realizes a dynamical system that supplies each vehicle with a common reference to be used for cooperative motion. We prove a separation principle that decomposes formation stability into two components: Stability of this is achieved information flow for the given graph and stability of an individual vehicle for the given controller. The information flow can thus be rendered highly robust to changes in the graph, enabling tight formation control despite limitations in intervehicle communication capability.",fullPaper,jv329
p1901,725aa166223bf01ab21fb6b002b1e7f13b626d82,c22,International Conference on Data Technologies and Applications,Spectra of graphs : theory and application,Introduction. Basic Concepts of the Spectrum of a Graph. Operations on Graphs and the Resulting Spectra. Relations Between Spectral and Structural Properties of Graphs. The Divisor of a Graph. The Spectrum and the Group of Automorphisms. Characterization of Graphs by Means of Spectra. Spectra Techniques in Graph Theory and Combinatories. Applications in Chemistry an Physics. Some Additional Results. Appendix. Tables of Graph Spectra Biblgraphy. Index of Symbols. Index of Names. Subject Index.,poster,cp22
p1902,a55df99552d6093b6150fe2ca79d017644b73935,j108,PLoS ONE,BrainNet Viewer: A Network Visualization Tool for Human Brain Connectomics,"The human brain is a complex system whose topological organization can be represented using connectomics. Recent studies have shown that human connectomes can be constructed using various neuroimaging technologies and further characterized using sophisticated analytic strategies, such as graph theory. These methods reveal the intriguing topological architectures of human brain networks in healthy populations and explore the changes throughout normal development and aging and under various pathological conditions. However, given the huge complexity of this methodology, toolboxes for graph-based network visualization are still lacking. Here, using MATLAB with a graphical user interface (GUI), we developed a graph-theoretical network visualization toolbox, called BrainNet Viewer, to illustrate human connectomes as ball-and-stick models. Within this toolbox, several combinations of defined files with connectome information can be loaded to display different combinations of brain surface, nodes and edges. In addition, display properties, such as the color and size of network elements or the layout of the figure, can be adjusted within a comprehensive but easy-to-use settings panel. Moreover, BrainNet Viewer draws the brain surface, nodes and edges in sequence and displays brain networks in multiple views, as required by the user. The figure can be manipulated with certain interaction functions to display more detailed information. Furthermore, the figures can be exported as commonly used image file formats or demonstration video for further use. BrainNet Viewer helps researchers to visualize brain networks in an easy, flexible and quick manner, and this software is freely available on the NITRC website (www.nitrc.org/projects/bnv/).",fullPaper,jv108
p1903,9857955ee757b70bcbfa348c4489a0646f238155,c91,Workshop on Algorithms and Models for the Web-Graph,Automated planning - theory and practice,1 Introduction and Overview I Classical Planning 2 Representations for Classical Planning*3 Complexity of Classical Planning*4 State-Space Planning*5 Plan-Space Planning II Neoclassical Planning 6 Planning-Graph Techniques*7 Propositional Satisfiability Techniques*8 Constraint Satisfaction Techniques III Heuristics and Control Strategies 9 Heuristics in Planning*10 Control Rules in Planning*11 Hierarchical Task Network Planning*12 Control Strategies in Deductive Planning IV Planning with Time and Resources 13 Time for Planning*14 Temporal Planning*15 Planning and Resource Scheduling V Planning under Uncertainty 16 Planning based on Markov Decision Processes*17 Planning based on Model Checking*18 Uncertainty with Neo-Classical Techniques VI Case Studies and Applications 19 Space Applications*20 Planning in Robotics*21 Planning for Manufacturability Analysis*22 Emergency Evacuation Planning *23 Planning in the Game of Bridge VII Conclusion 24 Conclusion and Other Topics VIII Appendices A Search Procedures and Computational Complexity*B First Order Logic*C Model Checking,poster,cp91
p1904,98f84b38956ece37082c7bc2a8282f47454f4427,c101,International Conference on Automatic Face and Gesture Recognition,"Functional Analysis, Sobolev Spaces and Partial Differential Equations",Abstract content,poster,cp101
p1905,1e890895a38fe79be13636e563ea669ea63133e1,c41,Software Product Lines Conference,Applications of Hyperstructure Theory,Abstract content,poster,cp41
p1906,acfd9ea27a4183cc6ae1d74998e2e1e0c9e98093,c99,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,Computing topological parameters of biological networks,"UNLABELLED
Rapidly increasing amounts of molecular interaction data are being produced by various experimental techniques and computational prediction methods. In order to gain insight into the organization and structure of the resultant large complex networks formed by the interacting molecules, we have developed the versatile Cytoscape plugin NetworkAnalyzer. It computes and displays a comprehensive set of topological parameters, which includes the number of nodes, edges, and connected components, the network diameter, radius, density, centralization, heterogeneity, and clustering coefficient, the characteristic path length, and the distributions of node degrees, neighborhood connectivities, average clustering coefficients, and shortest path lengths. NetworkAnalyzer can be applied to both directed and undirected networks and also contains extra functionality to construct the intersection or union of two networks. It is an interactive and highly customizable application that requires no expert knowledge in graph theory from the user.


AVAILABILITY
NetworkAnalyzer can be downloaded via the Cytoscape web site: http://www.cytoscape.org",poster,cp99
p1907,a79d1c0f6e8bee2ca0cab3522e67b27a533e19e3,j329,IEEE Transactions on Automatic Control,On maximizing the second smallest eigenvalue of a state-dependent graph Laplacian,"We consider the set G consisting of graphs of fixed order and weighted edges. The vertex set of graphs in G will correspond to point masses and the weight for an edge between two vertices is a functional of the distance between them. We pose the problem of finding the best vertex positional configuration in the presence of an additional proximity constraint, in the sense that, the second smallest eigenvalue of the corresponding graph Laplacian is maximized. In many recent applications of algebraic graph theory in systems and control, the second smallest eigenvalue of Laplacian has emerged as a critical parameter that influences the stability and robustness properties of dynamic systems that operate over an information network. Our motivation in the present work is to ""assign"" this Laplacian eigenvalue when relative positions of various elements dictate the interconnection of the underlying weighted graph. In this venue, one would then be able to ""synthesize"" information graphs that have desirable system theoretic properties.",fullPaper,jv329
p1908,2e64c17ae0799e3995e927ec46b31e03ce26aa76,c108,International Conference on Information Integration and Web-based Applications & Services,Evolutionary Dynamics: Exploring the Equations of Life,Preface 1. Introduction 2. What Evolution Is 3. Fitness Landscapes and Sequence Spaces 4. Evolutionary Games 5. Prisoners of the Dilemma 6. Finite Populations 7. Games in Finite Populations 8. Evolutionary Graph Theory 9. Spatial Games 10. HIV Infection 11. The Evolution of Virulence 12. The Evolutionary Dynamics of Cancer 13. Language Evolution 14. Conclusion Further Reading References Index,poster,cp108
p1909,d2b93dfbe50f3c642c64b8ea581cb6e449c71f82,j330,Multiscale Modeling & simulation,Nonlocal Operators with Applications to Image Processing,"We propose the use of nonlocal operators to define new types of flows and functionals for image processing and elsewhere. A main advantage over classical PDE-based algorithms is the ability to handle better textures and repetitive structures. This topic can be viewed as an extension of spectral graph theory and the diffusion geometry framework to functional analysis and PDE-like evolutions. Some possible applications and numerical examples are given, as is a general framework for approximating Hamilton–Jacobi equations on arbitrary grids in high demensions, e.g., for control theory.",fullPaper,jv330
p1910,cc2e6e4fde1560bd4839a8cb47b00899cf146a31,c74,IEEE International Conference on Tools with Artificial Intelligence,Large Networks and Graph Limits,"The book Large Networks and Graph Limits, xiv + 475 pp., published in late 2012, comprises five parts, the first an illuminating introduction and the last a tantalizing taste of how the scope of the theory developed in its pages might be extended to other combinatorial structures than graphs. The three central parts treat in depth the topics of graph algebras, limits for sequences of dense graphs (this constitutes the most substantial part, occupying nearly half the book) and limits for sequences of bounded degree graphs. Primarily the book is aimed at graduate students and research mathematicians interested in graph theory and its application to networks (for example, the internet and networks in social science, biology, statistical physics and engineering). There are 23 chapters and an appendix, the latter conveniently giving necessary background from areas of mathematics outside mainstream graph theory. A bibliography collects together the extensive research in this area up to 2012, and a subject, author and notation index facilitate navigation of the book. The author maintains a webpage for corrections and supplementary material. Indeed, via the author’s homepage the reader can freely access the many papers he has written with collaborators on the topic of graph homomorphisms and graph limits. The book synthesizes much of the material in these papers, with some revision in",poster,cp74
p1911,c2d7f7428a81a18fb5269dd548755fcb1d6998d3,c71,IEEE International Conference on Information Reuse and Integration,"Mean Curvature, Threshold Dynamics, and Phase Field Theory on Finite Graphs",Abstract content,poster,cp71
p1912,636b0754486f29f0bbc68cc2b410f564a3dfefe6,c105,Biometrics and Identity Management,Spectra of Graphs,"This book gives an elementary treatment of the basic material about graph spectra, both for ordinary, and Laplace and Seidel spectra. The text progresses systematically, by covering standard topics before presenting some new material on trees, strongly regular graphs, two-graphs, association schemes, p-ranks of configurations and similar topics. Exercises at the end of each chapter provide practice and vary from easy yet interesting applications of the treated theory, to little excursions into related topics. Tables, references at the end of the book, an author and subject index enrich the text. Spectra of Graphs is written for researchers, teachers and graduate students interested in graph spectra. The reader is assumed to be familiar with basic linear algebra and eigenvalues, although some more advanced topics in linear algebra, like the Perron-Frobenius theorem and eigenvalue interlacing are included.",poster,cp105
p1913,221aa3be55a4ead8fc2aa83b12aac370bfba72f5,j331,IEEE Transactions on Systems Science and Cybernetics,A Formal Basis for the Heuristic Determination of Minimum Cost Paths,"Although the problem of determining the minimum cost path through a graph arises naturally in a number of interesting applications, there has been no underlying theory to guide the development of efficient search procedures. Moreover, there is no adequate conceptual framework within which the various ad hoc search strategies proposed to date can be compared. This paper describes how heuristic information from the problem domain can be incorporated into a formal mathematical theory of graph searching and demonstrates an optimality property of a class of search strategies.",fullPaper,jv331
p1914,08717babb3ce5a7db8971aad14504ee43826b627,c28,International Conference on Collaboration Technologies and Systems,Social Network Analysis,"This paper reports on the development of social network analysis, tracing its origins in classical sociology and its more recent formulation in social scientific and mathematical work. It is argued that the concept of social network provides a powerful model for social structure, and that a number of important formal methods of social network analysis can be discerned. Social network analysis has been used in studies of kinship structure, social mobility, science citations, contacts among members of deviant groups, corporate power, international trade exploitation, class structure, and many other areas. A review of the formal models proposed in graph theory, multidimensional scaling, and algebraic topology is followed by extended illustrations of social network analysis in the study of community structure and interlocking directorships.",poster,cp28
p1915,0b4497542279def00135e7a3140b3418606679e9,c30,IEEE Aerospace Conference,An Introduction to Hydrogen Bonding,"1. Brief History 2. Nature and Properties 3. Strong Hydrogen Bonds 4. Moderate Hydrogen Bonds 5. Weak Hydrogen Bonds 6. Cooperativity, Patterns, Graph Set Theory, Liquid Crystals 7. Disorder, Proton Transfer, Isotope Effect, Ferroelectrics, Transitions 8. Water, Water Dimers, Ices, Hydrates 9. Inclusion Compounds 10. Hydrogen Bonding in Biological Molecules 11. Methods",poster,cp30
p1916,e8b8c5f4a81e11576ee2c74ab65c66a42bbad270,c43,ACM Symposium on Applied Computing,Random graphs with arbitrary degree distributions and their applications.,"Recent work on the structure of social networks and the internet has focused attention on graphs with distributions of vertex degree that are significantly different from the Poisson degree distributions that have been widely studied in the past. In this paper we develop in detail the theory of random graphs with arbitrary degree distributions. In addition to simple undirected, unipartite graphs, we examine the properties of directed and bipartite graphs. Among other results, we derive exact expressions for the position of the phase transition at which a giant component first forms, the mean component size, the size of the giant component if there is one, the mean number of vertices a certain distance away from a randomly chosen vertex, and the average vertex-vertex distance within a graph. We apply our theory to some real-world graphs, including the world-wide web and collaboration graphs of scientists and Fortune 1000 company directors. We demonstrate that in some cases random graphs with appropriate distributions of vertex degree predict with surprising accuracy the behavior of the real world, while in others there is a measurable discrepancy between theory and reality, perhaps indicating the presence of additional social structure in the network that is not captured by the random graph.",poster,cp43
p1917,b9dd445a4e7ad794012db01339f8fe9967b923a8,j329,IEEE Transactions on Automatic Control,Consensus Problems on Networks With Antagonistic Interactions,"In a consensus protocol an agreement among agents is achieved thanks to the collaborative efforts of all agents, expresses by a communication graph with nonnegative weights. The question we ask in this paper is the following: is it possible to achieve a form of agreement also in presence of antagonistic interactions, modeled as negative weights on the communication graph? The answer to this question is affirmative: on signed networks all agents can converge to a consensus value which is the same for all agents except for the sign. Necessary and sufficient conditions are obtained to describe cases in which this is possible. These conditions have strong analogies with the theory of monotone systems. Linear and nonlinear Laplacian feedback designs are proposed.",fullPaper,jv329
p1918,7a498d6cef22d72dc6bf8da90f145ea8f5d9fece,c51,Conference of the Centre for Advanced Studies on Collaborative Research,Wiener Index of Trees: Theory and Applications,Abstract content,poster,cp51
p1919,4ff0de33c6c9e63055413f0ec3e8b75e1a23ac45,c88,Symposium on the Theory of Computing,The knowledge complexity of interactive proof-systems,"Usually, a proof of a theorem contains more knowledge than the mere fact that the theorem is true. For instance, to prove that a graph is Hamiltonian it suffices to exhibit a Hamiltonian tour in it; however, this seems to contain more knowledge than the single bit Hamiltonian/non-Hamiltonian.In this paper a computational complexity theory of the “knowledge” contained in a proof is developed. Zero-knowledge proofs are defined as those proofs that convey no additional knowledge other than the correctness of the proposition in question. Examples of zero-knowledge proof systems are given for the languages of quadratic residuosity and 'quadratic nonresiduosity. These are the first examples of zero-knowledge proofs for languages not known to be efficiently recognizable.",fullPaper,cp88
p1920,49db0fd3b64d03531255e2bdb80ae747d8fe0ffb,j291,IEEE Transactions on Information Theory,"The Dynamics of Message Passing on Dense Graphs, with Applications to Compressed Sensing","“Approximate message passing” (AMP) algorithms have proved to be effective in reconstructing sparse signals from a small number of incoherent linear measurements. Extensive numerical experiments further showed that their dynamics is accurately tracked by a simple one-dimensional iteration termed state evolution. In this paper, we provide rigorous foundation to state evolution. We prove that indeed it holds asymptotically in the large system limit for sensing matrices with independent and identically distributed Gaussian entries. While our focus is on message passing algorithms for compressed sensing, the analysis extends beyond this setting, to a general class of algorithms on dense graphs. In this context, state evolution plays the role that density evolution has for sparse graphs. The proof technique is fundamentally different from the standard approach to density evolution, in that it copes with a large number of short cycles in the underlying factor graph. It relies instead on a conditioning technique recently developed by Erwin Bolthausen in the context of spin glass theory.",fullPaper,jv291
p1921,088ab372dff19fb8837d0f48d395d1a987251f3f,c64,Experimental Software Engineering Network,Geometry of cuts and metrics,Abstract content,poster,cp64
p1922,cb346929220d4df38edc314056ee1933496c6cf5,c43,ACM Symposium on Applied Computing,Geometric Algorithms and Combinatorial Optimization,Abstract content,poster,cp43
p1923,c6b745c7ecc3fc89d0df71727e1a0f456be7187a,j332,Annual Review of Clinical Psychology,Brain graphs: graphical models of the human brain connectome.,"Brain graphs provide a relatively simple and increasingly popular way of modeling the human brain connectome, using graph theory to abstractly define a nervous system as a set of nodes (denoting anatomical regions or recording electrodes) and interconnecting edges (denoting structural or functional connections). Topological and geometrical properties of these graphs can be measured and compared to random graphs and to graphs derived from other neuroscience data or other (nonneural) complex systems. Both structural and functional human brain graphs have consistently demonstrated key topological properties such as small-worldness, modularity, and heterogeneous degree distributions. Brain graphs are also physically embedded so as to nearly minimize wiring cost, a key geometric property. Here we offer a conceptual review and methodological guide to graphical analysis of human neuroimaging data, with an emphasis on some of the key assumptions, issues, and trade-offs facing the investigator.",fullPaper,jv332
p1924,a8a18497987e8b4715cba7cd6d2f8e6a1d58b2fa,j333,Neuroinformatics,The small world of the cerebral cortex,Abstract content,fullPaper,jv333
p1925,59d86a93c4ef54b5489bc375cd02e64205823f42,j299,IEEE Transactions on Pattern Analysis and Machine Intelligence,Random Walks for Image Segmentation,"A novel method is proposed for performing multilabel, interactive image segmentation. Given a small number of pixels with user-defined (or predefined) labels, one can analytically and quickly determine the probability that a random walker starting at each unlabeled pixel will first reach one of the prelabeled pixels. By assigning each pixel to the label for which the greatest probability is calculated, a high-quality image segmentation may be obtained. Theoretical properties of this algorithm are developed along with the corresponding connections to discrete potential theory and electrical circuits. This algorithm is formulated in discrete space (i.e., on a graph) using combinatorial analogues of standard operators and principles from continuous potential theory, allowing it to be applied in arbitrary dimension on arbitrary graphs",fullPaper,jv299
p1926,b8523a1ae8f7ebc53b2af47b6541f6348383c0f8,c30,IEEE Aerospace Conference,Challenges with graph interpretation: a review of the literature,"With the growing emphasis on the development of scientific inquiry skills, the display and interpretation of data are becoming increasingly important. Graph interpretation competence is, in fact, essential to understanding today’s world and to be scientifically literate. However, graph interpretation is a complex and challenging activity. Graph interpretation competence is affected by many factors, including aspects of graph characteristics, the content of the graph and viewers’ prior knowledge. For instance, the prior theory and expectations that students have may lead to biases and misinterpretation of graphs. One basic controversy that remains unanswered, for example, is what should we teach first in order to make students scientific literate, how to graph or how to interpret a graph? If it is the case that the ability to interpret a graph be developed prior to the ability to create, then it is important to understand what graph interpretation entails. This paper reviews current literature on graph interpretation competence and argues that it should be explicitly taught given its importance and its complexity.",poster,cp30
p1927,7fe0ef2ddacd193101dc5ba3df97b0241a5e8fc6,j329,IEEE Transactions on Automatic Control,Stability of multiagent systems with time-dependent communication links,"We study a simple but compelling model of network of agents interacting via time-dependent communication links. The model finds application in a variety of fields including synchronization, swarming and distributed decision making. In the model, each agent updates his current state based upon the current information received from neighboring agents. Necessary and/or sufficient conditions for the convergence of the individual agents' states to a common value are presented, thereby extending recent results reported in the literature. The stability analysis is based upon a blend of graph-theoretic and system-theoretic tools with the notion of convexity playing a central role. The analysis is integrated within a formal framework of set-valued Lyapunov theory, which may be of independent interest. Among others, it is observed that more communication does not necessarily lead to faster convergence and may eventually even lead to a loss of convergence, even for the simple models discussed in the present paper.",fullPaper,jv329
p1928,ef32407a7947a1051c7ecdcdeb857ed835bbed99,j334,IEEE Control Systems,Rigid graph control architectures for autonomous formations,"This article sets out the rudiments of a theory for analyzing and creating architectures appropriate to the control of formations of autonomous vehicles. The theory rests on ideas of rigid graph theory, some but not all of which are old. The theory, however, has some gaps in it, and their elimination would help in applications. Some of the gaps in the relevant graph theory are as follows. First, there is as yet no analogue for three-dimensional graphs of Laman's theorem, which provides a combinatorial criterion for rigidity in two-dimensional graphs. Second, for three-dimensional graphs there is no analogue of the two-dimensional Henneberg construction for growing or deconstructing minimally rigid graphs although there are conjectures. Third, global rigidity can easily be characterized for two-dimensional graphs, but not for three-dimensional graphs.",fullPaper,jv334
p1929,a8921b3462a3575b0b5de602a975bd608f6f6652,j291,IEEE Transactions on Information Theory,Constructing free-energy approximations and generalized belief propagation algorithms,"Important inference problems in statistical physics, computer vision, error-correcting coding theory, and artificial intelligence can all be reformulated as the computation of marginal probabilities on factor graphs. The belief propagation (BP) algorithm is an efficient way to solve these problems that is exact when the factor graph is a tree, but only approximate when the factor graph has cycles. We show that BP fixed points correspond to the stationary points of the Bethe approximation of the free energy for a factor graph. We explain how to obtain region-based free energy approximations that improve the Bethe approximation, and corresponding generalized belief propagation (GBP) algorithms. We emphasize the conditions a free energy approximation must satisfy in order to be a ""valid"" or ""maxent-normal"" approximation. We describe the relationship between four different methods that can be used to generate valid approximations: the ""Bethe method"", the ""junction graph method"", the ""cluster variation method"", and the ""region graph method"". Finally, we explain how to tell whether a region-based approximation, and its corresponding GBP algorithm, is likely to be accurate, and describe empirical results showing that GBP can significantly outperform BP.",fullPaper,jv291
p1930,eb524f7c1e29bdd7d27c33a90921c5ea7f347234,j168,Proceedings of the IEEE,Frequency assignment: Theory and applications,"In this paper we introduce the minimum-order approach to frequency assignment and present a theory which relates this approach to the traditional one. This new approach is potentially more desirable than the traditional one. We model assignment problems as both frequency-distance constrained and frequency constrained optimization problems. The frequency constrained approach should be avoided if distance separation is employed to mitigate interference. A restricted class of graphs, called disk graphs, plays a central role in frequency-distance constrained problems. We introduce two generalizations of chromatic number and show that many frequency assignment problems are equivalent to generalized graph coloring problems. Using these equivalences and recent results concerning the complexity of graph coloring, we classify many frequency assignment problems according to the ""execution time efficiency"" of algorithms that may be devised for their solution. We discuss applications to important real world problems and identify areas for further work.",fullPaper,jv168
p1931,ada56e1f7575d7f542215c48625c161ab060bed0,c98,North American Chapter of the Association for Computational Linguistics,Static scheduling algorithms for allocating directed task graphs to multiprocessors,"Static scheduling of a program represented by a directed task graph on a multiprocessor system to minimize the program completion time is a well-known problem in parallel processing. Since finding an optimal schedule is an NP-complete problem in general, researchers have resorted to devising efficient heuristics. A plethora of heuristics have been proposed based on a wide spectrum of techniques, including branch-and-bound, integer-programming, searching, graph-theory, randomization, genetic algorithms, and evolutionary methods. The objective of this survey is to describe various scheduling algorithms and their functionalities in a contrasting fashion as well as examine their relative merits in terms of performance and time-complexity. Since these algorithms are based on diverse assumptions, they differ in their functionalities, and hence are difficult to describe in a unified context. We propose a taxonomy that classifies these algorithms into different categories. We consider 27 scheduling algorithms, with each algorithm explained through an easy-to-understand description followed by an illustrative example to demonstrate its operation. We also outline some of the novel and promising optimization approaches and current research trends in the area. Finally, we give an overview of the software tools that provide scheduling/mapping functionalities.",poster,cp98
p1932,2e15e356a1368e65b42417676276160110daf272,c67,Enterprise Application Integration,Second-Order Consensus for Multiagent Systems With Directed Topologies and Nonlinear Dynamics,"This paper considers a second-order consensus problem for multiagent systems with nonlinear dynamics and directed topologies where each agent is governed by both position and velocity consensus terms with a time-varying asymptotic velocity. To describe the system's ability for reaching consensus, a new concept about the generalized algebraic connectivity is defined for strongly connected networks and then extended to the strongly connected components of the directed network containing a spanning tree. Some sufficient conditions are derived for reaching second-order consensus in multiagent systems with nonlinear dynamics based on algebraic graph theory, matrix theory, and Lyapunov control approach. Finally, simulation examples are given to verify the theoretical analysis.",poster,cp67
p1933,1db1447bc61a68500ec31e94daf27cf057831f83,c81,IEEE Annual Symposium on Foundations of Computer Science,Shock Waves on the Highway,A simple theory of traffic flow is developed by replacing individual vehicles with a continuous “fluid” density and applying an empirical relation between speed and density. Characteristic features of the resulting theory are a simple “graph-shearing” process for following the development of traffic waves in time and the frequent appearance of shock waves. The effect of a traffic signal on traffic streams is studied and found to exhibit a threshold effect wherein the disturbances are minor for light traffic but suddenly build to large values when a critical density is exceeded.,poster,cp81
p1934,c81698f8a3014854f44744152d377421041da70f,j310,Journal of Graph Theory,The rainbow connection of a graph is (at most) reciprocal to its minimum degree,"An edge‐colored graph Gis rainbow edge‐connected if any two vertices are connected by a path whose edges have distinct colors. The rainbow connection of a connected graph G, denoted by rc(G), is the smallest number of colors that are needed in order to make Grainbow edge‐connected. We prove that if Ghas nvertices and minimum degree δ then rc(G)<20n/δ. This solves open problems from Y. Caro, A. Lev, Y. Roditty, Z. Tuza, and R. Yuster (Electron J Combin 15 (2008), #R57) and S. Chakrborty, E. Fischer, A. Matsliah, and R. Yuster (Hardness and algorithms for rainbow connectivity, Freiburg (2009), pp. 243–254). A vertex‐colored graph Gis rainbow vertex‐connected if any two vertices are connected by a path whose internal vertices have distinct colors. The rainbow vertex‐connection of a connected graph G, denoted by rvc(G), is the smallest number of colors that are needed in order to make Grainbow vertex‐connected. One cannot upper‐bound one of these parameters in terms of the other. Nevertheless, we prove that if Ghas nvertices and minimum degree δ then rvc(G)<11n/δ. We note that the proof in this case is different from the proof for the edge‐colored case, and we cannot deduce one from the other. © 2009 Wiley Periodicals, Inc. J Graph Theory 63: 185–191, 2010",fullPaper,jv310
p1935,e5a0fff54abb5eca5e61f2b8d73a5f2acaad6c3a,j62,Nature,Topological quantum chemistry,Abstract content,fullPaper,jv62
p1936,86a8e3b54eb7b0dd8076d73494f5c82f853ab860,c21,Grid Computing Environments,A Theory of Graphs,Abstract content,poster,cp21
p1937,3e502fb40768c140ef24ea742a212c263f380f71,c35,EUROMICRO Conference on Software Engineering and Advanced Applications,Modeling and control of formations of nonholonomic mobile robots,"This paper addresses the control of a team of nonholonomic mobile robots navigating in a terrain with obstacles while maintaining a desired formation and changing formations when required, using graph theory. We model the team as a triple, (g, r, H), consisting of a group element g that describes the gross position of the lead robot, a set of shape variables r that describe the relative positions of robots, and a control graph H that describes the behaviors of the robots in the formation. Our framework enables the representation and enumeration of possible control graphs and the coordination of transitions between any two formations.",poster,cp35
p1938,1808c2d7476a536a4a4b6618ab65b5961da57949,j20,Proceedings of the National Academy of Sciences of the United States of America,Spectral redemption in clustering sparse networks,"Significance Spectral algorithms are widely applied to data clustering problems, including finding communities or partitions in graphs and networks. We propose a way of encoding sparse data using a “nonbacktracking” matrix, and show that the corresponding spectral algorithm performs optimally for some popular generative models, including the stochastic block model. This is in contrast with classical spectral algorithms, based on the adjacency matrix, random walk matrix, and graph Laplacian, which perform poorly in the sparse case, failing significantly above a recently discovered phase transition for the detectability of communities. Further support for the method is provided by experiments on real networks as well as by theoretical arguments and analogies from probability theory, statistical physics, and the theory of random matrices. Spectral algorithms are classic approaches to clustering and community detection in networks. However, for sparse networks the standard versions of these algorithms are suboptimal, in some cases completely failing to detect communities even when other algorithms such as belief propagation can do so. Here, we present a class of spectral algorithms based on a nonbacktracking walk on the directed edges of the graph. The spectrum of this operator is much better-behaved than that of the adjacency matrix or other commonly used matrices, maintaining a strong separation between the bulk eigenvalues and the eigenvalues relevant to community structure even in the sparse case. We show that our algorithm is optimal for graphs generated by the stochastic block model, detecting communities all of the way down to the theoretical limit. We also show the spectrum of the nonbacktracking operator for some real-world networks, illustrating its advantages over traditional spectral clustering.",fullPaper,jv20
p1939,260c1afc3a6a593f7ba121c91bccfb695a938b8e,j335,Journal of Statistical Software,"ergm: A Package to Fit, Simulate and Diagnose Exponential-Family Models for Networks.","We describe some of the capabilities of the ergm package and the statistical theory underlying it. This package contains tools for accomplishing three important, and interrelated, tasks involving exponential-family random graph models (ERGMs): estimation, simulation, and goodness of fit. More precisely, ergm has the capability of approximating a maximum likelihood estimator for an ERGM given a network data set; simulating new network data sets from a fitted ERGM using Markov chain Monte Carlo; and assessing how well a fitted ERGM does at capturing characteristics of a particular network data set.",fullPaper,jv335
p1940,65d61afd9c35b0a75d9de77c2a4a2428af0f7f7b,j327,IEEE Signal Processing Magazine,Big Data Analysis with Signal Processing on Graphs: Representation and processing of massive data sets with irregular structure,"Analysis and processing of very large data sets, or big data, poses a significant challenge. Massive data sets are collected and studied in numerous domains, from engineering sciences to social networks, biomolecular research, commerce, and security. Extracting valuable information from big data requires innovative approaches that efficiently process large amounts of data as well as handle and, moreover, utilize their structure. This article discusses a paradigm for large-scale data analysis based on the discrete signal processing (DSP) on graphs (DSPG). DSPG extends signal processing concepts and methodologies from the classical signal processing theory to data indexed by general graphs. Big data analysis presents several challenges to DSPG, in particular, in filtering and frequency analysis of very large data sets. We review fundamental concepts of DSPG, including graph signals and graph filters, graph Fourier transform, graph frequency, and spectrum ordering, and compare them with their counterparts from the classical signal processing theory. We then consider product graphs as a graph model that helps extend the application of DSPG methods to large data sets through efficient implementation based on parallelization and vectorization. We relate the presented framework to existing methods for large-scale data processing and illustrate it with an application to data compression.",fullPaper,jv327
p1941,796cd1df17ac1eedbc504dd9eaf2f1ca30b8a6be,c69,International Conference on Parallel Processing,KEGGgraph: a graph approach to KEGG PATHWAY in R and bioconductor,"Motivation: KEGG PATHWAY is a service of Kyoto Encyclopedia of Genes and Genomes (KEGG), constructing manually curated pathway maps that represent current knowledge on biological networks in graph models. While valuable graph tools have been implemented in R/Bioconductor, to our knowledge there is currently no software package to parse and analyze KEGG pathways with graph theory. Results: We introduce the software package KEGGgraph in R and Bioconductor, an interface between KEGG pathways and graph models as well as a collection of tools for these graphs. Superior to existing approaches, KEGGgraph captures the pathway topology and allows further analysis or dissection of pathway graphs. We demonstrate the use of the package by the case study of analyzing human pancreatic cancer pathway. Availability:KEGGgraph is freely available at the Bioconductor web site (http://www.bioconductor.org). KGML files can be downloaded from KEGG FTP site (ftp://ftp.genome.jp/pub/kegg/xml). Contact: j.zhang@dkfz-heidelberg.de Supplementary information: Supplementary data are available at Bioinformatics online.",poster,cp69
p1942,439f26a0938b1c4617655d0f951eda77033aedbe,c0,International Conference on Human Factors in Computing Systems,Introduction to Quantum Graphs,"A ""quantum graph"" is a graph considered as a one-dimensional complex and equipped with a differential operator (""Hamiltonian""). Quantum graphs arise naturally as simplified models in mathematics, physics, chemistry, and engineering when one considers propagation of waves of various nature through a quasi-one-dimensional (e.g., ""meso-"" or ""nano-scale"") system that looks like a thin neighborhood of a graph. Works that currently would be classified as discussing quantum graphs have been appearing since at least the 1930s, and since then, quantum graphs techniques have been applied successfully in various areas of mathematical physics, mathematics in general and its applications. One can mention, for instance, dynamical systems theory, control theory, quantum chaos, Anderson localization, microelectronics, photonic crystals, physical chemistry, nano-sciences, superconductivity theory, etc. Quantum graphs present many non-trivial mathematical challenges, which makes them dear to a mathematician's heart. Work on quantum graphs has brought together tools and intuition coming from graph theory, combinatorics, mathematical physics, PDEs, and spectral theory. This book provides a comprehensive introduction to the topic, collecting the main notions and techniques. It also contains a survey of the current state of the quantum graph research and applications.",poster,cp0
p1943,afc34303fcd5a4175f33d5161eb056826f64b880,j336,IEEE Transactions on Circuits and Systems Part 1: Regular Papers,Kron Reduction of Graphs With Applications to Electrical Networks,"Consider a weighted undirected graph and its corresponding Laplacian matrix, possibly augmented with additional diagonal elements corresponding to self-loops. The Kron reduction of this graph is again a graph whose Laplacian matrix is obtained by the Schur complement of the original Laplacian matrix with respect to a specified subset of nodes. The Kron reduction process is ubiquitous in classic circuit theory and in related disciplines such as electrical impedance tomography, smart grid monitoring, transient stability assessment, and analysis of power electronics. Kron reduction is also relevant in other physical domains, in computational applications, and in the reduction of Markov chains. Related concepts have also been studied as purely theoretic problems in the literature on linear algebra. In this paper we analyze the Kron reduction process from the viewpoint of algebraic graph theory. Specifically, we provide a comprehensive and detailed graph-theoretic analysis of Kron reduction encompassing topological, algebraic, spectral, resistive, and sensitivity analyses. Throughout our theoretic elaborations we especially emphasize the practical applicability of our results to various problem setups arising in engineering, computation, and linear algebra. Our analysis of Kron reduction leads to novel insights both on the mathematical and the physical side.",fullPaper,jv336
p1944,5ce9e830146f9e4b095511f693a939e749686430,j337,IEEE Transactions on Mobile Computing,A Theory of Network Localization,"In this paper, we provide a theoretical foundation for the problem of network localization in which some nodes know their locations and other nodes determine their locations by measuring the distances to their neighbors. We construct grounded graphs to model network localization and apply graph rigidity theory to test the conditions for unique localizability and to construct uniquely localizable networks. We further study the computational complexity of network localization and investigate a subclass of grounded graphs where localization can be computed efficiently. We conclude with a discussion of localization in sensor networks where the sensors are placed randomly",fullPaper,jv337
p1945,4fcfc3a3263d3d8487e887165ae4200cdac269c5,c35,EUROMICRO Conference on Software Engineering and Advanced Applications,GRAPH LIMITS AND EXCHANGEABLE RANDOM GRAPHS,"We develop a clear connection between de Finetti’s theorem for exchangeable arrays (work of Aldous–Hoover–Kallenberg) and the emerging area of graph limits (work of Lovász and many coauthors). Along the way, we translate the graph theory into more classical prob-",poster,cp35
p1946,9f5b65929d14447a77063ceb854660b5f9f8ad05,c11,Hawaii International Conference on System Sciences,Graph-based Knowledge Representation - Computational Foundations of Conceptual Graphs,Abstract content,poster,cp11
p1947,8dd09cfe9d7b2d4a13e02693112b1f8afa37f222,j338,Internet Mathematics,"Towards a Theory of Scale-Free Graphs: Definition, Properties, and Implications","There is a large, popular, and growing literature on ""scale-free"" networks with the Internet along with metabolic networks representing perhaps the canonical examples. While this has in many ways reinvigorated graph theory, there is unfortunately no consistent, precise definition of scale-free graphs and few rigorous proofs of many of their claimed properties. In fact, it is easily shown that the existing theory has many inherent contradictions and that the most celebrated claims regarding the Internet and biology are verifiably false. In this paper, we introduce a structural metric that allows us to differentiate between all simple, connected graphs having an identical degree sequence, which is of particular interest when that sequence satisfies a power law relationship. We demonstrate that the proposed structural metric yields considerable insight into the claimed properties of SF graphs and provides one possible measure of the extent to which a graph is scale-free. This structural view can be related to previously studied graph properties such as the various notions of self-similarity, likelihood, betweenness and assortativity. Our approach clarifies much of the confusion surrounding the sensational qualitative claims in the current literature, and offers a rigorous and quantitative alternative, while suggesting the potential for a rich and interesting theory. This paper is aimed at readers familiar with the basics of Internet technology and comfortable with a theorem-proof style of exposition, but who may be unfamiliar with the existing literature on scale-free networks.",fullPaper,jv338
p1948,b623b893faa4c6ef54ba87af04a970b8250c5274,c46,Brazilian Symposium on Software Engineering,"Sharp thresholds of graph properties, and the -sat problem","Consider G(n, p) to be the probability space of random graphs on n vertices with edge probability p. We will be considering subsets of this space defined by monotone graph properties. A monotone graph property P is a property of graphs such that a) P is invariant under graph automorphisims. b) If graph H has property P , then so does any graph G having H as a subgraph. A monotone symmetric family of graphs is a family defined by such a property. One of the first observations made about random graphs by Erdos and Renyi in their seminal work on random graph theory [12] was the existence of threshold phenomena, the fact that for many interesting properties P , the probability of P appearing in G(n, p) exhibits a sharp increase at a certain critical value of the parameter p. Bollobas and Thomason proved the existence of threshold functions for all monotone set properties ([6]), and in [14] it is shown that this behavior is quite general, and that all monotone graph properties exhibit threshold behavior, i.e. the probability of their appearance increases from values very close to 0 to values close to 1 in a very small interval. More precise analysis of the size of the threshold interval is done in [7]. This threshold behavior which occurs in various settings which arise in combinatorics and computer science is an instance of the phenomenon of phase transitions which is the subject of much interest in statistical physics. One of the main questions that arises in studying phase transitions is: how “sharp” is the transition? For example, one of the motivations for this paper arose from the question of the sharpness of the phase transition for the property of satisfiability of a random kCNF Boolean formula. Nati Linial, who introduced me to this problem, suggested that although much concrete analysis was being performed on this problem the best approach would be to find general conditions for sharpness of the phase transition, answering the question posed in [14] as to the relation between the length of the threshold interval and the value of the critical probability. In this paper we indeed introduce a simple condition and prove it is sufficient. Stated roughly, in the setting of random graphs, the main theorem states that if a property has a coarse threshold, then it can be approximated by the property of having certain given graphs as a subgraph. This condition can be applied in a more",poster,cp46
p1949,ca47be74efccb005d88f8455aff73e0622949e96,c1,Technical Symposium on Computer Science Education,Mathematical Concepts in Organic Chemistry,Abstract content,poster,cp1
p1950,86ee21d690eec2a73806c2086949e944f8b46f7c,j339,Optics Express,Automatic segmentation of seven retinal layers in SDOCT images congruent with expert manual segmentation,"Segmentation of anatomical and pathological structures in ophthalmic images is crucial for the diagnosis and study of ocular diseases. However, manual segmentation is often a time-consuming and subjective process. This paper presents an automatic approach for segmenting retinal layers in Spectral Domain Optical Coherence Tomography images using graph theory and dynamic programming. Results show that this method accurately segments eight retinal layer boundaries in normal adult eyes more closely to an expert grader as compared to a second expert grader.",fullPaper,jv339
p1951,f7b91f04795c4fc1449587d9c900a3ec6d39d79a,c64,Experimental Software Engineering Network,A graph-theoretic approach to the method of global Lyapunov functions,"A class of global Lyapunov functions is revisited and used to resolve a long-standing open problem on the uniqueness and global stability of the endemic equilibrium of a class of multi-group models in mathematical epidemiology. We show how the group structure of the models, as manifested in the derivatives of the Lyapunov function, can be completely described using graph theory.",poster,cp64
p1952,6350d7697aa098fadc46296218223325076826a3,c101,International Conference on Automatic Face and Gesture Recognition,Topological properties of hypercubes,"The n-dimensional hypercube is a highly concurrent loosely coupled multiprocessor based on the binary n-cube topology. Machines based on the hypercube topology have been advocated as ideal parallel architectures for their powerful interconnection features. The authors examine the hypercube from the graph-theory point of view and consider those features that make its connectivity so appealing. Among other things, they propose a theoretical characterization of the n-cube as a graph and and show how to map various other topologies into a hypercube. >",poster,cp101
p1953,2c03d0e5113cc34ff607c652c68a5e542e607735,c107,British Machine Vision Conference,Property testing and its connection to learning and approximation,"The authors study the question of determining whether an unknown function has a particular property or is /spl epsiv/-far from any function with that property. A property testing algorithm is given a sample of the value of the function on instances drawn according to some distribution, and possibly may query the function on instances of its choice. First, they establish some connections between property testing and problems in learning theory. Next, they focus on testing graph properties, and devise algorithms to test whether a graph has properties such as being k-colorable or having a /spl rho/-clique (clique of density /spl rho/ w.r.t. the vertex set). The graph property testing algorithms are probabilistic and make assertions which are correct with high probability utilizing only poly(1//spl epsiv/) edge-queries into the graph, where /spl epsiv/ is the distance parameter. Moreover, the property testing algorithms can be used to efficiently (i.e., in time linear in the number of vertices) construct partitions of the graph which correspond to the property being tested, if it holds for the input graph.",poster,cp107
p1954,eac8fb1c9883da002a8a9fb1d514bde116219dc1,c57,IEEE International Conference on Engineering of Complex Computer Systems,Resistance distance,Abstract content,poster,cp57
p1955,e1b10e80013766521e82bc56babaab63c2265847,c99,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,Renormalization in Quantum Field Theory and the Riemann–Hilbert Problem I: The Hopf Algebra Structure of Graphs and the Main Theorem,Abstract content,poster,cp99
p1956,eff3e2a802a63b15ce57498611165eccca0ddbe3,j20,Proceedings of the National Academy of Sciences of the United States of America,The average distances in random graphs with given expected degrees,"Random graph theory is used to examine the “small-world phenomenon”; any two strangers are connected through a short chain of mutual acquaintances. We will show that for certain families of random graphs with given expected degrees the average distance is almost surely of order log n/log d̃, where d̃ is the weighted average of the sum of squares of the expected degrees. Of particular interest are power law random graphs in which the number of vertices of degree k is proportional to 1/kβ for some fixed exponent β. For the case of β > 3, we prove that the average distance of the power law graphs is almost surely of order log n/log d̃. However, many Internet, social, and citation networks are power law graphs with exponents in the range 2 < β < 3 for which the power law random graphs have average distance almost surely of order log log n, but have diameter of order log n (provided having some mild constraints for the average distance and maximum degree). In particular, these graphs contain a dense subgraph, which we call the core, having nc/log log n vertices. Almost all vertices are within distance log log n of the core although there are vertices at distance log n from the core.",fullPaper,jv20
p1957,8fc012941dccba5017bfd73dfd40e414001b74c4,c13,International Conference on Data Science and Advanced Analytics,Complex Graphs and Networks,Graph theory in the information age Old and new concentration inequalities A generative model--the preferential attachment scheme Duplication models for biological networks Random graphs with given expected degrees The rise of the giant component Average distance and the diameter Eigenvalues of the adjacency matrix of $G(\mathbf{w})$ The semi-circle law for $G(\mathbf{w})$ Coupling on-line and off-line analyses of random graphs The configuration model for power law graphs The small world phenomenon in hybrid graphs Bibliography Index.,poster,cp13
p1958,4e892deef85c4cb62716776657d66dc417574bcc,j340,SIAM Journal on Scientific Computing,An Improved Spectral Graph Partitioning Algorithm for Mapping Parallel Computations,Efficient use of a distributed memory parallel computer requires that the computational load be balanced across processors in a way that minimizes interprocessor communication. A new domain mapping algorithm is presented that extends recent work in which ideas from spectral graph theory have been applied to this problem. The generalization of spectral graph bisection involves a novel use of multiple eigenvectors to allow for division of a computation into four or eight parts at each stage of a recursive decomposition. The resulting method is suitable for scientific computations like irregular finite elements or differences performed on hypercube or mesh architecture machines. Experimental results confirm that the new method provides better decompositions arrived at more economically and robustly than with previous spectral methods. This algorithm allows for arbitrary nonnegative weights on both vertices and edges to model inhomogeneous computation and communication. A new spectral lower bound for graph bi...,fullPaper,jv340
p1959,77c4a58c801f233400e71ebd2591df62345f3616,c89,Conference on Uncertainty in Artificial Intelligence,A New Theory of Deadlock-Free Adaptive Routing in Wormhole Networks,"The theoretical background for the design of deadlock-free adaptive routing algorithms for wormhole networks is developed. The author proposes some basic definitions and two theorems. These create the conditions to verify that an adaptive algorithm is deadlock-free, even when there are cycles in the channel dependency graph. Two design methodologies are also proposed. The first supplies algorithms with a high degree of freedom, without increasing the number of physical channels. The second methodology is intended for the design of fault-tolerant algorithms. Some examples are given to show the application of the methodologies. Simulations show the performance improvement that can be achieved by designing the routing algorithms with the new theory. >",poster,cp89
p1960,c45b789b42e9a85d3193f43e3f0ddffb7d6aa423,c9,Pacific Symposium on Biocomputing,Handbook of Combinatorics,"Part 1 Structures: graphs - basic graph theory - paths and circuits, J.A. Bondy, connectivity and network flows, A. Frank, matchings and extensions, W.R. Pulleyblank, colouring, stable sets and perfect graphs, B. Toft, embeddings and minors, C. Thomassen, random graphs, M. Karonski finite sets and relations - hypergraphs, P. Duchet, partially ordered sets, W.T. Trotter matroids - matroids - fundamental concepts, D.J.A. Welsh, matroid minors, P.D. Seymour, matroid optimization and algorithms, R.E. Bixby and W.H. Cunningham symmetric structures - permutation groups, P.J. Cameron, finite geometries, P.J. Cameron, block designs, A.E. Brouwer, association schemes, A.E. Brouwer and W. Haemers, codes, J.H. van Lint combinatorial structures in geometry and number theory - extremal problems in combinatorial geometry, P. Erdos and G. Purdy, convex polytopes and related complexes, V. Klee and P. Kleinschmidt, point lattices, J.C. Lagarias, combinatorial number theory, C. Pomerance and A. Sarkozy. Part 2 Aspects: algebraic enumeration, I.M. Gessel and R.P. Stanley asymptotic enumeration methods, A.M. Odlyzko extremal graph theory, B. Bollobas extremal set systems, P. Frankl Ramsey theory, J. Nesetril discrepancy theory, J. Beck and V.T. Sos automorphism groups, isomorphism, reconstruction, L. Babai optimization, M. Grotschel and L. Lovasz computational complexity, D.B. Shmoys and E. Tardos. Part 3 Methods: polyhedral combinatorics, A. Schrijver tools from linear algebra, C.D. Godsil tools from higher algebra, N. Alon probabilistic methods, J. Spencer topological methods, A. Bjorner. Part 4 Applications: combinatorics in operations research, A. Kolen and J.K. Lenstra combinatorics in electrical engineering and statics, A. Recski combinatorics in statistical mechanics, C.D. Godsil et al combinatorics in chemistry, D.H. Rouvray applications of combinatorics to molecular biology, M.S. Waterman combinatorics in computer science, L. Lovasz et al combinatorics in pure mathematics, L. Lovasz et al. Part 5 Horizons: infinite combinatorics, A. Hajnal combinatorial games, R.K. Guy the history of combinatorics, N.L. Biggs et al.",poster,cp9
p1961,d89cc6a8911156f671ee60ddbf6af20ff33cd146,c68,International Workshop on Software Engineering for Large-Scale Multi-Agent Systems,Shock Graphs and Shape Matching,Abstract content,poster,cp68
p1962,be4bebe5282233f1fa94a5d8a6fc6e452310a27b,c98,North American Chapter of the Association for Computational Linguistics,Topological index based on the ratios of geometrical and arithmetical means of end-vertex degrees of edges,Abstract content,poster,cp98
p1963,49ab911541401d4bb031870d0691379da63d6d28,c45,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",Algebraic Approaches to Graph Transformation - Part I: Basic Concepts and Double Pushout Approach,"The algebraic approaches to graph transformation are based on the concept of gluing of graphs, modelled by pushouts in suitable categories of graphs and graph morphisms. This allows one not only to give an explicit algebraic or set theoretical description of the constructions, but also to use concepts and results from category theory in order to build up a rich theory and to give elegant proofs even in complex situations. In this chapter we start with an overwiev of the basic notions common to the two algebraic approaches, the ""double-pushout (DPO) approach"" and the ""single-pushout (SPO) approach""; next we present the classical theory and some recent development of the double-pushout approach. The next chapter is devoted instead to the single-pushout approach, and it is closed by a comparison between the two approaches. -- This document will appear as a chapter of the ""The Handbook of Graph Grammars. Volume I: Foundations"", G. Rozenberg (Ed.), World Scientific.",poster,cp45
p1964,7d41e80b97db14ea86d6c41e5dc090fd7e8da938,j329,IEEE Transactions on Automatic Control,Consensus Conditions of Multi-Agent Systems With Time-Varying Topologies and Stochastic Communication Noises,"This paper investigates the average-consensus problem of first-order discrete-time multi-agent networks in uncertain communication environments. Each agent can only use its own and neighbors' information to design its control input. To attenuate the communication noises, a distributed stochastic approximation type protocol is used. By using probability limit theory and algebraic graph theory, consensus conditions for this kind of protocols are obtained: (A) For the case of fixed topologies, a necessary and sufficient condition for mean square average-consensus is given, which is also sufficient for almost sure consensus. (B) For the case of time-varying topologies, sufficient conditions for mean square average-consensus and almost sure consensus are given, respectively. Especially, if the network switches between jointly-containing-spanning-tree, instantaneously balanced graphs, then the designed protocol can guarantee that each individual state converges, both almost surely and in mean square, to a common random variable, whose expectation is right the average of the initial states of the whole system, and whose variance describes the static maximum mean square error between each individual state and the average of the initial states of the whole system.",fullPaper,jv329
p1965,1e6f2b06416eb3e2df0c73c90e1bb3f81627a32a,c89,Conference on Uncertainty in Artificial Intelligence,Graphical Models for Game Theory,We introduce a compact graph-theoretic representation for multi-party game theory. Our main result is a provably correct and efficient algorithm for computing approximate Nash equilibria in one-stage games represented by trees or sparse graphs.,fullPaper,cp89
p1966,dce4848f314d1bc4eae80ea5efd1390654a4c341,c52,Workshop on Learning from Authoritative Security Experiment Results,Mathematics of networks,"An introduction to the mathematical tools used in the study of networks. Topics discussed include: the adjacency matrix; weighted, directed, acyclic, and bipartite networks; multilayer and dynamic networks; trees; planar networks. Some basic properties of networks are then discussed, including degrees, density and sparsity, paths on networks, component structure, and connectivity and cut sets. The final part of the chapter focuses on the graph Laplacian and its applications to network visualization, graph partitioning, the theory of random walks, and other problems.",poster,cp52
p1967,a83e9a3dce59292d7dbdab2a8dd20c6b73db3005,j299,IEEE Transactions on Pattern Analysis and Machine Intelligence,"The image foresting transform: theory, algorithms, and applications","The image foresting transform (IFT) is a graph-based approach to the design of image processing operators based on connectivity. It naturally leads to correct and efficient implementations and to a better understanding of how different operators relate to each other. We give here a precise definition of the IFT, and a procedure to compute it-a generalization of Dijkstra's algorithm-with a proof of correctness. We also discuss implementation issues and illustrate the use of the IFT in a few applications.",fullPaper,jv299
p1968,1467a3ff88a9b9fbd54e8c8afa0cb1d62bcf6a22,c30,IEEE Aerospace Conference,Applications of Combinatorial Matrix Theory to Laplacian Matrices of Graphs,"Matrix Theory Preliminaries Vector Norms, Matrix Norms, and the Spectral Radius of a Matrix Location of Eigenvalues Perron-Frobenius Theory M-Matrices Doubly Stochastic Matrices Generalized Inverses Graph Theory Preliminaries Introduction to Graphs Operations of Graphs and Special Classes of Graphs Trees Connectivity of Graphs Degree Sequences and Maximal Graphs Planar Graphs and Graphs of Higher Genus Introduction to Laplacian Matrices Matrix Representations of Graphs The Matrix Tree Theorem The Continuous Version of the Laplacian Graph Representations and Energy Laplacian Matrices and Networks The Spectra of Laplacian Matrices The Spectra of Laplacian Matrices Under Certain Graph Operations Upper Bounds on the Set of Laplacian Eigenvalues The Distribution of Eigenvalues Less than One and Greater than One The Grone-Merris Conjecture Maximal (Threshold) Graphs and Integer Spectra Graphs with Distinct Integer Spectra The Algebraic Connectivity Introduction to the Algebraic Connectivity of Graphs The Algebraic Connectivity as a Function of Edge Weight The Algebraic Connectivity with Regard to Distances and Diameters The Algebraic Connectivity in Terms of Edge Density and the Isoperimetric Number The Algebraic Connectivity of Planar Graphs The Algebraic Connectivity as a Function Genus k where k is greater than 1 The Fiedler Vector and Bottleneck Matrices for Trees The Characteristic Valuation of Vertices Bottleneck Matrices for Trees Excursion: Nonisomorphic Branches in Type I Trees Perturbation Results Applied to Extremizing the Algebraic Connectivity of Trees Application: Joining Two Trees by an Edge of Infinite Weight The Characteristic Elements of a Tree The Spectral Radius of Submatrices of Laplacian Matrices for Trees Bottleneck Matrices for Graphs Constructing Bottleneck Matrices for Graphs Perron Components of Graphs Minimizing the Algebraic Connectivity of Graphs with Fixed Girth Maximizing the Algebraic Connectivity of Unicyclic Graphs with Fixed Girth Application: The Algebraic Connectivity and the Number of Cut Vertices The Spectral Radius of Submatrices of Laplacian Matrices for Graphs The Group Inverse of the Laplacian Matrix Constructing the Group Inverse for a Laplacian Matrix of a Weighted Tree The Zenger Function as a Lower Bound on the Algebraic Connectivity The Case of the Zenger Equalling the Algebraic Connectivity in Trees Application: The Second Derivative of the Algebraic Connectivity as a Function of Edge Weight",poster,cp30
p1969,60fca57bca813e06d2bdf7acb1b970bfce3e858d,c111,International Society for Music Information Retrieval Conference,Strategic Interaction and Networks,"This paper brings a general network analysis to a wide class of economic games. A network, or interaction matrix, tells who directly interacts with whom. A major challenge is determining how network structure shapes overall outcomes. We have a striking result. Equilibrium conditions depend on a single number: the lowest eigenvalue of a network matrix. Combining tools from potential games, optimization, and spectral graph theory, we study games with linear best replies and characterize the Nash and stable equilibria for any graph and for any impact of players’ actions. When the graph is sufficiently absorptive (as measured by this eigenvalue), there is a unique equilibrium. When it is less absorptive, stable equilibria always involve extreme play where some agents take no actions at all. This paper is the first to show the importance of this measure to social and economic outcomes, and we relate it to different network link patterns.",poster,cp111
p1970,0cd7469b7fc4ad90a0552e7124f60f1d216f467f,j341,Journal of Computational Neuroscience,"Two’s company, three (or more) is a simplex",Abstract content,fullPaper,jv341
p1971,f7dccb8cb2e795b9a6d882523b316aa2930b72ad,c38,"IEEE International Conference on Software Analysis, Evolution, and Reengineering",On Communicating Finite-State Machines,"A model of commumcations protocols based on finite-state machines is investigated. The problem addressed is how to ensure certain generally desirable properties, which make protocols ""wellformed,"" that is, specify a response to those and only those events that can actually occur. It is determined to what extent the problem is solvable, and one approach to solving it ts described. Categories and SubJect Descriptors' C 2 2 [Computer-Conununication Networks]: Network Protocols-protocol verification; F 1 1 [Computation by Abstract Devices] Models of Computation--automata; G.2.2 [Discrete Mathematics] Graph Theory--graph algoruhms; trees General Terms: Reliability, Verification Additional",poster,cp38
p1972,bc3c2d97e967e18e1eafd9f2b1b887bf79c9d545,c90,Computer Vision and Pattern Recognition,Graph embedding: a general framework for dimensionality reduction,"In the last decades, a large family of algorithms - supervised or unsupervised; stemming from statistic or geometry theory - have been proposed to provide different solutions to the problem of dimensionality reduction. In this paper, beyond the different motivations of these algorithms, we propose a general framework, graph embedding along with its linearization and kernelization, which in theory reveals the underlying objective shared by most previous algorithms. It presents a unified perspective to understand these algorithms; that is, each algorithm can be considered as the direct graph embedding or its linear/kernel extension of some specific graph characterizing certain statistic or geometry property of a data set. Furthermore, this framework is a general platform to develop new algorithm for dimensionality reduction. To this end, we propose a new supervised algorithm, Marginal Fisher Analysis (MFA), for dimensionality reduction by designing two graphs that characterize the intra-class compactness and inter-class separability, respectively. MFA measures the intra-class compactness with the distance between each data point and its neighboring points of the same class, and measures the inter-class separability with the class margins; thus it overcomes the limitations of traditional Linear Discriminant Analysis algorithm in terms of data distribution assumptions and available projection directions. The toy problem on artificial data and the real face recognition experiments both show the superiority of our proposed MFA in comparison to LDA.",fullPaper,cp90
p1973,a0095ea6b13b95073088f2afb6e33e4d7761b04e,c32,International Conference on Software Technology: Methods and Tools,Entanglement in Graph States and its Applications,"Graph states form a rich class of entangled states that exhibit important aspects of multi-partite entanglement. At the same time, they can be described by a number of parameters that grows only moderately with the system size. They have a variety of applications in quantum information theory, most prominently as algorithmic resources in the context of the one-way quantum computer, but also in other fields such as quantum error correction and multi-partite quantum communication, as well as in the study of foundational issues such as non-locality and decoherence. In this review, we give a tutorial introduction into the theory of graph states. We introduce various equivalent ways how to define graph states, and discuss the basic notions and properties of these states. The focus of this review is on their entanglement properties. These include aspects of non-locality, bi-partite and multi-partite entanglement and its classification in terms of the Schmidt measure, the distillability properties of mixed entangled states close to a pure graph state, as well as the robustness of their entanglement under decoherence. We review some of the known applications of graph states, as well as proposals for their experimental implementation.",poster,cp32
p1974,8d03e9bccb5efb02fee168ab968c6cfa8884905c,c70,International Conference on Intelligent Robotics and Applications,Combinatorial Algebraic Topology,Abstract content,poster,cp70
p1975,0dbffa44f08aec11d35cb404beac6c0f748498b1,j313,Canadian Journal of Mathematics - Journal Canadien de Mathematiques,A Contribution to the Theory of Chromatic Polynomials,"Summary Two polynomials θ(G, n) and ϕ(G, n) connected with the colourings of a graph G or of associated maps are discussed. A result believed to be new is proved for the lesser-known polynomial ϕ(G, n). Attention is called to some unsolved problems concerning ϕ(G, n) which are natural generalizations of the Four Colour Problem from planar graphs to general graphs. A polynomial χ(G, x, y) in two variables x and y, which can be regarded as generalizing both θ(G, n) and ϕ(G, n) is studied. For a connected graph χ(G, x, y) is defined in terms of the “spanning” trees of G (which include every vertex) and in terms of a fixed enumeration of the edges.",fullPaper,jv313
p1976,d20399ead61efbf3ac4abed80836b5fe1bda64d9,c20,Research Conference of the South African Institute of Computer Scientists and Information Technologists,"Towards a spectral theory of graphs based on the signless Laplacian, I","A spectral graph theory is a theory in which graphs are studied by means of eigenvalues of a matrix �� which is in a prescribed way defined for any graph. This theory is called �� -theory. We outline a spectral theory of graphs based on the signless Laplacians �� and compare it with other spectral theories, in particular with those based on the adjacency matrix �� and the Laplacian �� . The �� -theory can be composed using various connections to other theories: equivalency with �� -theory and �� -theory for regular graphs, or with �� -theory for bipartite graphs, general analogies with �� -theory and analogies with �� -theory via line graphs and subdivision graphs. We present results on graph operations, inequalities for eigenvalues and reconstruction problems.",poster,cp20
p1977,3bc4736f9b8512043ed47357a81f26b93a1204b6,c5,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,Semi-supervised learning with graphs,"In traditional machine learning approaches to classification, one uses only a labeled set to train the classifier. Labeled instances however are often difficult, expensive, or time consuming to obtain, as they require the efforts of experienced human annotators. Meanwhile unlabeled data may be relatively easy to collect, but there has been few ways to use them. Semi-supervised learning addresses this problem by using large amount of unlabeled data, together with the labeled data, to build better classifiers. Because semi-supervised learning requires less human effort and gives higher accuracy, it is of great interest both in theory and in practice. 
We present a series of novel semi-supervised learning approaches arising from a graph representation, where labeled and unlabeled instances are represented as vertices, and edges encode the similarity between instances. They address the following questions: How to use unlabeled data? (label propagation); What is the probabilistic interpretation? (Gaussian fields and harmonic functions); What if we can choose labeled data? (active learning); How to construct good graphs? (hyperparameter learning); How to work with kernel machines like SVM? (graph kernels); How to handle complex data like sequences? (kernel conditional random fields); How to handle scalability and induction? (harmonic mixtures). An extensive literature review is included at the end.",poster,cp5
p1978,ba5f513fc3be3432018a3f93b12e17a3f1580324,c42,Joint Conference of International Workshop on Software Measurement and International Conference on Software Process and Product Measurement,"Protein-to-protein interactions: Technologies, databases, and algorithms","Studying proteins and their structures has an important role for understanding protein functionalities. Recently, due to important results obtained with proteomics, a great interest has been given to interactomics, that is, the study of protein-to-protein interactions, called PPI, or more generally, interactions among macromolecules, particularly within cells. Interactomics means studying, modeling, storing, and retrieving protein-to-protein interactions as well as algorithms for manipulating, simulating, and predicting interactions. PPI data can be obtained from biological experiments studying interactions. Modeling and storing PPIs can be realized by using graph theory and graph data management, thus graph databases can be queried for further experiments. PPI graphs can be used as input for data-mining algorithms, where raw data are binary interactions forming interaction graphs, and analysis algorithms retrieve biological interactions among proteins (i.e., PPI biological meanings). For instance, predicting the interactions between two or more proteins can be obtained by mining interaction networks stored in databases. In this article we survey modeling, storing, analyzing, and manipulating PPI data. After describing the main PPI models, mostly based on graphs, the article reviews PPI data representation and storage, as well as PPI databases. Algorithms and software tools for analyzing and managing PPI networks are discussed in depth. The article concludes by discussing the main challenges and research directions in PPI networks.",poster,cp42
p1979,3742c17f24437a310b05888de2618212fe82cedf,c97,Interspeech,Ramanujan Graphs,"In the last two decades, the theory of Ramanujan graphs has gained prominence primarily for two reasons. First, from a practical viewpoint, these graphs resolve an extremal problem in communication network theory (see for example [2]). Second, from a more aesthetic viewpoint, they fuse diverse branches of pure mathematics, namely, number theory, representation theory and algebraic geometry. The purpose of this survey is to unify some of the recent developments and expose certain open problems in the area. This survey is by no means an exhaustive one and demonstrates a highly number-theoretic bias. For more comprehensive surveys, we refer the reader to [27], [9] or [13]. For a more up-to-date survey highlighting the connection between graph theory and automorphic representations, we refer the reader to Winnie Li’s recent survey article [11]. A is a triple consisting of avertex (X), an and a map that associates to each edge two vertices (not necessarily distinct) called itsendpoints. A is an edge whose endpoints are equal. Multiple edges are edges having the same pair of endpoints. A is one having no loops or multiple edges. If a graph has loops or multiple edges, we will call it a multigraph. When two verticesu andv are endpoints of an edge, we say they are and (sometimes) write to indicate this. To any graph, we may associate the which is ann matrix (wheren |) with rows and columns indexed by the elements of the vertex set and the y)-th entry is the number of edges connecting and y. Since our graphs are undirected, the matrix is symmetric. Consequently, all of its eigenvalues are real. The convention regarding terminology is not clear in the literature. Most use the term ‘graph’ to mean a simple graph as we have defined it above. Thus, the",poster,cp97
p1980,b1cda064decc073881941c6d4f4ef3d6a3daafb4,j342,Mathematical programming,Statistical ranking and combinatorial Hodge theory,Abstract content,fullPaper,jv342
p1981,76b76de8318457348973d8a655d1212fe51cf142,c54,International Workshop on Agent-Oriented Software Engineering,The topology of multidimensional potential energy surfaces: Theory and application to peptide structure and kinetics,"Topological characteristics of multidimensional potential energy surfaces are explored and the full conformation space is mapped on the set of local minima. This map partitions conformation space into energy-dependent or temperature-dependent “attraction basins’’ and generates a “disconnectivity’’ graph that reflects the basin connectivity and characterizes the shape of the multidimensional surface. The partitioning of the conformation space is used to express the temporal behavior of the system in terms of basin-to-basin kinetics instead of the usual state-to-state transitions. For this purpose the transition matrix of the system is expressed in terms of basin-to-basin transitions and the corresponding master equation is solved. As an example, the approach is applied to the tetrapeptide, isobutyryl-(ala)3-NH-methyl (IAN), which is the shortest peptide that can form a full helical turn. A nearly complete list of minima and barriers is available for this system from the work of Czerminiski and Elber. The m...",poster,cp54
p1982,b4642da8677ca40ec888c6d450c3308473281b52,c103,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,Incidence matrices and interval graphs,"Abstract : According to present genetic theory, the fine structure of genes consists of linearly ordered elements. A mutant gene is obtained by alteration of some connected portion of this structure. By examining data obtained from suitable experiments, it can be determined whether or not the blemished portions of two mutant genes intersect or not, and thus intersection data for a large number of mutants can be represented as an undirected graph. If this graph is an interval graph, then the observed data is consistent with a linear model of the gene. The problem of determining when a graph is an interval graph is a special case of the following problem concerning (0, 1)-matrices: When can the rows of such a matrix be permuted so as to make the 1's in each colum appear consecutively. A complete theory is obtained for this latter problem, culminating in a decomposition theorem which leads to a rapid algorithm for deciding the question, and for constructing the desired permutation when one exists.",poster,cp103
p1983,3ecd2ef597822fee6f9f7752bcfcbb0242405eb0,c91,Workshop on Algorithms and Models for the Web-Graph,Workshop on Algorithms and Models for the Web Graph,Abstract content,fullPaper,cp91
p1984,2a5bcf67137c98b80ad18d8a76089832f848ceb5,c88,Symposium on the Theory of Computing,Differential and Integral Inequalities,Abstract content,poster,cp88
p1985,6a70edfc099614866996222893e91826b4da3677,c33,International Conference on Agile Software Development,Thermodynamic perturbation theory of polymerization,"We derive several extensions of a previously given first‐order perturbation theory (TPT 1) for fluids in which chain and ring polymers can be formed, due to the presence of two singly bondable molecular attraction sites. We retain graphs which contain a single chain of attraction bonds, and evaluate second‐order perturbation theory (TPT 2) within this framework. The previous formulation with sites fixed in the molecule is generalized to allow movable sites, thus permitting calculations for flexible bead polymers. The TPT 1 result for the equation of state of an equilibrium mixture of chain lengths is in good agreement with simulations of flexible bead polymers of fixed bead number N, when the mean number ν of beads is equated to the fixed N of the simulations. TPT 2 differs from TPT 1 by a rather small term, with improved agreement. If the distribution of movable sites includes configurations such that bonding of one site blocks bonding of the other, then graph resummation must be used. Resummed TPT yield...",poster,cp33
p1986,5ccd1eefd94af0a20e10a0a5e0da22aa2f481c76,c105,Biometrics and Identity Management,Every monotone graph property has a sharp threshold,"In their seminal work which initiated random graph theory Erdos and Renyi discovered that many graph properties have sharp thresholds as the number of vertices tends to infinity. We prove a conjecture of Linial that every monotone graph property has a sharp threshold. This follows from the following theorem. Let Vn(p) = {0, 1}n denote the Hamming space endowed with the probability measure μp defined by μp( 1, 2, . . . , n) = pk · (1 − p)n−k, where k = 1 + 2 + · · · + n. Let A be a monotone subset of Vn. We say that A is symmetric if there is a transitive permutation group Γ on {1, 2, . . . , n} such that A is invariant under Γ. Theorem. For every symmetric monotone A, if μp(A) > then μq(A) > 1− for q = p+ c1 log(1/2 )/ logn. (c1 is an absolute constant.)",poster,cp105
p1987,6356186ad49a7be6ef076a9625e8a47bca9dab20,c58,Australian Software Engineering Conference,Geometric Group Theory,"The aim of this talk is to define the space of ends of a f.g. (finitely generated) group. First, define the space of ends Ends(X) for a metric space X. Define what quasi-isometries are, briefly sketch the concept of proper and geodesic spaces and show that quasi-isometries between such spaces induce homeomorphisms between their spaces of ends. This can be found in [3], p.144/45. Define the Cayley graph for a group presentation and give the main ideas of Theorem 1.5 in [10]. Conclude.",poster,cp58
p1988,9496a84e79463f1004e7880669e7661d5203ed4a,c8,The Compass,Koszul duality for Operads,"(0.1) The purpose of this paper is to relate two seemingly disparate developments. One is the theory of graph cohomology of Kontsevich [Kon 2 3] which arose out of earlier works of Penner [Pe] and Kontsevich [Kon 1] on the cell decomposition and intersection theory on the moduli spaces of curves. The other is the theory of Koszul duality for quadratic associative algebras which was introduced by Priddy [Pr] and has found many applications in homological algebra, algebraic geometry and representation theory (see e.g., [Be] [BGG] [BGS] [Ka 1] [Man]). The unifying concept here is that of an operad. This paper can be divided into two parts consisting of chapters 1, 3 and 2, 4, respectively. The purpose of the first part is to establish a relationship between operads, moduli spaces of stable curves and graph complexes. To each operad we associate a collection of sheaves on moduli spaces. We introduce, in a natural way, the cobar complex of an operad and show that it is nothing but a (special case of the) graph complex, and that both constructions can be interpreted as the Verdier duality functor on sheaves. In the second part we introduce a class of operads, called quadratic, and introduce a distinguished subclass of Koszul operads. The main reason for introducing Koszul operads (and in fact for writing this paper) is that most of the operads ”arising from nature” are Koszul, cf. (0.8) below. We define a natural duality on quadratic operads (which is",poster,cp8
p1989,f276c00bac7594107c291947f560b7b48b1439d7,j343,Mathematics of Operations Research,A Best Possible Heuristic for the k-Center Problem,"In this paper we present a 2-approximation algorithm for the k-center problem with triangle inequality. This result is “best possible” since for any δ < 2 the existence of δ-approximation algorithm would imply that P = NP. It should be noted that no δ-approximation algorithm, for any constant δ, has been reported to date. Linear programming duality theory provides interesting insight to the problem and enables us to derive, in O|E| log |E| time, a solution with value no more than twice the k-center optimal value. 
 
A by-product of the analysis is an O|E| algorithm that identifies a dominating set in G2, the square of a graph G, the size of which is no larger than the size of the minimum dominating set in the graph G. The key combinatorial object used is called a strong stable set, and we prove the NP-completeness of the corresponding decision problem.",fullPaper,jv343
p1990,167261fe1af94863ea1824f6bc58fd4266b954af,c75,International Conference on Machine Learning,On higher rank graph C ∗ -algebras,"Given a row-finite k-graph Λ with no sources we investigate the K-theory of the higher rank graph C *-algebra, C * (Λ). When k = 2 we are able to give explicit formulae to calculate the K-groups of C * (Λ). The K-groups of C * (Λ) for k > 2 can be calculated under certain circumstances. We state that for all k, the torsion-free rank of K 0 (C * (Λ)) and K 1 (C * (Λ)) are equal when C * (Λ) is unital, and we determine the position of the class of the unit of C * (Λ) in K 0 (C * (Λ)).",poster,cp75
p1991,c20ef537e85b010e2f7e68cb328d1388e6c76de7,c22,International Conference on Data Technologies and Applications,"Instantons on ALE spaces, quiver varieties, and Kac-Moody algebras","To Professor Shoshichi Kobayashi on his 60th birthday 1. Introduction. In this paper we shall introduce a new family of varieties, which we call quiver varieties, and study their geometric structures. They have close relation to the singularity theory and the representation theory of the Kac-Moody algebras. Our original motivation was to study solutions of the anti-self-dual Yang-Mills equations on a particular class of 4-dimensional noncompact complete manifolds, the so-called ALE spaces (or the ALE gravitational instantons), which were constructed by Kronheimer [Krl]. In [KN] we gave a description of the framed moduli space of all solutions in terms of solutions of a system of quadratic equations (called the ADHM equations) for representations of a quiver on an affine, simply laced Dynkin graph. It is an analogue of the description, given by Atiyah, Drinfeld, Hitchin, and Manin [ADHM], of the moduli space for IR 4 (or S4) in terms of solutions of a quadratic equation for certain finite-dimensional matrices. Once we set aside their gauge-theoretic origin, there is no longer reason to restrict ourselves to affine Dynkin graphs. Definitions can be generalized to arbitrary finite graphs. We get what we call quiver varieties. We study geometric structures of quiver varieties in this paper. In [Nal] it was noticed that the moduli space of anti-self-dual connections on ALE spaces has a hyper-K/ihler structure, namely a Riemannian metric equipped with three endo-morphisms I, J, K of the tangent bundle which satisfy the relations of quaternion algebra and are covariant constant with respect to the Levi-Civita connection: The same holds for general quiver varieties. In particular, quiver varieties have holomorphic symplectic forms. We study further properties of the quiver variety, such as a natural *-action, symplectic geometry, topology, and so on. As ALE spaces closely related to simple singularities, quiver varieties have very special kinds of singularities that enjoy very nice properties. Surprisingly, the ADHM equation appears in a very different context. In [L3] Lusztig used it to construct ""canonical bases"" of the part U-of the quantized enveloping algebra U associated by Drinfeld and Jimbo to the graph. Motivated by his results, we give a geometric construction of irreducible highest-weight integrable representations of the Kac-Moody algebra associated to the graph (Theorem 10.14). The weight space of the representation space will be given as a vector space consisting of constructible functions on a Lagrangian subvariety of a quiver variety. The action of the Kac-Moody …",poster,cp22
p1992,28973306da1959a376780af9db8ac9ff7061c789,c92,Advances in Soft Computing,First Course on Fuzzy Theory and Applications,Abstract content,fullPaper,cp92
p1993,897aa57d2811be3f2d5aca74123cf7bfa9e75344,c61,Jahrestagung der Gesellschaft für Informatik,Computational Invariant Theory,Abstract content,poster,cp61
p1994,bedf65261e1518cf1cf959c0f30c65dbb56b5d65,j338,Internet Mathematics,The Average Distance in a Random Graph with Given Expected Degrees,"Random graph theory is used to examine the ""small-world phenomenon""– any two strangers are connected through a short chain of mutual acquaintances. We will show that for certain families of random graphs with given expected degrees, the average distance is almost surely of order log n/ logd̃ where d̃ is the weighted average of the sum of squares of the expected degrees. Of particular interest are power law random graphs in which the number of vertices of degree k is proportional to 1/k β for some fixed exponent β. For the case of β > 3, we prove that the average distance of the power law graphs is almost surely of order log n/ log d̃. However, many Internet, social, and citation networks are power law graphs with exponents in the range 2 < β < 3 for which the power law random graphs have average distance almost surely of order log log n, but have diameter of order log n (provided having some mild constraints for the average distance and maximum degree). In particular, these graphs contain a dense subgraph, that we call the core, having n c/ log log n vertices. Almost all vertices are within distance log log n of the core although there are vertices at distance log n from the core.",fullPaper,jv338
p1995,959258cb7ff636fee908e6f6877388081ca706b6,c51,Conference of the Centre for Advanced Studies on Collaborative Research,How to Draw a Graph,"W E use the definitions of (11). However, in deference to some recent attempts to unify the terminology of graph theory we replace the term 'circuit' by 'polygon', and 'degree' by 'valency'. A graph G is 3-connected (nodally 3-connected) if it is simple and non-separable and satisfies the following condition; if G is the union of two proper subgraphs H and K such that HnK consists solely of two vertices u and v, then one of H and K is a link-graph (arc-graph) with ends u and v. It should be noted that the union of two proper subgraphs H and K of G can be the whole of G only if each of H and K includes at least one edge or vertex not belonging to the other. In this paper we are concerned mainly with nodally 3-connected graphs, but a specialization to 3-connected graphs is made in § 12. In § 3 we discuss conditions for a nodally 3-connected graph to be planar, and in § 5 we discuss conditions for the existence of Kuratowski subgraphs of a given graph. In §§ 6-9 we show how to obtain a convex representation of a nodally 3-connected graph, without Kuratowski subgraphs, by solving a set of linear equations. Some extensions of these results to general graphs, with a proof of Kuratowski's theorem, are given in §§ 10-11. In § 12 we discuss the representation in the plane of a pair of dual graphs, and in § 13 we draw attention to some unsolved problems.",poster,cp51
p1996,d3ecad6e490d22e0bac1dc86e3a363ad791171a2,c61,Jahrestagung der Gesellschaft für Informatik,I-path analysis,"A circuit at the register transfer level is denoted as an RTL circuit. The paper describes a method for extracting the RTL circuit structure from the circuit formal description, using the I-path concept. The way of representing the RTL circuit structure by a labelled directed graph where nodes represent components and arcs represent connections between them, is presented. Labels identifying the component type are attached to the nodes, and other labels are attached to arcs to identify attributes of connections. It is shown, how the graph theory algorithms can be used to derive the information about the accessibility of circuit components, i.e., the existence of I-paths between them, and the sequences of control and clock signals which must be generated to transfer the information along the existing I-paths.<<ETX>>",poster,cp61
p1997,90b1e6a18c6a5dbbfd03752506b5744e257ef5b4,c46,Brazilian Symposium on Software Engineering,"Sparsity - Graphs, Structures, and Algorithms",Abstract content,poster,cp46
p1998,09d298f8418f7cdb1eececea66605e1b20667bc0,c85,International Conference on Graph Transformation,A course on the Web graph,"A Course on the Web Graph provides a comprehensive introduction to state-of-the-art research on the applications of graph theory to real-world networks such as the web graph. It is the first mathematically rigorous textbook discussing both models of the web graph and algorithms for searching the web. After introducing key tools required for the study of web graph mathematics, an overview is given of the most widely studied models for the web graph. A discussion of popular web search algorithms, e.g. PageRank, is followed by additional topics, such as applications of infinite graph theory to the web graph, spectral properties of power law graphs, domination in the web graph, and the spread of viruses in networks. The book is based on a graduate course taught at the AARMS 2006 Summer School at Dalhousie University. As such it is self-contained and includes over 100 exercises. The reader of the book will gain a working knowledge of current research in graph theory and its modern applications. In addition, the reader will learn first-hand about models of the web, and the mathematics underlying modern search engines.",poster,cp85
p1999,15af7f0606ad1a2a438b08a9b05a56455d5ebfee,c74,IEEE International Conference on Tools with Artificial Intelligence,Network theory of microscopic and macroscopic behavior of master equation systems,"A general microscopic and macroscopic theory is developed for systems which are governed by a (linear) master equation. The theory is based on a network representation of the master equation, and the results are obtained mostly by application of some basic theorems of mathematical graph theory. In the microscopic part of the theory, the construction of a steady state solution of the master equation in terms of graph theoretical elements is described (Kirchhoff's theorem), and it is shown that the master equation satisfies a global asymptotic Liapunov stability criterion with respect to this state. The Glansdorff-Prigogine criterion turns out to be the differential version and thus a special case of the global criterion. In the macroscopic part of the theory, a general prescription is given describing macrostates of the systems arbitrarily far from equilibrium in the language of generalized forces and fluxes of nonlinear irreversible thermodynamics. As a particular result, Onsager's reciprocity relations for the phenomenological coefficients are obtained as coinciding with the reciprocity relations of a near-to-equilibrium network.",poster,cp74
p2000,46c0b71ceea130bead6c9120cf6d32b2835db831,c92,Advances in Soft Computing,The Cambridge Structural Database,"This paper is the definitive article describing the creation, maintenance, information content and availability of the Cambridge Structural Database (CSD), the world’s repository of small molecule crystal structures.",poster,cp92
p2001,948fd800ecdd3c99488dde36b41480ca1b8acce3,c104,IEEE International Conference on Multimedia and Expo,The PRIDE database and related tools and resources in 2019: improving support for quantification data,"Abstract The PRoteomics IDEntifications (PRIDE) database (https://www.ebi.ac.uk/pride/) is the world’s largest data repository of mass spectrometry-based proteomics data, and is one of the founding members of the global ProteomeXchange (PX) consortium. In this manuscript, we summarize the developments in PRIDE resources and related tools since the previous update manuscript was published in Nucleic Acids Research in 2016. In the last 3 years, public data sharing through PRIDE (as part of PX) has definitely become the norm in the field. In parallel, data re-use of public proteomics data has increased enormously, with multiple applications. We first describe the new architecture of PRIDE Archive, the archival component of PRIDE. PRIDE Archive and the related data submission framework have been further developed to support the increase in submitted data volumes and additional data types. A new scalable and fault tolerant storage backend, Application Programming Interface and web interface have been implemented, as a part of an ongoing process. Additionally, we emphasize the improved support for quantitative proteomics data through the mzTab format. At last, we outline key statistics on the current data contents and volume of downloads, and how PRIDE data are starting to be disseminated to added-value resources including Ensembl, UniProt and Expression Atlas.",poster,cp104
p2002,95cd83603a0d2b6918a8e34a5637a8f382da96f5,j14,Scientific Data,"MIMIC-III, a freely accessible critical care database",Abstract content,fullPaper,jv14
p2003,da692ee969d9c33986196372c3f7cb87fa6b6f8f,c29,International Conference on Software Engineering,Database resources of the National Center for Biotechnology Information,"Abstract The National Center for Biotechnology Information (NCBI) provides a large suite of online resources for biological information and data, including the GenBank® nucleic acid sequence database and the PubMed database of citations and abstracts for published life science journals. The Entrez system provides search and retrieval operations for most of these data from 39 distinct databases. The E-utilities serve as the programming interface for the Entrez system. Augmenting many of the Web applications are custom implementations of the BLAST program optimized to search specialized data sets. New resources released in the past year include PubMed Data Management, RefSeq Functional Elements, genome data download, variation services API, Magic-BLAST, QuickBLASTp, and Identical Protein Groups. Resources that were updated in the past year include the genome data viewer, a human genome resources page, Gene, virus variation, OSIRIS, and PubChem. All of these resources can be accessed through the NCBI home page at www.ncbi.nlm.nih.gov.",poster,cp29
p2004,98128fd412ebfa90201a276f2c59020ccc696a75,c62,International Conference on Software Reuse,DrugBank 5.0: a major update to the DrugBank database for 2018,"Abstract DrugBank (www.drugbank.ca) is a web-enabled database containing comprehensive molecular information about drugs, their mechanisms, their interactions and their targets. First described in 2006, DrugBank has continued to evolve over the past 12 years in response to marked improvements to web standards and changing needs for drug research and development. This year’s update, DrugBank 5.0, represents the most significant upgrade to the database in more than 10 years. In many cases, existing data content has grown by 100% or more over the last update. For instance, the total number of investigational drugs in the database has grown by almost 300%, the number of drug-drug interactions has grown by nearly 600% and the number of SNP-associated drug effects has grown more than 3000%. Significant improvements have been made to the quantity, quality and consistency of drug indications, drug binding data as well as drug-drug and drug-food interactions. A great deal of brand new data have also been added to DrugBank 5.0. This includes information on the influence of hundreds of drugs on metabolite levels (pharmacometabolomics), gene expression levels (pharmacotranscriptomics) and protein expression levels (pharmacoprotoemics). New data have also been added on the status of hundreds of new drug clinical trials and existing drug repurposing trials. Many other important improvements in the content, interface and performance of the DrugBank website have been made and these should greatly enhance its ease of use, utility and potential applications in many areas of pharmacological research, pharmaceutical science and drug education.",poster,cp62
p2005,6e1e6afb314f9c5a24d744252a30aa5efc313571,c64,Experimental Software Engineering Network,"The STRING database in 2017: quality-controlled protein–protein association networks, made broadly accessible","A system-wide understanding of cellular function requires knowledge of all functional interactions between the expressed proteins. The STRING database aims to collect and integrate this information, by consolidating known and predicted protein–protein association data for a large number of organisms. The associations in STRING include direct (physical) interactions, as well as indirect (functional) interactions, as long as both are specific and biologically meaningful. Apart from collecting and reassessing available experimental data on protein–protein interactions, and importing known pathways and protein complexes from curated databases, interaction predictions are derived from the following sources: (i) systematic co-expression analysis, (ii) detection of shared selective signals across genomes, (iii) automated text-mining of the scientific literature and (iv) computational transfer of interaction knowledge between organisms based on gene orthology. In the latest version 10.5 of STRING, the biggest changes are concerned with data dissemination: the web frontend has been completely redesigned to reduce dependency on outdated browser technologies, and the database can now also be queried from inside the popular Cytoscape software framework. Further improvements include automated background analysis of user inputs for functional enrichments, and streamlined download options. The STRING resource is available online, at http://string-db.org/.",poster,cp64
p2006,51da1eab2d350b5aa0eeebf83fba7caae3a3bc29,j240,International Journal of Systematic and Evolutionary Microbiology,Introducing EzBioCloud: a taxonomically united database of 16S rRNA gene sequences and whole-genome assemblies,"The recent advent of DNA sequencing technologies facilitates the use of genome sequencing data that provide means for more informative and precise classification and identification of members of the Bacteria and Archaea. Because the current species definition is based on the comparison of genome sequences between type and other strains in a given species, building a genome database with correct taxonomic information is of paramount need to enhance our efforts in exploring prokaryotic diversity and discovering novel species as well as for routine identifications. Here we introduce an integrated database, called EzBioCloud, that holds the taxonomic hierarchy of the Bacteria and Archaea, which is represented by quality-controlled 16S rRNA gene and genome sequences. Whole-genome assemblies in the NCBI Assembly Database were screened for low quality and subjected to a composite identification bioinformatics pipeline that employs gene-based searches followed by the calculation of average nucleotide identity. As a result, the database is made of 61 700 species/phylotypes, including 13 132 with validly published names, and 62 362 whole-genome assemblies that were identified taxonomically at the genus, species and subspecies levels. Genomic properties, such as genome size and DNA G+C content, and the occurrence in human microbiome data were calculated for each genus or higher taxa. This united database of taxonomy, 16S rRNA gene and genome sequences, with accompanying bioinformatics tools, should accelerate genome-based classification and identification of members of the Bacteria and Archaea. The database and related search tools are available at www.ezbiocloud.net/.",fullPaper,jv240
p2007,0f5c63182b5d40850c741888a89e6c055a3593af,c16,Knowledge Discovery and Data Mining,The Pfam protein families database: towards a more sustainable future,"In the last two years the Pfam database (http://pfam.xfam.org) has undergone a substantial reorganisation to reduce the effort involved in making a release, thereby permitting more frequent releases. Arguably the most significant of these changes is that Pfam is now primarily based on the UniProtKB reference proteomes, with the counts of matched sequences and species reported on the website restricted to this smaller set. Building families on reference proteomes sequences brings greater stability, which decreases the amount of manual curation required to maintain them. It also reduces the number of sequences displayed on the website, whilst still providing access to many important model organisms. Matches to the full UniProtKB database are, however, still available and Pfam annotations for individual UniProtKB sequences can still be retrieved. Some Pfam entries (1.6%) which have no matches to reference proteomes remain; we are working with UniProt to see if sequences from them can be incorporated into reference proteomes. Pfam-B, the automatically-generated supplement to Pfam, has been removed. The current release (Pfam 29.0) includes 16 295 entries and 559 clans. The facility to view the relationship between families within a clan has been improved by the introduction of a new tool.",poster,cp16
p2008,788b43b7c62b497cf69b31544c6f81c6f4856d42,c92,Advances in Soft Computing,Pfam: the protein families database,"Pfam, available via servers in the UK (http://pfam.sanger.ac.uk/) and the USA (http://pfam.janelia.org/), is a widely used database of protein families, containing 14 831 manually curated entries in the current release, version 27.0. Since the last update article 2 years ago, we have generated 1182 new families and maintained sequence coverage of the UniProt Knowledgebase (UniProtKB) at nearly 80%, despite a 50% increase in the size of the underlying sequence database. Since our 2012 article describing Pfam, we have also undertaken a comprehensive review of the features that are provided by Pfam over and above the basic family data. For each feature, we determined the relevance, computational burden, usage statistics and the functionality of the feature in a website context. As a consequence of this review, we have removed some features, enhanced others and developed new ones to meet the changing demands of computational biology. Here, we describe the changes to Pfam content. Notably, we now provide family alignments based on four different representative proteome sequence data sets and a new interactive DNA search interface. We also discuss the mapping between Pfam and known 3D structures.",poster,cp92
p2009,b204970b0503a923359bff532726666f5e0e971b,c79,International Conference on Medical Image Computing and Computer-Assisted Intervention,The SILVA ribosomal RNA gene database project: improved data processing and web-based tools,"SILVA (from Latin silva, forest, http://www.arb-silva.de) is a comprehensive web resource for up to date, quality-controlled databases of aligned ribosomal RNA (rRNA) gene sequences from the Bacteria, Archaea and Eukaryota domains and supplementary online services. The referred database release 111 (July 2012) contains 3 194 778 small subunit and 288 717 large subunit rRNA gene sequences. Since the initial description of the project, substantial new features have been introduced, including advanced quality control procedures, an improved rRNA gene aligner, online tools for probe and primer evaluation and optimized browsing, searching and downloading on the website. Furthermore, the extensively curated SILVA taxonomy and the new non-redundant SILVA datasets provide an ideal reference for high-throughput classification of data from next-generation sequencing approaches.",poster,cp79
p2010,93d5369a0be3134c6018373d5290923f3d718815,c48,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",The Molecular Signatures Database Hallmark Gene Set Collection,Abstract content,poster,cp48
p2011,7f19972754ac0c15329666b3a6efbf569b27d8d5,c0,International Conference on Human Factors in Computing Systems,The Pfam protein families database in 2019,"Abstract The last few years have witnessed significant changes in Pfam (https://pfam.xfam.org). The number of families has grown substantially to a total of 17,929 in release 32.0. New additions have been coupled with efforts to improve existing families, including refinement of domain boundaries, their classification into Pfam clans, as well as their functional annotation. We recently began to collaborate with the RepeatsDB resource to improve the definition of tandem repeat families within Pfam. We carried out a significant comparison to the structural classification database, namely the Evolutionary Classification of Protein Domains (ECOD) that led to the creation of 825 new families based on their set of uncharacterized families (EUFs). Furthermore, we also connected Pfam entries to the Sequence Ontology (SO) through mapping of the Pfam type definitions to SO terms. Since Pfam has many community contributors, we recently enabled the linking between authorship of all Pfam entries with the corresponding authors’ ORCID identifiers. This effectively permits authors to claim credit for their Pfam curation and link them to their ORCID record.",poster,cp0
p2012,d2c733e34d48784a37d717fe43d9e93277a8c53e,c19,ACM Conference on Economics and Computation,ImageNet: A large-scale hierarchical image database,"The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.",poster,cp19
p2013,02613d6e3ecf67ed9ae8ce67a35a92f3986bc4cf,j102,Nucleic Acids Research,Gapped BLAST and PSI-BLAST: a new generation of protein database search programs.,"The BLAST programs are widely used tools for searching protein and DNA databases for sequence similarities. For protein comparisons, a variety of definitional, algorithmic and statistical refinements described here permits the execution time of the BLAST programs to be decreased substantially while enhancing their sensitivity to weak similarities. A new criterion for triggering the extension of word hits, combined with a new heuristic for generating gapped alignments, yields a gapped BLAST program that runs at approximately three times the speed of the original. In addition, a method is introduced for automatically combining statistically significant alignments produced by BLAST into a position-specific score matrix, and searching the database using this matrix. The resulting Position-Specific Iterated BLAST (PSI-BLAST) program runs at approximately the same speed per iteration as gapped BLAST, but in many cases is much more sensitive to weak but biologically relevant sequence similarities. PSI-BLAST is used to uncover several new and interesting members of the BRCT superfamily.",fullPaper,jv102
p2014,0e5bccdedb82fbafece8ca71d64b16ff05ec9145,c20,Research Conference of the South African Institute of Computer Scientists and Information Technologists,The carbohydrate-active enzymes database (CAZy) in 2013,"The Carbohydrate-Active Enzymes database (CAZy; http://www.cazy.org) provides online and continuously updated access to a sequence-based family classification linking the sequence to the specificity and 3D structure of the enzymes that assemble, modify and breakdown oligo- and polysaccharides. Functional and 3D structural information is added and curated on a regular basis based on the available literature. In addition to the use of the database by enzymologists seeking curated information on CAZymes, the dissemination of a stable nomenclature for these enzymes is probably a major contribution of CAZy. The past few years have seen the expansion of the CAZy classification scheme to new families, the development of subfamilies in several families and the power of CAZy for the analysis of genomes and metagenomes. This article outlines the changes that have occurred in CAZy during the past 5 years and presents our novel effort to display the resolution and the carbohydrate ligands in crystallographic complexes of CAZymes.",poster,cp20
p2015,f986968735459e789890f24b6b277b0920a9725d,j299,IEEE Transactions on Pattern Analysis and Machine Intelligence,Places: A 10 Million Image Database for Scene Recognition,"The rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach near-human semantic classification performance at tasks such as visual object and scene recognition. Here we describe the Places Database, a repository of 10 million scene photographs, labeled with scene semantic categories, comprising a large and diverse list of the types of environments encountered in the world. Using the state-of-the-art Convolutional Neural Networks (CNNs), we provide scene classification CNNs (Places-CNNs) as baselines, that significantly outperform the previous approaches. Visualization of the CNNs trained on Places shows that object detectors emerge as an intermediate representation of scene classification. With its high-coverage and high-diversity of exemplars, the Places Database along with the Places-CNNs offer a novel resource to guide future progress on scene recognition problems.",fullPaper,jv299
p2016,57dfc18815bba1c3737dbc2e5497fd1fc595edb5,j240,International Journal of Systematic and Evolutionary Microbiology,Introducing EzTaxon-e: a prokaryotic 16S rRNA gene sequence database with phylotypes that represent uncultured species.,"Despite recent advances in commercially optimized identification systems, bacterial identification remains a challenging task in many routine microbiological laboratories, especially in situations where taxonomically novel isolates are involved. The 16S rRNA gene has been used extensively for this task when coupled with a well-curated database, such as EzTaxon, containing sequences of type strains of prokaryotic species with validly published names. Although the EzTaxon database has been widely used for routine identification of prokaryotic isolates, sequences from uncultured prokaryotes have not been considered. Here, the next generation database, named EzTaxon-e, is formally introduced. This new database covers not only species within the formal nomenclatural system but also phylotypes that may represent species in nature. In addition to an identification function based on Basic Local Alignment Search Tool (blast) searches and pairwise global sequence alignments, a new objective method of assessing the degree of completeness in sequencing is proposed. All sequences that are held in the EzTaxon-e database have been subjected to phylogenetic analysis and this has resulted in a complete hierarchical classification system. It is concluded that the EzTaxon-e database provides a useful taxonomic backbone for the identification of cultured and uncultured prokaryotes and offers a valuable means of communication among microbiologists who routinely encounter taxonomically novel isolates. The database and its analytical functions can be found at http://eztaxon-e.ezbiocloud.net/.",fullPaper,jv240
p2017,9f7626c7af925b7b69f1ba86ceb916d21bc03dbe,c29,International Conference on Software Engineering,Pfam: The protein families database in 2021,"Abstract The Pfam database is a widely used resource for classifying protein sequences into families and domains. Since Pfam was last described in this journal, over 350 new families have been added in Pfam 33.1 and numerous improvements have been made to existing entries. To facilitate research on COVID-19, we have revised the Pfam entries that cover the SARS-CoV-2 proteome, and built new entries for regions that were not covered by Pfam. We have reintroduced Pfam-B which provides an automatically generated supplement to Pfam and contains 136 730 novel clusters of sequences that are not yet matched by a Pfam family. The new Pfam-B is based on a clustering by the MMseqs2 software. We have compared all of the regions in the RepeatsDB to those in Pfam and have started to use the results to build and refine Pfam repeat families. Pfam is freely available for browsing and download at http://pfam.xfam.org/.",poster,cp29
p2018,16b0744424f02e01fe2f01b3ea03e2862f1359fc,c33,International Conference on Agile Software Development,"Reference sequence (RefSeq) database at NCBI: current status, taxonomic expansion, and functional annotation","The RefSeq project at the National Center for Biotechnology Information (NCBI) maintains and curates a publicly available database of annotated genomic, transcript, and protein sequence records (http://www.ncbi.nlm.nih.gov/refseq/). The RefSeq project leverages the data submitted to the International Nucleotide Sequence Database Collaboration (INSDC) against a combination of computation, manual curation, and collaboration to produce a standard set of stable, non-redundant reference sequences. The RefSeq project augments these reference sequences with current knowledge including publications, functional features and informative nomenclature. The database currently represents sequences from more than 55 000 organisms (>4800 viruses, >40 000 prokaryotes and >10 000 eukaryotes; RefSeq release 71), ranging from a single record to complete genomes. This paper summarizes the current status of the viral, prokaryotic, and eukaryotic branches of the RefSeq project, reports on improvements to data access and details efforts to further expand the taxonomic representation of the collection. We also highlight diverse functional curation initiatives that support multiple uses of RefSeq data including taxonomic validation, genome annotation, comparative genomics, and clinical testing. We summarize our approach to utilizing available RNA-Seq and other data types in our manual curation process for vertebrate, plant, and other species, and describe a new direction for prokaryotic genomes and protein name management.",poster,cp33
p2019,d87ceda3042f781c341ac17109d1e94a717f5f60,c82,Workshop on Interdisciplinary Software Engineering Research,WordNet : an electronic lexical database,"Part 1 The lexical database: nouns in WordNet, George A. Miller modifiers in WordNet, Katherine J. Miller a semantic network of English verbs, Christiane Fellbaum design and implementation of the WordNet lexical database and searching software, Randee I. Tengi. Part 2: automated discovery of WordNet relations, Marti A. Hearst representing verb alterations in WordNet, Karen T. Kohl et al the formalization of WordNet by methods of relational concept analysis, Uta E. Priss. Part 3 Applications of WordNet: building semantic concordances, Shari Landes et al performance and confidence in a semantic annotation task, Christiane Fellbaum et al WordNet and class-based probabilities, Philip Resnik combining local context and WordNet similarity for word sense identification, Claudia Leacock and Martin Chodorow using WordNet for text retrieval, Ellen M. Voorhees lexical chains as representations of context for the detection and correction of malapropisms, Graeme Hirst and David St-Onge temporal indexing through lexical chaining, Reem Al-Halimi and Rick Kazman COLOR-X - using knowledge from WordNet for conceptual modelling, J.F.M. Burg and R.P. van de Riet knowledge processing on an extended WordNet, Sanda M. Harabagiu and Dan I Moldovan appendix - obtaining and using WordNet.",poster,cp82
p2020,68c03788224000794d5491ab459be0b2a2c38677,c93,Human Language Technology - The Baltic Perspectiv,WordNet: A Lexical Database for English,"Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].",fullPaper,cp93
p2021,3aca912f21d54b3931fa1fdfac0c199c557374a4,c88,Symposium on the Theory of Computing,GTDB-Tk: a toolkit to classify genomes with the Genome Taxonomy Database,Abstract Summary The Genome Taxonomy Database Toolkit (GTDB-Tk) provides objective taxonomic assignments for bacterial and archaeal genomes based on the GTDB. GTDB-Tk is computationally efficient and able to classify thousands of draft genomes in parallel. Here we demonstrate the accuracy of the GTDB-Tk taxonomic assignments by evaluating its performance on a phylogenetically diverse set of 10 156 bacterial and archaeal metagenome-assembled genomes. Availability and implementation GTDB-Tk is implemented in Python and licenced under the GNU General Public Licence v3.0. Source code and documentation are available at: https://github.com/ecogenomics/gtdbtk. Supplementary information Supplementary data are available at Bioinformatics online.,poster,cp88
p2022,80777d42513103bede188b2eebbdce7fb6f91390,c89,Conference on Uncertainty in Artificial Intelligence,HMDB 4.0: the human metabolome database for 2018,"Abstract The Human Metabolome Database or HMDB (www.hmdb.ca) is a web-enabled metabolomic database containing comprehensive information about human metabolites along with their biological roles, physiological concentrations, disease associations, chemical reactions, metabolic pathways, and reference spectra. First described in 2007, the HMDB is now considered the standard metabolomic resource for human metabolic studies. Over the past decade the HMDB has continued to grow and evolve in response to emerging needs for metabolomics researchers and continuing changes in web standards. This year's update, HMDB 4.0, represents the most significant upgrade to the database in its history. For instance, the number of fully annotated metabolites has increased by nearly threefold, the number of experimental spectra has grown by almost fourfold and the number of illustrated metabolic pathways has grown by a factor of almost 60. Significant improvements have also been made to the HMDB’s chemical taxonomy, chemical ontology, spectral viewing, and spectral/text searching tools. A great deal of brand new data has also been added to HMDB 4.0. This includes large quantities of predicted MS/MS and GC–MS reference spectral data as well as predicted (physiologically feasible) metabolite structures to facilitate novel metabolite identification. Additional information on metabolite-SNP interactions and the influence of drugs on metabolite levels (pharmacometabolomics) has also been added. Many other important improvements in the content, the interface, and the performance of the HMDB website have been made and these should greatly enhance its ease of use and its potential applications in nutrition, biochemistry, clinical chemistry, clinical genetics, medicine, and metabolomics science.",poster,cp89
p2023,bbe6e5fcc96e685db714d6aa11ffe6f49567c585,j48,Nature Human Behaviour,A global database of COVID-19 vaccinations,Abstract content,fullPaper,jv48
p2024,9dd54cd7ce4ebf2e52b762817c2688b56bb9e652,c113,International Conference on Image Analysis and Processing,2016 update of the PRIDE database and its related tools,"The PRoteomics IDEntifications (PRIDE) database is one of the world-leading data repositories of mass spectrometry (MS)-based proteomics data. Since the beginning of 2014, PRIDE Archive (http://www.ebi.ac.uk/pride/archive/) is the new PRIDE archival system, replacing the original PRIDE database. Here we summarize the developments in PRIDE resources and related tools since the previous update manuscript in the Database Issue in 2013. PRIDE Archive constitutes a complete redevelopment of the original PRIDE, comprising a new storage backend, data submission system and web interface, among other components. PRIDE Archive supports the most-widely used PSI (Proteomics Standards Initiative) data standard formats (mzML and mzIdentML) and implements the data requirements and guidelines of the ProteomeXchange Consortium. The wide adoption of ProteomeXchange within the community has triggered an unprecedented increase in the number of submitted data sets (around 150 data sets per month). We outline some statistics on the current PRIDE Archive data contents. We also report on the status of the PRIDE related stand-alone tools: PRIDE Inspector, PRIDE Converter 2 and the ProteomeXchange submission tool. Finally, we will give a brief update on the resources under development 'PRIDE Cluster' and 'PRIDE Proteomes', which provide a complementary view and quality-scored information of the peptide and protein identification data available in PRIDE Archive.",poster,cp113
p2025,c6b3ca4f939e36a9679a70e14ce8b1bbbc5618f3,c73,Workshop on Algorithms in Bioinformatics,Labeled Faces in the Wild: A Database forStudying Face Recognition in Unconstrained Environments,"Most face databases have been created under controlled conditions to facilitate the study of specific parameters on the face recognition problem. These parameters include such variables as position, pose, lighting, background, camera quality, and gender. While there are many applications for face recognition technology in which one can control the parameters of image acquisition, there are also many applications in which the practitioner has little or no control over such parameters. This database, Labeled Faces in the Wild, is provided as an aid in studying the latter, unconstrained, recognition problem. The database contains labeled face photographs spanning the range of conditions typically encountered in everyday life. The database exhibits “natural” variability in factors such as pose, lighting, race, accessories, occlusions, and background. In addition to describing the details of the database, we provide specific experimental paradigms for which the database is suitable. This is done in an effort to make research performed with the database as consistent and comparable as possible. We provide baseline results, including results of a state of the art face recognition system combined with a face alignment system. To facilitate experimentation on the database, we provide several parallel databases, including an aligned version.",poster,cp73
p2026,7b1d8dfb9e6260685d9fbb8c41bfc0a35710fe41,c104,IEEE International Conference on Multimedia and Expo,CARD 2020: antibiotic resistome surveillance with the comprehensive antibiotic resistance database,"Abstract The Comprehensive Antibiotic Resistance Database (CARD; https://card.mcmaster.ca) is a curated resource providing reference DNA and protein sequences, detection models and bioinformatics tools on the molecular basis of bacterial antimicrobial resistance (AMR). CARD focuses on providing high-quality reference data and molecular sequences within a controlled vocabulary, the Antibiotic Resistance Ontology (ARO), designed by the CARD biocuration team to integrate with software development efforts for resistome analysis and prediction, such as CARD’s Resistance Gene Identifier (RGI) software. Since 2017, CARD has expanded through extensive curation of reference sequences, revision of the ontological structure, curation of over 500 new AMR detection models, development of a new classification paradigm and expansion of analytical tools. Most notably, a new Resistomes & Variants module provides analysis and statistical summary of in silico predicted resistance variants from 82 pathogens and over 100 000 genomes. By adding these resistance variants to CARD, we are able to summarize predicted resistance using the information included in CARD, identify trends in AMR mobility and determine previously undescribed and novel resistance variants. Here, we describe updates and recent expansions to CARD and its biocuration process, including new resources for community biocuration of AMR molecular reference data.",poster,cp104
p2027,e6ce8255f48e3736f0a5fa0d85fb43c700d4f743,j314,Acta Crystallographica Section B Structural Science,The Cambridge Structural Database: a quarter of a million crystal structures and rising.,"The Cambridge Structural Database (CSD) now contains data for more than a quarter of a million small-molecule crystal structures. The information content of the CSD, together with methods for data acquisition, processing and validation, are summarized, with particular emphasis on the chemical information added by CSD editors. Nearly 80% of new structural data arrives electronically, mostly in CIF format, and the CCDC acts as the official crystal structure data depository for 51 major journals. The CCDC now maintains both a CIF archive (more than 73,000 CIFs dating from 1996), as well as the distributed binary CSD archive; the availability of data in both archives is discussed. A statistical survey of the CSD is also presented and projections concerning future accession rates indicate that the CSD will contain at least 500,000 crystal structures by the year 2010.",fullPaper,jv314
p2028,41abf43dc718e271299457bce65bccfe3feeb9d6,j344,Applied and Environmental Microbiology,"Greengenes, a Chimera-Checked 16S rRNA Gene Database and Workbench Compatible with ARB","ABSTRACT A 16S rRNA gene database (http://greengenes.lbl.gov ) addresses limitations of public repositories by providing chimera screening, standard alignment, and taxonomic classification using multiple published taxonomies. It was found that there is incongruent taxonomic nomenclature among curators even at the phylum level. Putative chimeras were identified in 3% of environmental sequences and in 0.2% of records derived from isolates. Environmental sequences were classified into 100 phylum-level lineages in the Archaea and Bacteria.",fullPaper,jv344
p2029,cdad2f8ca559f425ab7fa402535354a86b0a370a,c97,Interspeech,CDD/SPARCLE: the conserved domain database in 2020,"As NLM's Conserved Domain Database (CDD) enters its 20th year of operations as a publicly available resource, CDD curation staff continues to develop hierarchical classifications of widely distributed protein domain families, and to record conserved sites associated with molecular function, so that they can be mapped onto user queries in support of hypothesis-driven biomolecular research. CDD offers both an archive of pre-computed domain annotations as well as live search services for both single protein or nucleotide queries and larger sets of protein query sequences. CDD staff has continued to characterize protein families via conserved domain architectures and has built up a significant corpus of curated domain architectures in support of naming bacterial proteins in RefSeq. These architecture definitions are available via SPARCLE, the Subfamily Protein Architecture Labeling Engine. CDD can be accessed at https://www.ncbi.nlm.nih.gov/Structure/cdd/cdd.shtml.",poster,cp97
p2030,1976c9eeccc7115d18a04f1e7fb5145db6b96002,c33,International Conference on Agile Software Development,Freebase: a collaboratively created graph database for structuring human knowledge,"Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Freebase currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.",poster,cp33
p2031,76eb8e5688ee2951e5f04fb14956abf93a890149,c2,International Symposium on Intelligent Data Analysis,The Carbohydrate-Active EnZymes database (CAZy): an expert resource for Glycogenomics,"The Carbohydrate-Active Enzyme (CAZy) database is a knowledge-based resource specialized in the enzymes that build and breakdown complex carbohydrates and glycoconjugates. As of September 2008, the database describes the present knowledge on 113 glycoside hydrolase, 91 glycosyltransferase, 19 polysaccharide lyase, 15 carbohydrate esterase and 52 carbohydrate-binding module families. These families are created based on experimentally characterized proteins and are populated by sequences from public databases with significant similarity. Protein biochemical information is continuously curated based on the available literature and structural information. Over 6400 proteins have assigned EC numbers and 700 proteins have a PDB structure. The classification (i) reflects the structural features of these enzymes better than their sole substrate specificity, (ii) helps to reveal the evolutionary relationships between these enzymes and (iii) provides a convenient framework to understand mechanistic properties. This resource has been available for over 10 years to the scientific community, contributing to information dissemination and providing a transversal nomenclature to glycobiologists. More recently, this resource has been used to improve the quality of functional predictions of a number genome projects by providing expert annotation. The CAZy resource resides at URL: http://www.cazy.org/.",poster,cp2
p2032,c07eca9862e0144aa4c0c29c7978caa6eff60e2f,c88,Symposium on the Theory of Computing,The Ribosomal Database Project: improved alignments and new tools for rRNA analysis,"The Ribosomal Database Project (RDP) provides researchers with quality-controlled bacterial and archaeal small subunit rRNA alignments and analysis tools. An improved alignment strategy uses the Infernal secondary structure aware aligner to provide a more consistent higher quality alignment and faster processing of user sequences. Substantial new analysis features include a new Pyrosequencing Pipeline that provides tools to support analysis of ultra high-throughput rRNA sequencing data. This pipeline offers a collection of tools that automate the data processing and simplify the computationally intensive analysis of large sequencing libraries. In addition, a new Taxomatic visualization tool allows rapid visualization of taxonomic inconsistencies and suggests corrections, and a new class Assignment Generator provides instructors with a lesson plan and individualized teaching materials. Details about RDP data and analytical functions can be found at http://rdp.cme.msu.edu/.",poster,cp88
p2033,80e394ee3e1834091596e8b55c9ad9bf11456e09,j5,Genome Biology,"DAVID: Database for Annotation, Visualization, and Integrated Discovery",Abstract content,fullPaper,jv5
p2034,716000409a3a2e2c75801b3d58b9b17b68eeaef7,c91,Workshop on Algorithms and Models for the Web-Graph,An improved method of constructing a database of monthly climate observations and associated high‐resolution grids,"A database of monthly climate observations from meteorological stations is constructed. The database includes six climate elements and extends over the global land surface. The database is checked for inhomogeneities in the station records using an automated method that refines previous methods by using incomplete and partially overlapping records and by detecting inhomogeneities with opposite signs in different seasons. The method includes the development of reference series using neighbouring stations. Information from different sources about a single station may be combined, even without an overlapping period, using a reference series. Thus, a longer station record may be obtained and fragmentation of records reduced. The reference series also enables 1961–90 normals to be calculated for a larger proportion of stations.",poster,cp91
p2035,6a074a3fa856e86b2e6bc60e83d66cc488090ae9,j345,The International Journal of Life Cycle Assessment,The ecoinvent database version 3 (part I): overview and methodology,Abstract content,fullPaper,jv345
p2036,dc52d1ede1b90bf9d296bc5b34c9310b7eaa99a2,c69,International Conference on Parallel Processing,The mnist database of handwritten digits,Disclosed is an improved articulated bar flail having shearing edges for efficiently shredding materials. An improved shredder cylinder is disclosed with a plurality of these flails circumferentially spaced and pivotally attached to the periphery of a rotatable shaft. Also disclosed is an improved shredder apparatus which has a pair of these shredder cylinders mounted to rotate about spaced parallel axes which cooperates with a conveyer apparatus which has a pair of inclined converging conveyer belts with one of the belts mounted to move with respect to the other belt to allow the transport of articles of various sizes therethrough.,poster,cp69
p2037,317325439a0ce543d7629848a35adea04b6e7d12,c99,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,The InterPro protein families and domains database: 20 years on,"Abstract The InterPro database (https://www.ebi.ac.uk/interpro/) provides an integrative classification of protein sequences into families, and identifies functionally important domains and conserved sites. InterProScan is the underlying software that allows protein and nucleic acid sequences to be searched against InterPro's signatures. Signatures are predictive models which describe protein families, domains or sites, and are provided by multiple databases. InterPro combines signatures representing equivalent families, domains or sites, and provides additional information such as descriptions, literature references and Gene Ontology (GO) terms, to produce a comprehensive resource for protein classification. Founded in 1999, InterPro has become one of the most widely used resources for protein family annotation. Here, we report the status of InterPro (version 81.0) in its 20th year of operation, and its associated software, including updates to database content, the release of a new website and REST API, and performance improvements in InterProScan.",poster,cp99
p2038,8c1a1e761b715b23668b4f850e2bcc958fa21ad2,j206,Analytical Chemistry,Empirical statistical model to estimate the accuracy of peptide identifications made by MS/MS and database search.,"We present a statistical model to estimate the accuracy of peptide assignments to tandem mass (MS/MS) spectra made by database search applications such as SEQUEST. Employing the expectation maximization algorithm, the analysis learns to distinguish correct from incorrect database search results, computing probabilities that peptide assignments to spectra are correct based upon database search scores and the number of tryptic termini of peptides. Using SEQUEST search results for spectra generated from a sample of known protein components, we demonstrate that the computed probabilities are accurate and have high power to discriminate between correctly and incorrectly assigned peptides. This analysis makes it possible to filter large volumes of MS/MS database search results with predictable false identification error rates and can serve as a common standard by which the results of different research groups are compared.",fullPaper,jv206
p2039,d7c78b7071ea150346320e5b43a03824263e0fa9,c74,IEEE International Conference on Tools with Artificial Intelligence,Ribosomal Database Project: data and tools for high throughput rRNA analysis,"Ribosomal Database Project (RDP; http://rdp.cme.msu.edu/) provides the research community with aligned and annotated rRNA gene sequence data, along with tools to allow researchers to analyze their own rRNA gene sequences in the RDP framework. RDP data and tools are utilized in fields as diverse as human health, microbial ecology, environmental microbiology, nucleic acid chemistry, taxonomy and phylogenetics. In addition to aligned and annotated collections of bacterial and archaeal small subunit rRNA genes, RDP now includes a collection of fungal large subunit rRNA genes. RDP tools, including Classifier and Aligner, have been updated to work with this new fungal collection. The use of high-throughput sequencing to characterize environmental microbial populations has exploded in the past several years, and as sequence technologies have improved, the sizes of environmental datasets have increased. With release 11, RDP is providing an expanded set of tools to facilitate analysis of high-throughput data, including both single-stranded and paired-end reads. In addition, most tools are now available as open source packages for download and local use by researchers with high-volume needs or who would like to develop custom analysis pipelines.",poster,cp74
p2040,633f318876c41fed36b3905b8af5fdc27f734615,j346,Cell Systems,The Molecular Signatures Database (MSigDB) hallmark gene set collection.,"The Molecular Signatures Database (MSigDB) is one of the most widely used and comprehensive databases of gene sets for performing gene set enrichment analysis. Since its creation, MSigDB has grown beyond its roots in metabolic disease and cancer to include >10,000 gene sets. These better represent a wider range of biological processes and diseases, but the utility of the database is reduced by increased redundancy across, and heterogeneity within, gene sets. To address this challenge, here we use a combination of automated approaches and expert curation to develop a collection of ""hallmark"" gene sets as part of MSigDB. Each hallmark in this collection consists of a ""refined"" gene set, derived from multiple ""founder"" sets, that conveys a specific biological state or process and displays coherent expression. The hallmarks effectively summarize most of the relevant information of the original founder sets and, by reducing both variation and redundancy, provide more refined and concise inputs for gene set enrichment analysis.",fullPaper,jv346
p2041,ceee6447b291f8052a28c9eb00ca360d6f39f9b1,c41,Software Product Lines Conference,"The STRING database in 2011: functional interaction networks of proteins, globally integrated and scored","An essential prerequisite for any systems-level understanding of cellular functions is to correctly uncover and annotate all functional interactions among proteins in the cell. Toward this goal, remarkable progress has been made in recent years, both in terms of experimental measurements and computational prediction techniques. However, public efforts to collect and present protein interaction information have struggled to keep up with the pace of interaction discovery, partly because protein–protein interaction information can be error-prone and require considerable effort to annotate. Here, we present an update on the online database resource Search Tool for the Retrieval of Interacting Genes (STRING); it provides uniquely comprehensive coverage and ease of access to both experimental as well as predicted interaction information. Interactions in STRING are provided with a confidence score, and accessory information such as protein domains and 3D structures is made available, all within a stable and consistent identifier space. New features in STRING include an interactive network viewer that can cluster networks on demand, updated on-screen previews of structural information including homology models, extensive data updates and strongly improved connectivity and integration with third-party resources. Version 9.0 of STRING covers more than 1100 completely sequenced organisms; the resource can be reached at http://string-db.org.",poster,cp41
p2042,9667f8264745b626c6173b1310e2ff0298b09cfc,c54,International Workshop on Agent-Oriented Software Engineering,Learning Deep Features for Scene Recognition using Places Database,"Scene recognition is one of the hallmark tasks of computer vision, allowing definition of a context for object recognition. Whereas the tremendous recent progress in object recognition tasks is due to the availability of large datasets like ImageNet and the rise of Convolutional Neural Networks (CNNs) for learning high-level features, performance at scene recognition has not attained the same level of success. This may be because current deep features trained from ImageNet are not competitive enough for such tasks. Here, we introduce a new scene-centric database called Places with over 7 million labeled pictures of scenes. We propose new methods to compare the density and diversity of image datasets and show that Places is as dense as other scene datasets and has more diversity. Using CNN, we learn deep features for scene recognition tasks, and establish new state-of-the-art results on several scene-centric datasets. A visualization of the CNN layers' responses allows us to show differences in the internal representations of object-centric and scene-centric networks.",poster,cp54
p2043,5ef2cf7b7aa6f7e44488d5db5409ef7f76b9ef9a,j173,Journal of Molecular Biology,SCOP: a structural classification of proteins database for the investigation of sequences and structures.,Abstract content,fullPaper,jv173
p2044,f80a6ab4b0cfae0d00747f0f41f3e643f22f33ee,c4,Annual Conference on Genetic and Evolutionary Computation,Molecular signatures database (MSigDB) 3.0,"MOTIVATION
Well-annotated gene sets representing the universe of the biological processes are critical for meaningful and insightful interpretation of large-scale genomic data. The Molecular Signatures Database (MSigDB) is one of the most widely used repositories of such sets.


RESULTS
We report the availability of a new version of the database, MSigDB 3.0, with over 6700 gene sets, a complete revision of the collection of canonical pathways and experimental signatures from publications, enhanced annotations and upgrades to the web site.


AVAILABILITY AND IMPLEMENTATION
MSigDB is freely available for non-commercial use at http://www.broadinstitute.org/msigdb.",poster,cp4
p2045,298d799da82395a64a3bda38ef9d2a4646828ccb,c88,Symposium on the Theory of Computing,A fast quantum mechanical algorithm for database search,"were proposed in the early 1980’s [Benioff80] and shown to be at least as powerful as classical computers an important but not surprising result, since classical computers, at the deepest level, ultimately follow the laws of quantum mechanics. The description of quantum mechanical computers was formalized in the late 80’s and early 90’s [Deutsch85][BB92] [BV93] [Yao93] and they were shown to be more powerful than classical computers on various specialized problems. In early 1994, [Shor94] demonstrated that a quantum mechanical computer could efficiently solve a well-known problem for which there was no known efficient algorithm using classical computers. This is the problem of integer factorization, i.e. testing whether or not a given integer, N, is prime, in a time which is a finite power of o (logN) . ----------------------------------------------",fullPaper,cp88
p2046,11f647b95a7c9a94c346cd8dc53987105cb0f7c1,c1,Technical Symposium on Computer Science Education,dbSNP: the NCBI database of genetic variation,"In response to a need for a general catalog of genome variation to address the large-scale sampling designs required by association studies, gene mapping and evolutionary biology, the National Center for Biotechnology Information (NCBI) has established the dbSNP database [S.T.Sherry, M.Ward and K. Sirotkin (1999) Genome Res., 9, 677-679]. Submissions to dbSNP will be integrated with other sources of information at NCBI such as GenBank, PubMed, LocusLink and the Human Genome Project data. The complete contents of dbSNP are available to the public at website: http://www.ncbi.nlm.nih.gov/SNP. The complete contents of dbSNP can also be downloaded in multiple formats via anonymous FTP at ftp://ncbi.nlm.nih.gov/snp/.",poster,cp1
p2047,b7599c8ba88e7c93edbce57df513152e8f5693e7,j85,BMC Bioinformatics,The COG database: an updated version includes eukaryotes,Abstract content,fullPaper,jv85
p2048,4bd970a37c59c97804ff93cbb2c108e081de3a37,c9,Pacific Symposium on Biocomputing,Introduction to WordNet: An On-line Lexical Database,"Standard alphabetical procedures for organizing lexical information put together words that are spelled alike and scatter words with similar or related meanings haphazardly through the list. Unfortunately, there is no obvious alternative, no other simple way for lexicographers to keep track of what has been done or for readers to find the word they are looking for. But a frequent objection to this solution is that finding things on an alphabetical list can be tedious and time-consuming. Many people who would like to refer to a dictionary decide not to bother with it because finding the information would interrupt their work and break their train of thought.",poster,cp9
p2049,d6a5a1e8f56260608d2f7651a2f6aac6a041b57a,c28,International Conference on Collaboration Technologies and Systems,The Pfam protein families database,"Pfam is a widely used database of protein families, currently containing more than 13 000 manually curated protein families as of release 26.0. Pfam is available via servers in the UK (http://pfam.sanger.ac.uk/), the USA (http://pfam.janelia.org/) and Sweden (http://pfam.sbc.su.se/). Here, we report on changes that have occurred since our 2010 NAR paper (release 24.0). Over the last 2 years, we have generated 1840 new families and increased coverage of the UniProt Knowledgebase (UniProtKB) to nearly 80%. Notably, we have taken the step of opening up the annotation of our families to the Wikipedia community, by linking Pfam families to relevant Wikipedia pages and encouraging the Pfam and Wikipedia communities to improve and expand those pages. We continue to improve the Pfam website and add new visualizations, such as the ‘sunburst’ representation of taxonomic distribution of families. In this work we additionally address two topics that will be of particular interest to the Pfam community. First, we explain the definition and use of family-specific, manually curated gathering thresholds. Second, we discuss some of the features of domains of unknown function (also known as DUFs), which constitute a rapidly growing class of families within Pfam.",poster,cp28
p2050,bb967168ead7a14adcb0121dcf24a930d1a383b3,c42,Joint Conference of International Workshop on Software Measurement and International Conference on Software Process and Product Measurement,The HITRAN 2008 molecular spectroscopic database,Abstract content,poster,cp42
p2051,d364903a626ad70e6ce057209d9b7e004dafd4be,c54,International Workshop on Agent-Oriented Software Engineering,"PlantCARE, a database of plant cis-acting regulatory elements and a portal to tools for in silico analysis of promoter sequences","PlantCARE is a database of plant cis-acting regulatory elements, enhancers and repressors. Regulatory elements are represented by positional matrices, consensus sequences and individual sites on particular promoter sequences. Links to the EMBL, TRANSFAC and MEDLINE databases are provided when available. Data about the transcription sites are extracted mainly from the literature, supplemented with an increasing number of in silico predicted data. Apart from a general description for specific transcription factor sites, levels of confidence for the experimental evidence, functional information and the position on the promoter are given as well. New features have been implemented to search for plant cis-acting regulatory elements in a query sequence. Furthermore, links are now provided to a new clustering and motif search method to investigate clusters of co-expressed genes. New regulatory elements can be sent automatically and will be added to the database after curation. The PlantCARE relational database is available via the World Wide Web at http://sphinx.rug.ac.be:8080/PlantCARE/.",poster,cp54
p2052,90bc0ca3feebe0215079cf575b90017170a0089f,c20,Research Conference of the South African Institute of Computer Scientists and Information Technologists,CDD: NCBI's conserved domain database,"NCBI's CDD, the Conserved Domain Database, enters its 15th year as a public resource for the annotation of proteins with the location of conserved domain footprints. Going forward, we strive to improve the coverage and consistency of domain annotation provided by CDD. We maintain a live search system as well as an archive of pre-computed domain annotation for sequences tracked in NCBI's Entrez protein database, which can be retrieved for single sequences or in bulk. We also maintain import procedures so that CDD contains domain models and domain definitions provided by several collections available in the public domain, as well as those produced by an in-house curation effort. The curation effort aims at increasing coverage and providing finer-grained classifications of common protein domains, for which a wealth of functional and structural data has become available. CDD curation generates alignment models of representative sequence fragments, which are in agreement with domain boundaries as observed in protein 3D structure, and which model the structurally conserved cores of domain families as well as annotate conserved features. CDD can be accessed at http://www.ncbi.nlm.nih.gov/Structure/cdd/cdd.shtml.",poster,cp20
p2053,8b3b8848a311c501e704c45c6d50430ab7068956,c94,Vision,HMDB: A large video database for human motion recognition,"With nearly one billion online videos viewed everyday, an emerging new frontier in computer vision research is recognition and search in video. While much effort has been devoted to the collection and annotation of large scalable static image datasets containing thousands of image categories, human action datasets lag far behind. Current action recognition databases contain on the order of ten different action categories collected under fairly controlled conditions. State-of-the-art performance on these datasets is now near ceiling and thus there is a need for the design and creation of new benchmarks. To address this issue we collected the largest action video database to-date with 51 action categories, which in total contain around 7,000 manually annotated clips extracted from a variety of sources ranging from digitized movies to YouTube. We use this database to evaluate the performance of two representative computer vision systems for action recognition and explore the robustness of these methods under various conditions such as camera motion, viewpoint, video quality and occlusion.",fullPaper,cp94
p2054,82524ddee00fa0895dfca43995a7ec8bdb16f0d5,c106,Chinese Conference on Biometric Recognition,Fundamentals of Database Systems,"From the Publisher: 
Fundamentals of Database Systems combines clear explanations of theory and design, broad coverage of models and real systems, and excellent examples with up-to-date introductions to modern database technologies. This edition is completely revised and updated, and reflects the latest trends in technological and application development. Professors Elmasri and Navathe focus on the relational model and include coverage of recent object-oriented developments. They also address advanced modeling and system enhancements in the areas of active databases, temporal and spatial databases, and multimedia information systems. This edition also surveys the latest application areas of data warehousing, data mining, web databases, digital libraries, GIS, and genome databases. New to the Third Edition 
Reorganized material on data modeling to clearly separate entity relationship modeling, extended entity relationship modeling, and object-oriented modeling Expanded coverage of the object-oriented and object/relational approach to data management, including ODMG and SQL3 Uses examples from real database systems including OracleTM and Microsoft AccessAE Includes discussion of decision support applications of data warehousing and data mining, as well as emerging technologies of web databases, multimedia, and mobile databases Covers advanced modeling in the areas of active, temporal, and spatial databases Provides coverage of issues of physical database tuning Discusses current database application areas of GIS, genome, and digital libraries",poster,cp106
p2055,62f5ffb09a4c9543509c38f005b9c6eb308c6974,c112,Very Large Data Bases Conference,The COG database: a tool for genome-scale analysis of protein functions and evolution,"Rational classification of proteins encoded in sequenced genomes is critical for making the genome sequences maximally useful for functional and evolutionary studies. The database of Clusters of Orthologous Groups of proteins (COGs) is an attempt on a phylogenetic classification of the proteins encoded in 21 complete genomes of bacteria, archaea and eukaryotes (http://www. ncbi.nlm. nih.gov/COG). The COGs were constructed by applying the criterion of consistency of genome-specific best hits to the results of an exhaustive comparison of all protein sequences from these genomes. The database comprises 2091 COGs that include 56-83% of the gene products from each of the complete bacterial and archaeal genomes and approximately 35% of those from the yeast Saccharomyces cerevisiae genome. The COG database is accompanied by the COGNITOR program that is used to fit new proteins into the COGs and can be applied to functional and phylogenetic annotation of newly sequenced genomes.",poster,cp112
p2056,3b073a5e7de5513705a7e2a7b1c88d3acbeed82c,j347,Journal of Cheminformatics,TCMSP: a database of systems pharmacology for drug discovery from herbal medicines,Abstract content,fullPaper,jv347
p2057,073df13736ed5407374af2f397bd65560f156dd2,j348,Mobile DNA,"Repbase Update, a database of repetitive elements in eukaryotic genomes",Abstract content,fullPaper,jv348
p2058,092c275005ae49dc1303214f6d02d134457c7053,j349,International Journal of Computer Vision,LabelMe: A Database and Web-Based Tool for Image Annotation,Abstract content,fullPaper,jv349
p2059,b307d55ba07058d6183991d2d2a81b340d558186,c80,International Conference on Learning Representations,"NCBI reference sequences (RefSeq): a curated non-redundant sequence database of genomes, transcripts and proteins","The National Center for Biotechnology Information (NCBI) Reference Sequence (RefSeq) database (http://www.ncbi.nlm.nih.gov/RefSeq/) provides a non-redundant collection of sequences representing genomic data, transcripts and proteins. Although the goal is to provide a comprehensive dataset representing the complete sequence information for any given species, the database pragmatically includes sequence data that are currently publicly available in the archival databases. The database incorporates data from over 2400 organisms and includes over one million proteins representing significant taxonomic diversity spanning prokaryotes, eukaryotes and viruses. Nucleotide and protein sequences are explicitly linked, and the sequences are linked to other resources including the NCBI Map Viewer and Gene. Sequences are annotated to include coding regions, conserved domains, variation, references, names, database cross-references, and other features using a combined approach of collaboration and other input from the scientific community, automated annotation, propagation from GenBank and curation by NCBI staff.",poster,cp80
p2060,cb56121bc38e0f4b44bcb5296a12038626152e96,c24,Decision Support Systems,CARD 2017: expansion and model-centric curation of the comprehensive antibiotic resistance database,"The Comprehensive Antibiotic Resistance Database (CARD; http://arpcard.mcmaster.ca) is a manually curated resource containing high quality reference data on the molecular basis of antimicrobial resistance (AMR), with an emphasis on the genes, proteins and mutations involved in AMR. CARD is ontologically structured, model centric, and spans the breadth of AMR drug classes and resistance mechanisms, including intrinsic, mutation-driven and acquired resistance. It is built upon the Antibiotic Resistance Ontology (ARO), a custom built, interconnected and hierarchical controlled vocabulary allowing advanced data sharing and organization. Its design allows the development of novel genome analysis tools, such as the Resistance Gene Identifier (RGI) for resistome prediction from raw genome sequence. Recent improvements include extensive curation of additional reference sequences and mutations, development of a unique Model Ontology and accompanying AMR detection models to power sequence analysis, new visualization tools, and expansion of the RGI for detection of emergent AMR threats. CARD curation is updated monthly based on an interplay of manual literature curation, computational text mining, and genome analysis.",poster,cp24
p2061,1f53996347086be3bd3a32da0976ba2db7687988,c7,European Conference on Modelling and Simulation,miRDB: an online database for prediction of functional microRNA targets,"Abstract MicroRNAs (miRNAs) are small noncoding RNAs that act as master regulators in many biological processes. miRNAs function mainly by downregulating the expression of their gene targets. Thus, accurate prediction of miRNA targets is critical for characterization of miRNA functions. To this end, we have developed an online database, miRDB, for miRNA target prediction and functional annotations. Recently, we have performed major updates for miRDB. Specifically, by employing an improved algorithm for miRNA target prediction, we now present updated transcriptome-wide target prediction data in miRDB, including 3.5 million predicted targets regulated by 7000 miRNAs in five species. Further, we have implemented the new prediction algorithm into a web server, allowing custom target prediction with user-provided sequences. Another new database feature is the prediction of cell-specific miRNA targets. miRDB now hosts the expression profiles of over 1000 cell lines and presents target prediction data that are tailored for specific cell models. At last, a new web query interface has been added to miRDB for prediction of miRNA functions by integrative analysis of target prediction and Gene Ontology data. All data in miRDB are freely accessible at http://mirdb.org.",poster,cp7
p2062,0e466ea033b982519f351022304dccb64a46b93c,c111,International Society for Music Information Retrieval Conference,IPD-IMGT/HLA Database,"Abstract The IPD-IMGT/HLA Database, http://www.ebi.ac.uk/ipd/imgt/hla/, currently contains over 25 000 allele sequence for 45 genes, which are located within the Major Histocompatibility Complex (MHC) of the human genome. This region is the most polymorphic region of the human genome, and the levels of polymorphism seen exceed most other genes. Some of the genes have several thousand variants and are now termed hyperpolymorphic, rather than just simply polymorphic. The IPD-IMGT/HLA Database has provided a stable, highly accessible, user-friendly repository for this information, providing the scientific and medical community access to the many variant sequences of this gene system, that are critical for the successful outcome of transplantation. The number of currently known variants, and dramatic increase in the number of new variants being identified has necessitated a dedicated resource with custom tools for curation and publication. The challenge for the database is to continue to provide a highly curated database of sequence variants, while supporting the increased number of submissions and complexity of sequences. In order to do this, traditional methods of accessing and presenting data will be challenged, and new methods will need to be utilized to keep pace with new discoveries.",poster,cp111
p2063,908091b4a8757c3b2f7d9cfa2c4f616ee12c5157,c111,International Society for Music Information Retrieval Conference,SUN database: Large-scale scene recognition from abbey to zoo,"Scene categorization is a fundamental problem in computer vision. However, scene understanding research has been constrained by the limited scope of currently-used databases which do not capture the full variety of scene categories. Whereas standard databases for object categorization contain hundreds of different classes of objects, the largest available dataset of scene categories contains only 15 classes. In this paper we propose the extensive Scene UNderstanding (SUN) database that contains 899 categories and 130,519 images. We use 397 well-sampled categories to evaluate numerous state-of-the-art algorithms for scene recognition and establish new bounds of performance. We measure human scene classification performance on the SUN database and compare this with computational methods. Additionally, we study a finer-grained scene representation to detect scenes embedded inside of larger scenes.",poster,cp111
p2064,6d96f946aaabc734af7fe3fc4454cf8547fcd5ed,c6,Americas Conference on Information Systems,The AR face database,Abstract content,poster,cp6
p2065,d71af418eeb9f5a68062929bae12af74773ffcb2,c16,Knowledge Discovery and Data Mining,The ChEMBL database in 2017,"ChEMBL is an open large-scale bioactivity database (https://www.ebi.ac.uk/chembl), previously described in the 2012 and 2014 Nucleic Acids Research Database Issues. Since then, alongside the continued extraction of data from the medicinal chemistry literature, new sources of bioactivity data have also been added to the database. These include: deposited data sets from neglected disease screening; crop protection data; drug metabolism and disposition data and bioactivity data from patents. A number of improvements and new features have also been incorporated. These include the annotation of assays and targets using ontologies, the inclusion of targets and indications for clinical candidates, addition of metabolic pathways for drugs and calculation of structural alerts. The ChEMBL data can be accessed via a web-interface, RDF distribution, data downloads and RESTful web-services.",poster,cp16
p2066,fc90bd571818dacf9f19d4198c6e00d948632d2b,c97,Interspeech,Human Protein Reference Database—2009 update,"Human Protein Reference Database (HPRD—http://www.hprd.org/), initially described in 2003, is a database of curated proteomic information pertaining to human proteins. We have recently added a number of new features in HPRD. These include PhosphoMotif Finder, which allows users to find the presence of over 320 experimentally verified phosphorylation motifs in proteins of interest. Another new feature is a protein distributed annotation system—Human Proteinpedia (http://www.humanproteinpedia.org/)—through which laboratories can submit their data, which is mapped onto protein entries in HPRD. Over 75 laboratories involved in proteomics research have already participated in this effort by submitting data for over 15 000 human proteins. The submitted data includes mass spectrometry and protein microarray-derived data, among other data types. Finally, HPRD is also linked to a compendium of human signaling pathways developed by our group, NetPath (http://www.netpath.org/), which currently contains annotations for several cancer and immune signaling pathways. Since the last update, more than 5500 new protein sequences have been added, making HPRD a comprehensive resource for studying the human proteome.",poster,cp97
p2067,5a2892f91addeea2f4600d28b23e684be32f5b2c,j350,IEEE Transactions on Affective Computing,DEAP: A Database for Emotion Analysis ;Using Physiological Signals,"We present a multimodal data set for the analysis of human affective states. The electroencephalogram (EEG) and peripheral physiological signals of 32 participants were recorded as each watched 40 one-minute long excerpts of music videos. Participants rated each video in terms of the levels of arousal, valence, like/dislike, dominance, and familiarity. For 22 of the 32 participants, frontal face video was also recorded. A novel method for stimuli selection is proposed using retrieval by affective tags from the last.fm website, video highlight detection, and an online assessment tool. An extensive analysis of the participants' ratings during the experiment is presented. Correlates between the EEG signal frequencies and the participants' ratings are investigated. Methods and results are presented for single-trial classification of arousal, valence, and like/dislike ratings using the modalities of EEG, peripheral physiological signals, and multimedia content analysis. Finally, decision fusion of the classification results from different modalities is performed. The data set is made publicly available and we encourage other researchers to use it for testing their own affective state estimation methods.",fullPaper,jv350
p2068,bc744742f1644c9cab6b9535ab0bd6f2eed320bb,c2,International Symposium on Intelligent Data Analysis,CDD: a Conserved Domain Database for the functional annotation of proteins,"NCBI’s Conserved Domain Database (CDD) is a resource for the annotation of protein sequences with the location of conserved domain footprints, and functional sites inferred from these footprints. CDD includes manually curated domain models that make use of protein 3D structure to refine domain models and provide insights into sequence/structure/function relationships. Manually curated models are organized hierarchically if they describe domain families that are clearly related by common descent. As CDD also imports domain family models from a variety of external sources, it is a partially redundant collection. To simplify protein annotation, redundant models and models describing homologous families are clustered into superfamilies. By default, domain footprints are annotated with the corresponding superfamily designation, on top of which specific annotation may indicate high-confidence assignment of family membership. Pre-computed domain annotation is available for proteins in the Entrez/Protein dataset, and a novel interface, Batch CD-Search, allows the computation and download of annotation for large sets of protein queries. CDD can be accessed via http://www.ncbi.nlm.nih.gov/Structure/cdd/cdd.shtml.",poster,cp2
p2069,3d1ba71a1c3b7302e12ab3d07bf4a8451db5aad0,c105,Biometrics and Identity Management,Plant cis-acting regulatory DNA elements (PLACE) database: 1999,"PLACE (http://www.dna.affrc.go.jp/htdocs/PLACE/) is a database of nucleotide sequence motifs found in plant cis-acting regulatory DNA elements. Motifs were extracted from previously published reports on genes in vascular plants. In addition to the motifs originally reported, their variations in other genes or in other plant species in later reports are also compiled. Documents for each motif in the PLACE database contains, in addition to a motif sequence, a brief definition and description of each motif, and relevant literature with PubMed ID numbers and GenBank accession numbers where available. Users can search their query sequences for cis-elements using the Signal Scan program at our web site. The results will be reported in one of the three forms. Clicking the PLACE accession numbers in the result report will open the pertinent motif document. Clicking the PubMed or GenBank accession number in the document will allow users to access to these databases, and to read the of the literature or the annotation in the DNA database. This report summarizes the present status of this database and available tools.",poster,cp105
p2070,61533dd9e41f20e2f5deaf22afb04c94b4071eac,j351,Cytogenetic and Genome Research,"Repbase Update, a database of eukaryotic repetitive elements","Repbase Update is a comprehensive database of repetitive elements from diverse eukaryotic organisms. Currently, it contains over 3600 annotated sequences representing different families and subfamilies of repeats, many of which are unreported anywhere else. Each sequence is accompanied by a short description and references to the original contributors. Repbase Update includes Repbase Reports, an electronic journal publishing newly discovered transposable elements, and the Transposon Pub, a web-based browser of selected chromosomal maps of transposable elements. Sequences from Repbase Update are used to screen and annotate repetitive elements using programs such as Censor and RepeatMasker. Repbase Update is available on the worldwide web at http://www.girinst.org/Repbase_Update.html.",fullPaper,jv351
p2071,a8db50edfe26a6ae33a6787e2049de5bacd18666,c107,British Machine Vision Conference,ChEMBL: a large-scale bioactivity database for drug discovery,"ChEMBL is an Open Data database containing binding, functional and ADMET information for a large number of drug-like bioactive compounds. These data are manually abstracted from the primary published literature on a regular basis, then further curated and standardized to maximize their quality and utility across a wide range of chemical biology and drug-discovery research problems. Currently, the database contains 5.4 million bioactivity measurements for more than 1 million compounds and 5200 protein targets. Access is available through a web-based interface, data downloads and web services at: https://www.ebi.ac.uk/chembldb.",poster,cp107
p2072,f8928221d290a9cdd84d1de52e121373bc836caa,c52,Workshop on Learning from Authoritative Security Experiment Results,New tools in comparative political economy : the database of political institutions,"This article introduces a large new cross-country database, the database of political institutions. It covers 177 countries over 21 years, 1975-95. The article presents the intuition, construction, and definitions of the different variables. Among the novel variables introduced are several measures of checks and balances, tenure and stability, identification of party affiliation with government or opposition, and fragmentation of opposition and government parties in the legislature.",poster,cp52
p2073,072a0db716fb6f8332323f076b71554716a7271c,j352,IEEE Engineering in Medicine and Biology Magazine,The impact of the MIT-BIH Arrhythmia Database,"The MIT-BIH Arrhythmia Database was the first generally available set of standard test material for evaluation of arrhythmia detectors, and it has been used for that purpose as well as for basic research into cardiac dynamics at about 500 sites worldwide since 1980. It has lived a far longer life than any of its creators ever expected. Together with the American Heart Association Database, it played an interesting role in stimulating manufacturers of arrhythmia analyzers to compete on the basis of objectively measurable performance, and much of the current appreciation of the value of common databases, both for basic research and for medical device development and evaluation, can be attributed to this experience. In this article, we briefly review the history of the database, describe its contents, discuss what we have learned about database design and construction, and take a look at some of the later projects that have been stimulated by both the successes and the limitations of the MIT-BIH Arrhythmia Database.",fullPaper,jv352
p2074,e8104b335a4e499f7b79b913a92d23263c82f6b6,c44,International Workshop on Green and Sustainable Software,The HITRAN2012 molecular spectroscopic database,Abstract content,poster,cp44
p2075,e274c1b6e17825feab52de205fd0bc4917d5be6c,j353,Journal of Biomolecular NMR,Protein backbone angle restraints from searching a database for chemical shift and sequence homology,Abstract content,fullPaper,jv353
p2076,288b317e427c6bf4c94d455049bd1368ff2071eb,c98,North American Chapter of the Association for Computational Linguistics,The Immune Epitope Database (IEDB): 2018 update,"Abstract The Immune Epitope Database (IEDB, iedb.org) captures experimental data confined in figures, text and tables of the scientific literature, making it freely available and easily searchable to the public. The scope of the IEDB extends across immune epitope data related to all species studied and includes antibody, T cell, and MHC binding contexts associated with infectious, allergic, autoimmune, and transplant related diseases. Having been publicly accessible for >10 years, the recent focus of the IEDB has been improved query and reporting functionality to meet the needs of our users to access and summarize data that continues to grow in quantity and complexity. Here we present an update on our current efforts and future goals.",poster,cp98
p2077,00bc156bac2b39fab1689fe047b23c9f216d7f29,c1,Technical Symposium on Computer Science Education,Database resources of the National Center for Biotechnology Information: update,"In addition to maintaining the GenBank(R) nucleic acid sequence database, the National Center for Biotechnology Information (NCBI) provides data analysis and retrieval resources for the data in GenBank and other biological data made available through NCBI’s website. NCBI resources include Entrez, PubMed, PubMed Central, LocusLink, the NCBI Taxonomy Browser, BLAST, BLAST Link (BLink), Electronic PCR, OrfFinder, Spidey, RefSeq, UniGene, HomoloGene, ProtEST, dbMHC, dbSNP, Cancer Chromosome Aberration Project (CCAP), Entrez Genomes and related tools, the Map Viewer, Model Maker, Evidence Viewer, Clusters of Orthologous Groups (COGs) database, Retroviral Genotyping Tools, SARS Coronavirus Resource, SAGEmap, Gene Expression Omnibus (GEO), Online Mendelian Inheritance in Man (OMIM), the Molecular Modeling Database (MMDB), the Conserved Domain Database (CDD) and the Conserved Domain Architecture Retrieval Tool (CDART). Augmenting many of the web applications are custom implementations of the BLAST program optimized to search specialized data sets. All of the resources can be accessed through the NCBI home page at: http://www.ncbi.nlm.nih.gov.",poster,cp1
p2078,8035e5002b7b0898ca7fa8263d09fe4454c6e4fd,c41,Software Product Lines Conference,The BioGRID interaction database: 2019 update,"Abstract The Biological General Repository for Interaction Datasets (BioGRID: https://thebiogrid.org) is an open access database dedicated to the curation and archival storage of protein, genetic and chemical interactions for all major model organism species and humans. As of September 2018 (build 3.4.164), BioGRID contains records for 1 598 688 biological interactions manually annotated from 55 809 publications for 71 species, as classified by an updated set of controlled vocabularies for experimental detection methods. BioGRID also houses records for >700 000 post-translational modification sites. BioGRID now captures chemical interaction data, including chemical–protein interactions for human drug targets drawn from the DrugBank database and manually curated bioactive compounds reported in the literature. A new dedicated aspect of BioGRID annotates genome-wide CRISPR/Cas9-based screens that report gene–phenotype and gene–gene relationships. An extension of the BioGRID resource called the Open Repository for CRISPR Screens (ORCS) database (https://orcs.thebiogrid.org) currently contains over 500 genome-wide screens carried out in human or mouse cell lines. All data in BioGRID is made freely available without restriction, is directly downloadable in standard formats and can be readily incorporated into existing applications via our web service platforms. BioGRID data are also freely distributed through partner model organism databases and meta-databases.",poster,cp41
p2079,e3f2391513693647e0ea87bfa86cd89e468f51d0,c66,Annual Conference on Innovation and Technology in Computer Science Education,Comprehensive database for facial expression analysis,"Within the past decade, significant effort has occurred in developing methods of facial expression analysis. Because most investigators have used relatively limited data sets, the generalizability of these various methods remains unknown. We describe the problem space for facial expression analysis, which includes level of description, transitions among expressions, eliciting conditions, reliability and validity of training and test data, individual differences in subjects, head orientation and scene complexity image characteristics, and relation to non-verbal behavior. We then present the CMU-Pittsburgh AU-Coded Face Expression Image Database, which currently includes 2105 digitized image sequences from 182 adult subjects of varying ethnicity, performing multiple tokens of most primary FACS action units. This database is the most comprehensive testbed to date for comparative studies of facial expression analysis.",poster,cp66
p2080,dd31f1439a0b80cb9447a112347836b3325e953e,c59,British Computer Society Conference on Human-Computer Interaction,The Standardized World Income Inequality Database,"Cross-national research on the causes and consequences of income inequality has been hindered by the limitations of existing inequality datasets: greater coverage across countries and over time is available from these sources only at the cost of significantly reduced comparability across observations. The goal of the Standardized World Income Inequality Database (SWIID) is to overcome these limitations. A custom missing-data algorithm was used to standardize the United Nations University's World Income Inequality Database and data from other sources; data collected by the Luxembourg Income Study served as the standard. The SWIID provides comparable Gini indices of gross and net income inequality for 173 countries for as many years as possible from 1960 to the present along with estimates of uncertainty in these statistics. By maximizing comparability for the largest possible sample of countries and years, the SWIID is better suited to broadly cross-national research on income inequality than previously available sources. 
 
In any papers or publications that use the SWIID, authors are asked to cite the article of record for the data set and give the version number as follows: 
 
Solt, Frederick. 2009. ""Standardizing the World Income Inequality Database."" Social Science Quarterly 90(2):231-242. SWIID Version 3.1, December 2011.",poster,cp59
p2081,804836b8ad86ef8042e3dcbd45442a52f031ee03,c95,IEEE International Conference on Computer Vision,A Database and Evaluation Methodology for Optical Flow,Abstract content,fullPaper,cp95
p2082,eb960b5d56ed1368991eaa4f40cb7afee66edb1f,c43,ACM Symposium on Applied Computing,ONCOMINE: a cancer microarray database and integrated data-mining platform.,Abstract content,poster,cp43
p2083,1cf4a6954b419b5478c96119fc1e79aa90f87dea,c46,Brazilian Symposium on Software Engineering,The Cambridge Structural Database in retrospect and prospect.,"The Cambridge Crystallographic Data Centre (CCDC) was established in 1965 to record numerical, chemical and bibliographic data relating to published organic and metal-organic crystal structures. The Cambridge Structural Database (CSD) now stores data for nearly 700,000 structures and is a comprehensive and fully retrospective historical archive of small-molecule crystallography. Nearly 40,000 new structures are added each year. As X-ray crystallography celebrates its centenary as a subject, and the CCDC approaches its own 50th year, this article traces the origins of the CCDC as a publicly funded organization and its onward development into a self-financing charitable institution. Principally, however, we describe the growth of the CSD and its extensive associated software system, and summarize its impact and value as a basis for research in structural chemistry, materials science and the life sciences, including drug discovery and drug development. Finally, the article considers the CCDC's funding model in relation to open access and open data paradigms.",poster,cp46
p2084,46f74231b9afeb0c290d6d550043c55045284e5f,j327,IEEE Signal Processing Magazine,The MNIST Database of Handwritten Digit Images for Machine Learning Research [Best of the Web],"In this issue, “Best of the Web” presents the modified National Institute of Standards and Technology (MNIST) resources, consisting of a collection of handwritten digit images used extensively in optical character recognition and machine learning research.",fullPaper,jv327
p2085,a1b62db5c838df5630b1c238d4862dc69bdbc874,j14,Scientific Data,"The eICU Collaborative Research Database, a freely available multi-center database for critical care research",Abstract content,fullPaper,jv14
p2086,41e2692d9ac1434d1841d5a29e7ccb927b82b677,c14,International Conference on Exploring Services Science,The Comparative Toxicogenomics Database: update 2019,"Abstract The Comparative Toxicogenomics Database (CTD; http://ctdbase.org/) is a premier public resource for literature-based, manually curated associations between chemicals, gene products, phenotypes, diseases, and environmental exposures. In this biennial update, we present our new chemical–phenotype module that codes chemical-induced effects on phenotypes, curated using controlled vocabularies for chemicals, phenotypes, taxa, and anatomical descriptors; this module provides unique opportunities to explore cellular and system-level phenotypes of the pre-disease state and allows users to construct predictive adverse outcome pathways (linking chemical–gene molecular initiating events with phenotypic key events, diseases, and population-level health outcomes). We also report a 46% increase in CTD manually curated content, which when integrated with other datasets yields more than 38 million toxicogenomic relationships. We describe new querying and display features for our enhanced chemical–exposure science module, providing greater scope of content and utility. As well, we discuss an updated MEDIC disease vocabulary with over 1700 new terms and accession identifiers. To accommodate these increases in data content and functionality, CTD has upgraded its computational infrastructure. These updates continue to improve CTD and help inform new testable hypotheses about the etiology and mechanisms underlying environmentally influenced diseases.",poster,cp14
p2087,5cf0d213f3253cd46673d955209f8463db73cc51,j354,Language Resources and Evaluation,IEMOCAP: interactive emotional dyadic motion capture database,Abstract content,fullPaper,jv354
p2088,8c8b7c1adb6f077bb3045928767b8bc6763e0c06,c112,Very Large Data Bases Conference,HMDB 3.0—The Human Metabolome Database in 2013,"The Human Metabolome Database (HMDB) (www.hmdb.ca) is a resource dedicated to providing scientists with the most current and comprehensive coverage of the human metabolome. Since its first release in 2007, the HMDB has been used to facilitate research for nearly 1000 published studies in metabolomics, clinical biochemistry and systems biology. The most recent release of HMDB (version 3.0) has been significantly expanded and enhanced over the 2009 release (version 2.0). In particular, the number of annotated metabolite entries has grown from 6500 to more than 40 000 (a 600% increase). This enormous expansion is a result of the inclusion of both ‘detected’ metabolites (those with measured concentrations or experimental confirmation of their existence) and ‘expected’ metabolites (those for which biochemical pathways are known or human intake/exposure is frequent but the compound has yet to be detected in the body). The latest release also has greatly increased the number of metabolites with biofluid or tissue concentration data, the number of compounds with reference spectra and the number of data fields per entry. In addition to this expansion in data quantity, new database visualization tools and new data content have been added or enhanced. These include better spectral viewing tools, more powerful chemical substructure searches, an improved chemical taxonomy and better, more interactive pathway maps. This article describes these enhancements to the HMDB, which was previously featured in the 2009 NAR Database Issue. (Note to referees, HMDB 3.0 will go live on 18 September 2012.).",poster,cp112
p2089,68e3ccd3e8f24ad1a15caaddd99178edbf17f8b9,c17,International Conference on Enterprise Information Systems,MODOMICS: a database of RNA modification pathways. 2017 update,"Abstract MODOMICS is a database of RNA modifications that provides comprehensive information concerning the chemical structures of modified ribonucleosides, their biosynthetic pathways, the location of modified residues in RNA sequences, and RNA-modifying enzymes. In the current database version, we included the following new features and data: extended mass spectrometry and liquid chromatography data for modified nucleosides; links between human tRNA sequences and MINTbase - a framework for the interactive exploration of mitochondrial and nuclear tRNA fragments; new, machine-friendly system of unified abbreviations for modified nucleoside names; sets of modified tRNA sequences for two bacterial species, updated collection of mammalian tRNA modifications, 19 newly identified modified ribonucleosides and 66 functionally characterized proteins involved in RNA modification. Data from MODOMICS have been linked to the RNAcentral database of RNA sequences. MODOMICS is available at http://modomics.genesilico.pl.",poster,cp17
p2090,aca92bbbe7ebc8b6b0f272aca209ac5a027222bd,c83,International Conference on Computer Graphics and Interactive Techniques,The IPD and IMGT/HLA database: allele variant databases,"The Immuno Polymorphism Database (IPD) was developed to provide a centralized system for the study of polymorphism in genes of the immune system. Through the IPD project we have established a central platform for the curation and publication of locus-specific databases involved either directly or related to the function of the Major Histocompatibility Complex in a number of different species. We have collaborated with specialist groups or nomenclature committees that curate the individual sections before they are submitted to IPD for online publication. IPD consists of five core databases, with the IMGT/HLA Database as the primary database. Through the work of the various nomenclature committees, the HLA Informatics Group and in collaboration with the European Bioinformatics Institute we are able to provide public access to this data through the website http://www.ebi.ac.uk/ipd/. The IPD project continues to develop with new tools being added to address scientific developments, such as Next Generation Sequencing, and to address user feedback and requests. Regular updates to the website ensure that new and confirmatory sequences are dispersed to the immunogenetics community, and the wider research and clinical communities.",poster,cp83
p2091,f3c56ba45b39ec915477a19779cfc5090be09a73,c36,Conference on Software Engineering Education and Training,The Pfam protein families database,"Pfam is a large collection of protein multiple sequence alignments and profile hidden Markov models. Pfam is available on the WWW in the UK at http://www.sanger.ac.uk/Software/Pfam/, in Sweden at http://www.cgr.ki.se/Pfam/ and in the US at http://pfam.wustl.edu/. The latest version (4.3) of Pfam contains 1815 families. These Pfam families match 63% of proteins in SWISS-PROT 37 and TrEMBL 9. For complete genomes Pfam currently matches up to half of the proteins. Genomic DNA can be directly searched against the Pfam library using the Wise2 package.",poster,cp36
p2092,12d8a9991ee7aecc65bc0991959c5b58a367b2ae,c105,Biometrics and Identity Management,APD3: the antimicrobial peptide database as a tool for research and education,"The antimicrobial peptide database (APD, http://aps.unmc.edu/AP/) is an original database initially online in 2003. The APD2 (2009 version) has been regularly updated and further expanded into the APD3. This database currently focuses on natural antimicrobial peptides (AMPs) with defined sequence and activity. It includes a total of 2619 AMPs with 261 bacteriocins from bacteria, 4 AMPs from archaea, 7 from protists, 13 from fungi, 321 from plants and 1972 animal host defense peptides. The APD3 contains 2169 antibacterial, 172 antiviral, 105 anti-HIV, 959 antifungal, 80 antiparasitic and 185 anticancer peptides. Newly annotated are AMPs with antibiofilm, antimalarial, anti-protist, insecticidal, spermicidal, chemotactic, wound healing, antioxidant and protease inhibiting properties. We also describe other searchable annotations, including target pathogens, molecule-binding partners, post-translational modifications and animal models. Amino acid profiles or signatures of natural AMPs are important for peptide classification, prediction and design. Finally, we summarize various database applications in research and education.",poster,cp105
p2093,2c0aaeb420e1cd2d767a1797b2ded62e0d2ee426,j224,Plant Physiology,GENEVESTIGATOR. Arabidopsis Microarray Database and Analysis Toolbox1[w],"High-throughput gene expression analysis has become a frequent and powerful research tool in biology. At present, however, few software applications have been developed for biologists to query large microarray gene expression databases using a Web-browser interface. We present GENEVESTIGATOR, a database and Web-browser data mining interface for Affymetrix GeneChip data. Users can query the database to retrieve the expression patterns of individual genes throughout chosen environmental conditions, growth stages, or organs. Reversely, mining tools allow users to identify genes specifically expressed during selected stresses, growth stages, or in particular organs. Using GENEVESTIGATOR, the gene expression profiles of more than 22,000 Arabidopsis genes can be obtained, including those of 10,600 currently uncharacterized genes. The objective of this software application is to direct gene functional discovery and design of new experiments by providing plant biologists with contextual information on the expression of genes. The database and analysis toolbox is available as a community resource at https://www.genevestigator.ethz.ch.",fullPaper,jv224
p2094,dc8b25e35a3acb812beb499844734081722319b4,j355,Image and Vision Computing,The FERET database and evaluation procedure for face-recognition algorithms,Abstract content,fullPaper,jv355
p2095,26c075104d0ea1177cce4bd2d5c5d9eef93b8a3b,c96,USENIX Symposium on Operating Systems Design and Implementation,"The MEROPS database of proteolytic enzymes, their substrates and inhibitors in 2017 and a comparison with peptidases in the PANTHER database","Abstract The MEROPS database (http://www.ebi.ac.uk/merops/) is an integrated source of information about peptidases, their substrates and inhibitors. The hierarchical classification is: protein-species, family, clan, with an identifier at each level. The MEROPS website moved to the EMBL-EBI in 2017, requiring refactoring of the code-base and services provided. The interface to sequence searching has changed and the MEROPS protein sequence libraries can be searched at the EMBL-EBI with HMMER, FastA and BLASTP. Cross-references have been established between MEROPS and the PANTHER database at both the family and protein-species level, which will help to improve curation and coverage between the resources. Because of the increasing size of the MEROPS sequence collection, in future only sequences of characterized proteins, and from completely sequenced genomes of organisms of evolutionary, medical or commercial significance will be added. As an example, peptidase homologues in four proteomes from the Asgard superphylum of Archaea have been identified and compared to other archaean, bacterial and eukaryote proteomes. This has given insights into the origins and evolution of peptidase families, including an expansion in the number of proteasome components in Asgard archaeotes and as organisms increase in complexity. Novel structures for proteasome complexes in archaea are postulated.",poster,cp96
p2096,ba90ae48b30594b57a5ca7bfd37cae150458ecfa,c50,International Conference on Automated Software Engineering,TRRUST v2: an expanded reference database of human and mouse transcriptional regulatory interactions,"Abstract Transcription factors (TFs) are major trans-acting factors in transcriptional regulation. Therefore, elucidating TF–target interactions is a key step toward understanding the regulatory circuitry underlying complex traits such as human diseases. We previously published a reference TF–target interaction database for humans—TRRUST (Transcriptional Regulatory Relationships Unraveled by Sentence-based Text mining)—which was constructed using sentence-based text mining, followed by manual curation. Here, we present TRRUST v2 (www.grnpedia.org/trrust) with a significant improvement from the previous version, including a significantly increased size of the database consisting of 8444 regulatory interactions for 800 TFs in humans. More importantly, TRRUST v2 also contains a database for TF–target interactions in mice, including 6552 TF–target interactions for 828 mouse TFs. TRRUST v2 is also substantially more comprehensive and less biased than other TF–target interaction databases. We also improved the web interface, which now enables prioritization of key TFs for a physiological condition depicted by a set of user-input transcriptional responsive genes. With the significant expansion in the database size and inclusion of the new web tool for TF prioritization, we believe that TRRUST v2 will be a versatile database for the study of the transcriptional regulation involved in human diseases.",poster,cp50
p2097,73a254f05fa694dc11a5efc5a033a8f1a4c84fd0,c49,International Symposium on Search Based Software Engineering,The Gene Expression Omnibus Database,Abstract content,poster,cp49
p2098,2157f202c8c89d924dd4da4d1bcf92d16fcd8893,j356,Signal processing. Image communication,"Image database TID2013: Peculiarities, results and perspectives",Abstract content,fullPaper,jv356
p2099,439a453090e28f0858ad5ba0765cc2eeffb23626,c93,Human Language Technology - The Baltic Perspectiv,A New Database on Financial Development and Structure,"The authors introduce a new database of indicators of financial development and structure across countries and over time. This database is unique in that it unites a variety of indicators that measure the size, activity, and efficiency of financial intermediaries and markets. It improves on previous efforts by presenting data on the public share of commercial banks, by introducing indicators of the size and activity of non bank financial institutions, and by presenting measures of the size of bond and primary equity markets. The compiled data permit the construction of financial structure indicators to measure whether, for example, a country's banks are larger, more active, and more efficient than its stock markets. These indicators can then be used to investigate the empirical link between the legal, regulatory, and policy environment and indicators of financial structure. They can also be used to analyze the implications of financial structure for economic growth. The authors describe the sources and construction of, and the intuition behind, different indicators and present descriptive statistics.",poster,cp93
p2100,3765df816dc5a061bc261e190acc8bdd9d47bec0,c76,International Conference on Artificial Neural Networks,Presentation and validation of the Radboud Faces Database,"Many research fields concerned with the processing of information contained in human faces would benefit from face stimulus sets in which specific facial characteristics are systematically varied while other important picture characteristics are kept constant. Specifically, a face database in which displayed expressions, gaze direction, and head orientation are parametrically varied in a complete factorial design would be highly useful in many research domains. Furthermore, these stimuli should be standardised in several important, technical aspects. The present article presents the freely available Radboud Faces Database offering such a stimulus set, containing both Caucasian adult and children images. This face database is described both procedurally and in terms of content, and a validation study concerning its most important characteristics is presented. In the validation study, all frontal images were rated with respect to the shown facial expression, intensity of expression, clarity of expression, genuineness of expression, attractiveness, and valence. The results show very high recognition of the intended facial expressions.",poster,cp76
p2101,2b20c0d15c4ec8a48bfd916f73b163a2decd0852,c63,IEEE International Software Metrics Symposium,The InterPro protein families database: the classification resource after 15 years,"The InterPro database (http://www.ebi.ac.uk/interpro/) is a freely available resource that can be used to classify sequences into protein families and to predict the presence of important domains and sites. Central to the InterPro database are predictive models, known as signatures, from a range of different protein family databases that have different biological focuses and use different methodological approaches to classify protein families and domains. InterPro integrates these signatures, capitalizing on the respective strengths of the individual databases, to produce a powerful protein classification resource. Here, we report on the status of InterPro as it enters its 15th year of operation, and give an overview of new developments with the database and its associated Web interfaces and software. In particular, the new domain architecture search tool is described and the process of mapping of Gene Ontology terms to InterPro is outlined. We also discuss the challenges faced by the resource given the explosive growth in sequence data in recent years. InterPro (version 48.0) contains 36 766 member database signatures integrated into 26 238 InterPro entries, an increase of over 3993 entries (5081 signatures), since 2012.",poster,cp63
p2102,ddd1969dc3cd2fcaaaf0b2f16e506b22f463a9a5,c88,Symposium on the Theory of Computing,The BioGRID interaction database: 2017 update,"The Biological General Repository for Interaction Datasets (BioGRID: https://thebiogrid.org) is an open access database dedicated to the annotation and archival of protein, genetic and chemical interactions for all major model organism species and humans. As of September 2016 (build 3.4.140), the BioGRID contains 1 072 173 genetic and protein interactions, and 38 559 post-translational modifications, as manually annotated from 48 114 publications. This dataset represents interaction records for 66 model organisms and represents a 30% increase compared to the previous 2015 BioGRID update. BioGRID curates the biomedical literature for major model organism species, including humans, with a recent emphasis on central biological processes and specific human diseases. To facilitate network-based approaches to drug discovery, BioGRID now incorporates 27 501 chemical–protein interactions for human drug targets, as drawn from the DrugBank database. A new dynamic interaction network viewer allows the easy navigation and filtering of all genetic and protein interaction data, as well as for bioactive compounds and their established targets. BioGRID data are directly downloadable without restriction in a variety of standardized formats and are freely distributed through partner model organism databases and meta-databases.",poster,cp88
p2103,3bbc9400429ad3d6bda6d12e4449053afa1114a2,j357,Antimicrobial Agents and Chemotherapy,The Comprehensive Antibiotic Resistance Database,"ABSTRACT The field of antibiotic drug discovery and the monitoring of new antibiotic resistance elements have yet to fully exploit the power of the genome revolution. Despite the fact that the first genomes sequenced of free living organisms were those of bacteria, there have been few specialized bioinformatic tools developed to mine the growing amount of genomic data associated with pathogens. In particular, there are few tools to study the genetics and genomics of antibiotic resistance and how it impacts bacterial populations, ecology, and the clinic. We have initiated development of such tools in the form of the Comprehensive Antibiotic Research Database (CARD; http://arpcard.mcmaster.ca). The CARD integrates disparate molecular and sequence data, provides a unique organizing principle in the form of the Antibiotic Resistance Ontology (ARO), and can quickly identify putative antibiotic resistance genes in new unannotated genome sequences. This unique platform provides an informatic tool that bridges antibiotic resistance concerns in health care, agriculture, and the environment.",fullPaper,jv357
p2104,026668472fd8f0fa2ca710ce276be35d362637c2,c20,Research Conference of the South African Institute of Computer Scientists and Information Technologists,"Federated database systems for managing distributed, heterogeneous, and autonomous databases","A federated database system (FDBS) is a collection of cooperating database systems that are autonomous and possibly heterogeneous. In this paper, we define a reference architecture for distributed database management systems from system and schema viewpoints and show how various FDBS architectures can be developed. We then define a methodology for developing one of the popular architectures of an FDBS. Finally, we discuss critical issues related to developing and operating an FDBS.",poster,cp20
p2105,50819fdfb666fdabb12f00e93e0d7e43ba4ba4bd,j358,Journal of Chemical Information and Modeling,ZINC - A Free Database of Commercially Available Compounds for Virtual Screening,"A critical barrier to entry into structure-based virtual screening is the lack of a suitable, easy to access database of purchasable compounds. We have therefore prepared a library of 727,842 molecules, each with 3D structure, using catalogs of compounds from vendors (the size of this library continues to grow). The molecules have been assigned biologically relevant protonation states and are annotated with properties such as molecular weight, calculated LogP, and number of rotatable bonds. Each molecule in the library contains vendor and purchasing information and is ready for docking using a number of popular docking programs. Within certain limits, the molecules are prepared in multiple protonation states and multiple tautomeric forms. In one format, multiple conformations are available for the molecules. This database is available for free download (http://zinc.docking.org) in several common file formats including SMILES, mol2, 3D SDF, and DOCK flexibase format. A Web-based query tool incorporating a molecular drawing interface enables the database to be searched and browsed and subsets to be created. Users can process their own molecules by uploading them to a server. Our hope is that this database will bring virtual screening libraries to a wide community of structural biologists and medicinal chemists.",fullPaper,jv358
p2106,b7e54a5d6149cde03fd5144d38a0647d25a795f6,c95,IEEE International Conference on Computer Vision,The ChEMBL bioactivity database: an update,"ChEMBL is an open large-scale bioactivity database (https://www.ebi.ac.uk/chembl), previously described in the 2012 Nucleic Acids Research Database Issue. Since then, a variety of new data sources and improvements in functionality have contributed to the growth and utility of the resource. In particular, more comprehensive tracking of compounds from research stages through clinical development to market is provided through the inclusion of data from United States Adopted Name applications; a new richer data model for representing drug targets has been developed; and a number of methods have been put in place to allow users to more easily identify reliable data. Finally, access to ChEMBL is now available via a new Resource Description Framework format, in addition to the web-based interface, data downloads and web services.",poster,cp95
p2107,60258897d250a41f11cfee27de828a0130110b5e,c95,IEEE International Conference on Computer Vision,The CELEX Lexical Database (CD-ROM),Abstract content,poster,cp95
p2108,d2272dd9ff850edc448f7fde86eef6bcd57af2cc,c23,International Conference on Open and Big Data,Development of a global land cover characteristics database and IGBP DISCover from 1 km AVHRR data,"Researchers from the U.S. Geological Survey, University of Nebraska-Lincoln and the European Commission's Joint Research Centre, Ispra, Italy produced a 1 km resolution global land cover characteristics database for use in a wide range of continental-to global-scale environmental studies. This database provides a unique view of the broad patterns of the biogeographical and ecoclimatic diversity of the global land surface, and presents a detailed interpretation of the extent of human development. The project was carried out as an International Geosphere-Biosphere Programme, Data and Information Systems (IGBP-DIS) initiative. The IGBP DISCover global land cover product is an integral component of the global land cover database. DISCover includes 17 general land cover classes defined to meet the needs of IGBP core science projects. A formal accuracy assessment of the DISCover data layer will be completed in 1998. The 1 km global land cover database was developed through a continent-by-continent unsupervised classification of 1 km monthly Advanced Very High Resolution Radiometer (AVHRR) Normalized Difference Vegetation Index (NDVI) composites covering 1992-1993. Extensive post-classification stratification was necessary to resolve spectral/temporal confusion between disparate land cover types. The complete global database consists of 961 seasonal land cover regions that capture patterns of land cover, seasonality and relative primary productivity. The seasonal land cover regions were aggregated to produce seven separate land cover data sets used for global environmental modelling and assessment. The data sets include IGBP DISCover, U.S. Geological Survey Anderson System, Simple Biosphere Model, Simple Biosphere Model 2, Biosphere-Atmosphere Transfer Scheme, Olson Ecosystems and Running Global Remote Sensing Land Cover. The database also includes all digital sources that were used in the classification. The complete database can be sourced from the website: http://edcwww.cr.usgs.gov/landdaac/glcc/glcc.html.",poster,cp23
p2109,df3a207258f3febb98d3dcaf890e8a0f09cd5c12,j245,IEEE Transactions on Visualization and Computer Graphics,FaceWarehouse: A 3D Facial Expression Database for Visual Computing,"We present FaceWarehouse, a database of 3D facial expressions for visual computing applications. We use Kinect, an off-the-shelf RGBD camera, to capture 150 individuals aged 7-80 from various ethnic backgrounds. For each person, we captured the RGBD data of her different expressions, including the neutral expression and 19 other expressions such as mouth-opening, smile, kiss, etc. For every RGBD raw data record, a set of facial feature points on the color image such as eye corners, mouth contour, and the nose tip are automatically localized, and manually adjusted if better accuracy is required. We then deform a template facial mesh to fit the depth data as closely as possible while matching the feature points on the color image to their corresponding points on the mesh. Starting from these fitted face meshes, we construct a set of individual-specific expression blendshapes for each person. These meshes with consistent topology are assembled as a rank-3 tensor to build a bilinear face model with two attributes: identity and expression. Compared with previous 3D facial databases, for every person in our database, there is a much richer matching collection of expressions, enabling depiction of most human facial actions. We demonstrate the potential of FaceWarehouse for visual computing with four applications: facial image manipulation, face component transfer, real-time performance-based facial image animation, and facial animation retargeting from video to image.",fullPaper,jv245
p2110,0414f6ffc086bf6c2015176d4b46a051d436cd2b,c113,International Conference on Image Analysis and Processing,Mouse Genome Database (MGD) 2019,"Abstract The Mouse Genome Database (MGD; http://www.informatics.jax.org) is the community model organism genetic and genome resource for the laboratory mouse. MGD is the authoritative source for biological reference data sets related to mouse genes, gene functions, phenotypes, and mouse models of human disease. MGD is the primary outlet for official gene, allele and mouse strain nomenclature based on the guidelines set by the International Committee on Standardized Nomenclature for Mice. In this report we describe significant enhancements to MGD, including two new graphical user interfaces: (i) the Multi Genome Viewer for exploring the genomes of multiple mouse strains and (ii) the Phenotype-Gene Expression matrix which was developed in collaboration with the Gene Expression Database (GXD) and allows researchers to compare gene expression and phenotype annotations for mouse genes. Other recent improvements include enhanced efficiency of our literature curation processes and the incorporation of Transcriptional Start Site (TSS) annotations from RIKEN’s FANTOM 5 initiative.",poster,cp113
p2111,e4b52a1a00e9db941326fc857b95245cbfb60bce,c14,International Conference on Exploring Services Science,Reactome graph database: Efficient access to complex pathway data,"Reactome is a free, open-source, open-data, curated and peer-reviewed knowledgebase of biomolecular pathways. One of its main priorities is to provide easy and efficient access to its high quality curated data. At present, biological pathway databases typically store their contents in relational databases. This limits access efficiency because there are performance issues associated with queries traversing highly interconnected data. The same data in a graph database can be queried more efficiently. Here we present the rationale behind the adoption of a graph database (Neo4j) as well as the new ContentService (REST API) that provides access to these data. The Neo4j graph database and its query language, Cypher, provide efficient access to the complex Reactome data model, facilitating easy traversal and knowledge discovery. The adoption of this technology greatly improved query efficiency, reducing the average query time by 93%. The web service built on top of the graph database provides programmatic access to Reactome data by object oriented queries, but also supports more complex queries that take advantage of the new underlying graph-based data storage. By adopting graph database technology we are providing a high performance pathway data resource to the community. The Reactome graph database use case shows the power of NoSQL database engines for complex biological data types.",poster,cp14
p2112,0b87f6c1f2be4ad82b3ba77514083449232d8d72,c35,EUROMICRO Conference on Software Engineering and Advanced Applications,FRED-MD: A Monthly Database for Macroeconomic Research,"This article describes a large, monthly frequency, macroeconomic database with the goal of establishing a convenient starting point for empirical analysis that requires “big data.” The dataset mimics the coverage of those already used in the literature but has three appealing features. First, it is designed to be updated monthly using the Federal Reserve Economic Data (FRED) database. Second, it will be publicly accessible, facilitating comparison of related research and replication of empirical work. Third, it will relieve researchers from having to manage data changes and revisions. We show that factors extracted from our dataset share the same predictive content as those based on various vintages of the so-called Stock–Watson dataset. In addition, we suggest that diffusion indexes constructed as the partial sum of the factor estimates can potentially be useful for the study of business cycle chronology. Supplementary materials for this article are available online.",poster,cp35
p2113,0d0bdeafd7b74a790a4fc036c0a54bfe0de722f3,c66,Annual Conference on Innovation and Technology in Computer Science Education,Principles of Distributed Database Systems,Abstract content,poster,cp66
p2114,f4b3598f49fbb81c0ea21b8130bbdd5133403efc,j359,JAMA Oncology,Using the National Cancer Database for Outcomes Research: A Review,"Importance The National Cancer Database (NCDB), a joint quality improvement initiative of the American College of Surgeons Commission on Cancer and the American Cancer Society, has created a shared research file that has changed the study of cancer care in the United States. A thorough understanding of the nuances, strengths, and limitations of the database by both readers and investigators is of critical importance. This review describes the use of the NCDB to study cancer care, with a focus on the advantages of using the database and important considerations that affect the interpretation of NCDB studies. Observations The NCDB is one of the largest cancer registries in the world and has rapidly become one of the most commonly used data resources to study the care of cancer in the United States. The NCDB paints a comprehensive picture of cancer care, including a number of less commonly available details that enable subtle nuances of treatment to be studied. On the other hand, several potentially important patient and treatment attributes are not collected in the NCDB, which may affect the extent to which comparisons can be adjusted. Finally, the NCDB has undergone several significant changes during the past decade that may affect its completeness and the types of available data. Conclusions and Relevance The NCDB offers a critically important perspective on cancer care in the United States. To capitalize on its strengths and adjust for its limitations, investigators and their audiences should familiarize themselves with the advantages and shortcomings of the NCDB, as well as its evolution over time.",fullPaper,jv359
p2115,948237a4c741c81759454e3b9ef501d4fa843817,j102,Nucleic Acids Research,The MetaCyc database of metabolic pathways and enzymes,"Abstract MetaCyc (https://MetaCyc.org) is a comprehensive reference database of metabolic pathways and enzymes from all domains of life. It contains more than 2570 pathways derived from >54 000 publications, making it the largest curated collection of metabolic pathways. The data in MetaCyc is strictly evidence-based and richly curated, resulting in an encyclopedic reference tool for metabolism. MetaCyc is also used as a knowledge base for generating thousands of organism-specific Pathway/Genome Databases (PGDBs), which are available in the BioCyc (https://BioCyc.org) and other PGDB collections. This article provides an update on the developments in MetaCyc during the past two years, including the expansion of data and addition of new features.",fullPaper,jv102
p2116,0e9a6cfa4fee0cfe310edf55346e147549bd90be,c112,Very Large Data Bases Conference,The Comparative Toxicogenomics Database: update 2017,"The Comparative Toxicogenomics Database (CTD; http://ctdbase.org/) provides information about interactions between chemicals and gene products, and their relationships to diseases. Core CTD content (chemical-gene, chemical-disease and gene-disease interactions manually curated from the literature) are integrated with each other as well as with select external datasets to generate expanded networks and predict novel associations. Today, core CTD includes more than 30.5 million toxicogenomic connections relating chemicals/drugs, genes/proteins, diseases, taxa, Gene Ontology (GO) annotations, pathways, and gene interaction modules. In this update, we report a 33% increase in our core data content since 2015, describe our new exposure module (that harmonizes exposure science information with core toxicogenomic data) and introduce a novel dataset of GO-disease inferences (that identify common molecular underpinnings for seemingly unrelated pathologies). These advancements centralize and contextualize real-world chemical exposures with molecular pathways to help scientists generate testable hypotheses in an effort to understand the etiology and mechanisms underlying environmentally influenced diseases.",poster,cp112
p2117,c6b5f8b0885e307d2dab6413036aec93e65bfe4f,c61,Jahrestagung der Gesellschaft für Informatik,PhenoScanner: a database of human genotype–phenotype associations,"Abstract Summary: PhenoScanner is a curated database of publicly available results from large-scale genetic association studies. This tool aims to facilitate ‘phenome scans’, the cross-referencing of genetic variants with many phenotypes, to help aid understanding of disease pathways and biology. The database currently contains over 350 million association results and over 10 million unique genetic variants, mostly single nucleotide polymorphisms. It is accompanied by a web-based tool that queries the database for associations with user-specified variants, providing results according to the same effect and non-effect alleles for each input variant. The tool provides the option of searching for trait associations with proxies of the input variants, calculated using the European samples from 1000 Genomes and Hapmap. Availability and Implementation: PhenoScanner is available at www.phenoscanner.medschl.cam.ac.uk. Contact: jrs95@medschl.cam.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.",poster,cp61
p2118,bca0c491e3758ae2973d3374b62e1d417a3dc9c7,c110,IEEE International Conference on Automatic Face & Gesture Recognition,Rfam 12.0: updates to the RNA families database,"The Rfam database (available at http://rfam.xfam.org) is a collection of non-coding RNA families represented by manually curated sequence alignments, consensus secondary structures and annotation gathered from corresponding Wikipedia, taxonomy and ontology resources. In this article, we detail updates and improvements to the Rfam data and website for the Rfam 12.0 release. We describe the upgrade of our search pipeline to use Infernal 1.1 and demonstrate its improved homology detection ability by comparison with the previous version. The new pipeline is easier for users to apply to their own data sets, and we illustrate its ability to annotate RNAs in genomic and metagenomic data sets of various sizes. Rfam has been expanded to include 260 new families, including the well-studied large subunit ribosomal RNA family, and for the first time includes information on short sequence- and structure-based RNA motifs present within families.",poster,cp110
p2119,c1e62b537f3d30018e7979a89b0e0f15e2b6eecc,c87,European Conference on Computer Vision,The SIDER database of drugs and side effects,"Unwanted side effects of drugs are a burden on patients and a severe impediment in the development of new drugs. At the same time, adverse drug reactions (ADRs) recorded during clinical trials are an important source of human phenotypic data. It is therefore essential to combine data on drugs, targets and side effects into a more complete picture of the therapeutic mechanism of actions of drugs and the ways in which they cause adverse reactions. To this end, we have created the SIDER (‘Side Effect Resource’, http://sideeffects.embl.de) database of drugs and ADRs. The current release, SIDER 4, contains data on 1430 drugs, 5880 ADRs and 140 064 drug–ADR pairs, which is an increase of 40% compared to the previous version. For more fine-grained analyses, we extracted the frequency with which side effects occur from the package inserts. This information is available for 39% of drug–ADR pairs, 19% of which can be compared to the frequency under placebo treatment. SIDER furthermore contains a data set of drug indications, extracted from the package inserts using Natural Language Processing. These drug indications are used to reduce the rate of false positives by identifying medical terms that do not correspond to ADRs.",poster,cp87
p2120,9c15ba340c2a44a7b2066d8ded94ce18ea68fe2f,c87,European Conference on Computer Vision,The BioGRID interaction database: 2015 update,"The Biological General Repository for Interaction Datasets (BioGRID: http://thebiogrid.org) is an open access database that houses genetic and protein interactions curated from the primary biomedical literature for all major model organism species and humans. As of September 2014, the BioGRID contains 749 912 interactions as drawn from 43 149 publications that represent 30 model organisms. This interaction count represents a 50% increase compared to our previous 2013 BioGRID update. BioGRID data are freely distributed through partner model organism databases and meta-databases and are directly downloadable in a variety of formats. In addition to general curation of the published literature for the major model species, BioGRID undertakes themed curation projects in areas of particular relevance for biomedical sciences, such as the ubiquitin-proteasome system and various human disease-associated interaction networks. BioGRID curation is coordinated through an Interaction Management System (IMS) that facilitates the compilation interaction records through structured evidence codes, phenotype ontologies, and gene annotation. The BioGRID architecture has been improved in order to support a broader range of interaction and post-translational modification types, to allow the representation of more complex multi-gene/protein interactions, to account for cellular phenotypes through structured ontologies, to expedite curation through semi-automated text-mining approaches, and to enhance curation quality control.",poster,cp87
p2121,3ef0c7784bf446de5ce5977a35f86c8b30fd668f,j360,Systematic Reviews,Optimal database combinations for literature searches in systematic reviews: a prospective exploratory study,Abstract content,fullPaper,jv360
p2122,8257167186837ff6840d6c2f552b4d23ff26ec81,j361,Medical Physics (Lancaster),The Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI): a completed reference database of lung nodules on CT scans.,"PURPOSE
The development of computer-aided diagnostic (CAD) methods for lung nodule detection, classification, and quantitative assessment can be facilitated through a well-characterized repository of computed tomography (CT) scans. The Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI) completed such a database, establishing a publicly available reference for the medical imaging research community. Initiated by the National Cancer Institute (NCI), further advanced by the Foundation for the National Institutes of Health (FNIH), and accompanied by the Food and Drug Administration (FDA) through active participation, this public-private partnership demonstrates the success of a consortium founded on a consensus-based process.


METHODS
Seven academic centers and eight medical imaging companies collaborated to identify, address, and resolve challenging organizational, technical, and clinical issues to provide a solid foundation for a robust database. The LIDC/IDRI Database contains 1018 cases, each of which includes images from a clinical thoracic CT scan and an associated XML file that records the results of a two-phase image annotation process performed by four experienced thoracic radiologists. In the initial blinded-read phase, each radiologist independently reviewed each CT scan and marked lesions belonging to one of three categories (""nodule > or =3 mm,"" ""nodule <3 mm,"" and ""non-nodule > or =3 mm""). In the subsequent unblinded-read phase, each radiologist independently reviewed their own marks along with the anonymized marks of the three other radiologists to render a final opinion. The goal of this process was to identify as completely as possible all lung nodules in each CT scan without requiring forced consensus.


RESULTS
The Database contains 7371 lesions marked ""nodule"" by at least one radiologist. 2669 of these lesions were marked ""nodule > or =3 mm"" by at least one radiologist, of which 928 (34.7%) received such marks from all four radiologists. These 2669 lesions include nodule outlines and subjective nodule characteristic ratings.


CONCLUSIONS
The LIDC/IDRI Database is expected to provide an essential medical imaging research resource to spur CAD development, validation, and dissemination in clinical practice.",fullPaper,jv361
p2123,d90293c06067d1a3718b4fb8c167285bfd099702,j362,RNA: A publication of the RNA Society,circBase: a database for circular RNAs,"Recently, several laboratories have reported thousands of circular RNAs (circRNAs) in animals. Numerous circRNAs are highly stable and have specific spatiotemporal expression patterns. Even though a function for circRNAs is unknown, these features make circRNAs an interesting class of RNAs as possible biomarkers and for further research. We developed a database and website, “circBase,” where merged and unified data sets of circRNAs and the evidence supporting their expression can be accessed, downloaded, and browsed within the genomic context. circBase also provides scripts to identify known and novel circRNAs in sequencing data. The database is freely accessible through the web server at http://www.circbase.org/.",fullPaper,jv362
p2124,20740b17fd91394cfdf17ebf1c227312d6bb56cb,c29,International Conference on Software Engineering,The Database of Interacting Proteins: 2004 update,"The Database of Interacting Proteins (http://dip.doe-mbi.ucla.edu) aims to integrate the diverse body of experimental evidence on protein-protein interactions into a single, easily accessible online database. Because the reliability of experimental evidence varies widely, methods of quality assessment have been developed and utilized to identify the most reliable subset of the interactions. This CORE set can be used as a reference when evaluating the reliability of high-throughput protein-protein interaction data sets, for development of prediction methods, as well as in the studies of the properties of protein interaction networks.",poster,cp29
p2125,2e5701b71ccf3352b30b584c2e48fdc307376385,c16,Knowledge Discovery and Data Mining,The immune epitope database (IEDB) 3.0,"The IEDB, www.iedb.org, contains information on immune epitopes—the molecular targets of adaptive immune responses—curated from the published literature and submitted by National Institutes of Health funded epitope discovery efforts. From 2004 to 2012 the IEDB curation of journal articles published since 1960 has caught up to the present day, with >95% of relevant published literature manually curated amounting to more than 15 000 journal articles and more than 704 000 experiments to date. The revised curation target since 2012 has been to make recent research findings quickly available in the IEDB and thereby ensure that it continues to be an up-to-date resource. Having gathered a comprehensive dataset in the IEDB, a complete redesign of the query and reporting interface has been performed in the IEDB 3.0 release to improve how end users can access this information in an intuitive and biologically accurate manner. We here present this most recent release of the IEDB and describe the user testing procedures as well as the use of external ontologies that have enabled it.",poster,cp16
p2126,4c987ffb492e44acc010cbeb2347b92e257d7b59,c96,USENIX Symposium on Operating Systems Design and Implementation,HMDB: the Human Metabolome Database,"The Human Metabolome Database (HMDB) is currently the most complete and comprehensive curated collection of human metabolite and human metabolism data in the world. It contains records for more than 2180 endogenous metabolites with information gathered from thousands of books, journal articles and electronic databases. In addition to its comprehensive literature-derived data, the HMDB also contains an extensive collection of experimental metabolite concentration data compiled from hundreds of mass spectra (MS) and Nuclear Magnetic resonance (NMR) metabolomic analyses performed on urine, blood and cerebrospinal fluid samples. This is further supplemented with thousands of NMR and MS spectra collected on purified, reference metabolites. Each metabolite entry in the HMDB contains an average of 90 separate data fields including a comprehensive compound description, names and synonyms, structural information, physico-chemical data, reference NMR and MS spectra, biofluid concentrations, disease associations, pathway information, enzyme data, gene sequence data, SNP and mutation data as well as extensive links to images, references and other public databases. Extensive searching, relational querying and data browsing tools are also provided. The HMDB is designed to address the broad needs of biochemists, clinical chemists, physicians, medical geneticists, nutritionists and members of the metabolomics community. The HMDB is available at:",poster,cp96
p2127,ca6ac75d2408d9fbc3ff49e8d62341821574337b,j363,Immunogenetics,SYFPEITHI: database for MHC ligands and peptide motifs,Abstract content,fullPaper,jv363
p2128,20c31b7a5d64ce111e6808a261e09d4a9a24f6c0,c96,USENIX Symposium on Operating Systems Design and Implementation,Spanner: Google's Globally-Distributed Database,"Spanner is Google’s scalable, multiversion, globally distributed, and synchronously replicated database. It is the first system to distribute data at global scale and support externally-consistent distributed transactions. This article describes how Spanner is structured, its feature set, the rationale underlying various design decisions, and a novel time API that exposes clock uncertainty. This API and its implementation are critical to supporting external consistency and a variety of powerful features: nonblocking reads in the past, lock-free snapshot transactions, and atomic schema changes, across all of Spanner.",fullPaper,cp96
p2129,62a134740314b4469c83c8921ae2e1beea22b8f5,j299,IEEE Transactions on Pattern Analysis and Machine Intelligence,A Database for Handwritten Text Recognition Research,"An image database for handwritten text recognition research is described. Digital images of approximately 5000 city names, 5000 state names, 10000 ZIP Codes, and 50000 alphanumeric characters are included. Each image was scanned from mail in a working post office at 300 pixels/in in 8-bit gray scale on a high-quality flat bed digitizer. The data were unconstrained for the writer, style, and method of preparation. These characteristics help overcome the limitations of earlier databases that contained only isolated characters or were prepared in a laboratory setting under prescribed circumstances. Also, the database is divided into explicit training and testing sets to facilitate the sharing of results among researchers as well as performance comparisons. >",fullPaper,jv299
p2130,bd49e7ecf477766790840eabbefcbbcb3b998d72,c53,International Conference on Software Engineering and Knowledge Engineering,Database Resources of the National Center for Biotechnology Information,"The National Center for Biotechnology Information (NCBI) provides a large suite of online resources for biological information and data, including the GenBank® nucleic acid sequence database and the PubMed database of citations and abstracts for published life science journals. The Entrez system provides search and retrieval operations for most of these data from 37 distinct databases. The E-utilities serve as the programming interface for the Entrez system. Augmenting many of the Web applications are custom implementations of the BLAST program optimized to search specialized data sets. New resources released in the past year include iCn3D, MutaBind, and the Antimicrobial Resistance Gene Reference Database; and resources that were updated in the past year include My Bibliography, SciENcv, the Pathogen Detection Project, Assembly, Genome, the Genome Data Viewer, BLAST and PubChem. All of these resources can be accessed through the NCBI home page at www.ncbi.nlm.nih.gov.",poster,cp53
p2131,0e17d9327501603b39200c5eb9db886de1581a09,c51,Conference of the Centre for Advanced Studies on Collaborative Research,Systemic Banking Crises Database,Abstract content,poster,cp51
p2132,e57683f3eea6176441230c2b30bec2fda4984697,c31,International Conference on Evaluation & Assessment in Software Engineering,The Open Quantum Materials Database (OQMD): assessing the accuracy of DFT formation energies,Abstract content,poster,cp31
p2133,683bc72f18d2eede7d4c46c55537cbce63a47f62,c2,International Symposium on Intelligent Data Analysis,Tmbase-A database of membrane spanning protein segments,Abstract content,poster,cp2
p2134,58a93e9cd60ce331606d31ebed62599a2b7db805,c13,International Conference on Data Science and Advanced Analytics,The SWISS-PROT protein sequence database and its supplement TrEMBL in 2000,"SWISS-PROT is a curated protein sequence database which strives to provide a high level of annotation (such as the description of the function of a protein, its domains structure, post-translational modifications, variants, etc.), a minimal level of redundancy and high level of integration with other databases. Recent developments of the database include format and content enhancements, cross-references to additional databases, new documentation files and improvements to TrEMBL, a computer-annotated supplement to SWISS-PROT. TrEMBL consists of entries in SWISS-PROT-like format derived from the translation of all coding sequences (CDSs) in the EMBL Nucleotide Sequence Database, except the CDSs already included in SWISS-PROT. We also describe the Human Proteomics Initiative (HPI), a major project to annotate all known human sequences according to the quality standards of SWISS-PROT. SWISS-PROT is available at: http://www.expasy.ch/sprot/ and http://www.ebi.ac.uk/swissprot/",poster,cp13
p2135,e45a8d7176bd738c1e63de1f6791a88e704f8b4b,j107,Nature Communications,Universal database search tool for proteomics,Abstract content,fullPaper,jv107
p2136,e4beeff8cf47dcc0faf6efc8f4c1b3fefc052afb,c97,Interspeech,A database of German emotional speech,"The article describes a database of emotional speech. Ten actors (5 female and 5 male) simulated the emotions, producing 10 German utterances (5 short and 5 longer sentences) which could be used in everyday communication and are interpretable in all applied emotions. The recordings were taken in an anechoic chamber with high-quality recording equipment. In addition to the sound electro-glottograms were recorded. The speech material comprises about 800 sentences (seven emotions * ten actors * ten sentences + some second versions). The complete database was evaluated in a perception test regarding the recognisability of emotions and their naturalness. Utterances recognised better than 80% and judged as natural by more than 60% of the listeners were phonetically labelled in a narrow transcription with special markers for voice-quality, phonatory and articulatory settings and articulatory features. The database can be accessed by the public via the internet (http://www.expressive-speech.net/emodb/).",fullPaper,cp97
p2137,7ffa7a36e5414a0f2b16b1d8f93442ab15e2235d,j299,IEEE Transactions on Pattern Analysis and Machine Intelligence,"The CMU Pose, Illumination, and Expression Database","In the Fall of 2000, we collected a database of more than 40,000 facial images of 68 people. Using the Carnegie Mellon University 3D Room, we imaged each person across 13 different poses, under 43 different illumination conditions, and with four different expressions. We call this the CMU pose, illumination, and expression (PIE) database. We describe the imaging hardware, the collection procedure, the organization of the images, several possible uses, and how to obtain the database.",fullPaper,jv299
p2138,2ea5d8555fe5efb7b35bc40e55ead122cfac03b4,j364,Global Change Biology,TRY – a global database of plant traits,"Plant traits – the morphological, anatomical, physiological, biochemical and phenological characteristics of plants and their organs – determine how primary producers respond to environmental factors, affect other trophic levels, influence ecosystem processes and services and provide a link from species richness to ecosystem functional diversity. Trait data thus represent the raw material for a wide range of research from evolutionary biology, community and functional ecology to biogeography. Here we present the global database initiative named TRY, which has united a wide range of the plant trait research community worldwide and gained an unprecedented buy‐in of trait data: so far 93 trait databases have been contributed. The data repository currently contains almost three million trait entries for 69 000 out of the world's 300 000 plant species, with a focus on 52 groups of traits characterizing the vegetative and regeneration stages of the plant life cycle, including growth, dispersal, establishment and persistence. A first data analysis shows that most plant traits are approximately log‐normally distributed, with widely differing ranges of variation across traits. Most trait variation is between species (interspecific), but significant intraspecific variation is also documented, up to 40% of the overall variation. Plant functional types (PFTs), as commonly used in vegetation models, capture a substantial fraction of the observed variation – but for several traits most variation occurs within PFTs, up to 75% of the overall variation. In the context of vegetation models these traits would better be represented by state variables rather than fixed parameter values. The improved availability of plant trait data in the unified global database is expected to support a paradigm shift from species to trait‐based ecology, offer new opportunities for synthetic plant trait research and enable a more realistic and empirically grounded representation of terrestrial vegetation in Earth system models.",fullPaper,jv364
p2139,5fec3cda0d994c217c737c10b0b7e56d4574edca,c67,Enterprise Application Integration,"MEROPS: the database of proteolytic enzymes, their substrates and inhibitors","Peptidases, their substrates and inhibitors are of great relevance to biology, medicine and biotechnology. The MEROPS database (http://merops.sanger.ac.uk) aims to fulfill the need for an integrated source of information about these. The database has hierarchical classifications in which homologous sets of peptidases and protein inhibitors are grouped into protein species, which are grouped into families, which are in turn grouped into clans. Recent developments include the following. A community annotation project has been instigated in which acknowledged experts are invited to contribute summaries for peptidases. Software has been written to provide an Internet-based data entry form. Contributors are acknowledged on the relevant web page. A new display showing the intron/exon structures of eukaryote peptidase genes and the phasing of the junctions has been implemented. It is now possible to filter the list of peptidases from a completely sequenced bacterial genome for a particular strain of the organism. The MEROPS filing pipeline has been altered to circumvent the restrictions imposed on non-interactive blastp searches, and a HMMER search using specially generated alignments to maximize the distribution of organisms returned in the search results has been added.",poster,cp67
p2140,3ff0d2c7621c40e6245c7ca0964b4b856255e0c2,c23,International Conference on Open and Big Data,"Development and validation of a global database of lakes, reservoirs and wetlands",Abstract content,poster,cp23
p2141,269b9e182815a5d805cfa1f5c1137763b0b2b7cf,c39,International Conference on Global Software Engineering,Genevestigator V3: A Reference Expression Database for the Meta-Analysis of Transcriptomes,"The Web-based software tool Genevestigator provides powerful tools for biologists to explore gene expression across a wide variety of biological contexts. Its first releases, however, were limited by the scaling ability of the system architecture, multiorganism data storage and analysis capability, and availability of computationally intensive analysis methods. Genevestigator V3 is a novel meta-analysis system resulting from new algorithmic and software development using a client/server architecture, large-scale manual curation and quality control of microarray data for several organisms, and curation of pathway data for mouse and Arabidopsis. In addition to improved querying features, Genevestigator V3 provides new tools to analyze the expression of genes in many different contexts, to identify biomarker genes, to cluster genes into expression modules, and to model expression responses in the context of metabolic and regulatory networks. Being a reference expression database with user-friendly tools, Genevestigator V3 facilitates discovery research and hypothesis validation.",poster,cp39
p2142,e049bd1ef18a0f1cdb45477d957393cf9ef41c6d,c10,Big Data,The UCSC Genome Browser Database,"The University of California Santa Cruz (UCSC) Genome Browser Database is an up to date source for genome sequence data integrated with a large collection of related annotations. The database is optimized to support fast interactive performance with the web-based UCSC Genome Browser, a tool built on top of the database for rapid visualization and querying of the data at many levels. The annotations for a given genome are displayed in the browser as a series of tracks aligned with the genomic sequence. Sequence data and annotations may also be viewed in a text-based tabular format or downloaded as tab-delimited flat files. The Genome Browser Database, browsing tools and downloadable data files can all be found on the UCSC Genome Bioinformatics website (http://genome.ucsc.edu), which also contains links to documentation and related technical information.",poster,cp10
p2143,66470cf9df2f932f80094a309abcc14bcc1b9373,j102,Nucleic Acids Research,2016 update of the PRIDE database and its related tools,"The PRoteomics IDEntifications (PRIDE) database is one of the world-leading data repositories of mass spectrometry (MS)-based proteomics data. Since the beginning of 2014, PRIDE Archive (http://www.ebi.ac.uk/pride/archive/) is the new PRIDE archival system, replacing the original PRIDE database. Here we summarize the developments in PRIDE resources and related tools since the previous update manuscript in the Database Issue in 2013. PRIDE Archive constitutes a complete redevelopment of the original PRIDE, comprising a new storage backend, data submission system and web interface, among other components. PRIDE Archive supports the most-widely used PSI (Proteomics Standards Initiative) data standard formats (mzML and mzIdentML) and implements the data requirements and guidelines of the ProteomeXchange Consortium. The wide adoption of ProteomeXchange within the community has triggered an unprecedented increase in the number of submitted data sets (around 150 data sets per month). We outline some statistics on the current PRIDE Archive data contents. We also report on the status of the PRIDE related stand-alone tools: PRIDE Inspector, PRIDE Converter 2 and the ProteomeXchange submission tool. Finally, we will give a brief update on the resources under development ‘PRIDE Cluster’ and ‘PRIDE Proteomes’, which provide a complementary view and quality-scored information of the peptide and protein identification data available in PRIDE Archive.",fullPaper,jv102
p2144,537ab55f4bdfcbe83ab68b2032d83ae9c7d55d31,c30,IEEE Aerospace Conference,The Transporter Classification Database (TCDB): recent advances,"The Transporter Classification Database (TCDB; http://www.tcdb.org) is a freely accessible reference database for transport protein research, which provides structural, functional, mechanistic, evolutionary and disease/medical information about transporters from organisms of all types. TCDB is the only transport protein classification database adopted by the International Union of Biochemistry and Molecular Biology (IUBMB). It consists of more than 10 000 non-redundant transport systems with more than 11 000 reference citations, classified into over 1000 transporter families. Transporters in TCDB can be single or multi-component systems, categorized in a functional/phylogenetic hierarchical system of classes, subclasses, families, subfamilies and transport systems. TCDB also includes updated software designed to analyze the distinctive features of transport proteins, extending its usefulness. Here we present a comprehensive update of the database contents and features and summarize recent discoveries recorded in TCDB.",poster,cp30
p2145,5995a2b4645bd73cd4b19d3c44d6a96382d5c880,c89,Conference on Uncertainty in Artificial Intelligence,Saccharomyces Genome Database: the genomics resource of budding yeast,"The Saccharomyces Genome Database (SGD, http://www.yeastgenome.org) is the community resource for the budding yeast Saccharomyces cerevisiae. The SGD project provides the highest-quality manually curated information from peer-reviewed literature. The experimental results reported in the literature are extracted and integrated within a well-developed database. These data are combined with quality high-throughput results and provided through Locus Summary pages, a powerful query engine and rich genome browser. The acquisition, integration and retrieval of these data allow SGD to facilitate experimental design and analysis by providing an encyclopedia of the yeast genome, its chromosomal features, their functions and interactions. Public access to these data is provided to researchers and educators via web pages designed for optimal ease of use.",poster,cp89
p2146,f49dfde9529b07f5a4edb1ebda1d3b0e6aabab44,c67,Enterprise Application Integration,The UCSC Genome Browser database: 2015 update,"Launched in 2001 to showcase the draft human genome assembly, the UCSC Genome Browser database (http://genome.ucsc.edu) and associated tools continue to grow, providing a comprehensive resource of genome assemblies and annotations to scientists and students worldwide. Highlights of the past year include the release of a browser for the first new human genome reference assembly in 4 years in December 2013 (GRCh38, UCSC hg38), a watershed comparative genomics annotation (100-species multiple alignment and conservation) and a novel distribution mechanism for the browser (GBiB: Genome Browser in a Box). We created browsers for new species (Chinese hamster, elephant shark, minke whale), ‘mined the web’ for DNA sequences and expanded the browser display with stacked color graphs and region highlighting. As our user community increasingly adopts the UCSC track hub and assembly hub representations for sharing large-scale genomic annotation data sets and genome sequencing projects, our menu of public data hubs has tripled.",poster,cp67
p2147,2fb5f8ec6ac6a8ef4bcf6f6add2526a769eee3a2,c83,International Conference on Computer Graphics and Interactive Techniques,An Overview of the Global Historical Climatology Network-Daily Database,"AbstractA database is described that has been designed to fulfill the need for daily climate data over global land areas. The dataset, known as Global Historical Climatology Network (GHCN)-Daily, was developed for a wide variety of potential applications, including climate analysis and monitoring studies that require data at a daily time resolution (e.g., assessments of the frequency of heavy rainfall, heat wave duration, etc.). The dataset contains records from over 80 000 stations in 180 countries and territories, and its processing system produces the official archive for U.S. daily data. Variables commonly include maximum and minimum temperature, total daily precipitation, snowfall, and snow depth; however, about two-thirds of the stations report precipitation only. Quality assurance checks are routinely applied to the full dataset, but the data are not homogenized to account for artifacts associated with the various eras in reporting practice at any particular station (i.e., for changes in systematic...",poster,cp83
p2148,2de0a40e9a5d4f1feb07d61af5a5d87a069653f0,j1,IEEE Transactions on Knowledge and Data Engineering,Data Mining: An Overview from a Database Perspective,"Mining information and knowledge from large databases has been recognized by many researchers as a key research topic in database systems and machine learning, and by many industrial companies as an important area with an opportunity of major revenues. Researchers in many different fields have shown great interest in data mining. Several emerging applications in information-providing services, such as data warehousing and online services over the Internet, also call for various data mining techniques to better understand user behavior, to improve the service provided and to increase business opportunities. In response to such a demand, this article provides a survey, from a database researcher's point of view, on the data mining techniques developed recently. A classification of the available data mining techniques is provided and a comparative study of such techniques is presented.",fullPaper,jv1
p2149,87752bbb228c597e12cc7e86d9dba4539a04769e,c20,Research Conference of the South African Institute of Computer Scientists and Information Technologists,An Overview of the China Meteorological Administration Tropical Cyclone Database,"AbstractThe China Meteorological Administration (CMA)’s tropical cyclone (TC) database includes not only the best-track dataset but also TC-induced wind and precipitation data. This article summarizes the characteristics and key technical details of the CMA TC database. In addition to the best-track data, other phenomena that occurred with the TCs are also recorded in the dataset, such as the subcenters, extratropical transitions, outer-range severe winds associated with TCs over the South China Sea, and coastal severe winds associated with TCs landfalling in China. These data provide additional information for researchers. The TC-induced wind and precipitation data, which map the distribution of severe wind and rainfall, are also helpful for investigating the impacts of TCs. The study also considers the changing reliability of the various data sources used since the database was created and the potential causes of temporal and spatial inhomogeneities within the datasets. Because of the greater number of ...",poster,cp20
p2150,a7a581f7f052570fa099dcb887b9934617760791,j235,Proteomics,Comet: An open‐source MS/MS sequence database search tool,"Proteomics research routinely involves identifying peptides and proteins via MS/MS sequence database search. Thus the database search engine is an integral tool in many proteomics research groups. Here, we introduce the Comet search engine to the existing landscape of commercial and open‐source database search tools. Comet is open source, freely available, and based on one of the original sequence database search tools that has been widely used for many years.",fullPaper,jv235
p2151,84b38eeaeee5fa3118af89283291e2a413f9fa5f,c105,Biometrics and Identity Management,Database resources of the National Center for Biotechnology Information,"The National Center for Biotechnology Information (NCBI) provides a large suite of online resources for biological information and data, including the GenBank® nucleic acid sequence database and the PubMed database of citations and abstracts for published life science journals. Additional NCBI resources focus on literature (PubMed Central (PMC), Bookshelf and PubReader), health (ClinVar, dbGaP, dbMHC, the Genetic Testing Registry, HIV-1/Human Protein Interaction Database and MedGen), genomes (BioProject, Assembly, Genome, BioSample, dbSNP, dbVar, Epigenomics, the Map Viewer, Nucleotide, Probe, RefSeq, Sequence Read Archive, the Taxonomy Browser and the Trace Archive), genes (Gene, Gene Expression Omnibus (GEO), HomoloGene, PopSet and UniGene), proteins (Protein, the Conserved Domain Database (CDD), COBALT, Conserved Domain Architecture Retrieval Tool (CDART), the Molecular Modeling Database (MMDB) and Protein Clusters) and chemicals (Biosystems and the PubChem suite of small molecule databases). The Entrez system provides search and retrieval operations for most of these databases. Augmenting many of the web applications are custom implementations of the BLAST program optimized to search specialized datasets. All of these resources can be accessed through the NCBI home page at www.ncbi.nlm.nih.gov.",poster,cp105
p2152,34284e452a59db6a3d0a867cb01626151a7781b4,c61,Jahrestagung der Gesellschaft für Informatik,The Dartmouth Stellar Evolution Database,"The ever-expanding depth and quality of photometric and spectroscopic observations of stellar populations increase the need for theoretical models in regions of age-composition parameter space that are largely unexplored at present. Stellar evolution models that employ the most advanced physics and cover a wide range of compositions are needed to extract the most information from current observations of both resolved and unresolved stellar populations. The Dartmouth Stellar Evolution Database is a collection of stellar evolution tracks and isochrones that spans a range of [Fe/H] from –2.5 to +0.5, [α/Fe] from –0.2 to +0.8 (for [Fe/H] ⩽ 0) or +0.2 (for [Fe/H] > 0), and initial He mass fractions from Y = 0.245 to 0.40. Stellar evolution tracks were computed for masses between 0.1 and 4 M☉, allowing isochrones to be generated for ages as young as 250 Myr. For the range in masses where the core He flash occurs, separate He-burning tracks were computed starting from the zero age horizontal branch. The tracks and isochrones have been transformed to the observational plane in a variety of photometric systems including standard UBV(RI)C, Stromgren uvby, SDSS ugriz, 2MASS JHKs, and HST ACS/WFC and WFPC2. The Dartmouth Stellar Evolution Database is accessible through a Web site at http://stellar.dartmouth.edu/~models/ where all tracks, isochrones, and additional files can be downloaded.",poster,cp61
p2153,88fc814fc568fdfa4704ba2326eb623e95621bec,c69,International Conference on Parallel Processing,Notes on CEPII’s Distances Measures: The GeoDist Database,"GeoDist makes available the exhaustive set of gravity variables used in Mayer and Zignago (2005). GeoDist provides several geographical variables, in particular bilateral distances measured using citylevel data to assess the geographic distribution of population inside each nation. We have calculated different measures of bilateral distances available for most countries across the world (225 countries in the current version of the database). For most of them, different calculations of “intra-national distances” are also available. The GeoDist webpage provides two distinct files: a country-specific one (geo_cepii)and a dyadic one (dist_cepii) including a set of different distance and common dummy variables used in gravity equations to identify particular links between countries such as colonial past, common languages, contiguity. We try to improve upon the existing similar datasets in terms of geographical coverage, quality of measurement and number of variables provided.",poster,cp69
p2154,d1de096375c58d18e167b0f2324f0288750b7411,c7,European Conference on Modelling and Simulation,NGA-West2 Database,"The NGA-West2 project database expands on its predecessor to include worldwide ground motion data recorded from shallow crustal earthquakes in active tectonic regimes post-2000 and a set of small-to-moderate-magnitude earthquakes in California between 1998 and 2011. The database includes 21,336 (mostly) three-component records from 599 events. The parameter space covered by the database is M 3.0 to M 7.9, closest distance of 0.05 to 1,533 km, and site time-averaged shear-wave velocity in the top 30 m of VS30 = 94 m/s to 2,100 m/s (although data becomes sparse for distances >400 km and VS30 > 1,200 m/s or <150 m/s). The database includes uniformly processed time series and response spectral ordinates for 111 periods ranging from 0.01 s to 20 s at 11 damping ratios. Ground motions and metadata for source, path, and site conditions were subject to quality checks by ground motion prediction equation developers and topical working groups.",poster,cp7
p2155,ef12383f516840ec1ec998cd5921dfc6e197c9b2,c98,North American Chapter of the Association for Computational Linguistics,PPDB: The Paraphrase Database,"We present the 1.0 release of our paraphrase database, PPDB. Its English portion, PPDB:Eng, contains over 220 million paraphrase pairs, consisting of 73 million phrasal and 8 million lexical paraphrases, as well as 140 million paraphrase patterns, which capture many meaning-preserving syntactic transformations. The paraphrases are extracted from bilingual parallel corpora totaling over 100 million sentence pairs and over 2 billion English words. We also release PPDB:Spa, a collection of 196 million Spanish paraphrases. Each paraphrase pair in PPDB contains a set of associated scores, including paraphrase probabilities derived from the bitext data and a variety of monolingual distributional similarity scores computed from the Google n-grams and the Annotated Gigaword corpus. Our release includes pruning tools that allow users to determine their own precision/recall tradeoff.",fullPaper,cp98
p2156,429c25227c2225447fd3bf3d17582a19671cc872,c43,ACM Symposium on Applied Computing,The PLANTS Database,Abstract content,poster,cp43
p2157,4e802642a64727baad108ca9b01628d8d908308b,c60,IEEE International Conference on Software Engineering and Formal Methods,The MetaCyc database of metabolic pathways and enzymes and the BioCyc collection of pathway/genome databases,"The MetaCyc database (MetaCyc.org) is a freely accessible comprehensive database describing metabolic pathways and enzymes from all domains of life. The majority of MetaCyc pathways are small-molecule metabolic pathways that have been experimentally determined. MetaCyc contains more than 2400 pathways derived from >46 000 publications, and is the largest curated collection of metabolic pathways. BioCyc (BioCyc.org) is a collection of 5700 organism-specific Pathway/Genome Databases (PGDBs), each containing the full genome and predicted metabolic network of one organism, including metabolites, enzymes, reactions, metabolic pathways, predicted operons, transport systems, and pathway-hole fillers. The BioCyc website offers a variety of tools for querying and analyzing PGDBs, including Omics Viewers and tools for comparative analysis. This article provides an update of new developments in MetaCyc and BioCyc during the last two years, including addition of Gibbs free energy values for compounds and reactions; redesign of the primary gene/protein page; addition of a tool for creating diagrams containing multiple linked pathways; several new search capabilities, including searching for genes based on sequence patterns, searching for databases based on an organism's phenotypes, and a cross-organism search; and a metabolite identifier translation service.",poster,cp60
p2158,10b40befe5942e997f88ab40bac1acd931147893,c84,The Web Conference,"PATRIC, the bacterial bioinformatics database and analysis resource","The Pathosystems Resource Integration Center (PATRIC) is the all-bacterial Bioinformatics Resource Center (BRC) (http://www.patricbrc.org). A joint effort by two of the original National Institute of Allergy and Infectious Diseases-funded BRCs, PATRIC provides researchers with an online resource that stores and integrates a variety of data types [e.g. genomics, transcriptomics, protein–protein interactions (PPIs), three-dimensional protein structures and sequence typing data] and associated metadata. Datatypes are summarized for individual genomes and across taxonomic levels. All genomes in PATRIC, currently more than 10 000, are consistently annotated using RAST, the Rapid Annotations using Subsystems Technology. Summaries of different data types are also provided for individual genes, where comparisons of different annotations are available, and also include available transcriptomic data. PATRIC provides a variety of ways for researchers to find data of interest and a private workspace where they can store both genomic and gene associations, and their own private data. Both private and public data can be analyzed together using a suite of tools to perform comparative genomic or transcriptomic analysis. PATRIC also includes integrated information related to disease and PPIs. All the data and integrated analysis and visualization tools are freely available. This manuscript describes updates to the PATRIC since its initial report in the 2007 NAR Database Issue.",poster,cp84
p2159,3b3ad5eaddd5a970519b8c9b4097816fe374e8ec,c5,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,The Proteomics Identifications (PRIDE) database and associated tools: status in 2013,"The PRoteomics IDEntifications (PRIDE, http://www.ebi.ac.uk/pride) database at the European Bioinformatics Institute is one of the most prominent data repositories of mass spectrometry (MS)-based proteomics data. Here, we summarize recent developments in the PRIDE database and related tools. First, we provide up-to-date statistics in data content, splitting the figures by groups of organisms and species, including peptide and protein identifications, and post-translational modifications. We then describe the tools that are part of the PRIDE submission pipeline, especially the recently developed PRIDE Converter 2 (new submission tool) and PRIDE Inspector (visualization and analysis tool). We also give an update about the integration of PRIDE with other MS proteomics resources in the context of the ProteomeXchange consortium. Finally, we briefly review the quality control efforts that are ongoing at present and outline our future plans.",poster,cp5
p2160,66fb37fee3250f68d598e45b3097b50045edc499,j314,Acta Crystallographica Section B Structural Science,New software for searching the Cambridge Structural Database and visualizing crystal structures.,"Two new programs have been developed for searching the Cambridge Structural Database (CSD) and visualizing database entries: ConQuest and Mercury. The former is a new search interface to the CSD, the latter is a high-performance crystal-structure visualizer with extensive facilities for exploring networks of intermolecular contacts. Particular emphasis has been placed on making the programs as intuitive as possible. Both ConQuest and Mercury run under Windows and various types of Unix, including Linux.",fullPaper,jv314
p2161,af1468b0878e05b6a757de97ec342aa8d5aef916,c62,International Conference on Software Reuse,FEEDBACK ON A PUBLICLY DISTRIBUTED IMAGE DATABASE: THE MESSIDOR DATABASE,"The Messidor database, which contains hundreds of eye fundus images, has been publicly distributed since 2008. It was created by the Messidor project in order to evaluate automatic lesion segmentation and diabetic retinopathy grading methods. Designing, producing and maintaining such a database entails significant costs. By publicly sharing it, one hopes to bring a valuable resource to the public research community. However, the real interest and benefit of the research community is not easy to quantify. We analyse here the feedback on the Messidor database, after more than 6 years of diffusion. This analysis should apply to other similar research databases.",poster,cp62
p2162,3a2b869533620d2dfa076522321983c537b3c175,c53,International Conference on Software Engineering and Knowledge Engineering,Gene Ontology Consortium: The Gene Ontology (GO) database and informatics resource,"The Gene Ontology (GO) project (http://www. geneontology.org/) provides structured, controlled vocabularies and classifications that cover several domains of molecular and cellular biology and are freely available for community use in the annotation of genes, gene products and sequences. Many model organism databases and genome annotation groups use the GO and contribute their annotation sets to the GO resource. The GO database integrates the vocabularies and contributed annotations and provides full access to this information in several formats. Members of the GO Consortium continually work collectively, involving outside experts as needed, to expand and update the GO vocabularies. The GO Web resource also provides access to extensive documentation about the GO project and links to applications that use GO data for functional analyses.",poster,cp53
p2163,dc0e51b870dba980b79a3c34a044e491e7cfd5c4,c24,Decision Support Systems,CHIANTI - an atomic database for emission lines - I. Wavelengths greater than 50 Å,"CHIANTI consists of a critically evaluated set of atomic data and transition probabilities necessary to calculate the emission line spectrum of astrophysical plasmas. The data consist of atomic energy levels, atomic radiative data such as wavelengths, weighted oscillator strengths and A values, and electron collisional excitation rates. A set of programs that use these data to calculate the spectrum in a desired wavelength range as a function of temperature and density is also provided. A suite of programs has been developed to carry out plasma diagnostics of astrophysical plasmas. The state-of-the-art contents of the CHIANTI database will be described and some of the most important results obtained from the use of the CHIANTI database will be reviewed.",poster,cp24
p2164,3a60678ad2b862fa7c27b11f04c93c010cc6c430,j350,IEEE Transactions on Affective Computing,A Multimodal Database for Affect Recognition and Implicit Tagging,"MAHNOB-HCI is a multimodal database recorded in response to affective stimuli with the goal of emotion recognition and implicit tagging research. A multimodal setup was arranged for synchronized recording of face videos, audio signals, eye gaze data, and peripheral/central nervous system physiological signals. Twenty-seven participants from both genders and different cultural backgrounds participated in two experiments. In the first experiment, they watched 20 emotional videos and self-reported their felt emotions using arousal, valence, dominance, and predictability as well as emotional keywords. In the second experiment, short videos and images were shown once without any tag and then with correct or incorrect tags. Agreement or disagreement with the displayed tags was assessed by the participants. The recorded videos and bodily responses were segmented and stored in a database. The database is made available to the academic community via a web-based system. The collected data were analyzed and single modality and modality fusion results for both emotion recognition and implicit tagging experiments are reported. These results show the potential uses of the recorded modalities and the significance of the emotion elicitation protocol.",fullPaper,jv350
p2165,b89f0e4f43570688dd983813c9a3efa2fa7e7ebc,c0,International Conference on Human Factors in Computing Systems,"The CMU Pose, Illumination, and Expression (PIE) database","Between October 2000 and December 2000, we collected a database of over 40,000 facial images of 68 people. Using the CMU (Carnegie Mellon University) 3D Room, we imaged each person across 13 different poses, under 43 different illumination conditions, and with four different expressions. We call this database the CMU Pose, Illumination and Expression (PIE) database. In this paper, we describe the imaging hardware, the collection procedure, the organization of the database, several potential uses of the database, and how to obtain the database.",poster,cp0
p2166,b02d2cf90e5b06e3278e23e7984c29b0307c5ef3,c35,EUROMICRO Conference on Software Engineering and Advanced Applications,Principles of database and knowledge- base systems,Abstract content,poster,cp35
p2167,cf4dc5ff06fc8c17b05b399a98c164c60a834e09,j40,Social Science Research Network,Systemic Banking Crises: A New Database,"This paper presents a new database on the timing of systemic banking crises and policy responses to resolve them. The database covers the universe of systemic banking crises for the period 1970-2007, with detailed data on crisis containment and resolution policies for 42 crisis episodes, and also includes data on the timing of currency crises and sovereign debt crises. The database extends and builds on the Caprio, Klingebiel, Laeven, and Noguera (2005) banking crisis database, and is the most complete and detailed database on banking crises to date.",fullPaper,jv40
p2168,9162a7e9434022c2ed6f249b129d6e50b90eb1a3,j365,Biocontrol News and Information,Biological control of insect pests by insect parasitoids and predators: the BIOCAT database.,"The structure of the BIOCAT database, which contains records of the introductions of insect natural enemies for the control of insect pests worldwide, and is now available online, is explained. It is a useful summary of biological control effort and a guide to factors which may influence the success of introduction programmes, but is not detailed enough for making firm predictions.",fullPaper,jv365
p2169,24019050c30b7e5bf1be28e48b8cb5278c4286fd,c99,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,PH2 - A dermoscopic image database for research and benchmarking,"The increasing incidence of melanoma has recently promoted the development of computer-aided diagnosis systems for the classification of dermoscopic images. Unfortunately, the performance of such systems cannot be compared since they are evaluated in different sets of images by their authors and there are no public databases available to perform a fair evaluation of multiple systems. In this paper, a dermoscopic image database, called PH2, is presented. The PH2 database includes the manual segmentation, the clinical diagnosis, and the identification of several dermoscopic structures, performed by expert dermatologists, in a set of 200 dermoscopic images. The PH2 database will be made freely available for research and benchmarking purposes.",fullPaper,cp99
p2170,928e4178fca984b1d49da75c5120e05cff7a3fa9,c24,Decision Support Systems,Development of a 2001 National land-cover database for the United States,"Multi-Resolution Land Characterization 2001 (MRLC 2001) is a second-generation Federal consortium designed to create an updated pool of nation-wide Landsat 5 and 7 imagery and derive a second-generation National Land Cover Database (NLCD 2001). The objectives of this multi-layer, multi-source database are two fold: first, to provide consistent land cover for all 50 States, and second, to provide a data framework which allows flexibility in developing and applying each independent data component to a wide variety of other applications. Components in the database include the following: (1) normalized imagery for three time periods per path/row, (2) ancillary data, including a 30 m Digital Elevation Model (DEM) derived into slope, aspect and slope position, (3) perpixel estimates of percent imperviousness and percent tree canopy (4) 29 classes of land cover data derived from the imagery, ancillary data, and derivatives, (5) classification rules, confidence estimates, and metadata from the land cover classification. This database is now being developed using a Mapping Zone approach, with 66 Zones in the continental United States and 23 Zones in Alaska. Results from three initial mapping Zones show single-pixel land cover accuracies ranging from 73 to 77 percent, imperviousness accuracies ranging from 83 to 91 percent, tree canopy accuracies ranging from 78 to 93 percent, and an estimated 50 percent increase in mapping efficiency over previous methods. The database has now entered the production phase and is being created using extensive partnering in the Federal government with planned completion by 2006.",poster,cp24
p2171,d594c19d91c55a4e7f341fd1d45fc30c5fac0b1a,c34,IEEE Working Conference on Mining Software Repositories,Atlantic Hurricane Database Uncertainty and Presentation of a New Database Format,"Abstract“Best tracks” are National Hurricane Center (NHC) poststorm analyses of the intensity, central pressure, position, and size of Atlantic and eastern North Pacific basin tropical and subtropical cyclones. This paper estimates the uncertainty (average error) for Atlantic basin best track parameters through a survey of the NHC Hurricane Specialists who maintain and update the Atlantic hurricane database. A comparison is then made with a survey conducted over a decade ago to qualitatively assess changes in the uncertainties. Finally, the implications of the uncertainty estimates for NHC analysis and forecast products as well as for the prediction goals of the Hurricane Forecast Improvement Program are discussed.",poster,cp34
p2172,60add4035c2b6d4b9888cb1028c31e6bed793d60,c114,IEEE International Conference on Robotics and Automation,The UCSC Genome Browser database: 2014 update,"The University of California Santa Cruz (UCSC) Genome Browser (http://genome.ucsc.edu) offers online public access to a growing database of genomic sequence and annotations for a large collection of organisms, primarily vertebrates, with an emphasis on the human and mouse genomes. The Browser’s web-based tools provide an integrated environment for visualizing, comparing, analysing and sharing both publicly available and user-generated genomic data sets. As of September 2013, the database contained genomic sequence and a basic set of annotation ‘tracks’ for ∼90 organisms. Significant new annotations include a 60-species multiple alignment conservation track on the mouse, updated UCSC Genes tracks for human and mouse, and several new sets of variation and ENCODE data. New software tools include a Variant Annotation Integrator that returns predicted functional effects of a set of variants uploaded as a custom track, an extension to UCSC Genes that displays haplotype alleles for protein-coding genes and an expansion of data hubs that includes the capability to display remotely hosted user-provided assembly sequence in addition to annotation data. To improve European access, we have added a Genome Browser mirror (http://genome-euro.ucsc.edu) hosted at Bielefeld University in Germany.",poster,cp114
p2173,18867ef6b426e2d9c68cade3aa387948056141ba,c82,Workshop on Interdisciplinary Software Engineering Research,The Gene Ontology (GO) database and informatics resource,"The Gene Ontology (GO) project (http://www. geneontology.org/) provides structured, controlled vocabularies and classiﬁcations that cover several domains of molecular and cellular biology and are freely available for community use in the annotation of genes, gene products and sequences. Many model organism databases and genome annotation groups use the GO and contribute their annotation sets to the GO resource. The GO database inte-grates the vocabularies and contributed annotations and provides full access to this information in several formats. Members of the GO Consortium con-tinually work collectively, involving outside experts as needed, to expand and update the GO vocabularies. The GO Web resource also provides access to extensive documentation about the GO project and links to applications that use GO data for functional analyses.",poster,cp82
p2174,16cf42b0481042514a983e48b9994d60145553f1,c65,Formal Concept Analysis,InterPro: the integrative protein signature database,"The InterPro database (http://www.ebi.ac.uk/interpro/) integrates together predictive models or ‘signatures’ representing protein domains, families and functional sites from multiple, diverse source databases: Gene3D, PANTHER, Pfam, PIRSF, PRINTS, ProDom, PROSITE, SMART, SUPERFAMILY and TIGRFAMs. Integration is performed manually and approximately half of the total ∼58 000 signatures available in the source databases belong to an InterPro entry. Recently, we have started to also display the remaining un-integrated signatures via our web interface. Other developments include the provision of non-signature data, such as structural data, in new XML files on our FTP site, as well as the inclusion of matchless UniProtKB proteins in the existing match XML files. The web interface has been extended and now links out to the ADAN predicted protein–protein interaction database and the SPICE and Dasty viewers. The latest public release (v18.0) covers 79.8% of UniProtKB (v14.1) and consists of 16 549 entries. InterPro data may be accessed either via the web address above, via web services, by downloading files by anonymous FTP or by using the InterProScan search software (http://www.ebi.ac.uk/Tools/InterProScan/).",poster,cp65
p2175,d5837fd1cf0d51e547553c3612928de880a08589,c109,International Conference on Mobile Data Management,JASPAR: an open-access database for eukaryotic transcription factor binding profiles,"The analysis of regulatory regions in genome sequences is strongly based on the detection of potential transcription factor binding sites. The preferred models for representation of transcription factor binding specificity have been termed position-specific scoring matrices. JASPAR is an open-access database of annotated, high-quality, matrix-based transcription factor binding site profiles for multicellular eukaryotes. The profiles were derived exclusively from sets of nucleotide sequences experimentally demonstrated to bind transcription factors. The database is complemented by a web interface for browsing, searching and subset selection, an online sequence analysis utility and a suite of programming tools for genome-wide and comparative genomic analysis of regulatory regions. JASPAR is available at http://jaspar. cgb.ki.se.",poster,cp109
p2176,e3bc4caca9a5115c61281acb99ab9b978edd6387,j366,ACM Transactions on Graphics,The sketchy database,"We present the Sketchy database, the first large-scale collection of sketch-photo pairs. We ask crowd workers to sketch particular photographic objects sampled from 125 categories and acquire 75,471 sketches of 12,500 objects. The Sketchy database gives us fine-grained associations between particular photos and sketches, and we use this to train cross-domain convolutional networks which embed sketches and photographs in a common feature space. We use our database as a benchmark for fine-grained retrieval and show that our learned representation significantly outperforms both hand-crafted features as well as deep features trained for sketch or photo classification. Beyond image retrieval, we believe the Sketchy database opens up new opportunities for sketch and image understanding and synthesis.",fullPaper,jv366
p2177,6b092ee6d04b4fa76a0452cf0696086a7c04644d,c61,Jahrestagung der Gesellschaft für Informatik,"DIP, the Database of Interacting Proteins: a research tool for studying cellular networks of protein interactions","The Database of Interacting Proteins (DIP: http://dip.doe-mbi.ucla.edu) is a database that documents experimentally determined protein-protein interactions. It provides the scientific community with an integrated set of tools for browsing and extracting information about protein interaction networks. As of September 2001, the DIP catalogs approximately 11 000 unique interactions among 5900 proteins from >80 organisms; the vast majority from yeast, Helicobacter pylori and human. Tools have been developed that allow users to analyze, visualize and integrate their own experimental data with the information about protein-protein interactions available in the DIP database.",poster,cp61
p2178,31dbfb575ef802a89a5c08e632f42265bcf30684,c45,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",High Resolution XPS of Organic Polymers: The Scienta ESCA300 Database,Description of the spectrometer x-ray source monochromator electron lens hemispherical analyser multichannel detector sample analysis chamber charge compensation performance on conducting samples performance on insulating samples performance on testing of the spectrometer experimental protocol sample mounting data acquisition correction of binding energy scale for sample charging curve fitting lineshapes shake-up structure valence bands impurities x-ray degradation organization of the database list of polymers and acronyms the database appendix 1 - primary C 1s shifts appendix 2 - secondary C 1s shifts appendix 3.1 - 0 1s binding energies in CHO polymers appendix 3.2 - 0 1s binding energies in other polymers appendix 4 - N 1s binding energies appendix 5 - F 1s binding energies appendix 6 - binding energies and spin-orbit constants for core-line doublets apendix 7 - binding energies of peaks appearing in the valence band region.,poster,cp45
p2179,f8dc73630864b4903f8f57b5c8d7b098eb300e4f,c63,IEEE International Software Metrics Symposium,"Reactome: a database of reactions, pathways and biological processes","Reactome (http://www.reactome.org) is a collaboration among groups at the Ontario Institute for Cancer Research, Cold Spring Harbor Laboratory, New York University School of Medicine and The European Bioinformatics Institute, to develop an open source curated bioinformatics database of human pathways and reactions. Recently, we developed a new web site with improved tools for pathway browsing and data analysis. The Pathway Browser is an Systems Biology Graphical Notation (SBGN)-based visualization system that supports zooming, scrolling and event highlighting. It exploits PSIQUIC web services to overlay our curated pathways with molecular interaction data from the Reactome Functional Interaction Network and external interaction databases such as IntAct, BioGRID, ChEMBL, iRefIndex, MINT and STRING. Our Pathway and Expression Analysis tools enable ID mapping, pathway assignment and overrepresentation analysis of user-supplied data sets. To support pathway annotation and analysis in other species, we continue to make orthology-based inferences of pathways in non-human species, applying Ensembl Compara to identify orthologs of curated human proteins in each of 20 other species. The resulting inferred pathway sets can be browsed and analyzed with our Species Comparison tool. Collaborations are also underway to create manually curated data sets on the Reactome framework for chicken, Drosophila and rice.",poster,cp63
p2180,6dc7b1da99e30592b54f2148c85e8725563011f7,c100,ACM SIGMOD Conference,Storing and querying ordered XML using a relational database system,"XML is quickly becoming the de facto standard for data exchange over the Internet. This is creating a new set of data management requirements involving XML, such as the need to store and query XML documents. Researchers have proposed using relational database systems to satisfy these requirements by devising ways to ""shred"" XML documents into relations, and translate XML queries into SQL queries over these relations. However, a key issue with such an approach, which has largely been ignored in the research literature, is how (and whether) the ordered XML data model can be efficiently supported by the unordered relational data model. This paper shows that XML's ordered data model can indeed be efficiently supported by a relational database system. This is accomplished by encoding order as a data value. We propose three order encoding methods that can be used to represent XML order in the relational data model, and also propose algorithms for translating ordered XPath expressions into SQL using these encoding methods. Finally, we report the results of an experimental study that investigates the performance of the proposed order encoding methods on a workload of ordered XML queries and updates.",fullPaper,cp100
p2181,3a19b36d7f41ba20a507dee2585fd56fa019167a,j274,Nature Genetics,Systematic meta-analyses of Alzheimer disease genetic association studies: the AlzGene database,Abstract content,fullPaper,jv274
p2182,2a9cdd23f68b412fad551eab517c1e1618ff6ec2,c28,International Conference on Collaboration Technologies and Systems,A comparative analysis of methodologies for database schema integration,"One of the fundamental principles of the database approach is that a database allows a nonredundant, unified representation of all data managed in an organization. This is achieved only when methodologies are available to support integration across organizational and application boundaries.
Methodologies for database design usually perform the design activity by separately producing several schemas, representing parts of the application, which are subsequently merged. Database schema integration is the activity of integrating the schemas of existing or proposed databases into a global, unified schema.
The aim of the paper is to provide first a unifying framework for the problem of schema integration, then a comparative review of the work done thus far in this area. Such a framework, with the associated analysis of the existing approaches, provides a basis for identifying strengths and weaknesses of individual methodologies, as well as general guidelines for future improvements and extensions.",poster,cp28
p2183,1d6b1648e82fe01bcfa52488f46133784c29c58b,c114,IEEE International Conference on Robotics and Automation,MEROPS: the peptidase database,"Peptidases, their substrates and inhibitors are of great relevance to biology, medicine and biotechnology. The MEROPS database (http://merops.sanger.ac.uk) aims to fulfil the need for an integrated source of information about these. The database has a hierarchical classification in which homologous sets of peptidases and protein inhibitors are grouped into protein species, which are grouped into families, which are in turn grouped into clans. The classification framework is used for attaching information at each level. An important focus of the database has become distinguishing one peptidase from another through identifying the specificity of the peptidase in terms of where it will cleave substrates and with which inhibitors it will interact. We have collected over 39 000 known cleavage sites in proteins, peptides and synthetic substrates. These allow us to display peptidase specificity and alignments of protein substrates to give an indication of how well a cleavage site is conserved, and thus its probable physiological relevance. While the number of new peptidase families and clans has only grown slowly the number of complete genomes has greatly increased. This has allowed us to add an analysis tool to the relevant species pages to show significant gains and losses of peptidase genes relative to related species.",poster,cp114
p2184,475bbf493d8246031a5152c8005a5c567231307c,j358,Journal of Chemical Information and Modeling,Basis Set Exchange: A Community Database for Computational Sciences,"Basis sets are some of the most important input data for computational models in the chemistry, materials, biology, and other science domains that utilize computational quantum mechanics methods. Providing a shared, Web-accessible environment where researchers can not only download basis sets in their required format but browse the data, contribute new basis sets, and ultimately curate and manage the data as a community will facilitate growth of this resource and encourage sharing both data and knowledge. We describe the Basis Set Exchange (BSE), a Web portal that provides advanced browsing and download capabilities, facilities for contributing basis set data, and an environment that incorporates tools to foster development and interaction of communities. The BSE leverages and enables continued development of the basis set library originally assembled at the Environmental Molecular Sciences Laboratory.",fullPaper,jv358
p2185,ad9a8e6346a1259022abf526670eaf592270e59f,c53,International Conference on Software Engineering and Knowledge Engineering,Mouse Genome Database (MGD)-2017: community knowledge resource for the laboratory mouse,"The Mouse Genome Database (MGD: http://www.informatics.jax.org) is the primary community data resource for the laboratory mouse. It provides a highly integrated and highly curated system offering a comprehensive view of current knowledge about mouse genes, genetic markers and genomic features as well as the associations of those features with sequence, phenotypes, functional and comparative information, and their relationships to human diseases. MGD continues to enhance access to these data, to extend the scope of data content and visualizations, and to provide infrastructure and user support that ensures effective and efficient use of MGD in the advancement of scientific knowledge. Here, we report on recent enhancements made to the resource and new features.",poster,cp53
p2186,36698b7863c7d64cf3fb9d17d0f4df1ff6d286dc,c28,International Conference on Collaboration Technologies and Systems,Measuring Financial Inclusion: The Global Findex Database,"This paper provides the first analysis of the Global Financial Inclusion (Global Findex) Database, a new set of indicators that measure how adults in 148 economies save, borrow, make payments, and manage risk. The data show that 50 percent of adults worldwide have an account at a formal financial institution, though account penetration varies widely across regions, income groups and individual characteristics. In addition, 22 percent of adults report having saved at a formal financial institution in the past 12 months, and 9 percent report having taken out a new loan from a bank, credit union or microfinance institution in the past year. Although half of adults around the world remain unbanked, at least 35 percent of them report barriers to account use that might be addressed by public policy. Among the most commonly reported barriers are high cost, physical distance, and lack of proper documentation, though there are significant differences across regions and individual characteristics.",poster,cp28
p2187,0afa75ad56cc8ca3cfa176f89443e9a70e09434c,j40,Social Science Research Network,Systemic Banking Crises Database: An Update,"We update the widely used banking crises database by Laeven and Valencia (2008, 2010) with new information on recent and ongoing crises, including updated information on policy responses and outcomes (i.e. fiscal costs, output losses, and increases in public debt). We also update our dating of sovereign debt and currency crises. The database includes all systemic banking, currency, and sovereign debt crises during the period 1970-2011. The data show some striking differences in policy responses between advanced and emerging economies as well as many similarities between past and ongoing crises.",fullPaper,jv40
p2188,1fcf323ff79c46b401e7a3d8510f50da17073cb8,c10,Big Data,The UCSC Genome Browser Database: update 2006,"The University of California Santa Cruz Genome Browser Database (GBD) contains sequence and annotation data for the genomes of about a dozen vertebrate species and several major model organisms. Genome annotations typically include assembly data, sequence composition, genes and gene predictions, mRNA and expressed sequence tag evidence, comparative genomics, regulation, expression and variation data. The database is optimized to support fast interactive performance with web tools that provide powerful visualization and querying capabilities for mining the data. The Genome Browser displays a wide variety of annotations at all scales from single nucleotide level up to a full chromosome. The Table Browser provides direct access to the database tables and sequence data, enabling complex queries on genome-wide datasets. The Proteome Browser graphically displays protein properties. The Gene Sorter allows filtering and comparison of genes by several metrics including expression data and several gene properties. BLAT and In Silico PCR search for sequences in entire genomes in seconds. These tools are highly integrated and provide many hyperlinks to other databases and websites. The GBD, browsing tools, downloadable data files and links to documentation and other information can be found at .",poster,cp10
p2189,ecf1848db9f0e81a8a0dbffeb8162772cb6be733,c25,International Conference on Contemporary Computing,The COG database: new developments in phylogenetic classification of proteins from complete genomes,"The database of Clusters of Orthologous Groups of proteins (COGs), which represents an attempt on a phylogenetic classification of the proteins encoded in complete genomes, currently consists of 2791 COGs including 45 350 proteins from 30 genomes of bacteria, archaea and the yeast Saccharomyces cerevisiae (http://www.ncbi.nlm.nih. gov/COG). In addition, a supplement to the COGs is available, in which proteins encoded in the genomes of two multicellular eukaryotes, the nematode Caenorhabditis elegans and the fruit fly Drosophila melanogaster, and shared with bacteria and/or archaea were included. The new features added to the COG database include information pages with structural and functional details on each COG and literature references, improvements of the COGNITOR program that is used to fit new proteins into the COGs, and classification of genomes and COGs constructed by using principal component analysis.",poster,cp25
p2190,6af8b1efee79db7a87abc35b47bfbb7d096311d3,c77,Networks,"NCBI Reference Sequence (RefSeq): a curated non-redundant sequence database of genomes, transcripts and proteins","The National Center for Biotechnology Information (NCBI) Reference Sequence (RefSeq) database (http://www.ncbi.nlm.nih.gov/RefSeq/) provides a non-redundant collection of sequences representing genomic data, transcripts and proteins. Although the goal is to provide a comprehensive dataset representing the complete sequence information for any given species, the database pragmatically includes sequence data that are currently publicly available in the archival databases. The database incorporates data from over 2400 organisms and includes over one million proteins representing significant taxonomic diversity spanning prokaryotes, eukaryotes and viruses. Nucleotide and protein sequences are explicitly linked, and the sequences are linked to other resources including the NCBI Map Viewer and Gene. Sequences are annotated to include coding regions, conserved domains, variation, references, names, database cross-references, and other features using a combined approach of collaboration and other input from the scientific community, automated annotation, propagation from GenBank and curation by NCBI staff.",poster,cp77
p2191,cf6657ee417fb38bed405793e06a3656e2bd1d23,c100,ACM SIGMOD Conference,Access path selection in a relational database management system,"In a high level query and data manipulation language such as SQL, requests are stated non-procedurally, without reference to access paths. This paper describes how System R chooses access paths for both simple (single relation) and complex queries (such as joins), given a user specification of desired data as a boolean expression of predicates. System R is an experimental database management system developed to carry out research on the relational model of data. System R was designed and built by members of the IBM San Jose Research Laboratory.",fullPaper,cp100
p2192,3bde5bea972b91fa5599d5b07f2b45b5a2ff881f,c23,International Conference on Open and Big Data,The Cochrane Database of Systematic Reviews,"A thin film resonator comprising a piezoelectric material and having a controllable or tunable resonant frequency. The resonator is formed on a substrate having a cavity formed therein below the piezoelectric film material. A bending electrode is disposed within the cavity and the application of a voltage between the bending electrode and one of the resonator electrodes, creates an electric field that causes the substrate region to bend. These stresses caused: by the bending are transferred to the thin film resonator, subjecting the piezoelectric film to stresses and thereby changing the resonant properties of the thin film resonator.",poster,cp23
p2193,f780775b85986a2bf1d5849c56f621cb99efef3c,c74,IEEE International Conference on Tools with Artificial Intelligence,The Dfam database of repetitive DNA families,"Repetitive DNA, especially that due to transposable elements (TEs), makes up a large fraction of many genomes. Dfam is an open access database of families of repetitive DNA elements, in which each family is represented by a multiple sequence alignment and a profile hidden Markov model (HMM). The initial release of Dfam, featured in the 2013 NAR Database Issue, contained 1143 families of repetitive elements found in humans, and was used to produce more than 100 Mb of additional annotation of TE-derived regions in the human genome, with improved speed. Here, we describe recent advances, most notably expansion to 4150 total families including a comprehensive set of known repeat families from four new organisms (mouse, zebrafish, fly and nematode). We describe improvements to coverage, and to our methods for identifying and reducing false annotation. We also describe updates to the website interface. The Dfam website has moved to http://dfam.org. Seed alignments, profile HMMs, hit lists and other underlying data are available for download.",poster,cp74
p2194,5637a5e6cf3a858a8adc17efdf714ce21dbe1a2e,c107,British Machine Vision Conference,Mouse Genome Database (MGD)-2018: knowledgebase for the laboratory mouse,"Abstract The Mouse Genome Database (MGD; http://www.informatics.jax.org) is the key community mouse database which supports basic, translational and computational research by providing integrated data on the genetics, genomics, and biology of the laboratory mouse. MGD serves as the source for biological reference data sets related to mouse genes, gene functions, phenotypes and disease models with an increasing emphasis on the association of these data to human biology and disease. We report here on recent enhancements to this resource, including improved access to mouse disease model and human phenotype data and enhanced relationships of mouse models to human disease.",poster,cp107
p2195,0be6f2f0329230040e2a2dbd4b9e321ef33a2c11,c75,International Conference on Machine Learning,The UCSC Genome Browser database: update 2011,"The University of California, Santa Cruz Genome Browser (http://genome.ucsc.edu) offers online access to a database of genomic sequence and annotation data for a wide variety of organisms. The Browser also has many tools for visualizing, comparing and analyzing both publicly available and user-generated genomic data sets, aligning sequences and uploading user data. Among the features released this year are a gene search tool and annotation track drag-reorder functionality as well as support for BAM and BigWig/BigBed file formats. New display enhancements include overlay of multiple wiggle tracks through use of transparent coloring, options for displaying transformed wiggle data, a ‘mean+whiskers’ windowing function for display of wiggle data at high zoom levels, and more color schemes for microarray data. New data highlights include seven new genome assemblies, a Neandertal genome data portal, phenotype and disease association data, a human RNA editing track, and a zebrafish Conservation track. We also describe updates to existing tracks.",poster,cp75
p2196,d57ca29d73272e139c04f118d5c3107dfb964596,c72,Intelligent Systems in Molecular Biology,Survey of graph database models,"Graph database models can be defined as those in which data structures for the schema and instances are modeled as graphs or generalizations of them, and data manipulation is expressed by graph-oriented operations and type constructors. These models took off in the eighties and early nineties alongside object-oriented models. Their influence gradually died out with the emergence of other database models, in particular geographical, spatial, semistructured, and XML. Recently, the need to manage information with graph-like nature has reestablished the relevance of this area. The main objective of this survey is to present the work that has been conducted in the area of graph database modeling, concentrating on data structures, query languages, and integrity constraints.",poster,cp72
p2197,8df1903824290707e09ee754f3c40d90b088d65b,c61,Jahrestagung der Gesellschaft für Informatik,World Database on Protected Areas (WDPA),Abstract content,poster,cp61
p2198,a69e6053a0e717cebfe4bae52df85fe23d39a4aa,c21,Grid Computing Environments,PID: the Pathway Interaction Database,"The Pathway Interaction Database (PID, http://pid.nci.nih.gov) is a freely available collection of curated and peer-reviewed pathways composed of human molecular signaling and regulatory events and key cellular processes. Created in a collaboration between the US National Cancer Institute and Nature Publishing Group, the database serves as a research tool for the cancer research community and others interested in cellular pathways, such as neuroscientists, developmental biologists and immunologists. PID offers a range of search features to facilitate pathway exploration. Users can browse the predefined set of pathways or create interaction network maps centered on a single molecule or cellular process of interest. In addition, the batch query tool allows users to upload long list(s) of molecules, such as those derived from microarray experiments, and either overlay these molecules onto predefined pathways or visualize the complete molecular connectivity map. Users can also download molecule lists, citation lists and complete database content in extensible markup language (XML) and Biological Pathways Exchange (BioPAX) Level 2 format. The database is updated with new pathway content every month and supplemented by specially commissioned articles on the practical uses of other relevant online tools.",poster,cp21
p2199,b43ffeb049adb791a82521eaaf83c367d651da4c,c51,Conference of the Centre for Advanced Studies on Collaborative Research,The BioGRID interaction database: 2013 update,"The Biological General Repository for Interaction Datasets (BioGRID: http//thebiogrid.org) is an open access archive of genetic and protein interactions that are curated from the primary biomedical literature for all major model organism species. As of September 2012, BioGRID houses more than 500 000 manually annotated interactions from more than 30 model organisms. BioGRID maintains complete curation coverage of the literature for the budding yeast Saccharomyces cerevisiae, the fission yeast Schizosaccharomyces pombe and the model plant Arabidopsis thaliana. A number of themed curation projects in areas of biomedical importance are also supported. BioGRID has established collaborations and/or shares data records for the annotation of interactions and phenotypes with most major model organism databases, including Saccharomyces Genome Database, PomBase, WormBase, FlyBase and The Arabidopsis Information Resource. BioGRID also actively engages with the text-mining community to benchmark and deploy automated tools to expedite curation workflows. BioGRID data are freely accessible through both a user-defined interactive interface and in batch downloads in a wide variety of formats, including PSI-MI2.5 and tab-delimited files. BioGRID records can also be interrogated and analyzed with a series of new bioinformatics tools, which include a post-translational modification viewer, a graphical viewer, a REST service and a Cytoscape plugin.",poster,cp51
p2200,8df1903824290707e09ee754f3c40d90b088d65b,c5,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,World Database on Protected Areas (WDPA),Abstract content,poster,cp5
p2201,a69e6053a0e717cebfe4bae52df85fe23d39a4aa,c0,International Conference on Human Factors in Computing Systems,PID: the Pathway Interaction Database,"The Pathway Interaction Database (PID, http://pid.nci.nih.gov) is a freely available collection of curated and peer-reviewed pathways composed of human molecular signaling and regulatory events and key cellular processes. Created in a collaboration between the US National Cancer Institute and Nature Publishing Group, the database serves as a research tool for the cancer research community and others interested in cellular pathways, such as neuroscientists, developmental biologists and immunologists. PID offers a range of search features to facilitate pathway exploration. Users can browse the predefined set of pathways or create interaction network maps centered on a single molecule or cellular process of interest. In addition, the batch query tool allows users to upload long list(s) of molecules, such as those derived from microarray experiments, and either overlay these molecules onto predefined pathways or visualize the complete molecular connectivity map. Users can also download molecule lists, citation lists and complete database content in extensible markup language (XML) and Biological Pathways Exchange (BioPAX) Level 2 format. The database is updated with new pathway content every month and supplemented by specially commissioned articles on the practical uses of other relevant online tools.",poster,cp0
p2202,b43ffeb049adb791a82521eaaf83c367d651da4c,c9,Pacific Symposium on Biocomputing,The BioGRID interaction database: 2013 update,"The Biological General Repository for Interaction Datasets (BioGRID: http//thebiogrid.org) is an open access archive of genetic and protein interactions that are curated from the primary biomedical literature for all major model organism species. As of September 2012, BioGRID houses more than 500 000 manually annotated interactions from more than 30 model organisms. BioGRID maintains complete curation coverage of the literature for the budding yeast Saccharomyces cerevisiae, the fission yeast Schizosaccharomyces pombe and the model plant Arabidopsis thaliana. A number of themed curation projects in areas of biomedical importance are also supported. BioGRID has established collaborations and/or shares data records for the annotation of interactions and phenotypes with most major model organism databases, including Saccharomyces Genome Database, PomBase, WormBase, FlyBase and The Arabidopsis Information Resource. BioGRID also actively engages with the text-mining community to benchmark and deploy automated tools to expedite curation workflows. BioGRID data are freely accessible through both a user-defined interactive interface and in batch downloads in a wide variety of formats, including PSI-MI2.5 and tab-delimited files. BioGRID records can also be interrogated and analyzed with a series of new bioinformatics tools, which include a post-translational modification viewer, a graphical viewer, a REST service and a Cytoscape plugin.",poster,cp9
p2203,fa69e5e6d21650daa286ed936af9c53d1aca00df,c86,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,STRING: a database of predicted functional associations between proteins,"Functional links between proteins can often be inferred from genomic associations between the genes that encode them: groups of genes that are required for the same function tend to show similar species coverage, are often located in close proximity on the genome (in prokaryotes), and tend to be involved in gene-fusion events. The database STRING is a precomputed global resource for the exploration and analysis of these associations. Since the three types of evidence differ conceptually, and the number of predicted interactions is very large, it is essential to be able to assess and compare the significance of individual predictions. Thus, STRING contains a unique scoring-framework based on benchmarks of the different types of associations against a common reference set, integrated in a single confidence score per prediction. The graphical representation of the network of inferred, weighted protein interactions provides a high-level view of functional linkage, facilitating the analysis of modularity in biological processes. STRING is updated continuously, and currently contains 261 033 orthologs in 89 fully sequenced genomes. The database predicts functional interactions at an expected level of accuracy of at least 80% for more than half of the genes; it is online at http://www.bork.embl-heidelberg.de/STRING/.",poster,cp86
p2204,99a4acc4de2097d5ed6c4ec257f284c9dc86b3b8,c50,International Conference on Automated Software Engineering,A major upgrade of the VALD database,"Vienna atomic line database (VALD) is a collection of critically evaluated laboratory parameters for individual atomic transitions, complemented by theoretical calculations. VALD is actively used by astronomers for stellar spectroscopic studies—model atmosphere calculations, atmospheric parameter determinations, abundance analysis etc. The two first VALD releases contained parameters for atomic transitions only. In a major upgrade of VALD—VALD3, publically available from spring 2014, atomic data was complemented with parameters of molecular lines. The diatomic molecules C2, CH, CN, CO, OH, MgH, SiH, TiO are now included. For each transition VALD provides species name, wavelength, energy, quantum number J and Landé-factor of the lower and upper levels, radiative, Stark and van der Waals damping factors and a full description of electronic configurarion and term information of both levels. Compared to the previous versions we have revised and verify all of the existing data and added new measurements and calculations for transitions in the range between 20 Å and 200 microns. All transitions were complemented with term designations in a consistent way and electron configurations when available. All data were checked for consistency: listed wavelength versus Ritz, selection rules etc. A new bibliographic system keeps track of literature references for each parameter in a given transition throughout the merging process so that every selected data entry can be traced to the original source. The query language and the extraction tools can now handle various units, vacuum and air wavelengths. In the upgrade process we had an intensive interaction with data producers, which was very helpful for improving the quality of the VALD content.",poster,cp50
p2205,756686211e7d253ac7abb8cb9e290141880f0889,j221,Plant and Cell Physiology,Rice Annotation Project Database (RAP-DB): An Integrative and Interactive Database for Rice Genomics,"The Rice Annotation Project Database (RAP-DB, http://rapdb.dna.affrc.go.jp/) has been providing a comprehensive set of gene annotations for the genome sequence of rice, Oryza sativa (japonica group) cv. Nipponbare. Since the first release in 2005, RAP-DB has been updated several times along with the genome assembly updates. Here, we present our newest RAP-DB based on the latest genome assembly, Os-Nipponbare-Reference-IRGSP-1.0 (IRGSP-1.0), which was released in 2011. We detected 37,869 loci by mapping transcript and protein sequences of 150 monocot species. To provide plant researchers with highly reliable and up to date rice gene annotations, we have been incorporating literature-based manually curated data, and 1,626 loci currently incorporate literature-based annotation data, including commonly used gene names or gene symbols. Transcriptional activities are shown at the nucleotide level by mapping RNA-Seq reads derived from 27 samples. We also mapped the Illumina reads of a Japanese leading japonica cultivar, Koshihikari, and a Chinese indica cultivar, Guangluai-4, to the genome and show alignments together with the single nucleotide polymorphisms (SNPs) and gene functional annotations through a newly developed browser, Short-Read Assembly Browser (S-RAB). We have developed two satellite databases, Plant Gene Family Database (PGFD) and Integrative Database of Cereal Gene Phylogeny (IDCGP), which display gene family and homologous gene relationships among diverse plant species. RAP-DB and the satellite databases offer simple and user-friendly web interfaces, enabling plant and genome researchers to access the data easily and facilitating a broad range of plant research topics.",fullPaper,jv221
p2206,6787a08978444d97d6860359ba24b5a735d492e2,c107,British Machine Vision Conference,RegNetwork: an integrated database of transcriptional and post-transcriptional regulatory networks in human and mouse,"Transcriptional and post-transcriptional regulation of gene expression is of fundamental importance to numerous biological processes. Nowadays, an increasing amount of gene regulatory relationships have been documented in various databases and literature. However, to more efficiently exploit such knowledge for biomedical research and applications, it is necessary to construct a genome-wide regulatory network database to integrate the information on gene regulatory relationships that are widely scattered in many different places. Therefore, in this work, we build a knowledge-based database, named ‘RegNetwork’, of gene regulatory networks for human and mouse by collecting and integrating the documented regulatory interactions among transcription factors (TFs), microRNAs (miRNAs) and target genes from 25 selected databases. Moreover, we also inferred and incorporated potential regulatory relationships based on transcription factor binding site (TFBS) motifs into RegNetwork. As a result, RegNetwork contains a comprehensive set of experimentally observed or predicted transcriptional and post-transcriptional regulatory relationships, and the database framework is flexibly designed for potential extensions to include gene regulatory networks for other organisms in the future. Based on RegNetwork, we characterized the statistical and topological properties of genome-wide regulatory networks for human and mouse, we also extracted and interpreted simple yet important network motifs that involve the interplays between TF-miRNA and their targets. In summary, RegNetwork provides an integrated resource on the prior information for gene regulatory relationships, and it enables us to further investigate context-specific transcriptional and post-transcriptional regulatory interactions based on domain-specific experimental data. Database URL: http://www.regnetworkweb.org",poster,cp107
p2207,1c3588499908417312d172f1b5102e76a90fbb2c,c81,IEEE Annual Symposium on Foundations of Computer Science,"HITEMP, the high-temperature molecular spectroscopic database",Abstract content,poster,cp81
p2208,6d703e6eb1c72241720bafdb42b46b70c3bdd16e,c35,EUROMICRO Conference on Software Engineering and Advanced Applications,MRC Psycholinguistic Database,Abstract content,poster,cp35
p2209,fb1d51c023da037dac928d7991de1b77ade22728,j367,Therapeutic Drug Monitoring,METLIN: A Metabolite Mass Spectral Database,"Endogenous metabolites have gained increasing interest over the past 5 years largely for their implications in diagnostic and pharmaceutical biomarker discovery. METLIN (http://metlin.scripps.edu), a freely accessible web-based data repository, has been developed to assist in a broad array of metabolite research and to facilitate metabolite identification through mass analysis. METLIN includes an annotated list of known metabolite structural information that is easily cross-correlated with its catalogue of high-resolution Fourier transform mass spectrometry (FTMS) spectra, tandem mass spectrometry (MS/MS) spectra, and LC/MS data.",fullPaper,jv367
p2210,b1c6b16fc6184625c42e903af398a5bd2c845bcb,c11,Hawaii International Conference on System Sciences,Introduction to Database Systems,Preface About the Authors 1 What's in a Database? 2 Relational Model 3 Relational Calculus 4 Relational Algebra 5 SQL 6 SQL and Programming Languages 7 Entity-Relationship Model 8 Normalisation 9 Conclusion References Index,poster,cp11
p2211,fd2d9588818d4bd6d5cc59a26642c3d3c05ab915,c33,International Conference on Agile Software Development,IT’IS Database for Thermal and Electromagnetic Parameters of Biological Tissues,Abstract content,poster,cp33
p2212,854b86751d0d02bf205c6fb4bf2f2d13804430ee,j368,Journal of Human Genetics,"Human genetic variation database, a reference database of genetic variations in the Japanese population",Abstract content,fullPaper,jv368
p2213,5d0b1c9e2e24b38489c8021261217b023e42a652,c18,Conference on Innovative Data Systems Research,OPM database and PPM web server: resources for positioning of proteins in membranes,"The Orientations of Proteins in Membranes (OPM) database is a curated web resource that provides spatial positions of membrane-bound peptides and proteins of known three-dimensional structure in the lipid bilayer, together with their structural classification, topology and intracellular localization. OPM currently contains more than 1200 transmembrane and peripheral proteins and peptides from approximately 350 organisms that represent approximately 3800 Protein Data Bank entries. Proteins are classified into classes, superfamilies and families and assigned to 21 distinct membrane types. Spatial positions of proteins with respect to the lipid bilayer are optimized by the PPM 2.0 method that accounts for the hydrophobic, hydrogen bonding and electrostatic interactions of the proteins with the anisotropic water-lipid environment described by the dielectric constant and hydrogen-bonding profiles. The OPM database is freely accessible at http://opm.phar.umich.edu. Data can be sorted, searched or retrieved using the hierarchical classification, source organism, localization in different types of membranes. The database offers downloadable coordinates of proteins and peptides with membrane boundaries. A gallery of protein images and several visualization tools are provided. The database is supplemented by the PPM server (http://opm.phar.umich.edu/server.php) which can be used for calculating spatial positions in membranes of newly determined proteins structures or theoretical models.",poster,cp18
p2214,7bbe0235f583b27f96c0f288043876f86795d6c2,c94,Vision,AVA: A large-scale database for aesthetic visual analysis,"With the ever-expanding volume of visual content available, the ability to organize and navigate such content by aesthetic preference is becoming increasingly important. While still in its nascent stage, research into computational models of aesthetic preference already shows great potential. However, to advance research, realistic, diverse and challenging databases are needed. To this end, we introduce a new large-scale database for conducting Aesthetic Visual Analysis: AVA. It contains over 250,000 images along with a rich variety of meta-data including a large number of aesthetic scores for each image, semantic labels for over 60 categories as well as labels related to photographic style. We show the advantages of AVA with respect to existing databases in terms of scale, diversity, and heterogeneity of annotations. We then describe several key insights into aesthetic preference afforded by AVA. Finally, we demonstrate, through three applications, how the large scale of AVA can be leveraged to improve performance on existing preference tasks.",poster,cp94
p2215,448752b56fe4b2fc8fb15f22d9430c17aa306392,c67,Enterprise Application Integration,FEEDBACK ON A PUBLICLY DISTRIBUTED IMAGE DATABASE: THE MESSIDOR DATABASE,"The Messidor database, which contains hundreds of eye fundus images, has been publicly distributed since 2008. It was created by the Messidor project in order to evaluate automatic lesion segmentation and diabetic retinopathy grading methods. Designing, producing and maintaining such a database entails signiﬁcant costs. By publicly sharing it, one hopes to bring a valuable resource to the public research community. However, the real interest and beneﬁt of the research community is not easy to quantify. We analyse here the feedback on the Messidor database, after more than 6 years of diffusion. This analysis should apply to other similar research databases.",poster,cp67
p2216,1e9e0538ae54be22a515430199dab0befb9e29cb,c69,International Conference on Parallel Processing,Database: The Journal of Biological Databases and Curation,"Evolution provides the unifying framework with which to understand biology. The coherent investigation of genic and genomic data often requires comparative genomics analyses based on whole-genome alignments, sets of homologous genes and other relevant datasets in order to evaluate and answer evolutionary-related questions. However, the complexity and computational requirements of producing such data are substantial: this has led to only a small number of reference resources that are used for most comparative analyses. The Ensembl comparative genomics resources are one such reference set that facilitates comprehensive and reproducible analysis of chordate genome data. Ensembl computes pairwise and multiple whole-genome alignments from which large-scale synteny, per-base conservation scores and constrained elements are obtained. Gene alignments are used to define Ensembl Protein Families, GeneTrees and homologies for both protein-coding and non-coding RNA genes. These resources are updated frequently and have a consistent informatics infrastructure and data presentation across all supported species. Specialized web-based visualizations are also available including synteny displays, collapsible gene tree plots, a gene family locator and different alignment views. The Ensembl comparative genomics infrastructure is extensively reused for the analysis of non-vertebrate species by other projects including Ensembl Genomes and Gramene and much of the information here is relevant to these projects. The consistency of the annotation across species and the focus on vertebrates makes Ensembl an ideal system to perform and support vertebrate comparative genomic analyses. We use robust software and pipelines to produce reference comparative data and make it freely available.Database URL: http://www.ensembl.org.",poster,cp69
p2217,644a46f9ce7dc4e7f74aa57a52d87abb4de6ff97,c108,International Conference on Information Integration and Web-based Applications & Services,MODOMICS: a database of RNA modification pathways—2013 update,"MODOMICS is a database of RNA modifications that provides comprehensive information concerning the chemical structures of modified ribonucleosides, their biosynthetic pathways, RNA-modifying enzymes and location of modified residues in RNA sequences. In the current database version, accessible at http://modomics.genesilico.pl, we included new features: a census of human and yeast snoRNAs involved in RNA-guided RNA modification, a new section covering the 5′-end capping process, and a catalogue of ‘building blocks’ for chemical synthesis of a large variety of modified nucleosides. The MODOMICS collections of RNA modifications, RNA-modifying enzymes and modified RNAs have been also updated. A number of newly identified modified ribonucleosides and more than one hundred functionally and structurally characterized proteins from various organisms have been added. In the RNA sequences section, snRNAs and snoRNAs with experimentally mapped modified nucleosides have been added and the current collection of rRNA and tRNA sequences has been substantially enlarged. To facilitate literature searches, each record in MODOMICS has been cross-referenced to other databases and to selected key publications. New options for database searching and querying have been implemented, including a BLAST search of protein sequences and a PARALIGN search of the collected nucleic acid sequences.",poster,cp108
p2218,ad21c3cd8871347e3bdb7cb2800049f7e8a97aca,c100,ACM SIGMOD Conference,The IntAct molecular interaction database in 2012,"IntAct is an open-source, open data molecular interaction database populated by data either curated from the literature or from direct data depositions. Two levels of curation are now available within the database, with both IMEx-level annotation and less detailed MIMIx-compatible entries currently supported. As from September 2011, IntAct contains approximately 275 000 curated binary interaction evidences from over 5000 publications. The IntAct website has been improved to enhance the search process and in particular the graphical display of the results. New data download formats are also available, which will facilitate the inclusion of IntAct's data in the Semantic Web. IntAct is an active contributor to the IMEx consortium (http://www.imexconsortium.org). IntAct source code and data are freely available at http://www.ebi.ac.uk/intact.",poster,cp100
p2219,529124bc3ff7fa655729c49bf10aedaddd49787e,c88,Symposium on the Theory of Computing,Phenol-Explorer 3.0: a major update of the Phenol-Explorer database to incorporate data on the effects of food processing on polyphenol content,"Polyphenols are a major class of bioactive phytochemicals whose consumption may play a role in the prevention of a number of chronic diseases such as cardiovascular diseases, type II diabetes and cancers. Phenol-Explorer, launched in 2009, is the only freely available web-based database on the content of polyphenols in food and their in vivo metabolism and pharmacokinetics. Here we report the third release of the database (Phenol-Explorer 3.0), which adds data on the effects of food processing on polyphenol contents in foods. Data on >100 foods, covering 161 polyphenols or groups of polyphenols before and after processing, were collected from 129 peer-reviewed publications and entered into new tables linked to the existing relational design. The effect of processing on polyphenol content is expressed in the form of retention factor coefficients, or the proportion of a given polyphenol retained after processing, adjusted for change in water content. The result is the first database on the effects of food processing on polyphenol content and, following the model initially defined for Phenol-Explorer, all data may be traced back to original sources. The new update will allow polyphenol scientists to more accurately estimate polyphenol exposure from dietary surveys. Database URL: http://www.phenol-explorer.eu",poster,cp88
p2220,cc589c499dcf323fe4a143bbef0074c3e31f9b60,c101,International Conference on Automatic Face and Gesture Recognition,A 3D facial expression database for facial behavior research,"Traditionally, human facial expressions have been studied using either 2D static images or 2D video sequences. The 2D-based analysis is incapable of handing large pose variations. Although 3D modeling techniques have been extensively used for 3D face recognition and 3D face animation, barely any research on 3D facial expression recognition using 3D range data has been reported. A primary factor for preventing such research is the lack of a publicly available 3D facial expression database. In this paper, we present a newly developed 3D facial expression database, which includes both prototypical 3D facial expression shapes and 2D facial textures of 2,500 models from 100 subjects. This is the first attempt at making a 3D facial expression database available for the research community, with the ultimate goal of fostering the research on affective computing and increasing the general understanding of facial behavior and the fine 3D structure inherent in human facial expressions. The new database can be a valuable resource for algorithm assessment, comparison and evaluation",fullPaper,cp101
p2221,dc85ca80fb3d75fe63106f631a2f7cd251d2851e,j274,Nature Genetics,Identification of protein coding regions by database similarity search,Abstract content,fullPaper,jv274
p2222,fded6f9cefb06c1da7acddd5b9b89b1eabb08b7d,c42,Joint Conference of International Workshop on Software Measurement and International Conference on Software Process and Product Measurement,The Ensembl genome database project,"The Ensembl (http://www.ensembl.org/) database project provides a bioinformatics framework to organise biology around the sequences of large genomes. It is a comprehensive source of stable automatic annotation of the human genome sequence, with confirmed gene predictions that have been integrated with external data sources, and is available as either an interactive web site or as flat files. It is also an open source software engineering project to develop a portable system able to handle very large genomes and associated requirements from sequence analysis to data storage and visualisation. The Ensembl site is one of the leading sources of human genome sequence annotation and provided much of the analysis for publication by the international human genome project of the draft genome. The Ensembl system is being installed around the world in both companies and academic sites on machines ranging from supercomputers to laptops.",poster,cp42
p2223,b62628ac06bbac998a3ab825324a41a11bc3a988,c102,International Conference on Biometrics,XM2VTSDB: The Extended M2VTS Database,"Keywords: vision Reference EPFL-CONF-82502 URL: ftp://ftp.idiap.ch/pub/papers/vision/avbpa99.pdf Record created on 2006-03-10, modified on 2017-05-10",poster,cp102
p2224,7d25222fcf62dae782ae6b18fae0b33d9f7923fe,j209,Human Mutation,Human Gene Mutation Database (HGMD®): 2003 update,"The Human Gene Mutation Database (HGMD) constitutes a comprehensive core collection of data on germ‐line mutations in nuclear genes underlying or associated with human inherited disease (www.hgmd.org). Data catalogued includes: single base‐pair substitutions in coding, regulatory and splicing‐relevant regions; micro‐deletions and micro‐insertions; indels; triplet repeat expansions as well as gross deletions; insertions; duplications; and complex rearrangements. Each mutation is entered into HGMD only once in order to avoid confusion between recurrent and identical‐by‐descent lesions. By March 2003, the database contained in excess of 39,415 different lesions detected in 1,516 different nuclear genes, with new entries currently accumulating at a rate exceeding 5,000 per annum. Since its inception, HGMD has been expanded to include cDNA reference sequences for more than 87% of listed genes, splice junction sequences, disease‐associated and functional polymorphisms, as well as links to data present in publicly available online locus‐specific mutation databases. Although HGMD has recently entered into a licensing agreement with Celera Genomics (Rockville, MD), mutation data will continue to be made freely available via the Internet. Hum Mutat 21:577–581, 2003. © 2003 Wiley‐Liss, Inc.",fullPaper,jv209
p2225,bcc73dd05b7b7a6616c41df428e3624375c95e56,c102,International Conference on Biometrics,ATtRACT—a database of RNA-binding proteins and associated motifs,"RNA-binding proteins (RBPs) play a crucial role in key cellular processes, including RNA transport, splicing, polyadenylation and stability. Understanding the interaction between RBPs and RNA is key to improve our knowledge of RNA processing, localization and regulation in a global manner. Despite advances in recent years, a unified non-redundant resource that includes information on experimentally validated motifs, RBPs and integrated tools to exploit this information is lacking. Here, we developed a database named ATtRACT (available at http://attract.cnic.es) that compiles information on 370 RBPs and 1583 RBP consensus binding motifs, 192 of which are not present in any other database. To populate ATtRACT we (i) extracted and hand-curated experimentally validated data from CISBP-RNA, SpliceAid–F, RBPDB databases, (ii) integrated and updated the unavailable ASD database and (iii) extracted information from Protein-RNA complexes present in Protein Data Bank database through computational analyses. ATtRACT provides also efficient algorithms to search a specific motif and scan one or more RNA sequences at a time. It also allows discovering de novo motifs enriched in a set of related sequences and compare them with the motifs included in the database. Database URL: http:// attract. cnic. es",poster,cp102
p2226,6827419c860e6b9c825a1138d48633ecef00d4d5,c25,International Conference on Contemporary Computing,HPIDB 2.0: a curated database for host–pathogen interactions,"Identification and analysis of host–pathogen interactions (HPI) is essential to study infectious diseases. However, HPI data are sparse in existing molecular interaction databases, especially for agricultural host–pathogen systems. Therefore, resources that annotate, predict and display the HPI that underpin infectious diseases are critical for developing novel intervention strategies. HPIDB 2.0 (http://www.agbase.msstate.edu/hpi/main.html) is a resource for HPI data, and contains 45, 238 manually curated entries in the current release. Since the first description of the database in 2010, multiple enhancements to HPIDB data and interface services were made that are described here. Notably, HPIDB 2.0 now provides targeted biocuration of molecular interaction data. As a member of the International Molecular Exchange consortium, annotations provided by HPIDB 2.0 curators meet community standards to provide detailed contextual experimental information and facilitate data sharing. Moreover, HPIDB 2.0 provides access to rapidly available community annotations that capture minimum molecular interaction information to address immediate researcher needs for HPI network analysis. In addition to curation, HPIDB 2.0 integrates HPI from existing external sources and contains tools to infer additional HPI where annotated data are scarce. Compared to other interaction databases, our data collection approach ensures HPIDB 2.0 users access the most comprehensive HPI data from a wide range of pathogens and their hosts (594 pathogen and 70 host species, as of February 2016). Improvements also include enhanced search capacity, addition of Gene Ontology functional information, and implementation of network visualization. The changes made to HPIDB 2.0 content and interface ensure that users, especially agricultural researchers, are able to easily access and analyse high quality, comprehensive HPI data. All HPIDB 2.0 data are updated regularly, are publically available for direct download, and are disseminated to other molecular interaction resources. Database URL: http://www.agbase.msstate.edu/hpi/main.html",poster,cp25
p2227,7c301b598e6ba6b49fb2d14503158b88a87932ae,j369,Accounts of Chemical Research,"The Reticular Chemistry Structure Resource (RCSR) database of, and symbols for, crystal nets.","During the past decade, interest has grown tremendously in the design and synthesis of crystalline materials constructed from molecular clusters linked by extended groups of atoms. Most notable are metal-organic frameworks (MOFs), in which polyatomic inorganic metal-containing clusters are joined by polytopic linkers. (Although these materials are sometimes referred to as coordination polymers, we prefer to differentiate them, because MOFs are based on strong linkages that yield robust frameworks.) The realization that MOFs could be designed and synthesized in a rational way from molecular building blocks led to the emergence of a discipline that we call reticular chemistry. MOFs can be represented as a special kind of graph called a periodic net. Such descriptions date back to the earliest crystallographic studies but have become much more common recently because thousands of new structures and hundreds of underlying nets have been reported. In the simplest cases (e.g., the structure of diamond), the atoms in the crystal become the vertices of the net, and bonds are the links (edges) that connect them. In the case of MOFs, polyatomic groups act as the vertices and edges of the net. Because of the explosive growth in this area, a need has arisen for a universal system of nomenclature, classification, identification, and retrieval of these topological structures. We have developed a system of symbols for the identification of three periodic nets of interest, and this system is now in wide use. In this Account, we explain the underlying methodology of assigning symbols and describe the Reticular Chemistry Structure Resource (RCSR), in which about 1600 such nets are collected and illustrated in a database that can be searched by symbol, name, keywords, and attributes. The resource also contains searchable data for polyhedra and layers. The database entries come from systematic enumerations or from known chemical compounds or both. In the latter case, references to occurrences are provided. We describe some crystallographic, topological, and other attributes of nets and explain how they are reported in the database. We also describe how the database can be used as a tool for the design and structural analysis of new materials. Associated with each net is a natural tiling, which is a natural partition of space into space-filling tiles. The database allows export of data that can be used to analyze and illustrate such tilings.",fullPaper,jv369
p2228,768b97bd5a8d4944be4a6f71b12ffad372a6ce5e,c20,Research Conference of the South African Institute of Computer Scientists and Information Technologists,The UCSC Genome Browser database: extensions and updates 2013,"The University of California Santa Cruz (UCSC) Genome Browser (http://genome.ucsc.edu) offers online public access to a growing database of genomic sequence and annotations for a wide variety of organisms. The Browser is an integrated tool set for visualizing, comparing, analysing and sharing both publicly available and user-generated genomic datasets. As of September 2012, genomic sequence and a basic set of annotation ‘tracks’ are provided for 63 organisms, including 26 mammals, 13 non-mammal vertebrates, 3 invertebrate deuterostomes, 13 insects, 6 worms, yeast and sea hare. In the past year 19 new genome assemblies have been added, and we anticipate releasing another 28 in early 2013. Further, a large number of annotation tracks have been either added, updated by contributors or remapped to the latest human reference genome. Among these are an updated UCSC Genes track for human and mouse assemblies. We have also introduced several features to improve usability, including new navigation menus. This article provides an update to the UCSC Genome Browser database, which has been previously featured in the Database issue of this journal.",poster,cp20
p2229,c2b381b24aabf237394059fed7920cd6fd0e67b8,j1,IEEE Transactions on Knowledge and Data Engineering,Database Mining: A Performance Perspective,"The authors' perspective of database mining as the confluence of machine learning techniques and the performance emphasis of database technology is presented. Three classes of database mining problems involving classification, associations, and sequences are described. It is argued that these problems can be uniformly viewed as requiring discovery of rules embedded in massive amounts of data. A model and some basic operations for the process of rule discovery are described. It is shown how the database mining problems considered map to this model, and how they can be solved by using the basic operations proposed. An example is given of an algorithm for classification obtained by combining the basic rule discovery operations. This algorithm is efficient in discovering classification rules and has accuracy comparable to ID3, one of the best current classifiers. >",fullPaper,jv1
p2230,3962ddcab496d4a2a7196dcef370c87f58a3133d,c65,Formal Concept Analysis,The NCBI Taxonomy database,"The NCBI Taxonomy database (http://www.ncbi.nlm.nih.gov/taxonomy) is the standard nomenclature and classification repository for the International Nucleotide Sequence Database Collaboration (INSDC), comprising the GenBank, ENA (EMBL) and DDBJ databases. It includes organism names and taxonomic lineages for each of the sequences represented in the INSDC’s nucleotide and protein sequence databases. The taxonomy database is manually curated by a small group of scientists at the NCBI who use the current taxonomic literature to maintain a phylogenetic taxonomy for the source organisms represented in the sequence databases. The taxonomy database is a central organizing hub for many of the resources at the NCBI, and provides a means for clustering elements within other domains of NCBI web site, for internal linking between domains of the Entrez system and for linking out to taxon-specific external resources on the web. Our primary purpose is to index the domain of sequences as conveniently as possible for our user community.",poster,cp65
p2231,f707571329e1aa7180a1a289b1aa250eabdc8618,c22,International Conference on Data Technologies and Applications,InterPro in 2011: new developments in the family and domain prediction database,"InterPro (http://www.ebi.ac.uk/interpro/) is a database that integrates diverse information about protein families, domains and functional sites, and makes it freely available to the public via Web-based interfaces and services. Central to the database are diagnostic models, known as signatures, against which protein sequences can be searched to determine their potential function. InterPro has utility in the large-scale analysis of whole genomes and meta-genomes, as well as in characterizing individual protein sequences. Herein we give an overview of new developments in the database and its associated software since 2009, including updates to database content, curation processes and Web and programmatic interfaces.",poster,cp22
p2232,187f31d400d711e25fd3cddd195f0af730729912,c69,International Conference on Parallel Processing,Cloud-Screening and Quality Control Algorithms for the AERONET Database,Abstract content,poster,cp69
p2233,a094ea0a295db1b4d86d504cd885fde5d3396c75,c73,Workshop on Algorithms in Bioinformatics,The RDP-II (Ribosomal Database Project),"The Ribosomal Database Project (RDP-II), previously described by Maidak et al. [Nucleic Acids Res. (2000), 28, 173-174], continued during the past year to add new rRNA sequences to the aligned data and to improve the analysis commands. Release 8.0 (June 1, 2000) consisted of 16 277 aligned prokaryotic small subunit (SSU) rRNA sequences while the number of eukaryotic and mitochondrial SSU rRNA sequences in aligned form remained at 2055 and 1503, respectively. The number of prokaryotic SSU rRNA sequences more than doubled from the previous release 14 months earlier, and approximately 75% are longer than 899 bp. An RDP-II mirror site in Japan is now available (http://wdcm.nig.ac.jp/RDP/html/index.h tml). RDP-II provides aligned and annotated rRNA sequences, derived phylogenetic trees and taxonomic hierarchies, and analysis services through its WWW server (http://rdp.cme.msu.edu/). Analysis services include rRNA probe checking, approximate phylogenetic placement of user sequences, screening user sequences for possible chimeric rRNA sequences, automated alignment, production of similarity matrices and services to plan and analyze terminal restriction fragment polymorphism experiments. The RDP-II email address for questions and comments has been changed from curator@cme.msu.edu to rdpstaff@msu.edu.",poster,cp73
p2234,a175453ceff401c67ce503750418e66936331b06,c69,International Conference on Parallel Processing,NCBI GEO: mining tens of millions of expression profiles—database and tools update,"The Gene Expression Omnibus (GEO) repository at the National Center for Biotechnology Information (NCBI) archives and freely disseminates microarray and other forms of high-throughput data generated by the scientific community. The database has a minimum information about a microarray experiment (MIAME)-compliant infrastructure that captures fully annotated raw and processed data. Several data deposit options and formats are supported, including web forms, spreadsheets, XML and Simple Omnibus Format in Text (SOFT). In addition to data storage, a collection of user-friendly web-based interfaces and applications are available to help users effectively explore, visualize and download the thousands of experiments and tens of millions of gene expression patterns stored in GEO. This paper provides a summary of the GEO database structure and user facilities, and describes recent enhancements to database design, performance, submission format options, data query and retrieval utilities. GEO is accessible at",poster,cp69
p2235,b7340682cd94e4a40df2823cee68ff166be93f86,c50,International Conference on Automated Software Engineering,The ConsensusPathDB interaction database: 2013 update,"Knowledge of the various interactions between molecules in the cell is crucial for understanding cellular processes in health and disease. Currently available interaction databases, being largely complementary to each other, must be integrated to obtain a comprehensive global map of the different types of interactions. We have previously reported the development of an integrative interaction database called ConsensusPathDB (http://ConsensusPathDB.org) that aims to fulfill this task. In this update article, we report its significant progress in terms of interaction content and web interface tools. ConsensusPathDB has grown mainly due to the integration of 12 further databases; it now contains 215 541 unique interactions and 4601 pathways from overall 30 databases. Binary protein interactions are scored with our confidence assessment tool, IntScore. The ConsensusPathDB web interface allows users to take advantage of these integrated interaction and pathway data in different contexts. Recent developments include pathway analysis of metabolite lists, visualization of functional gene/metabolite sets as overlap graphs, gene set analysis based on protein complexes and induced network modules analysis that connects a list of genes through various interaction types. To facilitate the interactive, visual interpretation of interaction and pathway data, we have re-implemented the graph visualization feature of ConsensusPathDB using the Cytoscape.js library.",poster,cp50
p2236,0d3e8b3a4bb5ffc8bc0527c443a99eb1438956a0,c102,International Conference on Biometrics,A face antispoofing database with diverse attacks,"Face antispoofing has now attracted intensive attention, aiming to assure the reliability of face biometrics. We notice that currently most of face antispoofing databases focus on data with little variations, which may limit the generalization performance of trained models since potential attacks in real world are probably more complex. In this paper we release a face antispoofing database which covers a diverse range of potential attack variations. Specifically, the database contains 50 genuine subjects, and fake faces are made from the high quality records of the genuine faces. Three imaging qualities are considered, namely the low quality, normal quality and high quality. Three fake face attacks are implemented, which include warped photo attack, cut photo attack and video attack. Therefore each subject contains 12 videos (3 genuine and 9 fake), and the final database contains 600 video clips. Test protocol is provided, which consists of 7 scenarios for a thorough evaluation from all possible aspects. A baseline algorithm is also given for comparison, which explores the high frequency information in the facial region to determine the liveness. We hope such a database can serve as an evaluation platform for future researches in the literature.",fullPaper,cp102
p2237,30ae1edd4e6f13a28f87ec150c407e6820c7da60,j370,BMC Cancer,PROGgeneV2: enhancements on the existing database,Abstract content,fullPaper,jv370
p2238,f1d6cbac2f1ad7443c60c035e1c819bef402c5c7,c97,Interspeech,Systemic Banking Crises Database II,Abstract content,poster,cp97
p2239,a0e5802cf66257d0412de878682dc9caccca0719,c63,IEEE International Software Metrics Symposium,Rfam: an RNA family database,"Rfam is a collection of multiple sequence alignments and covariance models representing non-coding RNA families. Rfam is available on the web in the UK at http://www.sanger.ac.uk/Software/Rfam/ and in the US at http://rfam.wustl.edu/. These websites allow the user to search a query sequence against a library of covariance models, and view multiple sequence alignments and family annotation. The database can also be downloaded in flatfile form and searched locally using the INFERNAL package (http://infernal.wustl.edu/). The first release of Rfam (1.0) contains 25 families, which annotate over 50 000 non-coding RNA genes in the taxonomic divisions of the EMBL nucleotide database.",poster,cp63
p2240,0a4365f6c30d40ca7610e9afac6c339f3c72224c,j371,Pharmacoepidemiology and Drug Safety,Validation of the national health insurance research database with ischemic stroke cases in Taiwan,The National Health Insurance Research Database (NHIRD) is commonly used for pharmacoepidemiological research in Taiwan. This study evaluated the validity of the database for patients with a principal diagnosis of ischemic stroke.,fullPaper,jv371
p2241,71e9a23138a5d7c35b28bd98fd616c81719b1b7a,c61,Jahrestagung der Gesellschaft für Informatik,"NoSQL Database: New Era of Databases for Big data Analytics - Classification, Characteristics and Comparison","Digital world is growing very fast and become more complex in the volume (terabyte to petabyte), variety (structured and un-structured and hybrid), velocity (high speed in growth) in nature. This refers to as ‘Big Data’ that is a global phenomenon. This is typically considered to be a data collection that has grown so large it can’t be effectively managed or exploited using conventional data management tools: e.g., classic relational database management systems (RDBMS) or conventional search engines. To handle this problem, traditional RDBMS are complemented by specifically designed a rich set of alternative DBMS; such as - NoSQL, NewSQL and Search-based systems. This paper motivation is to provide - classification, characteristics and evaluation of NoSQL databases in Big Data Analytics. This report is intended to help users, especially to the organizations to obtain an independent understanding of the strengths and weaknesses of various NoSQL database approaches to supporting applications that process huge volumes of data.",poster,cp61
p2242,5808a6c7371fc3e787f6923484613fee29dca1fa,c85,International Conference on Graph Transformation,Completion of the 2001 National Land Cover Database for the conterminous United States,"Introduction Appropriate and relevant land cover information is increasingly required by a broad spectrum of scientific, economic and governmental applications as essential input to analyze such issues as assessing ecosystem status and health, understanding spatial patterns of biodiversity and developing land management policy. The publication of the first National Land Cover Dataset (NLCD 1992) (Vogelmann et al. 2001) created a 30-meter resolution land cover data layer over the conterminous United States from circa 1992 Landsat Thematic Mapper (TM) imagery. Information from this original NLCD 1992 has been used in thousands of applications in the private, public, and academic sectors ranging from assisting in placing cell phone towers to tracking how diseases spread. The national consistency of this information has provided critical analysis for many national applications such as the Heinz Center’s State of the Nation’s Ecosystems (Heinz Center 2002), the Environmental Protection Agency’s Draft Report on the Environment (USEPA 2003) and the U.S. Geological Survey National Water Quality Assessment program. Starting in 1999, new research was undertaken to expand and update NLCD 1992 into a full scale land cover database (with multiple instead of single products), and to produce it across all 50 states and Puerto Rico (Homer et al. 2004). This new database is called the National Land Cover Database 2001 (the 2001 refers to the nominal year from which most of the Landsat 5 and Landsat 7 imagery was acquired) and has been under production for 6 years. This article announces the completion of NLCD 2001 for the conterminous United States, with products that can identify one of 16 classes of land cover, the percent tree canopy, and the percent urban imperviousness for every 30-meter cell in the conterminous 48 states (approximately 27 billion cells). Both NLCD 1992 and NLCD 2001 have been produced and funded through an umbrella organization called the Multi-Resolution Land Characteristics Consortium (MRLC). This Consortium consists of 13 programs across 10 Federal agencies that require landcover data for addressing their agency needs (www.mrlc.gov). MRLC provided the umbrella to coordinate multi-agency NLCD mapping production and funding contributions. In addition to NLCD data, MRLC also offers approximately 6,200 terrain corrected Landsat 5 TM and Landsat 7 Enhanced Thematic Mapper (ETM+) scenes spanning growing season dates from 1982-2006 which are available for public web-enabled download from www.mrlc. gov. MRLC represents an excellent example of Federal government collaboration across many agencies to synergistically develop important geo-spatial data for the Nation.",poster,cp85
p2243,1bd82d1d7dea7002966538e881bc9d5d8d51ef6e,c30,IEEE Aerospace Conference,"The PROSITE database, its status in 1997","The PROSITE database (http://www.expasy.ch/sprot/prosite.htm l) consists of biologically significant patterns and profiles formulated in such a way that with appropriate computational tools it can help to determine to which known family of protein (if any) a new sequence belongs, or which known domain(s) it contains.",poster,cp30
p2244,c0e3333ec5098fc902bbd7ad30b820ad9b3a324f,c100,ACM SIGMOD Conference,Executing SQL over encrypted data in the database-service-provider model,"Rapid advances in networking and Internet technologies have fueled the emergence of the ""software as a service"" model for enterprise computing. Successful examples of commercially viable software services include rent-a-spreadsheet, electronic mail services, general storage services, disaster protection services. ""Database as a Service"" model provides users power to create, store, modify, and retrieve data from anywhere in the world, as long as they have access to the Internet. It introduces several challenges, an important issue being data privacy. It is in this context that we specifically address the issue of data privacy.There are two main privacy issues. First, the owner of the data needs to be assured that the data stored on the service-provider site is protected against data thefts from outsiders. Second, data needs to be protected even from the service providers, if the providers themselves cannot be trusted. In this paper, we focus on the second challenge. Specifically, we explore techniques to execute SQL queries over encrypted data. Our strategy is to process as much of the query as possible at the service providers' site, without having to decrypt the data. Decryption and the remainder of the query processing are performed at the client site. The paper explores an algebraic framework to split the query to minimize the computation at the client site. Results of experiments validating our approach are also presented.",fullPaper,cp100
p2245,6210e6f1814e0e1fc842ed97661e1b8e6937b35c,c49,International Symposium on Search Based Software Engineering,The notions of consistency and predicate locks in a database system,"In database systems, users access shared data under the assumption that the data satisfies certain consistency constraints. This paper defines the concepts of transaction, consistency and schedule and shows that consistency requires that a transaction cannot request new locks after releasing a lock. Then it is argued that a transaction needs to lock a logical rather than a physical subset of the database. These subsets may be specified by predicates. An implementation of predicate locks which satisfies the consistency condition is suggested.",poster,cp49
p2246,ff9ef51fbc40937d38dcea754b43a3e06ca39a01,c43,ACM Symposium on Applied Computing,EpiFactors: a comprehensive database of human epigenetic factors and complexes,"Epigenetics refers to stable and long-term alterations of cellular traits that are not caused by changes in the DNA sequence per se. Rather, covalent modifications of DNA and histones affect gene expression and genome stability via proteins that recognize and act upon such modifications. Many enzymes that catalyse epigenetic modifications or are critical for enzymatic complexes have been discovered, and this is encouraging investigators to study the role of these proteins in diverse normal and pathological processes. Rapidly growing knowledge in the area has resulted in the need for a resource that compiles, organizes and presents curated information to the researchers in an easily accessible and user-friendly form. Here we present EpiFactors, a manually curated database providing information about epigenetic regulators, their complexes, targets and products. EpiFactors contains information on 815 proteins, including 95 histones and protamines. For 789 of these genes, we include expressions values across several samples, in particular a collection of 458 human primary cell samples (for approximately 200 cell types, in many cases from three individual donors), covering most mammalian cell steady states, 255 different cancer cell lines (representing approximately 150 cancer subtypes) and 134 human postmortem tissues. Expression values were obtained by the FANTOM5 consortium using Cap Analysis of Gene Expression technique. EpiFactors also contains information on 69 protein complexes that are involved in epigenetic regulation. The resource is practical for a wide range of users, including biologists, pharmacologists and clinicians. Database URL: http://epifactors.autosome.ru",poster,cp43
p2247,efed8e0153ab5ee0d06a49388eb676c237ab788d,c83,International Conference on Computer Graphics and Interactive Techniques,Database Systems: The Complete Book,"From the Publisher: 
This introduction to database systems offers a readable comprehensive approach with engaging, real-world examplesusers will learn how to successfully plan a database application before building it. The first half of the book provides in-depth coverage of databases from the point of view of the database designer, user, and application programmer, while the second half of the book provides in-depth coverage of databases from the point of view of the DBMS implementor. The first half of the book focuses on database design, database use, and implementation of database applications and database management systemsit covers the latest database standards SQL:1999, SQL/PSM, SQL/CLI, JDBC, ODL, and XML, with broader coverage of SQL than most other books. The second half of the book focuses on storage structures, query processing, and transaction managementit covers the main techniques in these areas with broader coverage of query optimization than most other books, along with advanced topics including multidimensional and bitmap indexes, distributed transactions, and information integration techniques. A professional reference for database designers, users, and application programmers.",poster,cp83
p2248,148c4770b5fc2f841179f4c4f800f41c2171841a,c0,International Conference on Human Factors in Computing Systems,A New Database on the Structure and Development of the Financial Sector,"This article introduces a new database of indicators of financial structure and financial development across countries and over time. The database is unique in that it combines a wide variety of indicators that measure the size, activity, and efficiency of financial intermediaries and markets. It improves on previous efforts by presenting data on the public share of commercial banks, introducing indicators of the size and activity of nonbank financial institutions, and constructing measures of the size of bond and primary equity markets. This article introduces a new database, the first to provide comprehensive measures of the development, structure, and performance of the financial sector. This database is the first to define and construct indicators of the size and activity of nonbank financial intermediaries, such as insurance companies, pension funds, and non-deposit money banks. It is also the first to include indicators of the size of primary equity markets and primary and secondary bond markets. In constructing the database, authors carefully deflate measures and match stock and flow variables.",poster,cp0
p2249,54a4f558c0b4f916cf49ff2eceaadb94d5d3637d,c17,International Conference on Enterprise Information Systems,Database Management Systems,Abstract content,poster,cp17
p2250,355f5e57eab748d07ec768e4c07b06dfd66a3ba5,c32,International Conference on Software Technology: Methods and Tools,BindingDB: a web-accessible database of experimentally determined protein–ligand binding affinities,"BindingDB () is a publicly accessible database currently containing ∼20 000 experimentally determined binding affinities of protein–ligand complexes, for 110 protein targets including isoforms and mutational variants, and ∼11 000 small molecule ligands. The data are extracted from the scientific literature, data collection focusing on proteins that are drug-targets or candidate drug-targets and for which structural data are present in the Protein Data Bank. The BindingDB website supports a range of query types, including searches by chemical structure, substructure and similarity; protein sequence; ligand and protein names; affinity ranges and molecular weight. Data sets generated by BindingDB queries can be downloaded in the form of annotated SDfiles for further analysis, or used as the basis for virtual screening of a compound database uploaded by the user. The data in BindingDB are linked both to structural data in the PDB via PDB IDs and chemical and sequence searches, and to the literature in PubMed via PubMed IDs.",poster,cp32
p2251,755353e242bd4db190b6830d4bdcc608f9dd8744,c31,International Conference on Evaluation & Assessment in Software Engineering,The Transporter Classification Database,"The Transporter Classification Database (TCDB; http://www.tcdb.org) serves as a common reference point for transport protein research. The database contains more than 10 000 non-redundant proteins that represent all currently recognized families of transmembrane molecular transport systems. Proteins in TCDB are organized in a five level hierarchical system, where the first two levels are the class and subclass, the second two are the family and subfamily, and the last one is the transport system. Superfamilies that contain multiple families are included as hyperlinks to the five tier TC hierarchy. TCDB includes proteins from all types of living organisms and is the only transporter classification system that is both universal and recognized by the International Union of Biochemistry and Molecular Biology. It has been expanded by manual curation, contains extensive text descriptions providing structural, functional, mechanistic and evolutionary information, is supported by unique software and is interconnected to many other relevant databases. TCDB is of increasing usefulness to the international scientific community and can serve as a model for the expansion of database technologies. This manuscript describes an update of the database descriptions previously featured in NAR database issues.",poster,cp31
p2252,b0c5efdf2f90322784283290a052797eb073b554,j66,Data Science Journal,The World Ocean Database,"The World Ocean Database (WOD) is the most comprehensive global ocean profile-plankton database available internationally without restriction. All data are in one well-documented format and are available both on DVDs for a minimal charge and on-line without charge. The latest DVD version of the WOD is the World Ocean Database 2009 (WOD09). All data in the WOD are associated with as much metadata as possible, and every ocean data value has a quality control flag associated with it. The WOD is a product of the U.S. National Oceanographic Data Center and its co-located World Data Center for Oceanography. However, the WOD exists because of the international oceanographic data exchange that has occurred under the auspices of the Intergovernmental Oceanographic Commission (IOC) and the International Council of Science (ICSU) World Data Center (WDC) system. World Data Centers are part of the ICSU World Data System.",fullPaper,jv66
p2253,20bdfd777432f7eab7ab2cc146297df1db654090,c47,International Symposium on Empirical Software Engineering and Measurement,"MINT, the molecular interaction database: 2012 update","The Molecular INTeraction Database (MINT, http://mint.bio.uniroma2.it/mint/) is a public repository for protein–protein interactions (PPI) reported in peer-reviewed journals. The database grows steadily over the years and at September 2011 contains approximately 235 000 binary interactions captured from over 4750 publications. The web interface allows the users to search, visualize and download interactions data. MINT is one of the members of the International Molecular Exchange consortium (IMEx) and adopts the Molecular Interaction Ontology of the Proteomics Standard Initiative (PSI-MI) standards for curation and data exchange. MINT data are freely accessible and downloadable at http://mint.bio.uniroma2.it/mint/download.do. We report here the growth of the database, the major changes in curation policy and a new algorithm to assign a confidence to each interaction.",poster,cp47
p2254,f995e0be49ba43b6d3324af2ee00c6a3be84d210,c34,IEEE Working Conference on Mining Software Repositories,The UCSC Genome Browser database: extensions and updates 2011,"The University of California Santa Cruz Genome Browser (http://genome.ucsc.edu) offers online public access to a growing database of genomic sequence and annotations for a wide variety of organisms. The Browser is an integrated tool set for visualizing, comparing, analyzing and sharing both publicly available and user-generated genomic data sets. In the past year, the local database has been updated with four new species assemblies, and we anticipate another four will be released by the end of 2011. Further, a large number of annotation tracks have been either added, updated by contributors, or remapped to the latest human reference genome. Among these are new phenotype and disease annotations, UCSC genes, and a major dbSNP update, which required new visualization methods. Growing beyond the local database, this year we have introduced ‘track data hubs’, which allow the Genome Browser to provide access to remotely located sets of annotations. This feature is designed to significantly extend the number and variety of annotation tracks that are publicly available for visualization and analysis from within our site. We have also introduced several usability features including track search and a context-sensitive menu of options available with a right-click anywhere on the Browser's image.",poster,cp34
p2255,0d2072d81d03247949126e87d6201788cb646526,c93,Human Language Technology - The Baltic Perspectiv,"The PROSITE database, its status in 2002","PROSITE [Bairoch and Bucher (1994) Nucleic Acids Res., 22, 3583-3589; Hofmann et al. (1999) Nucleic Acids Res., 27, 215-219] is a method of identifying the functions of uncharacterized proteins translated from genomic or cDNA sequences. The PROSITE database (http://www.expasy.org/prosite/) consists of biologically significant patterns and profiles designed in such a way that with appropriate computational tools it can rapidly and reliably help to determine to which known family of proteins (if any) a new sequence belongs, or which known domain(s) it contains.",poster,cp93
p2256,151ec57b35f6a7431fdce934a57ae15451079d85,c18,Conference on Innovative Data Systems Research,NCBI’s Database of Genotypes and Phenotypes: dbGaP,"The Database of Genotypes and Phenotypes (dbGap, http://www.ncbi.nlm.nih.gov/gap) is a National Institutes of Health-sponsored repository charged to archive, curate and distribute information produced by studies investigating the interaction of genotype and phenotype. Information in dbGaP is organized as a hierarchical structure and includes the accessioned objects, phenotypes (as variables and datasets), various molecular assay data (SNP and Expression Array data, Sequence and Epigenomic marks), analyses and documents. Publicly accessible metadata about submitted studies, summary level data, and documents related to studies can be accessed freely on the dbGaP website. Individual-level data are accessible via Controlled Access application to scientists across the globe.",poster,cp18
p2257,2129930aa4803610ea25860e770cbc73440acbf4,j306,"Proteins: Structure, Function, and Bioinformatics",Database of homology‐derived protein structures and the structural meaning of sequence alignment,"The database of known protein three‐dimensional structures can be significantly increased by the use of sequence homology, based on the following observations. (1) The database of known sequences, currently at more than 12,000 proteins, is two orders of magnitude larger than the database of known structures. (2) The currently most powerful method of predicting protein structures is model building by homology. (3) Structural homology can be inferred from the level of sequence similarity. (4) The threshold of sequence similarity sufficient for structural homology depends strongly on the length of the alignment. Here, we first quantify the relation between sequence similarity, structure similarity, and alignment length by an exhaustive survey of alignments between proteins of known structure and report a homology threshold curve as a function of alignment length. We then produce a database of homology‐derived secondary structure of proteins (HSSP) by aligning to each protein of known structure all sequences deemed homologous on the basis of the threshold curve. For each known protein structure, the derived database contains the aligned sequences, secondary structure, sequence variability, and sequence profile. Tertiary structures of the aligned sequences are implied, but not modeled explicity. The database effectively increases the number of known protein structures by a factor of five to more than 1800. The results may be useful in assessing the structural significance of matches in sequence database searches, in deriving preferences and patterns for structure prediction, in elucidating the structural role of conserved residues, and in modeling three‐dimensional detail by homology.",fullPaper,jv306
p2258,01297b19ec00f5487a522a573ff6e0a9aeac4f05,c21,Grid Computing Environments,Database,Abstract content,poster,cp21
p2259,3c2d95624ede725cc629cfe63affb57237f009f7,c10,Big Data,A New Database of Financial Reforms,Abstract content,poster,cp10
p2260,03094c68e9333dbfb17426d22d4e61748d92e414,c67,Enterprise Application Integration,The BioGRID Interaction Database: 2011 update,"The Biological General Repository for Interaction Datasets (BioGRID) is a public database that archives and disseminates genetic and protein interaction data from model organisms and humans (http://www.thebiogrid.org). BioGRID currently holds 347 966 interactions (170 162 genetic, 177 804 protein) curated from both high-throughput data sets and individual focused studies, as derived from over 23 000 publications in the primary literature. Complete coverage of the entire literature is maintained for budding yeast (Saccharomyces cerevisiae), fission yeast (Schizosaccharomyces pombe) and thale cress (Arabidopsis thaliana), and efforts to expand curation across multiple metazoan species are underway. The BioGRID houses 48 831 human protein interactions that have been curated from 10 247 publications. Current curation drives are focused on particular areas of biology to enable insights into conserved networks and pathways that are relevant to human health. The BioGRID 3.0 web interface contains new search and display features that enable rapid queries across multiple data types and sources. An automated Interaction Management System (IMS) is used to prioritize, coordinate and track curation across international sites and projects. BioGRID provides interaction data to several model organism databases, resources such as Entrez-Gene and other interaction meta-databases. The entire BioGRID 3.0 data collection may be downloaded in multiple file formats, including PSI MI XML. Source code for BioGRID 3.0 is freely available without any restrictions.",poster,cp67
p2261,6dd37b57f3391b438fa588f98a1c2067365ae5ca,c13,International Conference on Data Science and Advanced Analytics,TID2008 – A database for evaluation of full-reference visual quality assessment metrics,"— In this paper, a new image database, TID2008, for evaluation of full-reference visual quality assessment metrics is described. It contains 1700 test images (25 reference images, 17 types of distortions for each reference image, 4 different levels of each type of distortion). Mean Opinion Scores (MOS) for this database have been obtained as a result of more than 800 experiments. During these tests, observers from three countries (Finland, Italy, and Ukraine) have carried out about 256000 individual human quality judgments. The obtained MOS can be used for effective testing of different visual quality metrics as well as for the design of new metrics. Using the designed image database, we have tested several known quality metrics. The designed test image database is freely available for downloading and utilization in scientific investigations.",poster,cp13
p2262,feae36b19d8f8566aefbb8b30e9fb55c1592f0a7,j274,Nature Genetics,A gene expression database for the molecular pharmacology of cancer,Abstract content,fullPaper,jv274
p2263,49b60b92201710d095684b6df1b1d93f92122dd0,j102,Nucleic Acids Research,MINT: the Molecular INTeraction database,Abstract content,fullPaper,jv102
p2264,1dd0140d51e870a713340ae30734c8438b03d1a3,c50,International Conference on Automated Software Engineering,Unit selection in a concatenative speech synthesis system using a large speech database,"One approach to the generation of natural-sounding synthesized speech waveforms is to select and concatenate units from a large speech database. Units (in the current work, phonemes) are selected to produce a natural realisation of a target phoneme sequence predicted from text which is annotated with prosodic and phonetic context information. We propose that the units in a synthesis database can be considered as a state transition network in which the state occupancy cost is the distance between a database unit and a target, and the transition cost is an estimate of the quality of concatenation of two consecutive units. This framework has many similarities to HMM-based speech recognition. A pruned Viterbi search is used to select the best units for synthesis from the database. This approach to waveform synthesis permits training from natural speech: two methods for training from speech are presented which provide weights which produce more natural speech than can be obtained by hand-tuning.",poster,cp50
p2265,afef030037e621538ac53e18d23a1a4660df79d8,c51,Conference of the Centre for Advanced Studies on Collaborative Research,Harmonized World Soil Database (version 1.2),"METIS-ID: 167825 The Harmonized World Soil Database is a 30 arc-second raster database with over 15000 different soil mapping units that combines existing regional and national updates of soil information worldwide (SOTER, ESD, Soil Map of China, ISRIC-WISE) with the information contained within the 1:5 000 000 scale FAO-UNESCO Soil Map of the World (FAO, 1971-1981). The resulting raster database consists of ... 21600 rows and 43200 columns, which are linked to harmonized soil property data. The use of a standardized structure allows for the linkage of the attribute data with the raster map to display or query the composition in terms of soil units and the characterization of selected soil parameters (organic Carbon, pH, water storage capacity, soil depth, cation exchange capacity of the soil and the clay fraction, total exchangeable nutrients, lime and gypsum contents, sodium exchange percentage, salinity, textural class and granulometry). Reliability of the information contained in the database is variable: the parts of the database that still make use of the Soil Map of the World such as North America, Australia, West Africa and South Asia are considered less reliable, while most of the areas covered by SOTER databases are considered to have the highest reliability (Southern Africa, Latin America and the Caribbean, Central and Eastern Europe). Further expansion and update of the HWSD is foreseen for the near future, notably with the excellent databases held in the USA (Natural Resources Conservation Service US General Soil Map, STATSGO), Canada (Agriculture and AgriFood Canada: The National Soil Database NSDB), and Australia (CSIRO, ACLEP, Nnatural Heritage Trust and National Land and Water Resources Audit: ASRIS), and with the recently released SOTER database for Central Africa (FAO/ISRIC/Univ. Gent, 2007)",poster,cp51
p2266,ff2218b349f89026ffaaccdf807228fa497c04bd,c75,International Conference on Machine Learning,THE DIGITAL DATABASE FOR SCREENING MAMMOGRAPHY,Abstract content,poster,cp75
p2267,107e98602c1be84b1654d6a1b241b7c97a94c71f,c44,International Workshop on Green and Sustainable Software,"ExoCarta 2012: database of exosomal proteins, RNA and lipids","Exosomes are membraneous nanovesicles of endocytic origin released by most cell types from diverse organisms; they play a critical role in cell–cell communication. ExoCarta (http://www.exocarta.org) is a manually curated database of exosomal proteins, RNA and lipids. The database catalogs information from both published and unpublished exosomal studies. The mode of exosomal purification and characterization, the biophysical and molecular properties are listed in the database aiding biomedical scientists in assessing the quality of the exosomal preparation and the corresponding data obtained. Currently, ExoCarta (Version 3.1) contains information on 11 261 protein entries, 2375 mRNA entries and 764 miRNA entries that were obtained from 134 exosomal studies. In addition to the data update, as a new feature, lipids identified in exosomes are added to ExoCarta. We believe that this free web-based community resource will aid researchers in identifying molecular signatures (proteins/RNA/lipids) that are specific to certain tissue/cell type derived exosomes and trigger new exosomal studies.",poster,cp44
p2268,acd36b17c486957cebacc3ad68bd83ed417bf9cc,j209,Human Mutation,The IARC TP53 database: New online mutation analysis and recommendations to users,"Mutations in the tumor suppressor gene TP53 are frequent in most human cancers. Comparison of the mutation patterns in different cancers may reveal clues on the natural history of the disease. Over the past 10 years, several databases of TP53 mutations have been developed. The most extensive of these databases is maintained and developed at the International Agency for Research on Cancer. The database compiles all mutations (somatic and inherited), as well as polymorphisms, that have been reported in the published literature since 1989. The IARC TP53 mutation dataset is the largest dataset available on the variations of any human gene. The database is available at www.iarc.fr/P53/. In this paper, we describe recent developments of the database. These developments include restructuring of the database, which is now patient‐centered, with more detailed annotations on the patient (carcinogen exposure, virus infection, genetic background). In addition, a new on‐line application to retrieve somatic mutation data and analyze mutation patterns is now available. We also discuss limitations on the use of the database and provide recommendations to users. Hum Mutat 19:607–614, 2002. © 2002 Wiley‐Liss, Inc.",fullPaper,jv209
p2269,e03a1c0f0a8d95aa84bb21f2aa95e5053e4cb655,c3,Frontiers in Education Conference,miR2Disease: a manually curated database for microRNA deregulation in human disease,"‘miR2Disease’, a manually curated database, aims at providing a comprehensive resource of microRNA deregulation in various human diseases. The current version of miR2Disease documents 1939 curated relationships between 299 human microRNAs and 94 human diseases by reviewing more than 600 published papers. Around one-seventh of the microRNA–disease relationships represent the pathogenic roles of deregulated microRNA in human disease. Each entry in the miR2Disease contains detailed information on a microRNA–disease relationship, including a microRNA ID, the disease name, a brief description of the microRNA–disease relationship, an expression pattern of the microRNA, the detection method for microRNA expression, experimentally verified target gene(s) of the microRNA and a literature reference. miR2Disease provides a user-friendly interface for a convenient retrieval of each entry by microRNA ID, disease name, or target gene. In addition, miR2Disease offers a submission page that allows researchers to submit established microRNA–disease relationships that are not documented. Once approved by the submission review committee, the submitted records will be included in the database. miR2Disease is freely available at http://www.miR2Disease.org.",poster,cp3
p2270,6bd04f666425fa98a0162cdd4f4d1339d57ac15d,c52,Workshop on Learning from Authoritative Security Experiment Results,Standardizing the World Income Inequality Database,"Cross-national research on the causes and consequences of income inequality has been hindered by the limitations of existing inequality datasets: greater coverage across countries and over time is available from these sources only at the cost of signicantly reduced comparability across observations. This article presents the Standardized World Income Inequality Database (SWIID), which standardizes the United Nations University database (UNU-WIDER 2008) while minimizing reliance on problematic assumptions by using as much information as possible from proximate years within the same country. The resulting series of gross and net income inequality data maximize comparability for the largest possible sample of countries and years and so are better suited to broadly cross-national research than other sources.",poster,cp52
p2271,7071b85c83035ff86c8ed3c1f3319a304bfb7fb5,c87,European Conference on Computer Vision,The MetaCyc Database of metabolic pathways and enzymes and the BioCyc collection of Pathway/Genome Databases,"MetaCyc (MetaCyc.org) is a universal database of metabolic pathways and enzymes from all domains of life. The pathways in MetaCyc are curated from the primary scientific literature, and are experimentally determined small-molecule metabolic pathways. Each reaction in a MetaCyc pathway is annotated with one or more well-characterized enzymes. Because MetaCyc contains only experimentally elucidated knowledge, it provides a uniquely high-quality resource for metabolic pathways and enzymes. BioCyc (BioCyc.org) is a collection of more than 350 organism-specific Pathway/Genome Databases (PGDBs). Each BioCyc PGDB contains the predicted metabolic network of one organism, including metabolic pathways, enzymes, metabolites and reactions predicted by the Pathway Tools software using MetaCyc as a reference database. BioCyc PGDBs also contain predicted operons and predicted pathway hole fillers—predictions of which enzymes may catalyze pathway reactions that have not been assigned to an enzyme. The BioCyc website offers many tools for computational analysis of PGDBs, including comparative analysis and analysis of omics data in a pathway context. The BioCyc PGDBs generated by SRI are offered for adoption by any interested party for the ongoing integration of metabolic and genome-related information about an organism.",poster,cp87
p2272,fefea2b9ed93a0c3163432c52a67cf34efa868f7,c8,The Compass,SAbDab: the structural antibody database,"Structural antibody database (SAbDab; http://opig.stats.ox.ac.uk/webapps/sabdab) is an online resource containing all the publicly available antibody structures annotated and presented in a consistent fashion. The data are annotated with several properties including experimental information, gene details, correct heavy and light chain pairings, antigen details and, where available, antibody–antigen binding affinity. The user can select structures, according to these attributes as well as structural properties such as complementarity determining region loop conformation and variable domain orientation. Individual structures, datasets and the complete database can be downloaded.",poster,cp8
p2273,334511feb95634c91d06359fd497d01fc60767f7,c91,Workshop on Algorithms and Models for the Web-Graph,DIP: The Database of Interacting Proteins: 2001 update,"The Database of Interacting Proteins (DIP; http://dip.doe-mbi.ucla. edu) is a database that documents experimentally determined protein-protein interactions. Since January 2000 the number of protein-protein interactions in DIP has nearly tripled to 3472 and the number of proteins to 2659. New interactive tools have been developed to aid in the visualization, navigation and study of networks of protein interactions.",poster,cp91
p2274,66c0e24d97786f0382ce9f7acde37de9a349537c,c94,Vision,The Reptile Database,Abstract content,poster,cp94
p2275,cd4ef5b8da1a543871357f4bca7483e89ff9e3b5,c72,Intelligent Systems in Molecular Biology,CDD: a Conserved Domain Database for protein classification,"The Conserved Domain Database (CDD) is the protein classification component of NCBI's Entrez query and retrieval system. CDD is linked to other Entrez databases such as Proteins, Taxonomy and PubMed®, and can be accessed at http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=cdd. CD-Search, which is available at http://www.ncbi.nlm.nih.gov/Structure/cdd/wrpsb.cgi, is a fast, interactive tool to identify conserved domains in new protein sequences. CD-Search results for protein sequences in Entrez are pre-computed to provide links between proteins and domain models, and computational annotation visible upon request. Protein–protein queries submitted to NCBI's BLAST search service at http://www.ncbi.nlm.nih.gov/BLAST are scanned for the presence of conserved domains by default. While CDD started out as essentially a mirror of publicly available domain alignment collections, such as SMART, Pfam and COG, we have continued an effort to update, and in some cases replace these models with domain hierarchies curated at the NCBI. Here, we report on the progress of the curation effort and associated improvements in the functionality of the CDD information retrieval system.",poster,cp72
p2276,c64e51ab702ecdfc65281eb06fae8722809a0756,j372,"Behavoir research methods, instruments & computers",A lifespan database of adult facial stimuli,Abstract content,fullPaper,jv372
p2277,8162d4f3bfce2055c9a53c267af66103c3bfd167,c59,British Computer Society Conference on Human-Computer Interaction,Survey on NoSQL database,"With the development of the Internet and cloud computing, there need databases to be able to store and process big data effectively, demand for high-performance when reading and writing, so the traditional relational database is facing many new challenges. Especially in large scale and high-concurrency applications, such as search engines and SNS, using the relational database to store and query dynamic user data has appeared to be inadequate. In this case, NoSQL database created. This paper describes the background, basic characteristics, data model of NoSQL. In addition, this paper classifies NoSQL databases according to the CAP theorem. Finally, the mainstream NoSQL databases are separately described in detail, and extract some properties to help enterprises to choose NoSQL.",poster,cp59
p2278,ab99f2afd0f09ee707540022f0895a09fa1107f1,c1,Technical Symposium on Computer Science Education,Database resources of the National Center for Biotechnology,"In addition to maintaining the GenBank(R) nucleic acid sequence database, the National Center for Biotechnology Information (NCBI) provides data analysis and retrieval resources for the data in GenBank and other biological data made available through NCBI's Web site. NCBI resources include Entrez, PubMed, PubMed Central (PMC), LocusLink, the NCBITaxonomy Browser, BLAST, BLAST Link (BLink), Electronic PCR (e-PCR), Open Reading Frame (ORF) Finder, References Sequence (RefSeq), UniGene, HomoloGene, ProtEST, Database of Single Nucleotide Polymorphisms (dbSNP), Human/Mouse Homology Map, Cancer Chromosome Aberration Project (CCAP), Entrez Genomes and related tools, the Map Viewer, Model Maker (MM), Evidence Viewer (EV), Clusters of Orthologous Groups (COGs) database, Retroviral Genotyping Tools, SAGEmap, Gene Expression Omnibus (GEO), Online Mendelian Inheritance in Man (OMIM), the Molecular Modeling Database (MMDB), the Conserved Domain Database (CDD), and the Conserved Domain Architecture Retrieval Tool (CDART). Augmenting many of the Web applications are custom implementations of the BLAST program optimized to search specialized data sets. All of the resources can be accessed through the NCBI home page at: http://www.ncbi.nlm.nih.gov.",poster,cp1
p2279,999db6b11c1fe6377118081c84f79f6ae6b4262d,c47,International Symposium on Empirical Software Engineering and Measurement,"MEROPS: the database of proteolytic enzymes, their substrates and inhibitors","Peptidases, their substrates and inhibitors are of great relevance to biology, medicine and biotechnology. The MEROPS database (http://merops.sanger.ac.uk) aims to fulfil the need for an integrated source of information about these. The database has hierarchical classifications in which homologous sets of peptidases and protein inhibitors are grouped into protein species, which are grouped into families, which are in turn grouped into clans. The database has been expanded to include proteolytic enzymes other than peptidases. Special identifiers for peptidases from a variety of model organisms have been established so that orthologues can be detected in other species. A table of predicted active-site residue and metal ligand positions and the residue ranges of the peptidase domains in orthologues has been added to each peptidase summary. New displays of tertiary structures, which can be rotated or have the surfaces displayed, have been added to the structure pages. New indexes for gene names and peptidase substrates have been made available. Among the enhancements to existing features are the inclusion of small-molecule inhibitors in the tables of peptidase–inhibitor interactions, a table of known cleavage sites for each protein substrate, and tables showing the substrate-binding preferences of peptidases derived from combinatorial peptide substrate libraries.",poster,cp47
p2280,187770872ace85e4ee613731d87c61503943eaf2,c53,International Conference on Software Engineering and Knowledge Engineering,The MetaCyc database of metabolic pathways and enzymes and the BioCyc collection of Pathway/Genome Databases,"The MetaCyc database (MetaCyc.org) is a comprehensive and freely accessible database describing metabolic pathways and enzymes from all domains of life. MetaCyc pathways are experimentally determined, mostly small-molecule metabolic pathways and are curated from the primary scientific literature. MetaCyc contains >2100 pathways derived from >37 000 publications, and is the largest curated collection of metabolic pathways currently available. BioCyc (BioCyc.org) is a collection of >3000 organism-specific Pathway/Genome Databases (PGDBs), each containing the full genome and predicted metabolic network of one organism, including metabolites, enzymes, reactions, metabolic pathways, predicted operons, transport systems and pathway-hole fillers. Additions to BioCyc over the past 2 years include YeastCyc, a PGDB for Saccharomyces cerevisiae, and 891 new genomes from the Human Microbiome Project. The BioCyc Web site offers a variety of tools for querying and analysis of PGDBs, including Omics Viewers and tools for comparative analysis. New developments include atom mappings in reactions, a new representation of glycan degradation pathways, improved compound structure display, better coverage of enzyme kinetic data, enhancements of the Web Groups functionality, improvements to the Omics viewers, a new representation of the Enzyme Commission system and, for the desktop version of the software, the ability to save display states.",poster,cp53
p2281,69114cb58049c3b06ce012cf41d493d2e947b6c8,c51,Conference of the Centre for Advanced Studies on Collaborative Research,DBatVir: the database of bat-associated viruses,"Emerging infectious diseases remain a significant threat to public health. Most emerging infectious disease agents in humans are of zoonotic origin. Bats are important reservoir hosts of many highly lethal zoonotic viruses and have been implicated in numerous emerging infectious disease events in recent years. It is essential to enhance our knowledge and understanding of the genetic diversity of the bat-associated viruses to prevent future outbreaks. To facilitate further research, we constructed the database of bat-associated viruses (DBatVir). Known viral sequences detected in bat samples were manually collected and curated, along with the related metadata, such as the sampling time, location, bat species and specimen type. Additional information concerning the bats, including common names, diet type, geographic distribution and phylogeny were integrated into the database to bridge the gap between virologists and zoologists. The database currently covers >4100 bat-associated animal viruses of 23 viral families detected from 196 bat species in 69 countries worldwide. It provides an overview and snapshot of the current research regarding bat-associated viruses, which is essential now that the field is rapidly expanding. With a user-friendly interface and integrated online bioinformatics tools, DBatVir provides a convenient and powerful platform for virologists and zoologists to analyze the virome diversity of bats, as well as for epidemiologists and public health researchers to monitor and track current and future bat-related infectious diseases. Database URL: http://www.mgc.ac.cn/DBatVir/",poster,cp51
p2282,2485c98aa44131d1a2f7d1355b1e372f2bb148ad,c73,Workshop on Algorithms in Bioinformatics,The CAS-PEAL Large-Scale Chinese Face Database and Baseline Evaluations,"In this paper, we describe the acquisition and contents of a large-scale Chinese face database: the CAS-PEAL face database. The goals of creating the CAS-PEAL face database include the following: 1) providing the worldwide researchers of face recognition with different sources of variations, particularly pose, expression, accessories, and lighting (PEAL), and exhaustive ground-truth information in one uniform database; 2) advancing the state-of-the-art face recognition technologies aiming at practical applications by using off-the-shelf imaging equipment and by designing normal face variations in the database; and 3) providing a large-scale face database of Mongolian. Currently, the CAS-PEAL face database contains 99 594 images of 1040 individuals (595 males and 445 females). A total of nine cameras are mounted horizontally on an arc arm to simultaneously capture images across different poses. Each subject is asked to look straight ahead, up, and down to obtain 27 images in three shots. Five facial expressions, six accessories, and 15 lighting changes are also included in the database. A selected subset of the database (CAS-PEAL-R1, containing 30 863 images of the 1040 subjects) is available to other researchers now. We discuss the evaluation protocol based on the CAS-PEAL-R1 database and present the performance of four algorithms as a baseline to do the following: 1) elementarily assess the difficulty of the database for face recognition algorithms; 2) preference evaluation results for researchers using the database; and 3) identify the strengths and weaknesses of the commonly used algorithms.",poster,cp73
p2283,f5f4a0933cf097fb14ab84ca295b1bdfe01f97e0,c97,Interspeech,PlasmoDB: a functional genomic database for malaria parasites,"PlasmoDB (http://PlasmoDB.org) is a functional genomic database for Plasmodium spp. that provides a resource for data analysis and visualization in a gene-by-gene or genome-wide scale. PlasmoDB belongs to a family of genomic resources that are housed under the EuPathDB (http://EuPathDB.org) Bioinformatics Resource Center (BRC) umbrella. The latest release, PlasmoDB 5.5, contains numerous new data types from several broad categories—annotated genomes, evidence of transcription, proteomics evidence, protein function evidence, population biology and evolution. Data in PlasmoDB can be queried by selecting the data of interest from a query grid or drop down menus. Various results can then be combined with each other on the query history page. Search results can be downloaded with associated functional data and registered users can store their query history for future retrieval or analysis.",poster,cp97
p2284,00899cac0e2e770fe6b28deac002eceb8c3c4bea,c21,Grid Computing Environments,Phenol-Explorer: an online comprehensive database on polyphenol contents in foods,"A number of databases on the plant metabolome describe the chemistry and biosynthesis of plant chemicals. However, no such database is specifically focused on foods and more precisely on polyphenols, one of the major classes of phytochemicals. As antoxidants, polyphenols influence human health and may play a role in the prevention of a number of chronic diseases such as cardiovascular diseases, some cancers or type 2 diabetes. To determine polyphenol intake in populations and study their association with health, it is essential to have detailed information on their content in foods. However this information is not easily collected due to the variety of their chemical structures and the variability of their content in a given food. Phenol-Explorer is the first comprehensive web-based database on polyphenol content in foods. It contains more than 37 000 original data points collected from 638 scientific articles published in peer-reviewed journals. The quality of these data has been evaluated before they were aggregated to produce final representative mean content values for 502 polyphenols in 452 foods. The web interface allows making various queries on the aggregated data to identify foods containing a given polyphenol or polyphenols present in a given food. For each mean content value, it is possible to trace all original content values and their literature sources. Phenol-Explorer is a major step forward in the development of databases on food constituents and the food metabolome. It should help researchers to better understand the role of phytochemicals in the technical and nutritional quality of food, and food manufacturers to develop tailor-made healthy foods. Database URL: http://www.phenol-explorer.eu",poster,cp21
p2285,a5edce377759894482464a133cb9ec6791709eb2,c50,International Conference on Automated Software Engineering,Database System Concepts,"From the Publisher: 
This acclaimed revision of a classic database systems text offers a complete background in the basics of database design, languages, and system implementation. It provides the latest information combined with real-world examples to help readers master concepts. All concepts are presented in a technically complete yet easy-to-understand style with notations kept to a minimum. A running example of a bank enterprise illustrates concepts at work. To further optimize comprehension, figures and examples, rather than proofs, portray concepts and anticipate results.",poster,cp50
p2286,a6e72ff479fb58f0b714f07b0292c612dfe4ff05,c67,Enterprise Application Integration,Principles Of Database And Knowledge-Base Systems,"This book goes into the details of database conception and use, it tells you everything on relational databases. from theory to the actual used algorithms.",poster,cp67
p2287,5c86baa92ece7425ceb09232dddd9538c224ce5e,c5,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,The Human Oral Microbiome Database: a web accessible resource for investigating oral microbe taxonomic and genomic information,"The human oral microbiome is the most studied human microflora, but 53% of the species have not yet been validly named and 35% remain uncultivated. The uncultivated taxa are known primarily from 16S rRNA sequence information. Sequence information tied solely to obscure isolate or clone numbers, and usually lacking accurate phylogenetic placement, is a major impediment to working with human oral microbiome data. The goal of creating the Human Oral Microbiome Database (HOMD) is to provide the scientific community with a body site-specific comprehensive database for the more than 600 prokaryote species that are present in the human oral cavity based on a curated 16S rRNA gene-based provisional naming scheme. Currently, two primary types of information are provided in HOMD—taxonomic and genomic. Named oral species and taxa identified from 16S rRNA gene sequence analysis of oral isolates and cloning studies were placed into defined 16S rRNA phylotypes and each given unique Human Oral Taxon (HOT) number. The HOT interlinks phenotypic, phylogenetic, genomic, clinical and bibliographic information for each taxon. A BLAST search tool is provided to match user 16S rRNA gene sequences to a curated, full length, 16S rRNA gene reference data set. For genomic analysis, HOMD provides comprehensive set of analysis tools and maintains frequently updated annotations for all the human oral microbial genomes that have been sequenced and publicly released. Oral bacterial genome sequences, determined as part of the Human Microbiome Project, are being added to the HOMD as they become available. We provide HOMD as a conceptual model for the presentation of microbiome data for other human body sites. Database URL: http://www.homd.org",poster,cp5
p2288,81e89f25baed869a690ffc6f93cd0306c58efe14,c39,International Conference on Global Software Engineering,The Mammographic Image Analysis Society digital mammogram database,"A clamp or grip for heavy duty work with twisted wire cables and the like, such as in marine and industrial uses and especially where reasonably easy application of the cable grip to the cable is important and undue bending moments on the heavy cables are to be avoided. The feature of a removable jaw is coupled with dual link bar structure for the jaws without sacrificing strength and with a considerable reduction in overall weight of the clamp as compared to presently available equipment, this being accomplished in part by elimination of a frame as such and providing the principal jaw with a slotted stabilizing arm having a sliding connection with a unique hanger bar, which latter is designed to be connected to the lift hook of a crane or the like.",poster,cp39
p2289,5ab169aed6e76c20621a23c411f651aac423efe3,c31,International Conference on Evaluation & Assessment in Software Engineering,"Principles of Database and Knowledge-Base Systems, Volume II",Abstract content,poster,cp31
p2290,2055c63fd081abf321ad0ff61987df112f8871c4,j373,ACM Transactions on Database Systems,Extending the database relational model to capture more meaning,"During the last three or four years several investigators have been exploring “semantic models” for formatted databases. The intent is to capture (in a more or less formal way) more of the meaning of the data so that database design can become more systematic and the database system itself can behave more intelligently. Two major thrusts are clear. (1) the search for meaningful units that are as small as possible—atomic semantics; (2) the search for meaningful units that are larger than the usual n-ary relation—molecular semantics. In this paper we propose extensions to the relational model to support certain atomic and molecular semantics. These extensions represent a synthesis of many ideas from the published work in semantic modeling plus the introduction of new rules for insertion, update, and deletion, as well as new algebraic operators.",fullPaper,jv373
p2291,db45667093e4fa4f95bc402c10b460052119717f,c5,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,mVOC: a database of microbial volatiles,"Scents are well known to be emitted from flowers and animals. In nature, these volatiles are responsible for inter- and intra-organismic communication, e.g. attraction and defence. Consequently, they influence and improve the establishment of organisms and populations in ecological niches by acting as single compounds or in mixtures. Despite the known wealth of volatile organic compounds (VOCs) from species of the plant and animal kingdom, in the past, less attention has been focused on volatiles of microorganisms. Although fast and affordable sequencing methods facilitate the detection of microbial diseases, however, the analysis of signature or fingerprint volatiles will be faster and easier. Microbial VOCs (mVOCs) are presently used as marker to detect human diseases, food spoilage or moulds in houses. Furthermore, mVOCs exhibited antagonistic potential against pathogens in vitro, but their biological roles in the ecosystems remain to be investigated. Information on volatile emission from bacteria and fungi is presently scattered in the literature, and no public and up-to-date collection on mVOCs is available. To address this need, we have developed mVOC, a database available online at http://bioinformatics.charite.de/mvoc.",poster,cp5
p2292,c378df148d9e42376e4f47888b04a1f679d2e25f,c29,International Conference on Software Engineering,XCOM : Photon Cross Sections Database,Abstract content,poster,cp29
p2293,49ed15db181c74c7067ec01800fb5392411c868c,c103,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,Epidemic algorithms for replicated database maintenance,"Whru a dilt~lhSC is replicated at, many sites2 maintaining mutual consistrnry among t,he sites iu the fac:e of updat,es is a signitirant problem. This paper descrikrs several randomized algorit,hms for dist,rihut.ing updates and driving t,he replicas toward consist,c>nc,y. The algorit Inns are very simple and require few guarant,ees from the underlying conllllunicat.ioll system, yc+ they rnsutc t.hat. the off(~c~t, of (‘very update is evcnt,uwlly rf+irt-ted in a11 rq1ica.s. The cost, and parformancc of t,hr algorithms arc tuned I>? c%oosing appropriat,c dist,rilMions in t,hc randoinizat,ioii step. TIN> idgoritlmls ilr(’ c*los~*ly analogoIls t,o epidemics, and t,he epidcWliolog)litc\ratiirc, ilitlh iii Illld~~rsti4lldill~ tlicir bc*liavior. One of tlW i$,oritlims 11&S brc>n implrmcWrd in the Clraringhousr sprv(brs of thr Xerox C’orporat~c~ Iiitcrnc4, solviiig long-standing prol>lf~lns of high traffic and tlatirl>ilsr inconsistcllcp.",fullPaper,cp103
p2294,fbbe83989ce11b91ab0dfb0d5e822b08cd885f48,c52,Workshop on Learning from Authoritative Security Experiment Results,The UCSC Genome Browser database: update 2010,"The University of California, Santa Cruz (UCSC) Genome Browser website (http://genome.ucsc.edu/) provides a large database of publicly available sequence and annotation data along with an integrated tool set for examining and comparing the genomes of organisms, aligning sequence to genomes, and displaying and sharing users’ own annotation data. As of September 2009, genomic sequence and a basic set of annotation ‘tracks’ are provided for 47 organisms, including 14 mammals, 10 non-mammal vertebrates, 3 invertebrate deuterostomes, 13 insects, 6 worms and a yeast. New data highlights this year include an updated human genome browser, a 44-species multiple sequence alignment track, improved variation and phenotype tracks and 16 new genome-wide ENCODE tracks. New features include drag-and-zoom navigation, a Wiki track for user-added annotations, new custom track formats for large datasets (bigBed and bigWig), a new multiple alignment output tool, links to variation and protein structure tools, in silico PCR utility enhancements, and improved track configuration tools.",poster,cp52
p2295,c9fc7bf985fde7246f6139809cc7b019fd1ae007,c65,Formal Concept Analysis,The Forest Inventory and Analysis Database: Database Description and Users Manual Version 4.0 for Phase 2,"This document is based on previous documentation of the nationally standardized Forest Inventory and Analysis database (Hansen and others 1992; Woudenberg and Farrenkopf 1995; Miles and others 2001). Documentation of the structure of the Forest Inventory and Analysis database (FIADB) for Phase 2 data, as well as codes and definitions, is provided. Examples for producing population level estimates are also presented. This database provides a consistent framework for storing forest inventory data across all ownerships for the entire United States. These data are available to the public.",poster,cp65
p2296,269a6271fa98bbdc2d456bae7fb419a77c88dc70,c77,Networks,D2P2: database of disordered protein predictions,"We present the Database of Disordered Protein Prediction (D2P2), available at http://d2p2.pro (including website source code). A battery of disorder predictors and their variants, VL-XT, VSL2b, PrDOS, PV2, Espritz and IUPred, were run on all protein sequences from 1765 complete proteomes (to be updated as more genomes are completed). Integrated with these results are all of the predicted (mostly structured) SCOP domains using the SUPERFAMILY predictor. These disorder/structure annotations together enable comparison of the disorder predictors with each other and examination of the overlap between disordered predictions and SCOP domains on a large scale. D2P2 will increase our understanding of the interplay between disorder and structure, the genomic distribution of disorder, and its evolutionary history. The parsed data are made available in a unified format for download as flat files or SQL tables either by genome, by predictor, or for the complete set. An interactive website provides a graphical view of each protein annotated with the SCOP domains and disordered regions from all predictors overlaid (or shown as a consensus). There are statistics and tools for browsing and comparing genomes and their disorder within the context of their position on the tree of life.",poster,cp77
p2297,42d2032f6a0372e211fe2908ef1349c3b3236723,c2,International Symposium on Intelligent Data Analysis,The ribosomal database project,"The Ribosomal Database Project (RDP) is a curated database that offers ribosome data along with related programs and services. The offerings include phylogenetically ordered alignments of ribosomal RNA (rRNA) sequences, derived phylogenetic trees, rRNA secondary structure diagrams and various software packages for handling, analyzing and displaying alignments and trees. The data are available via ftp and electronic mail. Certain analytic services are also provided by the electronic mail server.",poster,cp2
p2298,9ebe338e49e63ff97348aca0db521ac3ff01bcef,c71,IEEE International Conference on Information Reuse and Integration,CDD: specific functional annotation with the Conserved Domain Database,"NCBI's Conserved Domain Database (CDD) is a collection of multiple sequence alignments and derived database search models, which represent protein domains conserved in molecular evolution. The collection can be accessed at http://www.ncbi.nlm.nih.gov/Structure/cdd/cdd.shtml, and is also part of NCBI's Entrez query and retrieval system, cross-linked to numerous other resources. CDD provides annotation of domain footprints and conserved functional sites on protein sequences. Precalculated domain annotation can be retrieved for protein sequences tracked in NCBI's Entrez system, and CDD's collection of models can be queried with novel protein sequences via the CD-Search service at http://www.ncbi.nlm.nih.gov/Structure/cdd/wrpsb.cgi. Starting with the latest version of CDD, v2.14, information from redundant and homologous domain models is summarized at a superfamily level, and domain annotation on proteins is flagged as either ‘specific’ (identifying molecular function with high confidence) or as ‘non-specific’ (identifying superfamily membership only).",poster,cp71
p2299,b0bf5eb499c483b94efd57135cf9572f2bb4bb8f,c14,International Conference on Exploring Services Science,Systemic Banking Crises Database; An Update,"We update the widely used banking crises database by Laeven and Valencia (2008, 2010) with new information on recent and ongoing crises, including updated information on policy responses and outcomes (i.e. fiscal costs, output losses, and increases in public debt). We also update our dating of sovereign debt and currency crises. The database includes all systemic banking, currency, and sovereign debt crises during the period 1970-2011. The data show some striking differences in policy responses between advanced and emerging economies as well as many similarities between past and ongoing crises.",poster,cp14
p2300,2ba6fa80a995b5290b2e3c17f35f04c5d2187d5c,c60,IEEE International Conference on Software Engineering and Formal Methods,Database indexing for production MegaBLAST searches,"Motivation: The BLAST software package for sequence comparison speeds up homology search by preprocessing a query sequence into a lookup table. Numerous research studies have suggested that preprocessing the database instead would give better performance. However, production usage of sequence comparison methods that preprocess the database has been limited to programs such as BLAT and SSAHA that are designed to find matches when query and database subsequences are highly similar. Results: We developed a new version of the MegaBLAST module of BLAST that does the initial phase of finding short seeds for matches by searching a database index. We also developed a program makembindex that preprocesses the database into a data structure for rapid seed searching. We show that the new ‘indexed MegaBLAST’ is faster than the ‘non-indexed’ version for most practical uses. We show that indexed MegaBLAST is faster than miBLAST, another implementation of BLAST nucleotide searching with a preprocessed database, for most of the 200 queries we tested. To deploy indexed MegaBLAST as part of NCBI'sWeb BLAST service, the storage of databases and the queueing mechanism were modified, so that some machines are now dedicated to serving queries for a specific database. The response time for such Web queries is now faster than it was when each computer handled queries for multiple databases. Availability: The code for indexed MegaBLAST is part of the blastn program in the NCBI C++ toolkit. The preprocessor program makembindex is also in the toolkit. Indexed MegaBLAST has been used in production on NCBI's Web BLAST service to search one version of the human and mouse genomes since October 2007. The Linux command-line executables for blastn and makembindex, documentation, and some query sets used to carry out the tests described below are available in the directory: ftp://ftp.ncbi.nlm.nih.gov/pub/agarwala/indexed_megablast Contact: schaffer@helix.nih.gov Supplementary information: Supplementary data are available at Bioinformatics online.",poster,cp60
p2301,04a10e1b25f35a9ac1a4d4344bfbdb34b253cb59,j374,International Journal on Document Analysis and Recognition,The IAM-database: an English sentence database for offline handwriting recognition,Abstract content,fullPaper,jv374
p2302,c80b987fe2d52214772f435417cb6666f60613d2,c26,PS,An Introduction to Database Systems,"From the Publisher: 
For over 25 years, C. J. Date's An Introduction to Database Systems has been the authoritative resource for readers interested in gaining insight into and understanding of the principles of database systems. This revision continues to provide a solid grounding in the foundations of database technology and to provide some ideas as to how the field is likely to develop in the future.. ""Readers of this book will gain a strong working knowledge of the overall structure, concepts, and objectives of database systems and will become familiar with the theoretical principles underlying the construction of such systems.",poster,cp26
p2303,ff89306dcc77b387f01718b497df0116c87c260d,c64,Experimental Software Engineering Network,The IMGT/HLA database,"It is 10 years since the IMGT/HLA database was released, providing the HLA community with a searchable repository of highly curated HLA sequences. The HLA complex is located within the 6p21.3 region of human chromosome 6 and contains more than 220 genes of diverse function. Many of the genes encode proteins of the immune system and are highly polymorphic. The naming of these HLA genes and alleles, and their quality control is the responsibility of the WHO Nomenclature Committee for Factors of the HLA System. Through the work of the HLA Informatics Group and in collaboration with the European Bioinformatics Institute, we are able to provide public access to this data through the website http://www.ebi.ac.uk/imgt/hla/. The first release contained 964 sequences, the most recent release 3300 sequences, with around 450 new sequences been added each year. The tools provided on the website have been updated to allow more complex alignments, which include genomic sequence data, as well as the development of tools for probe and primer design and the inclusion of data from the HLA Dictionary. Regular updates to the website ensure that new and confirmatory sequences are dispersed to the HLA community, and the wider research and clinical communities.",poster,cp64
p2304,6082f99fd783e1c698441e881a311793a6b1bb4d,j242,Genome Medicine,The Human Gene Mutation Database: 2008 update,Abstract content,fullPaper,jv242
p2305,b7e3bec7efda0946a73d8cfa06550ef2b1a2b2bd,j375,British Journal of Cancer,The COSMIC (Catalogue of Somatic Mutations in Cancer) database and website,Abstract content,fullPaper,jv375
p2306,2a75f34663a60ab1b04a0049ed1d14335129e908,c104,IEEE International Conference on Multimedia and Expo,Web-based database for facial expression analysis,"In the last decade, the research topic of automatic analysis of facial expressions has become a central topic in machine vision research. Nonetheless, there is a glaring lack of a comprehensive, readily accessible reference set of face images that could be used as a basis for benchmarks for efforts in the field. This lack of easily accessible, suitable, common testing resource forms the major impediment to comparing and extending the issues concerned with automatic facial expression analysis. In this paper, we discuss a number of issues that make the problem of creating a benchmark facial expression database difficult. We then present the MMI facial expression database, which includes more than 1500 samples of both static images and image sequences of faces in frontal and in profile view displaying various expressions of emotion, single and multiple facial muscle activation. It has been built as a Web-based direct-manipulation application, allowing easy access and easy search of the available images. This database represents the most comprehensive reference set of images for studies on facial expression analysis to date.",fullPaper,cp104
p2307,015525f864ccaf28efbdaed46029598441121a9e,c26,PS,UCID: an uncompressed color image database,"Standardised image databases or rather the lack of them are one of the main weaknesses in the field of content based image retrieval (CBIR). Authors often use their own images or do not specify the source of their datasets. Naturally this makes comparison of results somewhat difficult. While a first approach towards a common colour image set has been taken by the MPEG 7 committee their database does not cater for all strands of research in the CBIR community. In particular as the MPEG-7 images only exist in compressed form it does not allow for an objective evaluation of image retrieval algorithms that operate in the compressed domain or to judge the influence image compression has on the performance of CBIR algorithms. In this paper we introduce a new dataset, UCID (pronounced ""use it"") - an Uncompressed Colour Image Dataset which tries to bridge this gap. The UCID dataset currently consists of 1338 uncompressed images together with a ground truth of a series of query images with corresponding models that an ideal CBIR algorithm would retrieve. While its initial intention was to provide a dataset for the evaluation of compressed domain algorithms, the UCID database also represents a good benchmark set for the evaluation of any kind of CBIR method as well as an image set that can be used to evaluate image compression and colour quantisation algorithms.",poster,cp26
p2308,a35934a0b4c63925ee27d2bd77c75b31cf6d8072,c42,Joint Conference of International Workshop on Software Measurement and International Conference on Software Process and Product Measurement,VFDB: a reference database for bacterial virulence factors,"Bacterial pathogens continue to impose a major threat to public health worldwide in the 21st century. Intensified studies on bacterial pathogenesis have greatly expanded our knowledge about the mechanisms of the disease processes at the molecular level over the last decades. To facilitate future research, it becomes necessary to form a database collectively presenting the virulence factors (VFs) of various medical significant bacterial pathogens. The aim of virulence factor database (VFDB) (http://www.mgc.ac.cn/VFs/) is to provide such a source for scientists to rapidly access to current knowledge about VFs from various bacterial pathogens. VFDB is comprehensive and user-friendly. One can search VFDB by browsing each genus or by typing keywords. Furthermore, a BLAST search tool against all known VF-related genes is also available. VFDB provides a unified gateway to store, search, retrieve and update information about VFs from various bacterial pathogens.",poster,cp42
p2309,9c1411c9ebc6260edc5798c9339e189e759b2168,c73,Workshop on Algorithms in Bioinformatics,GMD@CSB.DB: the Golm Metabolome Database,"UNLABELLED
Metabolomics, in particular gas chromatography-mass spectrometry (GC-MS) based metabolite profiling of biological extracts, is rapidly becoming one of the cornerstones of functional genomics and systems biology. Metabolite profiling has profound applications in discovering the mode of action of drugs or herbicides, and in unravelling the effect of altered gene expression on metabolism and organism performance in biotechnological applications. As such the technology needs to be available to many laboratories. For this, an open exchange of information is required, like that already achieved for transcript and protein data. One of the key-steps in metabolite profiling is the unambiguous identification of metabolites in highly complex metabolite preparations from biological samples. Collections of mass spectra, which comprise frequently observed metabolites of either known or unknown exact chemical structure, represent the most effective means to pool the identification efforts currently performed in many laboratories around the world. Here we present GMD, The Golm Metabolome Database, an open access metabolome database, which should enable these processes. GMD provides public access to custom mass spectral libraries, metabolite profiling experiments as well as additional information and tools, e.g. with regard to methods, spectral information or compounds. The main goal will be the representation of an exchange platform for experimental research activities and bioinformatics to develop and improve metabolomics by multidisciplinary cooperation.


AVAILABILITY
http://csbdb.mpimp-golm.mpg.de/gmd.html


CONTACT
Steinhauser@mpimp-golm.mpg.de


SUPPLEMENTARY INFORMATION
http://csbdb.mpimp-golm.mpg.de/",poster,cp73
p2310,fa7fa53b825228f29ac80e1a4b3d9c10e5870843,c46,Brazilian Symposium on Software Engineering,"Output, Input and Productivity Measures at the Industry Level: The EU KLEMS Database","This article describes the contents and the construction of the EU KLEMS Growth and Productivity Accounts. This database contains industry-level measures of output, inputs and productivity for 25 European countries, Japan and the US for the period from 1970 onwards. The article considers the methodology employed in constructing the database and shows how it can be useful in comparing productivity trends. Although growth accounts are the organising principle, it is argued that the database is useful for a wider range of applications. We give some guidance to prudent use and indicate possible extensions. Copyright © The Author(s). Journal compilation © Royal Economic Society 2009.",poster,cp46
p2311,20f5782a5fed99979ae406849d7d11bd59314996,j274,Nature Genetics,dbEST — database for “expressed sequence tags”,Abstract content,fullPaper,jv274
p2312,aec58dd1363d679e9e1d7917d74c0bcde504b65f,j376,Novartis Foundation symposium,The KEGG database.,"KEGG (http://www.genome.ad.jp/kegg/) is a suite of databases and associated software for understanding and simulating higher-order functional behaviours of the cell or the organism from its genome information. First, KEGG computerizes data and knowledge on protein interaction networks (PATHWAY database) and chemical reactions (LIGAND database) that are responsible for various cellular processes. Second, KEGG attempts to reconstruct protein interaction networks for all organisms whose genomes are completely sequenced (GENES and SSDB databases). Third, KEGG can be utilized as reference knowledge for functional genomics (EXPRESSION database) and proteomics (BRITE database) experiments. I will review the current status of KEGG and report on new developments in graph representation and graph computations.",fullPaper,jv376
p2313,87842063a79dbd0dbe550d9d9f613a218e179a30,c41,Software Product Lines Conference,Parallel database systems: the future of high performance database systems,"The success of these systems refutes a 1983 paper predicting the demise of database machines [3]. Ten years ago the future of highly parallel database machines seemed gloomy, even to their staunchest advocates. Most database machine research had focused on specialized, often trendy, hardware such as CCD memories, bubble memories, head-per-track disks, and optical disks. None of these technologies fulfilled their promises; so there was a sense that conventional CPUs , electronic RAM, and mcving-head magnetic disks would dominate the scene for many years to come. At that time, disk throughput was predicted to double while processor speeds were predicted to increase by much larger factors. Consequently , critics predicted that multiprocessor systems would scxm be I/O limited unless a solution to the I/O bottleneck was found. Whiie these predictions were fairly accurate about the future of hardware, the critics were certainly wrong about the overall future of parallel database systems. Over the last decade 'Eradata, Tandem, and a host of startup companies have successfully developed and marketed highly parallel machines.",poster,cp41
p2314,2acf7e58f0a526b957be2099c10aab693f795973,c105,Biometrics and Identity Management,Bosphorus Database for 3D Face Analysis,Abstract content,fullPaper,cp105
p2315,02e72b05d309cbc7652666c72a2dbb0bc68cd9ea,c28,International Conference on Collaboration Technologies and Systems,Immune epitope database analysis resource,"The immune epitope database analysis resource (IEDB-AR: http://tools.iedb.org) is a collection of tools for prediction and analysis of molecular targets of T- and B-cell immune responses (i.e. epitopes). Since its last publication in the NAR webserver issue in 2008, a new generation of peptide:MHC binding and T-cell epitope predictive tools have been added. As validated by different labs and in the first international competition for predicting peptide:MHC-I binding, their predictive performances have improved considerably. In addition, a new B-cell epitope prediction tool was added, and the homology mapping tool was updated to enable mapping of discontinuous epitopes onto 3D structures. Furthermore, to serve a wider range of users, the number of ways in which IEDB-AR can be accessed has been expanded. Specifically, the predictive tools can be programmatically accessed using a web interface and can also be downloaded as software packages.",poster,cp28
p2316,0758a501039f9e2dfb7607507f9734155c52c7fc,c41,Software Product Lines Conference,The Comparative Toxicogenomics Database: update 2013,"The Comparative Toxicogenomics Database (CTD; http://ctdbase.org/) provides information about interactions between environmental chemicals and gene products and their relationships to diseases. Chemical–gene, chemical–disease and gene–disease interactions manually curated from the literature are integrated to generate expanded networks and predict many novel associations between different data types. CTD now contains over 15 million toxicogenomic relationships. To navigate this sea of data, we added several new features, including DiseaseComps (which finds comparable diseases that share toxicogenomic profiles), statistical scoring for inferred gene–disease and pathway–chemical relationships, filtering options for several tools to refine user analysis and our new Gene Set Enricher (which provides biological annotations that are enriched for gene sets). To improve data visualization, we added a Cytoscape Web view to our ChemComps feature, included color-coded interactions and created a ‘slim list’ for our MEDIC disease vocabulary (allowing diseases to be grouped for meta-analysis, visualization and better data management). CTD continues to promote interoperability with external databases by providing content and cross-links to their sites. Together, this wealth of expanded chemical–gene–disease data, combined with novel ways to analyze and view content, continues to help users generate testable hypotheses about the molecular mechanisms of environmental diseases.",poster,cp41
p2317,aae0cdee99c3eca297f18f89c4b03b69c43fada8,c65,Formal Concept Analysis,Encyclopedia of Database Systems,Abstract content,poster,cp65
p2318,987a42cc5a8d7c8536e7e5a308b1ba6aa15d454f,c11,Hawaii International Conference on System Sciences,The UMIST database for astrochemistry 2012,"We present the fifth release of the UMIST Database for Astrochemistry (UDfA). The new reaction network contains 6173 gas-phase reactions, involving 467 species, 47 of which are new to this release. We have updated rate coefficients across all reaction types. We have included 1171 new anion reactions and updated and reviewed all photorates. In addition to the usual reaction network, we also now include, for download, state-specific deuterated rate coefficients, deuterium exchange reactions and a list of surface binding energies for many neutral species. Where possible, we have referenced the original source of all new and existing data. We have tested the main reaction network using a dark cloud model and a carbon-rich circumstellar envelope model. We present and briefly discuss the results of these models.",poster,cp11
p2319,aa7b1246e367b5a1154bdc877558a8a4a8474f96,c99,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,The MRC Psycholinguistic Database,"This paper describes a computerised database of psycholinguistic information. Semantic, syntactic, phonological and orthographic information about some or all of the 98,538 words in the database is accessible, by using a specially-written and very simple programming language. Word-association data are also included in the database. Some examples are given of the use of the database for selection of stimuli to be used in psycholinguistic experimentation or linguistic research.",poster,cp99
p2320,11fdda41735869a5962b698e9d4fc6524ee96d4c,j377,Journal of Applied Crystallography,Crystallography Open Database – an open-access collection of crystal structures,"The Crystallography Open Database (COD) is an ongoing initiative by crystallographers to gather all published inorganic, metal–organic and small organic molecule structures in one database, providing a straightforward search and retrieval interface. The COD adopts an open-access model for its >80 000 structure files.",fullPaper,jv377
p2321,995c2d648db2a6d6dcab19d58494b3127c1fa175,c80,International Conference on Learning Representations,The Object Database Standard: ODMG 2.0,"This book is the first of its kind and is produced as a result of the efforts by a consortium of database companies called the Object Database Management Group (ODMG). With this book, standards are defined for object management systems and this will be the foundational book for object-oriented database product.",poster,cp80
p2322,240faf3bbdea0673f5bd6e3668e4de1de905ceee,c87,European Conference on Computer Vision,"PROSITE, a protein domain database for functional characterization and annotation","PROSITE consists of documentation entries describing protein domains, families and functional sites, as well as associated patterns and profiles to identify them. It is complemented by ProRule, a collection of rules based on profiles and patterns, which increases the discriminatory power of these profiles and patterns by providing additional information about functionally and/or structurally critical amino acids. PROSITE is largely used for the annotation of domain features of UniProtKB/Swiss-Prot entries. Among the 983 (DNA-binding) domains, repeats and zinc fingers present in Swiss-Prot (release 57.8 of 22 September 2009), 696 (∼70%) are annotated with PROSITE descriptors using information from ProRule. In order to allow better functional characterization of domains, PROSITE developments focus on subfamily specific profiles and a new profile building method giving more weight to functionally important residues. Here, we describe AMSA, an annotated multiple sequence alignment format used to build a new generation of generalized profiles, the migration of ScanProsite to Vital-IT, a cluster of 633 CPUs, and the adoption of the Distributed Annotation System (DAS) to facilitate PROSITE data integration and interchange with other sources. The latest version of PROSITE (release 20.54, of 22 September 2009) contains 1308 patterns, 863 profiles and 869 ProRules. PROSITE is accessible at: http://www.expasy.org/prosite/.",poster,cp87
p2323,e72c49c059ac9926cc7e25d18971300f7ec2feef,c14,International Conference on Exploring Services Science,Principles of transaction-oriented database recovery,"In this paper, a terminological framework is provided for describing different transactionoriented recovery schemes for database systems in a conceptual rather than an implementation-dependent way. By introducing the terms materialized database, propagation strategy, and checkpoint, we obtain a means for classifying arbitrary implementations from a unified viewpoint. This is complemented by a classification scheme for logging techniques, which are precisely defined by using the other terms. It is shown that these criteria are related to all relevant questions such as speed and scope of recovery and amount of redundant information required. The primary purpose of this paper, however, is to establish an adequate and precise terminology for a topic in which the confusion of concepts and implementational aspects still imposes a lot of problems.",poster,cp14
p2324,755ef09cc0f7593b792482edc5bf799138243acf,j105,Behavior Research Methods,The Geneva affective picture database (GAPED): a new 730-picture database focusing on valence and normative significance,Abstract content,fullPaper,jv105
p2325,5745c2aff2a61d6f4a5b1083663fe7ed54d22672,c56,European Conference on Software Process Improvement,MODOMICS: a database of RNA modification pathways,"MODOMICS is the first comprehensive database resource for systems biology of RNA modification. It integrates information about the chemical structure of modified nucleosides, their localization in RNA sequences, pathways of their biosynthesis and enzymes that carry out the respective reactions. MODOMICS also provides literature information, and links to other databases, including the available protein sequence and structure data. The current list of modifications and pathways is comprehensive, while the dataset of enzymes is limited to Escherichia coli and Saccharomyces cerevisiae and sequence alignments are presented only for tRNAs from these organisms. RNAs and enzymes from other organisms will be included in the near future. MODOMICS can be queried by the type of nucleoside (e.g. A, G, C, U, I, m1A, nm5s2U, etc.), type of RNA, position of a particular nucleoside, type of reaction (e.g. methylation, thiolation, deamination, etc.) and name or sequence of an enzyme of interest. Options for data presentation include graphs of pathways involving the query nucleoside, multiple sequence alignments of RNA sequences and tabular forms with enzyme and literature data. The contents of MODOMICS can be accessed through the World Wide Web at .",poster,cp56
p2326,462098bcc81f5d8b8a067aa0b8988adba0eef91f,c7,European Conference on Modelling and Simulation,BioNumbers—the database of key numbers in molecular and cell biology,"BioNumbers (http://www.bionumbers.hms.harvard.edu) is a database of key numbers in molecular and cell biology—the quantitative properties of biological systems of interest to computational, systems and molecular cell biologists. Contents of the database range from cell sizes to metabolite concentrations, from reaction rates to generation times, from genome sizes to the number of mitochondria in a cell. While always of importance to biologists, having numbers in hand is becoming increasingly critical for experimenting, modeling, and analyzing biological systems. BioNumbers was motivated by an appreciation of how long it can take to find even the simplest number in the vast biological literature. All numbers are taken directly from a literature source and that reference is provided with the number. BioNumbers is designed to be highly searchable and queries can be performed by keywords or browsed by menus. BioNumbers is a collaborative community platform where registered users can add content and make comments on existing data. All new entries and commentary are curated to maintain high quality. Here we describe the database characteristics and implementation, demonstrate its use, and discuss future directions for its development.",poster,cp7
p2327,7b472238215ac399e119ef152c0ff93f2df1c8e6,c34,IEEE Working Conference on Mining Software Repositories,Rfam: updates to the RNA families database,"Rfam is a collection of RNA sequence families, represented by multiple sequence alignments and covariance models (CMs). The primary aim of Rfam is to annotate new members of known RNA families on nucleotide sequences, particularly complete genomes, using sensitive BLAST filters in combination with CMs. A minority of families with a very broad taxonomic range (e.g. tRNA and rRNA) provide the majority of the sequence annotations, whilst the majority of Rfam families (e.g. snoRNAs and miRNAs) have a limited taxonomic range and provide a limited number of annotations. Recent improvements to the website, methodologies and data used by Rfam are discussed. Rfam is freely available on the Web at http://rfam.sanger.ac.uk/and http://rfam.janelia.org/.",poster,cp34
p2328,d6cff906315e29b61afcf14bf7e6fe40f4d74ea5,c90,Computer Vision and Pattern Recognition,A large-scale hierarchical image database,Abstract content,fullPaper,cp90
p2329,250718af678dedb2938775f9ca6fb8601749778a,c102,International Conference on Biometrics,Social Conflict in Africa: A New Database,"We describe the Social Conflict in Africa Database (SCAD), a new event dataset for conducting research and analysis on various forms of social and political unrest in Africa. SCAD contains information on over 7,200 instances of protests, riots, strikes, government repression, communal violence, and other forms of unrest for 47 African countries from 1990–2010. SCAD includes information on event dates, actors and targets, lethality, georeferenced location information, and other conflict attributes. This article gives an overview of the data collection process, presents descriptive statistics and trends across the continent, and compares SCAD to the widely used Banks event data. We believe that SCAD will be a useful resource for scholars across multiple disciplines as well as for the policy community.",poster,cp102
p2330,100f4767f087e858976013b7117f38d26bb32a66,j206,Analytical Chemistry,Method to correlate tandem mass spectra of modified peptides to amino acid sequences in the protein database.,"A method to correlate uninterpreted tandem mass spectra of modified peptides, produced under low-energy (10-50 eV) collision conditions, with amino acid sequences in a protein database has been developed. The fragmentation patterns observed in the tandem mass spectra of peptides containing covalent modifications is used to directly search and fit linear amino acid sequences in the database. Specific information relevant to sites of modification is not contained in the character-based sequence information of the databases. The search method considers each putative modification site as both modified and unmodified in one pass through the database and simultaneously considers up to three different sites of modification. The search method will identify the correct sequence if the tandem mass spectrum did not represent a modified peptide. This approach is demonstrated with peptides containing modifications such as S-carboxymethylated cysteine, oxidized methionine, phosphoserine, phosphothreonine, or phosphotyrosine. In addition, a scanning approach is used in which neutral loss scans are used to initiate the acquisition of product ion MS/MS spectra of doubly charged phosphorylated peptides during a single chromatographic run for data analysis with the database-searching algorithm. The approach described in this paper provides a convenient method to match the nascent tandem mass spectra of modified peptides to sequences in a protein database and thereby identify previously unknown sites of modification.",fullPaper,jv206
p2331,4d130f394bd16320ac41112eebd3af74a129c6be,c33,International Conference on Agile Software Development,A KINETIC DATABASE FOR ASTROCHEMISTRY (KIDA),"We present a novel chemical database for gas-phase astrochemistry. Named the KInetic Database for Astrochemistry (KIDA), this database consists of gas-phase reactions with rate coefficients and uncertainties that will be vetted to the greatest extent possible. Submissions of measured and calculated rate coefficients are welcome, and will be studied by experts before inclusion into the database. Besides providing kinetic information for the interstellar medium, KIDA is planned to contain such data for planetary atmospheres and for circumstellar envelopes. Each year, a subset of the reactions in the database (kida.uva) will be provided as a network for the simulation of the chemistry of dense interstellar clouds with temperatures between 10 K and 300 K. We also provide a code, named Nahoon, to study the time-dependent gas-phase chemistry of zero-dimensional and one-dimensional interstellar sources.",poster,cp33
p2332,fd05e825c5d076b17626996a78bdff5e7752549d,c100,ACM SIGMOD Conference,ChEBI: a database and ontology for chemical entities of biological interest,"Chemical Entities of Biological Interest (ChEBI) is a freely available dictionary of molecular entities focused on ‘small’ chemical compounds. The molecular entities in question are either natural products or synthetic products used to intervene in the processes of living organisms. Genome-encoded macromolecules (nucleic acids, proteins and peptides derived from proteins by cleavage) are not as a rule included in ChEBI. In addition to molecular entities, ChEBI contains groups (parts of molecular entities) and classes of entities. ChEBI includes an ontological classification, whereby the relationships between molecular entities or classes of entities and their parents and/or children are specified. ChEBI is available online at http://www.ebi.ac.uk/chebi/",poster,cp100
p2333,607ca163b2635f9992e773d1b1d07d39d5d33f4e,j5,Genome Biology,"DAVID: Database for Annotation, Visualization, and Integrated Discovery",Abstract content,fullPaper,jv5
p2334,f322e882eec09709f7f7c2d7824722509b79f5e9,c46,Brazilian Symposium on Software Engineering,Principles of Database Systems,"A large part is a description of relations, their algebra and calculus, and the query languages that have been designed using these concepts. There are explanations of how the theory can be used to design good systems. A description of the optimization of queries in relation-based query languages is provided, and a chapter is devoted to the recently developed protocols for guaranteeing consistency in databases that are operated on by many processes concurrently",poster,cp46
p2335,a7ece6b4ad1f1688ba8afe3720e7f6942ec68f58,c70,International Conference on Intelligent Robotics and Applications,The MetaCyc database of metabolic pathways and enzymes and the BioCyc collection of pathway/genome databases,"The MetaCyc database (http://metacyc.org/) provides a comprehensive and freely accessible resource for metabolic pathways and enzymes from all domains of life. The pathways in MetaCyc are experimentally determined, small-molecule metabolic pathways and are curated from the primary scientific literature. MetaCyc contains more than 1800 pathways derived from more than 30 000 publications, and is the largest curated collection of metabolic pathways currently available. Most reactions in MetaCyc pathways are linked to one or more well-characterized enzymes, and both pathways and enzymes are annotated with reviews, evidence codes and literature citations. BioCyc (http://biocyc.org/) is a collection of more than 1700 organism-specific Pathway/Genome Databases (PGDBs). Each BioCyc PGDB contains the full genome and predicted metabolic network of one organism. The network, which is predicted by the Pathway Tools software using MetaCyc as a reference database, consists of metabolites, enzymes, reactions and metabolic pathways. BioCyc PGDBs contain additional features, including predicted operons, transport systems and pathway-hole fillers. The BioCyc website and Pathway Tools software offer many tools for querying and analysis of PGDBs, including Omics Viewers and comparative analysis. New developments include a zoomable web interface for diagrams; flux-balance analysis model generation from PGDBs; web services; and a new tool called Web Groups.",poster,cp70
p2336,633888a9e6ac257c3e1e3d480525231c1627dc8d,c76,International Conference on Artificial Neural Networks,"The RNA modification database, RNAMDB: 2011 update","Since its inception in 1994, The RNA Modification Database (RNAMDB, http://rna-mdb.cas.albany.edu/RNAmods/) has served as a focal point for information pertaining to naturally occurring RNA modifications. In its current state, the database employs an easy-to-use, searchable interface for obtaining detailed data on the 109 currently known RNA modifications. Each entry provides the chemical structure, common name and symbol, elemental composition and mass, CA registry numbers and index name, phylogenetic source, type of RNA species in which it is found, and references to the first reported structure determination and synthesis. Though newly transferred in its entirety to The RNA Institute, the RNAMDB continues to grow with two notable additions, agmatidine and 8-methyladenosine, appended in the last year. The RNA Modification Database is staying up-to-date with significant improvements being prepared for inclusion within the next year and the following year. The expanded future role of The RNA Modification Database will be to serve as a primary information portal for researchers across the entire spectrum of RNA-related research.",poster,cp76
p2337,13bc458634865e23ed8ecd54473a34e705f7da10,c27,ACM-SIAM Symposium on Discrete Algorithms,THE HITRAN MOLECULAR DATABASE: EDITIONS OF 1991 AND 1992,Abstract content,poster,cp27
p2338,8a61d0e598d1a1d47fa4f744081cd39255a3f508,j108,PLoS ONE,TCM Database@Taiwan: The World's Largest Traditional Chinese Medicine Database for Drug Screening In Silico,"Rapid advancing computational technologies have greatly speeded up the development of computer-aided drug design (CADD). Recently, pharmaceutical companies have increasingly shifted their attentions toward traditional Chinese medicine (TCM) for novel lead compounds. Despite the growing number of studies on TCM, there is no free 3D small molecular structure database of TCM available for virtual screening or molecular simulation. To address this shortcoming, we have constructed TCM Database@Taiwan (http://tcm.cmu.edu.tw/) based on information collected from Chinese medical texts and scientific publications. TCM Database@Taiwan is currently the world's largest non-commercial TCM database. This web-based database contains more than 20,000 pure compounds isolated from 453 TCM ingredients. Both cdx (2D) and Tripos mol2 (3D) formats of each pure compound in the database are available for download and virtual screening. The TCM database includes both simple and advanced web-based query options that can specify search clauses, such as molecular properties, substructures, TCM ingredients, and TCM classification, based on intended drug actions. The TCM database can be easily accessed by all researchers conducting CADD. Over the last eight years, numerous volunteers have devoted their time to analyze TCM ingredients from Chinese medical texts as well as to construct structure files for each isolated compound. We believe that TCM Database@Taiwan will be a milestone on the path towards modernizing traditional Chinese medicine.",fullPaper,jv108
p2339,e3f9ed5a6c1c2045adfb5f66845338d9352149d3,c55,Annual Workshop of the Psychology of Programming Interest Group,OPM: Orientations of Proteins in Membranes database,"SUMMARY
The Orientations of Proteins in Membranes (OPM) database provides a collection of transmembrane, monotopic and peripheral proteins from the Protein Data Bank whose spatial arrangements in the lipid bilayer have been calculated theoretically and compared with experimental data. The database allows analysis, sorting and searching of membrane proteins based on their structural classification, species, destination membrane, numbers of transmembrane segments and subunits, numbers of secondary structures and the calculated hydrophobic thickness or tilt angle with respect to the bilayer normal. All coordinate files with the calculated membrane boundaries are available for downloading.


AVAILABILITY
http://opm.phar.umich.edu.",poster,cp55
p2340,dd9c570d52a504b7fefac1bfbb65f03952b0132f,c93,Human Language Technology - The Baltic Perspectiv,The UCSC genome browser database: update 2007,"The University of California, Santa Cruz Genome Browser Database contains, as of September 2006, sequence and annotation data for the genomes of 13 vertebrate and 19 invertebrate species. The Genome Browser displays a wide variety of annotations at all scales from the single nucleotide level up to a full chromosome and includes assembly data, genes and gene predictions, mRNA and EST alignments, and comparative genomics, regulation, expression and variation data. The database is optimized for fast interactive performance with web tools that provide powerful visualization and querying capabilities for mining the data. In the past year, 22 new assemblies and several new sets of human variation annotation have been released. New features include VisiGene, a fully integrated in situ hybridization image browser; phyloGif, for drawing evolutionary tree diagrams; a redesigned Custom Track feature; an expanded SNP annotation track; and many new display options. The Genome Browser, other tools, downloadable data files and links to documentation and other information can be found at .",poster,cp93
p2341,5f47123f5d86019c79c89f75ef6b44a60039f347,j378,Multimedia tools and applications,SCface – surveillance cameras face database,Abstract content,fullPaper,jv378
p2342,1f159d534f2e150577718b92d3ccfd3e23b7e889,c90,Computer Vision and Pattern Recognition,Constructing a research database of social and environmental reporting by UK companies,"Responds to the widely‐reported methodological problems which have arisen in research into corporate social and environmental reporting. Reports on an attempt to build a database of UK company social and environmental disclosure. The motivation behind the database is an attempt to provide, first, a data set which both refines and develops earlier attempts to capture and interpret such disclosures; second, a data set covering several years to permit longitudinal analysis; and third, a public database for accounting researchers who wish to pursue, in a systematic and comparable way, more focused hypotheses about social and environmental reporting behaviour. Explains the motivation for, the background to, and process of establishing such a database and attempts to expose the difficulties met and the assumptions made in establishing the structure of the data capture. The resultant database has already proved useful to other UK researchers. Aims to help researchers in other countries to develop their own metho...",poster,cp90
p2343,9f37bd6500bcc8ed946c0fd3dc9deb6334c24c12,c43,ACM Symposium on Applied Computing,CHIANTI—AN ATOMIC DATABASE FOR EMISSION LINES. XII. VERSION 7 OF THE DATABASE,"The CHIANTI spectral code consists of an atomic database and a suite of computer programs to calculate the optically thin spectrum of astrophysical objects and carry out spectroscopic plasma diagnostics. The database includes atomic energy levels, wavelengths, radiative transition probabilities, collision excitation rate coefficients, and ionization and recombination rate coefficients, as well as data to calculate free–free, free–bound, and two-photon continuum emission. Version 7 has been released, which includes several new ions, significant updates to existing ions, as well as Chianti-Py, the implementation of CHIANTI software in the Python programming language. All data and programs are freely available at http://www.chiantidatabase.org, while the Python interface to CHIANTI can be found at http://chiantipy.sourceforge.net.",poster,cp43
p2344,eda424538bab229d38f03a97d3ed1731e2a2c871,c7,European Conference on Modelling and Simulation,"Semantic database modeling: survey, applications, and research issues","Most common database management systems represent information in a simple record-based format. Semantic modeling provides richer data structuring capabilities for database applications. In particular, research in this area has articulated a number of constructs that provide mechanisms for representing structurally complex interrelations among data typically arising in commercial applications. In general terms, semantic modeling complements work on knowledge representation (in artificial intelligence) and on the new generation of database models based on the object-oriented paradigm of programming languages.
This paper presents an in-depth discussion of semantic data modeling. It reviews the philosophical motivations of semantic models, including the need for high-level modeling abstractions and the reduction of semantic overloading of data type constructors. It then provides a tutorial introduction to the primary components of semantic models, which are the explicit representation of objects, attributes of and relationships among objects, type constructors for building complex types, ISA relationships, and derived schema components. Next, a survey of the prominent semantic models in the literature is presented. Further, since a broad area of research has developed around semantic modeling, a number of related topics based on these models are discussed, including data languages, graphical interfaces, theoretical investigations, and physical implementation strategies.",poster,cp7
p2345,ef50e6878d6addcbd5d1ca96e08eef51b9ddec9e,c10,Big Data,Reference database of Raman spectra of biological molecules,"Raman spectra of biological materials are very complex, because they consist of signals from all molecules present in cells. In order to obtain chemical information from these spectra, it is necessary to know the Raman patterns of the possible components of a cell. In this paper, we present a collection of Raman spectra of biomolecules that can serve as references for the interpretation of Raman spectra of biological materials. We included the most important components present in a cell: (1) DNA and RNA bases (adenine, cytosine, guanine, thymine and uracil), (2) amino acids (glycine, L-alanine, L-valine, L-serine, L-glutamic acid, L-arginine, L-phenylalanine, L-tyrosine, L-tryptophan, L-histidine, L-proline), (3) fatty acids and fats (lauric acid, myristic acid, palmitic acid, stearic acid, 12-methyltetradecanoic acid, 13-methylmyristic acid, 14-methylpentadecanoic acid, 14-methylhexadecanoic acid, 15-methylpalmitic acid, oleic acid, vaccenic acid, glycerol, triolein, trilinolein, trilinolenin), (4) saccharides (β-D-glucose, lactose, cellulose, D-(+)-dextrose, D-(+)-trehalose, amylose, amylopectine, D-(+)-mannose, D-(+)-fucose, D-(−)-arabinose, D-(+)-xylose, D-(−)-fructose, D-(+)-galactosamine, N-acetyl-D-glucosamine, chitin), (5) primary metabolites (citric acid, succinic acid, fumarate, malic acid, pyruvate, phosphoenolpyruvate, coenzyme A, acetyl coenzyme A, acetoacetate, D-fructose-6-phosphate) and (6) others (β-carotene, ascorbic acid, riboflavin, glutathione). Examples of Raman spectra of bacteria and fungal spores are shown, together with band assignments to the reference products. Copyright © 2007 John Wiley & Sons, Ltd.",poster,cp10
p2346,28e702e1a352854cf0748b9a6a9ad6679b1d4e83,c19,ACM Conference on Economics and Computation,Progressive skyline computation in database systems,"The skyline of a d-dimensional dataset contains the points that are not dominated by any other point on all dimensions. Skyline computation has recently received considerable attention in the database community, especially for progressive methods that can quickly return the initial results without reading the entire database. All the existing algorithms, however, have some serious shortcomings which limit their applicability in practice. In this article we develop branch-and-bound skyline (BBS), an algorithm based on nearest-neighbor search, which is I/O optimal, that is, it performs a single access only to those nodes that may contain skyline points. BBS is simple to implement and supports all types of progressive processing (e.g., user preferences, arbitrary dimensionality, etc). Furthermore, we propose several interesting variations of skyline computation, and show how BBS can be applied for their efficient processing.",poster,cp19
p2347,c4907ef7d044ad71cc8b292c8b1e146987422ec7,c18,Conference on Innovative Data Systems Research,IPD—the Immuno Polymorphism Database,"The Immuno Polymorphism Database (IPD), http://www.ebi.ac.uk/ipd/ is a set of specialist databases related to the study of polymorphic genes in the immune system. The IPD project works with specialist groups or nomenclature committees who provide and curate individual sections before they are submitted to IPD for online publication. The IPD project stores all the data in a set of related databases. IPD currently consists of four databases: IPD-KIR, contains the allelic sequences of killer-cell immunoglobulin-like receptors, IPD-MHC, a database of sequences of the major histocompatibility complex of different species; IPD-HPA, alloantigens expressed only on platelets; and IPD-ESTDAB, which provides access to the European Searchable Tumour Cell-Line Database, a cell bank of immunologically characterized melanoma cell lines. The data is currently available online from the website and FTP directory. This article describes the latest updates and additional tools added to the IPD project.",poster,cp18
p2348,311c0501c68f8cbe0d2e3a161de1ab12d49cb9ce,c47,International Symposium on Empirical Software Engineering and Measurement,Physical Properties of Ionic Liquids: Database and Evaluation,"A comprehensive database on physical properties of ionic liquids (ILs), which was collected from 109 kinds of literature sources in the period from 1984 through 2004, has been presented. There are 1680 pieces of data on the physical properties for 588 available ILs, from which 276 kinds of cations and 55 kinds of anions were extracted. In terms of the collected database, the structure-property relationship was evaluated. The correlation of melting points of two most common systems, disubstituted imidazolium tetrafluoroborate and disubstituted imidazolium hexafluorophosphate, was carried out using a quantitative structure-property relationship method.",poster,cp47
p2349,977fe5853db16e320917a43fb00f334456625a1e,c68,International Workshop on Software Engineering for Large-Scale Multi-Agent Systems,DIP: the Database of Interacting Proteins,"The Database of Interacting Proteins (DIP; http://dip.doe-mbi.ucla.edu) is a database that documents experimentally determined protein-protein interactions. This database is intended to provide the scientific community with a comprehensive and integrated tool for browsing and efficiently extracting information about protein interactions and interaction networks in biological processes. Beyond cataloging details of protein-protein interactions, the DIP is useful for understanding protein function and protein-protein relationships, studying the properties of networks of interacting proteins, benchmarking predictions of protein-protein interactions, and studying the evolution of protein-protein interactions.",poster,cp68
p2350,e606ccf581b507149e4bdaba972ab58682eef57b,c22,International Conference on Data Technologies and Applications,The SIMBAD astronomical database. The CDS reference database for astronomical objects,"Simbad is the reference database for identification and bibliography of astronomical objects. It contains identifications, “basic data”, bibliography, and selected observational measurements for several million astronomical objects.  Simbad is developed and maintained by CDS, Strasbourg. Building the database contents is achieved with the help of several contributing institutes. Scanning the bibliography is the result of the collaboration of CDS with bibliographers in Observatoire de Paris (DASGAL), Institut d'Astrophysique de Paris, and Observatoire de Bordeaux. When selecting catalogues and tables for inclusion, priority is given to optimal multi-wavelength coverage of the database, and to support of research developments linked to large projects. In parallel, the systematic scanning of the bibliography reflects the diversity and general trends of astronomical research. A WWW interface to Simbad is available at: http://simbad.u-strasbg.fr/Simbad.",poster,cp22
p2351,a03d8f591bc3c2dbdecbd9d515e0469953a3f7ef,j274,Nature Genetics,The NCBI dbGaP database of genotypes and phenotypes,Abstract content,fullPaper,jv274
p2352,61076194ec631a89daa30edbcc90bc7be37804cc,c64,Experimental Software Engineering Network,The NCBI BioSystems database,"The NCBI BioSystems database, found at http://www.ncbi.nlm.nih.gov/biosystems/, centralizes and cross-links existing biological systems databases, increasing their utility and target audience by integrating their pathways and systems into NCBI resources. This integration allows users of NCBI’s Entrez databases to quickly categorize proteins, genes and small molecules by metabolic pathway, disease state or other BioSystem type, without requiring time-consuming inference of biological relationships from the literature or multiple experimental datasets.",poster,cp64
p2353,b791d488eef45ef79da812f7569fc2cc83196aa5,j379,Springer Netherlands,EuroWordNet: A multilingual database with lexical semantic networks,Abstract content,fullPaper,jv379
p2354,a0883d134b5abb7928483eb0859832a66a51fbf9,c105,Biometrics and Identity Management,The PROSITE database,"The PROSITE database consists of a large collection of biologically meaningful signatures that are described as patterns or profiles. Each signature is linked to a documentation that provides useful biological information on the protein family, domain or functional site identified by the signature. The PROSITE database is now complemented by a series of rules that can give more precise information about specific residues. During the last 2 years, the documentation and the ScanProsite web pages were redesigned to add more functionalities. The latest version of PROSITE (release 19.11 of September 27, 2005) contains 1329 patterns and 552 profile entries. Over the past 2 years more than 200 domains have been added, and now 52% of UniProtKB/Swiss-Prot entries (release 48.1 of September 27, 2005) have a cross-reference to a PROSITE entry. The database is accessible at .",poster,cp105
p2355,7dabd56ccd524f78f0eda5073dc358f28893a45d,c97,Interspeech,The CIPIC HRTF database,"This paper describes a public-domain database of high-spatial-resolution head-related transfer functions measured at the UC Davis CIPIC Interface Laboratory and the methods used to collect the data.. Release 1.0 (see http://interface.cipic.ucdavis.edu) includes head-related impulse responses for 45 subjects at 25 different azimuths and 50 different elevations (1250 directions) at approximately 5/spl deg/ angular increments. In addition, the database contains anthropometric measurements for each subject. Statistics of anthropometric parameters and correlations between anthropometry and some temporal and spectral features of the HRTFs are reported.",poster,cp97
p2356,351bbaa6d0b597175a17f59f822c8e0d1fdebe03,c45,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",Defining and cataloging exoplanets: the exoplanet.eu database,"We describe an online database for extrasolar planetary-mass candidates, which is updated regularly as new data are available. We first discuss criteria for inclusion of objects in the catalog: “definition” of a planet and several aspects of the confidence level of planet candidates. We are led to point out the contradiction between the sharpness of criteria for belonging to a catalog and the fuzziness of the confidence level for an object to be a planet. We then describe the different tables of extrasolar planetary systems, including unconfirmed candidates (which will ultimately be confirmed, or not, by direct imaging). It also provides online tools: histograms of planet and host star data, cross-correlations between these parameters, and some Virtual Observatory services. Future evolutions of the database are presented.",poster,cp45
p2357,fdb3490482abb8864296f699d9b12435638c4c90,c18,Conference on Innovative Data Systems Research,Cambridge Structural Database,"The precision of crystal structure determinations is often assessed using the crystallographic R-factor, a measure of how well the structure factors computed using the refined structural model agree with structure factors given by the experimentally observed diffraction intensities. CSD structures with unreported R-factors often arise from short communications, and most frequently from the earlier literature.",poster,cp18
p2358,7427faf58516c01da79830a4edc6a98fdf902bc1,c52,Workshop on Learning from Authoritative Security Experiment Results,SGD: Saccharomyces Genome Database,"The Saccharomyces Genome Database (SGD) provides Internet access to the complete Saccharomyces cerevisiae genomic sequence, its genes and their products, the phenotypes of its mutants, and the literature supporting these data. The amount of information and the number of features provided by SGD have increased greatly following the release of the S.cerevisiae genomic sequence, which is currently the only complete sequence of a eukaryotic genome. SGD aids researchers by providing not only basic information, but also tools such as sequence similarity searching that lead to detailed information about features of the genome and relationships between genes. SGD presents information using a variety of user-friendly, dynamically created graphical displays illustrating physical, genetic and sequence feature maps. SGD can be accessed via the World Wide Web at http://genome-www.stanford.edu/Saccharomyces/",poster,cp52
p2359,bf9e27a62e100e46c5060c7ea79a0d97ce6c1a79,c26,PS,An atomic and molecular database for analysis of submillimetre line observations,"Atomic and molecular data for the transitions of a number of astrophysically interesting species are summarized, in- cluding energy levels, statistical weights, Einstein A-coefficients and collisional rate coefficients. Available collisional data from quantum chemical calculations and experiments are extrapolated to higher energies (up to E/k ∼ 1000 K). These data, which are made publically available through the WWW at http://www.strw.leidenuniv.nl/∼moldata, are essential input for non-LTE line radiative transfer programs. An online version of a computer program for performing statistical equilibrium calcu- lations is also made available as part of the database. Comparisons of calculated emission lines using different sets of collisional rate coefficients are presented. This database should form an important tool in analyzing observations from current and future (sub)millimetre and infrared telescopes.",poster,cp26
p2360,7f4754e970cada5e4746dcad0b4258f1a19d02cb,j34,Proceedings of the VLDB Endowment,Schism: a Workload-Driven Approach to Database Replication and Partitioning,"We present Schism, a novel workload-aware approach for database partitioning and replication designed to improve scalability of shared-nothing distributed databases. Because distributed transactions are expensive in OLTP settings (a fact we demonstrate through a series of experiments), our partitioner attempts to minimize the number of distributed transactions, while producing balanced partitions. Schism consists of two phases: i) a workload-driven, graph-based replication/partitioning phase and ii) an explanation and validation phase. The first phase creates a graph with a node per tuple (or group of tuples) and edges between nodes accessed by the same transaction, and then uses a graph partitioner to split the graph into k balanced partitions that minimize the number of cross-partition transactions. The second phase exploits machine learning techniques to find a predicate-based explanation of the partitioning strategy (i.e., a set of range predicates that represent the same replication/partitioning scheme produced by the partitioner). 
 
The strengths of Schism are: i) independence from the schema layout, ii) effectiveness on n-to-n relations, typical in social network databases, iii) a unified and fine-grained approach to replication and partitioning. We implemented and tested a prototype of Schism on a wide spectrum of test cases, ranging from classical OLTP workloads (e.g., TPC-C and TPC-E), to more complex scenarios derived from social network websites (e.g., Epinions.com), whose schema contains multiple n-to-n relationships, which are known to be hard to partition. Schism consistently outperforms simple partitioning schemes, and in some cases proves superior to the best known manual partitioning, reducing the cost of distributed transactions up to 30%.",fullPaper,jv34
p2361,c3b16176728c7f785802f84df5aacffbc82ad431,c7,European Conference on Modelling and Simulation,Database abstractions: aggregation and generalization,"Two kinds of abstraction that are fundamentally important in database design and usage are defined. Aggregation is an abstraction which turns a relationship between objects into an aggregate object. Generalization is an abstraction which turns a class of objects into a generic object. It is suggested that all objects (individual, aggregate, generic) should be given uniform treatment in models of the real world. A new data type, called generic, is developed as a primitive for defining such models. Models defined with this primitive are structured as a set of aggregation hierarchies intersecting with a set of generalization hierarchies. Abstract objects occur at the points of intersection. This high level structure provides a discipline for the organization of relational databases. In particular this discipline allows: (i) an important class of views to be integrated and maintained; (ii) stability of data and programs under certain evolutionary changes; (iii) easier understanding of complex models and more natural query formulation; (iv) a more systematic approach to database design; (v) more optimization to be performed at lower implementation levels. The generic type is formalized by a set of invariant properties. These properties should be satisfied by all relations in a database if abstractions are to be preserved. A triggering mechanism for automatically maintaining these invariants during update operations is proposed. A simple mapping of aggregation/generalization hierarchies onto owner-coupled set structures is given.",poster,cp7
p2362,2c9b060388b88841cf8095cd9efcbfeb805357f6,c27,ACM-SIAM Symposium on Discrete Algorithms,PubChem's BioAssay Database,"PubChem (http://pubchem.ncbi.nlm.nih.gov) is a public repository for biological activity data of small molecules and RNAi reagents. The mission of PubChem is to deliver free and easy access to all deposited data, and to provide intuitive data analysis tools. The PubChem BioAssay database currently contains 500 000 descriptions of assay protocols, covering 5000 protein targets, 30 000 gene targets and providing over 130 million bioactivity outcomes. PubChem's bioassay data are integrated into the NCBI Entrez information retrieval system, thus making PubChem data searchable and accessible by Entrez queries. Also, as a repository, PubChem constantly optimizes and develops its deposition system answering many demands of both high- and low-volume depositors. The PubChem information platform allows users to search, review and download bioassay description and data. The PubChem platform also enables researchers to collect, compare and analyze biological test results through web-based and programmatic tools. In this work, we provide an update for the PubChem BioAssay resource, including information content growth, data model extension and new developments of data submission, retrieval, analysis and download tools.",poster,cp27
p2363,09d73eecceb080eb1f7cea71d7df1411c712baf6,c38,"IEEE International Conference on Software Analysis, Evolution, and Reengineering",The National Land Cover Database,Abstract content,poster,cp38
p2364,9a0723e76b4fce1cdfd407ed31a2b45130b4b423,c86,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,"The PROSITE database, its status in 1999","The PROSITE database consists of biologically significant patterns and profiles formulated in such a way that with appropriate computational tools it can help to determine to which known family of protein (if any) a new sequence belongs, or which known domain(s) it contains.",poster,cp86
p2365,695557ab15e44bee66d532d52b81a37decd87d70,c7,European Conference on Modelling and Simulation,The Object-Oriented Database System Manifesto,Abstract content,poster,cp7
p2366,cd7763d7c118bc875ea34b30b52d0d95257b1418,c110,IEEE International Conference on Automatic Face & Gesture Recognition,TRANSFAC: a database on transcription factors and their DNA binding sites,TRANSFAC is a database about eukaryotic transcription regulating DNA sequence elements and the transcription factors binding to and acting through them. This report summarizes the present status of this database and accompanying retrieval tools.,poster,cp110
p2367,a6859f695e6b2bd967df7cdb8becf8c9465b472a,c43,ACM Symposium on Applied Computing,The 'Dresden Image Database' for benchmarking digital image forensics,"This paper introduces and documents a novel image database specifically built for the purpose of development and bench-marking of camera-based digital forensic techniques. More than 14,000 images of various indoor and outdoor scenes have been acquired under controlled and thus widely comparable conditions from altogether 73 digital cameras. The cameras were drawn from only 25 different models to ensure that device-specific and model-specific characteristics can be disentangled and studied separately, as validated with results in this paper. In addition, auxiliary images for the estimation of device-specific sensor noise pattern were collected for each camera. Another subset of images to study model-specific JPEG compression algorithms has been compiled for each model. The 'Dresden Image Database' will be made freely available for scientific purposes when this accompanying paper is presented. The database is intended to become a useful resource for researchers and forensic investigators. Using a standard database as a benchmark not only makes results more comparable and reproducible, but it is also more economical and avoids potential copyright and privacy issues that go along with self-sampled benchmark sets from public photo communities on the Internet.",fullPaper,cp43
p2368,5524cacaae93810945f1b21e77f565f6c8bdcdef,c18,Conference on Innovative Data Systems Research,Relational Cloud: a Database Service for the cloud,"This paper introduces a new transactional “database-as-a-service” (DBaaS) called Relational Cloud. A DBaaS promises to move much of the operational burden of provisioning, configuration, scaling, performance tuning, backup, privacy, and access control from the database users to the service operator, offering lower overall costs to users. Early DBaaS efforts include Amazon RDS and Microsoft SQL Azure, which are promising in terms of establishing the market need for such a service, but which do not address three important challenges: efficient multi-tenancy, elastic scalability, and database privacy. We argue that these three challenges must be overcome before outsourcing database software and management becomes attractive to many users, and cost-effective for service providers. The key technical features of Relational Cloud include: (1) a workload-aware approach to multi-tenancy that identifies the workloads that can be co-located on a database server, achieving higher consolidation and better performance than existing approaches; (2) the use of a graph-based data partitioning algorithm to achieve near-linear elastic scale-out even for complex transactional workloads; and (3) an adjustable security scheme that enables SQL queries to run over encrypted data, including ordering operations, aggregates, and joins. An underlying theme in the design of the components of Relational Cloud is the notion of workload awareness: by monitoring query patterns and data accesses, the system obtains information useful for various optimization and security functions, reducing the configuration effort for users and operators.",fullPaper,cp18
p2369,036a089cccfb31a7aeaff40373079d945b87eec0,c15,International Conference on Conceptual Structures,Using the KEGG Database Resource,"KEGG (Kyoto Encyclopedia of Genes and Genomes) is a bioinformatics resource for understanding the functions and utilities of cells and organisms from both high‐level and genomic perspectives. It is a self‐sufficient, integrated resource consisting of genomic, chemical, and network information, with cross‐references to numerous outside databases. The genomic and chemical information is a complete set of building blocks (genes and molecules) and the network information includes molecular wiring diagrams (interaction/reaction networks) and hierarchical classifications (relation networks) to represent high‐level functions. This unit describes protocols for using KEGG, focusing on molecular network information in KEGG PATHWAY, KEGG BRITE, and KEGG MODULE, perturbed molecular networks in KEGG DISEASE and KEGG DRUG, molecular building block information in KEGG GENES and KEGG LIGAND, and a mechanism for linking genomes to molecular networks in KEGG ORTHOLOGY (KO). All of these many protocols enable the user to take advantage of the full breadth of the functionality provided by KEGG. Curr. Protoc. Bioinform. 38:1.12.1‐1.12.43. © 2012 by John Wiley & Sons, Inc.",poster,cp15
p2370,6c26791be6a51844f2784cd402876b18f110c5e4,c62,International Conference on Software Reuse,IntAct: an open source molecular interaction database,"IntAct provides an open source database and toolkit for the storage, presentation and analysis of protein interactions. The web interface provides both textual and graphical representations of protein interactions, and allows exploring interaction networks in the context of the GO annotations of the interacting proteins. A web service allows direct computational access to retrieve interaction networks in XML format. IntAct currently contains approximately 2200 binary and complex interactions imported from the literature and curated in collaboration with the Swiss-Prot team, making intensive use of controlled vocabularies to ensure data consistency. All IntAct software, data and controlled vocabularies are available at http://www.ebi.ac.uk/intact.",poster,cp62
p2371,638f10c6cc396907b98424621f6420a4287d342f,c14,International Conference on Exploring Services Science,rrndb: the Ribosomal RNA Operon Copy Number Database,"The Ribosomal RNA Operon Copy Number Database (rrndb) is an Internet-accessible database containing annotated information on rRNA operon copy number among prokaryotes. Gene redundancy is uncommon in prokaryotic genomes, yet the rRNA genes can vary from one to as many as 15 copies. Despite the widespread use of 16S rRNA gene sequences for identification of prokaryotes, information on the number and sequence of individual rRNA genes in a genome is not readily accessible. In an attempt to understand the evolutionary implications of rRNA operon redundancy, we have created a phylogenetically arranged report on rRNA gene copy number for a diverse collection of prokaryotic microorganisms. Each entry (organism) in the rrndb contains detailed information linked directly to external websites including the Ribosomal Database Project, GenBank, PubMed and several culture collections. Data contained in the rrndb will be valuable to researchers investigating microbial ecology and evolution using 16S rRNA gene sequences. The rrndb web site is directly accessible on the WWW at http://rrndb.cme. msu.edu.",poster,cp14
p2372,f3cdea5fe196a7558afd9fcb8f3dacb69fdbe3d3,c56,European Conference on Software Process Improvement,"The InterPro database, an integrated documentation resource for protein families, domains and functional sites","Signature databases are vital tools for identifying distant relationships in novel sequences and hence for inferring protein function. InterPro is an integrated documentation resource for protein families, domains and functional sites, which amalgamates the efforts of the PROSITE, PRINTS, Pfam and ProDom database projects. Each InterPro entry includes a functional description, annotation, literature references and links back to the relevant member database(s). Release 2.0 of InterPro (October 2000) contains over 3000 entries, representing families, domains, repeats and sites of post-translational modification encoded by a total of 6804 different regular expressions, profiles, fingerprints and Hidden Markov Models. Each InterPro entry lists all the matches against SWISS-PROT and TrEMBL (more than 1,000,000 hits from 462,500 proteins in SWISS-PROT and TrEMBL). The database is accessible for text- and sequence-based searches at http://www.ebi.ac.uk/interpro/. Questions can be emailed to interhelp@ebi.ac.uk.",poster,cp56
p2373,ff9186e43abd68e55fbcb9ba992944c7497bacab,j227,Trends in Genetics,Repbase update: a database and an electronic journal of repetitive elements.,Abstract content,fullPaper,jv227
p2374,2ee12e17d94c6a39d1f25071d7fbc249fa79e72a,c4,Annual Conference on Genetic and Evolutionary Computation,BIND: the Biomolecular Interaction Network Database,"The Biomolecular Interaction Network Database (BIND: http://bind.ca) archives biomolecular interaction, complex and pathway information. A web-based system is available to query, view and submit records. BIND continues to grow with the addition of individual submissions as well as interaction data from the PDB and a number of large-scale interaction and complex mapping experiments using yeast two hybrid, mass spectrometry, genetic interactions and phage display. We have developed a new graphical analysis tool that provides users with a view of the domain composition of proteins in interaction and complex records to help relate functional domains to protein interactions. An interaction network clustering tool has also been developed to help focus on regions of interest. Continued input from users has helped further mature the BIND data specification, which now includes the ability to store detailed information about genetic interactions. The BIND data specification is available as ASN.1 and XML DTD.",poster,cp4
p2375,d70c182a71aea05a145391b24d6bc3cdeede32a5,c86,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,Database-friendly random projections,"A classic result of Johnson and Lindenstrauss asserts that any set of n points in d-dimensional Euclidean space can be embedded into k-dimensional Euclidean space where k is logarithmic in n and independent of d so that all pairwise distances are maintained within an arbitrarily small factor. All known constructions of such embeddings involve projecting the n points onto a random k-dimensional hyperplane. We give a novel construction of the embedding, suitable for database applications, which amounts to computing a simple aggregate over k random attribute partitions.",fullPaper,cp86
p2376,7a350beede1b8eda39ce22bca62732bcd6677ebd,c10,Big Data,Concurrency Control in Distributed Database Systems,"In this paper we survey, consolidate, and present the state of the art in distributed database concurrency control. The heart of our analysts is a decomposition of the concurrency control problem into two major subproblems: read-write and write-write synchronization. We describe a series of synchromzation techniques for solving each subproblem and show how to combine these techniques into algorithms for solving the entire concurrency control problem. Such algorithms are called ""concurrency control methods."" We describe 48 principal methods, including all practical algorithms that have appeared m the literature plus several new ones. We concentrate on the structure and correctness of concurrency control algorithms. Issues of performance are given only secondary treatment.",poster,cp10
p2377,ed431581f9537896d26b7c8d9935dce9ee73871d,j102,Nucleic Acids Research,The International Nucleotide Sequence Database Collaboration,"Under the International Nucleotide Sequence Database Collaboration (INSDC; http://www.insdc.org), globally comprehensive public domain nucleotide sequence is captured, preserved and presented. The partners of this long-standing collaboration work closely together to provide data formats and conventions that enable consistent data submission to their databases and support regular data exchange around the globe. Clearly defined policy and governance in relation to free access to data and relationships with journal publishers have positioned INSDC databases as a key provider of the scientific record and a core foundation for the global bioinformatics data infrastructure. While growth in sequence data volumes comes no longer as a surprise to INSDC partners, the uptake of next-generation sequencing technology by mainstream science that we have witnessed in recent years brings a step-change to growth, necessarily making a clear mark on INSDC strategy. In this article, we introduce the INSDC, outline data growth patterns and comment on the challenges of increased growth.",fullPaper,jv102
p2378,d03cab6781985ad3f62aec52048a7e15ee2dee61,c88,Symposium on the Theory of Computing,LMSD: LIPID MAPS structure database,"The LIPID MAPS Structure Database (LMSD) is a relational database encompassing structures and annotations of biologically relevant lipids. Structures of lipids in the database come from four sources: (i) LIPID MAPS Consortium's core laboratories and partners; (ii) lipids identified by LIPID MAPS experiments; (iii) computationally generated structures for appropriate lipid classes; (iv) biologically relevant lipids manually curated from LIPID BANK, LIPIDAT and other public sources. All the lipid structures in LMSD are drawn in a consistent fashion. In addition to a classification-based retrieval of lipids, users can search LMSD using either text-based or structure-based search options. The text-based search implementation supports data retrieval by any combination of these data fields: LIPID MAPS ID, systematic or common name, mass, formula, category, main class, and subclass data fields. The structure-based search, in conjunction with optional data fields, provides the capability to perform a substructure search or exact match for the structure drawn by the user. Search results, in addition to structure and annotations, also include relevant links to external databases. The LMSD is publicly available at",poster,cp88
p2379,fb2896bb515ad483260f2b937202d0e7289ddd16,c106,Chinese Conference on Biometric Recognition,SDUMLA-HMT: A Multimodal Biometric Database,Abstract content,fullPaper,cp106
p2380,9d79185d82c03c30778f3635bfbdcf605330f41b,c60,IEEE International Conference on Software Engineering and Formal Methods,The Immune Epitope Database 2.0,"The Immune Epitope Database (IEDB, www.iedb.org) provides a catalog of experimentally characterized B and T cell epitopes, as well as data on Major Histocompatibility Complex (MHC) binding and MHC ligand elution experiments. The database represents the molecular structures recognized by adaptive immune receptors and the experimental contexts in which these molecules were determined to be immune epitopes. Epitopes recognized in humans, nonhuman primates, rodents, pigs, cats and all other tested species are included. Both positive and negative experimental results are captured. Over the course of 4 years, the data from 180 978 experiments were curated manually from the literature, which covers ∼99% of all publicly available information on peptide epitopes mapped in infectious agents (excluding HIV) and 93% of those mapped in allergens. In addition, data that would otherwise be unavailable to the public from 129 186 experiments were submitted directly by investigators. The curation of epitopes related to autoimmunity is expected to be completed by the end of 2010. The database can be queried by epitope structure, source organism, MHC restriction, assay type or host organism, among other criteria. The database structure, as well as its querying, browsing and reporting interfaces, was completely redesigned for the IEDB 2.0 release, which became publicly available in early 2009.",poster,cp60
p2381,5bf9cebe3658cfbf7f67c0a2680c8233509aa5e4,c53,International Conference on Software Engineering and Knowledge Engineering,UCI Repository of Machine Learning Database,Abstract content,poster,cp53
p2382,5c06b60a6940df55271fe5917848abf7ab3ca706,c97,Interspeech,The RDP (Ribosomal Database Project),"The Ribosomal Database Project (RDP) is a curated database that offers ribosome-related data, analysis services and associated computer programs. The offerings include phylogenetically ordered alignments of ribosomal RNA (rRNA) sequences, derived phylogenetic trees, rRNA secondary structure diagrams, and various software for handling, analyzing and displaying alignments and trees. The data are available via anonymous FTP (rdp.life.uiuc.edu), electronic mail (server@rdp.life.uiuc.edu), gopher (rdpgopher.life.uiuc.edu) and WWW (http://rdpwww.life.uiuc.edu/ ). The electronic mail and WWW servers provide ribosomal probe checking, approximate phylogenetic placement of user-submitted sequences, screening for possible chimeric rRNA sequences, automated alignment, and a suggested placement of an unknown sequence on an existing phylogenetic tree.",poster,cp97
p2383,59b7919e89bca41d91362537f5dbdc2fa0c7f9e8,c21,Grid Computing Environments,The International Nucleotide Sequence Database Collaboration,"The members of the International Nucleotide Sequence Database Collaboration (INSDC; http://www.insdc.org) set out to capture, preserve and present globally comprehensive public domain nucleotide sequence information. The work of the long-standing collaboration includes the provision of data formats, annotation conventions and routine global data exchange. Among the many developments to INSDC resources in 2011 are the newly launched BioProject database and improved handling of assembly information. In this article, we outline INSDC services and update the reader on developments in 2011.",poster,cp21
p2384,062cea54e5d58ee41aea607cbf2ba0cf457aa4e7,c107,British Machine Vision Conference,The DIARETDB1 Diabetic Retinopathy Database and Evaluation Protocol,"Automatic diagnosis of diabetic retinopathy from digital fundus images has been an active research topic in the medical image processing community. The research interest is justified by the excellent potential for new products in the medical industry and significant reductions in health care costs. However, the maturity of proposed algorithms cannot be judged due to the lack of commonly accepted and representative image database with a verified ground truth and strict evaluation protocol. In this study, an evaluation methodology is proposed and an image database with ground truth is described. The database is publicly available for benchmarking diagnosis algorithms. With the proposed database and protocol, it is possible to compare different algorithms, and correspondingly, analyse their maturity for technology transfer from the research laboratories to the medical practice.",fullPaper,cp107
p2385,798e312dd67798024da74f9a8f92946af88c7cd4,c17,International Conference on Enterprise Information Systems,Comparative study of retinal vessel segmentation methods on a new publicly available database,"In this work we compare the performance of a number of vessel segmentation algorithms on a newly constructed retinal vessel image database. Retinal vessel segmentation is important for the detection of numerous eye diseases and plays an important role in automatic retinal disease screening systems. A large number of methods for retinal vessel segmentation have been published, yet an evaluation of these methods on a common database of screening images has not been performed. To compare the performance of retinal vessel segmentation methods we have constructed a large database of retinal images. The database contains forty images in which the vessel trees have been manually segmented. For twenty of those forty images a second independent manual segmentation is available. This allows for a comparison between the performance of automatic methods and the performance of a human observer. The database is available to the research community. Interested researchers are encouraged to upload their segmentation results to our website (http://www.isi.uu.nl/Research/Databases). The performance of five different algorithms has been compared. Four of these methods have been implemented as described in the literature. The fifth pixel classification based method was developed specifically for the segmentation of retinal vessels and is the only supervised method in this test. We define the segmentation accuracy with respect to our gold standard as the performance measure. Results show that the pixel classification method performs best, but the second observer still performs significantly better.",poster,cp17
p2386,a304faa860dd8d78881ad992c0115a3466847af9,c5,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,An Overview of the Global Historical Climatology Network Temperature Database,"Abstract The Global Historical Climatology Network version 2 temperature database was released in May 1997. This century-scale dataset consists of monthly surface observations from ∼7000 stations from around the world. This archive breaks considerable new ground in the field of global climate databases. The enhancements include 1) data for additional stations to improve regional-scale analyses, particularly in previously data-sparse areas; 2) the addition of maximum–minimum temperature data to provide climate information not available in mean temperature data alone; 3) detailed assessments of data quality to increase the confidence in research results; 4) rigorous and objective homogeneity adjustments to decrease the effect of nonclimatic factors on the time series; 5) detailed metadata (e.g., population, vegetation, topography) that allow more detailed analyses to be conducted; and 6) an infrastructure for updating the archive at regular intervals so that current climatic conditions can constantly be put...",poster,cp5
p2387,dd79f74b9f5537ceecd097563a20546dd60937f6,c26,PS,The VizieR database of astronomical catalogues,"VizieR is a database grouping in an homoge- neous way thousands of astronomical catalogues gath- ered for decades by the Centre de Donn ees de Strasbourg (CDS) and participating institutes. The history and cur- rent status of this large collection is briefly presented, and the way these catalogues are being standardized to t in the VizieR system is described. The architecture of the database is then presented, with emphasis on the man- agement of links and of accesses to very large catalogues. Several query interfaces are currently available, making use of the ASU protocol, for browsing purposes or for use by other data processing systems such as visualisa- tion tools.",poster,cp26
p2388,a7352bf3b88df27f0d0faa9d7ee7198a8b304c1d,c21,Grid Computing Environments,Development and use of a database of hydraulic properties of European soils,Abstract content,poster,cp21
p2389,c6c3f205d2aacf3163217186c419db596b2bad9e,j102,Nucleic Acids Research,The EMBL Nucleotide Sequence Database,"The EMBL Nucleotide Sequence Database (http://www.ebi.ac.uk/embl), maintained at the European Bioinformatics Institute (EBI) near Cambridge, UK, is a comprehensive collection of nucleotide sequences and annotation from available public sources. The database is part of an international collaboration with DDBJ (Japan) and GenBank (USA). Data are exchanged daily between the collaborating institutes to achieve swift synchrony. Webin is the preferred tool for individual submissions of nucleotide sequences, including Third Party Annotation (TPA) and alignments. Automated procedures are provided for submissions from large-scale sequencing projects and data from the European Patent Office. New and updated data records are distributed daily and the whole EMBL Nucleotide Sequence Database is released four times a year. Access to the sequence data is provided via ftp and several WWW interfaces. With the web-based Sequence Retrieval System (SRS) it is also possible to link nucleotide data to other specialist molecular biology databases maintained at the EBI. Other tools are available for sequence similarity searching (e.g. FASTA and BLAST). Changes over the past year include the removal of the sequence length limit, the launch of the EMBLCDSs dataset, extension of the Sequence Version Archive functionality and the revision of quality rules for TPA data.",fullPaper,jv102
p2390,97dcab33aa0f1b8c98eec95e52e13596f3fb890d,c96,USENIX Symposium on Operating Systems Design and Implementation,The ecoinvent Database: Overview and Methodological Framework (7 pp),Abstract content,poster,cp96
p2391,075082cfbedfe3161d15354b31859ca59dfbeafb,c16,Knowledge Discovery and Data Mining,A global database of soil respiration data,"Abstract. Soil respiration – RS, the flux of CO2 from the soil to the atmosphere – is probably the least well constrained component of the terrestrial carbon cycle. Here we introduce the SRDB database, a near-universal compendium of published RS data, and make it available to the scientific community both as a traditional static archive and as a dynamic community database that may be updated over time by interested users. The database encompasses all published studies that report one of the following data measured in the field (not laboratory): annual RS, mean seasonal RS, a seasonal or annual partitioning of RS into its sources fluxes, RS temperature response (Q10), or RS at 10 °C. Its orientation is thus to seasonal and annual fluxes, not shorter-term or chamber-specific measurements. To date, data from 818 studies have been entered into the database, constituting 3379 records. The data span the measurement years 1961–2007 and are dominated by temperate, well-drained forests. We briefly examine some aspects of the SRDB data – its climate space coverage, mean annual RS fluxes and their correlation with other carbon fluxes, RS variability, temperature sensitivities, and the partitioning of RS source flux – and suggest some potential lines of research that could be explored using these data. The SRDB database is available online in a permanent archive as well as via a project-hosting repository; the latter source leverages open-source software technologies to encourage wider participation in the database's future development. Ultimately, we hope that the updating of, and corrections to, the SRDB will become a shared project, managed by the users of these data in the scientific community.",poster,cp16
p2392,1784554a7bee71a24bb161957c4ffab98f0a9be1,c30,IEEE Aerospace Conference,Entity-relationship modeling - foundations of database technology,"From the Publisher: 
Database technology and entity-relationship (ER) modeling have meanwhile reached the level of an established technology. This book presents the achievements of research in this field in a comprehensive survey. It deals with the ER model and its extensions with regard to an integrated development and modeling of database applications and, consequently, the specification of structures, behavior, and interaction."" ""Apart from research on the ER model and the syntax, semantics, and pragmatics of database modeling the book also presents techniques for the translation of the ER model into classical database models and languages such as relational, hierarchical, and network models and languages, and also into object-oriented models."" ""The book is of interest for all database theoreticians as well as practitioners who are provided with the relevant fundamentals of database modeling.",poster,cp30
p2393,387e290dfd914cc2bfa4a8d76386cad4c0b882d0,c100,ACM SIGMOD Conference,On supporting containment queries in relational database management systems,"Virtually all proposals for querying XML include a class of query we term “containment queries”. It is also clear that in the foreseeable future, a substantial amount of XML data will be stored in relational database systems. This raises the question of how to support these containment queries. The inverted list technology that underlies much of Information Retrieval is well-suited to these queries, but should we implement this technology (a) in a separate loosely-coupled IR engine, or (b) using the native tables and query execution machinery of the RDBMS? With option (b), more than twenty years of work on RDBMS query optimization, query execution, scalability, and concurrency control and recovery immediately extend to the queries and structures that implement these new operations. But all this will be irrelevant if the performance of option (b) lags that of (a) by too much. In this paper, we explore some performance implications of both options using native implementations in two commercial relational database systems and in a special purpose inverted list engine. Our performance study shows that while RDBMSs are generally poorly suited for such queries, under conditions they can outperform an inverted list engine. Our analysis further identifies two significant causes that differentiate the performance of the IR and RDBMS implementations: the join algorithms employed and the hardware cache utilization. Our results suggest that contrary to most expectations, with some modifications, a native implementations in an RDBMS can support this class of query much more efficiently.",fullPaper,cp100
p2394,816b0957fa05347951a1b37c29e21b67d9257c91,c87,European Conference on Computer Vision,The ENZYME database in 2000,The ENZYME database is a repository of information related to the nomenclature of enzymes. In recent years it has became an indispensable resource for the development of metabolic databases. The current version contains information on 3705 enzymes. It is available through the ExPASy WWW server (http://www.expasy.ch/enzyme/ ).,poster,cp87
p2395,6e6eecc13a594edbb3392819026f5d8a23818536,c70,International Conference on Intelligent Robotics and Applications,ELM—the database of eukaryotic linear motifs,"Linear motifs are short, evolutionarily plastic components of regulatory proteins and provide low-affinity interaction interfaces. These compact modules play central roles in mediating every aspect of the regulatory functionality of the cell. They are particularly prominent in mediating cell signaling, controlling protein turnover and directing protein localization. Given their importance, our understanding of motifs is surprisingly limited, largely as a result of the difficulty of discovery, both experimentally and computationally. The Eukaryotic Linear Motif (ELM) resource at http://elm.eu.org provides the biological community with a comprehensive database of known experimentally validated motifs, and an exploratory tool to discover putative linear motifs in user-submitted protein sequences. The current update of the ELM database comprises 1800 annotated motif instances representing 170 distinct functional classes, including approximately 500 novel instances and 24 novel classes. Several older motif class entries have been also revisited, improving annotation and adding novel instances. Furthermore, addition of full-text search capabilities, an enhanced interface and simplified batch download has improved the overall accessibility of the ELM data. The motif discovery portion of the ELM resource has added conservation, and structural attributes have been incorporated to aid users to discriminate biologically relevant motifs from stochastically occurring non-functional instances.",poster,cp70
p2396,251c272eef27fed72dd4e4e07c202f20e6dbd55a,c97,Interspeech,A new version of the RDP (Ribosomal Database Project),"The Ribosomal Database Project (RDP-II), previously described by Maidak et al. [ Nucleic Acids Res. (1997), 25, 109-111], is now hosted by the Center for Microbial Ecology at Michigan State University. RDP-II is a curated database that offers ribosomal RNA (rRNA) nucleotide sequence data in aligned and unaligned forms, analysis services, and associated computer programs. During the past two years, data alignments have been updated and now include >9700 small subunit rRNA sequences. The recent development of an ObjectStore database will provide more rapid updating of data, better data accuracy and increased user access. RDP-II includes phylogenetically ordered alignments of rRNA sequences, derived phylogenetic trees, rRNA secondary structure diagrams, and various software programs for handling, analyzing and displaying alignments and trees. The data are available via anonymous ftp (ftp.cme.msu. edu) and WWW (http://www.cme.msu.edu/RDP). The WWW server provides ribosomal probe checking, approximate phylogenetic placement of user-submitted sequences, screening for possible chimeric rRNA sequences, automated alignment, and a suggested placement of an unknown sequence on an existing phylogenetic tree. Additional utilities also exist at RDP-II, including distance matrix, T-RFLP, and a Java-based viewer of the phylogenetic trees that can be used to create subtrees.",poster,cp97
p2397,472416ec7486fcea0a323d86b0db5db56391599e,c102,International Conference on Biometrics,The Cochrane Database of Systematic Reviews,"This paper reminds readers of this journal of an important source of summaries of the evidence for effectiveness of healthcare interventions, namely the Cochrane Database of Systematic Reviews.",poster,cp102
p2398,51f3fbc8b948dd93ed6d1e27e320141d0507603d,c68,International Workshop on Software Engineering for Large-Scale Multi-Agent Systems,CPPsite: a curated database of cell penetrating peptides,"Delivering drug molecules into the cell is one of the major challenges in the process of drug development. In past, cell penetrating peptides have been successfully used for delivering a wide variety of therapeutic molecules into various types of cells for the treatment of multiple diseases. These peptides have unique ability to gain access to the interior of almost any type of cell. Due to the huge therapeutic applications of CPPs, we have built a comprehensive database ‘CPPsite’, of cell penetrating peptides, where information is compiled from the literature and patents. CPPsite is a manually curated database of experimentally validated 843 CPPs. Each entry provides information of a peptide that includes ID, PubMed ID, peptide name, peptide sequence, chirality, origin, nature of peptide, sub-cellular localization, uptake efficiency, uptake mechanism, hydrophobicity, amino acid frequency and composition, etc. A wide range of user-friendly tools have been incorporated in this database like searching, browsing, analyzing, mapping tools. In addition, we have derived various types of information from these peptide sequences that include secondary/tertiary structure, amino acid composition and physicochemical properties of peptides. This database will be very useful for developing models for predicting effective cell penetrating peptides. Database URL: http://crdd.osdd.net/raghava/cppsite/.",poster,cp68
p2399,0d74d07ee64ae04e01a1893d798a93680a9211c8,c92,Advances in Soft Computing,Conceptual Database Design: An Entity-Relationship Approach,I. CONCEPTUAL DATABASE DESIGN. 1. An Introduction to Database Design. 2. Data Modeling Concepts. 3. Methodologies for Conceptual Design. 4. View Design. 5. View Integration. 6. Improving the Quality of a Database Schema. 7. Schema Documentation and Maintenance. II. FUNCTIONAL ANALYSIS FOR DATABASE DESIGN. 1. Functional Analysis Using the Dataflow Model. 2. Joint Data and Functional Analysis. 3. Case Study. III. LOGICAL DESIGN AND DESIGN TOOLS. 1. High-Level Logical Design Using the Entity-Relationship Model. 2. Logical Design for the Relational Model. 3. Logical Design for the Network Model. 4. Logical Design for the Hierarchical Model. 5. Database Design Tools. Index. 0805302441T04062001,poster,cp92
p2400,e70622a35880b552e12d664d5bf55eaa55f189df,c79,International Conference on Medical Image Computing and Computer-Assisted Intervention,RBPDB: a database of RNA-binding specificities,"The RNA-Binding Protein DataBase (RBPDB) is a collection of experimental observations of RNA-binding sites, both in vitro and in vivo, manually curated from primary literature. To build RBPDB, we performed a literature search for experimental binding data for all RNA-binding proteins (RBPs) with known RNA-binding domains in four metazoan species (human, mouse, fly and worm). In total, RPBDB contains binding data on 272 RBPs, including 71 that have motifs in position weight matrix format, and 36 sets of sequences of in vivo-bound transcripts from immunoprecipitation experiments. The database is accessible by a web interface which allows browsing by domain or by organism, searching and export of records, and bulk data downloads. Users can also use RBPDB to scan sequences for RBP-binding sites. RBPDB is freely available, without registration at http://rbpdb.ccbr.utoronto.ca/.",poster,cp79
p2401,f472655c370e4b3209f35a2834e01fb4e77ade9b,c108,International Conference on Information Integration and Web-based Applications & Services,NoSQL databases: a step to database scalability in web environment,"The paper is focused on so called NoSQL databases. In context of cloud computing, architectures and basic features of these databases are studied, particularly their horizontal scalability and concurrency model, that is mostly weaker than ACID transactions in relational SQL-like database systems. Some characteristics like a data model and querying capabilities are discussed in more detail. The paper also contains an overview of some representatives of NoSQL databases.",fullPaper,cp108
p2402,33b43080c8c1e9ec4d491125fac1621a0ed7f4fc,c44,International Workshop on Green and Sustainable Software,MIPS: a database for genomes and protein sequences,"The Munich Information Center for Protein Sequences (MIPS-GSF), Martinsried, near Munich, Germany, continues its longstanding tradition to develop and maintain high quality curated genome databases. In addition, efforts have been intensified to cover the wealth of complete genome sequences in a systematic, comprehensive form. Bioinformatics, supporting national as well as European sequencing and functional analysis projects, has resulted in several up-to-date genome-oriented databases. This report describes growing databases reflecting the progress of sequencing the Arabidopsis thaliana (MATDB) and Neurospora crassa genomes (MNCDB), the yeast genome database (MYGD) extended by functional analysis data, the database of annotated human EST-clusters (HIB) and the database of the complete cDNA sequences from the DHGP (German Human Genome Project). It also contains information on the up-to-date database of complete genomes (PEDANT), the classification of protein sequences (ProtFam) and the collection of protein sequence data within the framework of the PIR-International Protein Sequence Database. These databases can be accessed through the MIPS WWW server (http://www. mips.biochem.mpg.de).",poster,cp44
p2403,08705b00d9dcdfa69dc40f073c5a29e2cf8bcbed,c112,Very Large Data Bases Conference,The BioGRID Interaction Database: 2008 update,"The Biological General Repository for Interaction Datasets (BioGRID) database (http://www.thebiogrid.org) was developed to house and distribute collections of protein and genetic interactions from major model organism species. BioGRID currently contains over 198 000 interactions from six different species, as derived from both high-throughput studies and conventional focused studies. Through comprehensive curation efforts, BioGRID now includes a virtually complete set of interactions reported to date in the primary literature for both the budding yeast Saccharomyces cerevisiae and the fission yeast Schizosaccharomyces pombe. A number of new features have been added to the BioGRID including an improved user interface to display interactions based on different attributes, a mirror site and a dedicated interaction management system to coordinate curation across different locations. The BioGRID provides interaction data with monthly updates to Saccharomyces Genome Database, Flybase and Entrez Gene. Source code for the BioGRID and the linked Osprey network visualization system is now freely available without restriction.",poster,cp112
p2404,49578a040f3346f81759ac40cc174cd12cb40045,c14,International Conference on Exploring Services Science,"JASPAR, the open access database of transcription factor-binding profiles: new content and tools in the 2008 update","JASPAR is a popular open-access database for matrix models describing DNA-binding preferences for transcription factors and other DNA patterns. With its third major release, JASPAR has been expanded and equipped with additional functions aimed at both casual and power users. The heart of the JASPAR database—the JASPAR CORE sub-database—has increased by 12% in size, and three new specialized sub-databases have been added. New functions include clustering of matrix models by similarity, generation of random matrices by sampling from selected sets of existing models and a language-independent Web Service applications programming interface for matrix retrieval. JASPAR is available at http://jaspar.genereg.net.",poster,cp14
p2405,b37d1e07de30b7061be39f766fec1dd8c73de7be,j102,Nucleic Acids Research,BIND--The Biomolecular Interaction Network Database.,"The Biomolecular Interaction Network Database (BIND; http://binddb. org) is a database designed to store full descriptions of interactions, molecular complexes and pathways. Development of the BIND 2.0 data model has led to the incorporation of virtually all components of molecular mechanisms including interactions between any two molecules composed of proteins, nucleic acids and small molecules. Chemical reactions, photochemical activation and conformational changes can also be described. Everything from small molecule biochemistry to signal transduction is abstracted in such a way that graph theory methods may be applied for data mining. The database can be used to study networks of interactions, to map pathways across taxonomic branches and to generate information for kinetic simulations. BIND anticipates the coming large influx of interaction information from high-throughput proteomics efforts including detailed information about post-translational modifications from mass spectrometry. Version 2.0 of the BIND data model is discussed as well as implementation, content and the open nature of the BIND project. The BIND data specification is available as ASN.1 and XML DTD.",fullPaper,jv102
p2406,83a500fcc7cd98db063b73461277ac885c8fe7c3,c109,International Conference on Mobile Data Management,Towards Sensor Database Systems,Abstract content,fullPaper,cp109
p2407,e4b9a5ed3f838da72a8f3169a01be3268c4d3c2c,c96,USENIX Symposium on Operating Systems Design and Implementation,NGA Project Strong-Motion Database,"A key component of the NGA research project was the development of a strong-motion database with improved quality and content that could be used for ground-motion research as well as for engineering practice. Development of the NGA database was executed through the Lifelines program of the PEER Center with contributions from several research organizations and many individuals in the engineering and seismological communities. Currently, the data set consists of 3551 publicly available multi-component records from 173 shallow crustal earthquakes, ranging in magnitude from 4.2 to 7.9. Each acceleration time series has been corrected and filtered, and pseudo absolute spectral acceleration at multiple damping levels has been computed for each of the 3 components of the acceleration time series. The lowest limit of usable spectral frequency was determined based on the type of filter and the filter corner frequency. For NGA model development, the two horizontal acceleration components were further rotated to form the orientation-independent measure of horizontal ground motion (GMRotI50). In addition to the ground-motion parameters, a large and comprehensive list of metadata characterizing the recording conditions of each record was also developed. NGA data have been systematically checked and reviewed by experts and NGA developers.",poster,cp96
p2408,369f62edea6e6ace6f68c7ebe9bdde046b9514ad,c86,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,The asteroid lightcurve database,Abstract content,poster,cp86
p2409,0d6b4182d465f70e359e30372e550121a0fc94b0,c84,The Web Conference,SCOP: a structural classification of proteins database,"The Structural Classification of Proteins (SCOP) database provides a detailed and comprehensive description of the relationships of known protein structures. The classification is on hierarchical levels: the first two levels, family and superfamily, describe near and distant evolutionary relationships; the third, fold, describes geometrical relationships. The distinction between evolutionary relationships and those that arise from the physics and chemistry of proteins is a feature that is unique to this database so far. The sequences of proteins in SCOP provide the basis of the ASTRAL sequence libraries that can be used as a source of data to calibrate sequence search algorithms and for the generation of statistics on, or selections of, protein structures. Links can be made from SCOP to PDB-ISL: a library containing sequences homologous to proteins of known structure. Sequences of proteins of unknown structure can be matched to distantly related proteins of known structure by using pairwise sequence comparison methods to find homologues in PDB-ISL. The database and its associated files are freely accessible from a number of WWW sites mirrored from URL http://scop.mrc-lmb.cam.ac.uk/scop/",poster,cp84
p2410,cc42c8dac7c3fb8cd522136f1c7c31ae45a3121f,j102,Nucleic Acids Research,The MDM2 gene amplification database.,"The p53 tumor suppressor gene is inactivated in human tumors by several distinct mechanisms. The best characterized inactivation mechanisms are: (i) gene mutation; (ii) p53 protein association with viral proteins; (iii) p53 protein association with the MDM2 cellular oncoprotein. The MDM2 gene has been shown to be abnormally up-regulated in human tumors and tumor cell lines by gene amplification, increased transcript levels and enhanced translation. This communication presents a brief review of the spectrum of MDM2 abnormalities in human tumors and compares the tissue distribution of MDM2 amplification and p53 mutation frequencies. In this study, 3889 samples from tumors or xenografts from 28 tumor types were examined for MDM2 amplification from previously published sources. The overall frequency of MDM2 amplification in these human tumors was 7%. Gene amplification was observed in 19 tumor types, with the highest frequency observed in soft tissue tumors (20%), osteosarcomas (16%) and esophageal carcinomas (13%). Tumors which showed a higher incidence of MDM2 amplification than p53 mutation were soft tissue tumors, testicular germ cell cancers and neuro-blastomas. Data from studies where both MDM2 amplification and p53 mutations were analyzed within the same samples showed that mutations in these two genes do not generally occur within the same tumor. In these studies, 29 out of a total of 33 MDM2 amplification-positive tumors had wild-type p53. We hypothesize that heretofore uncharacterized carcinogens favor MDM2 amplification over p53 mutations in certain tumor types. A database listing the MDM2 gene amplifications is available on the World Wide Web at http://www. infosci.coh.org/mdm2 . Charts of MDM2 amplification frequencies and comparisons with p53 genetic alterations are also available at this Web site.",fullPaper,jv102
p2411,0b115b72214b501923852e6278b20401fee27f85,c80,International Conference on Learning Representations,Database Repairing and Consistent Query Answering,"Integrity constraints are semantic conditions that a database should satisfy in order to be an appropriate model of external reality. In practice, and for many reasons, a database may not satisfy those integrity constraints, and for that reason it is said to be inconsistent. However, and most likely, a large portion of the database is still semantically correct, in a sense that has to be made precise. After having provided a formal characterization of consistent data in an inconsistent database, the natural problem emerges of extracting that semantically correct data, as query answers. The consistent data in an inconsistent database is usually characterized as the data that persists across all the database instances that are consistent and minimally differ from the inconsistent instance. Those are the so-called repairs of the database. In particular, the consistent answers to a query posed to the inconsistent database are those answers that can be simultaneously obtained from all the database repairs. As expected, the notion of repair requires an adequate notion of distance that allows for the comparison of databases with respect to how much they differ from the inconsistent instance. On this basis, the minimality condition on repairs can be properly formulated. In this monograph we present and discuss these fundamental concepts, different repair semantics, algorithms for computing consistent answers to queries, and also complexity-theoretic results related to the computation of repairs and doing consistent query answering. Table of Contents: Introduction / The Notions of Repair and Consistent Answer / Tractable CQA and Query Rewriting / Logically Specifying Repairs / Decision Problems in CQA: Complexity and Algorithms / Repairs and Data Cleaning",poster,cp80
p2412,7463a0b934ac40a353773840485bb56d35fbbb66,c55,Annual Workshop of the Psychology of Programming Interest Group,Database on medicinal plants used in Ayurveda,Abstract content,poster,cp55
p2413,a68f8e1f7b9f4144049537c766be3faec5b54786,c80,International Conference on Learning Representations,The Exoplanet Orbit Database,"We present a database of well-determined orbital parameters of exoplanets, and their host stars’ properties. This database comprises spectroscopic orbital elements measured for 427 planets orbiting 363 stars from radial velocity and transit measurements as reported in the literature. We have also compiled fundamental transit parameters, stellar parameters, and the method used for the planets discovery. This Exoplanet Orbit Database includes all planets with robust, well measured orbital parameters reported in peer-reviewed articles. The database is available in a searchable, filterable, and sortable form online through the Exoplanets Data Explorer table, and the data can be plotted and explored through the Exoplanet Data Explorer plotter. We use the Data Explorer to generate publication-ready plots, giving three examples of the signatures of exoplanet migration and dynamical evolution: We illustrate the character of the apparent correlation between mass and period in exoplanet orbits, the different selection biases between radial velocity and transit surveys, and that the multiplanet systems show a distinct semimajor-axis distribution from apparently singleton systems.",poster,cp80
p2414,40691c4ba3c2eb44849273eb92dca66da6b634ad,c37,Asia-Pacific Software Engineering Conference,NIST Atomic Spectra Database (version 2.0),Abstract content,poster,cp37
p2415,17e6076b6761788684434d1e14e85e8877fc0146,c33,International Conference on Agile Software Development,LandScan: A Global Population Database for Estimating Populations at Risk,"The LandScan Global Population Project produced a world-wide 1998 population database at a 30-by 30-second resolution for estimating ambient populations at risk. Best available census counts were distributed to cells based on probability coefficients which, in turn, were based on road proximity, slope, land cover, and nighttime lights, LandScan 1998 has been completed for the entire world. Verification and validation (V&V) studies were conducted routinely for all regions and more extensively for Israel, Germany, and the southwestern United States. Geographic information systems (GIS) were essential for conflation of diverse input variables, computation of probability coefficients, allocation of population to cells, and reconciliation of cell totals with aggregate (usually province) control totals. Remote sensing was an essential source of two input variables-land cover and nighttime lights-and one ancillary database-high-resolution panchromatic imagery-used in V&V of the population model and resulting LandScan database.",poster,cp33
p2416,78597d4989a7f7f892067637dd3271e60569b087,c99,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,DisProt: the Database of Disordered Proteins,"The Database of Protein Disorder (DisProt) links structure and function information for intrinsically disordered proteins (IDPs). Intrinsically disordered proteins do not form a fixed three-dimensional structure under physiological conditions, either in their entireties or in segments or regions. We define IDP as a protein that contains at least one experimentally determined disordered region. Although lacking fixed structure, IDPs and regions carry out important biological functions, being typically involved in regulation, signaling and control. Such functions can involve high-specificity low-affinity interactions, the multiple binding of one protein to many partners and the multiple binding of many proteins to one partner. These three features are all enabled and enhanced by protein intrinsic disorder. One of the major hindrances in the study of IDPs has been the lack of organized information. DisProt was developed to enable IDP research by collecting and organizing knowledge regarding the experimental characterization and the functional associations of IDPs. In addition to being a unique source of biological information, DisProt opens doors for a plethora of bioinformatics studies. DisProt is openly available at .",poster,cp99
p2417,5a7f573a9f2493e1705ffadc0009122db4f646da,c46,Brazilian Symposium on Software Engineering,The Cochrane Database of Systematic Reviews,"1 Interventions for cutaneous molluscum contagiosum (Review) Copyright © 2017 The Cochrane Collaboration. Published by John Wiley & Sons, Ltd. Main results We found 11 new studies for this update, resulting in 22 included studies with a total of 1650 participants. The studies examined the effects of topical (20 studies) and systemic interventions (2 studies). Among the new included studies were the full trial reports of three large unpublished studies, brought to our attention by an expert in the field. They all provided moderate-quality evidence for a lack of effect of 5% imiquimod compared to vehicle (placebo) on shortterm clinical cure (4 studies, 850 participants, 12 weeks after start of treatment, risk ratio (RR) 1.33, 95% confidence interval (CI) 0.92 to 1.93), medium-term clinical cure (2 studies, 702 participants, 18 weeks after start of treatment, RR 0.88, 95% CI 0.67 to 1.14), and long-term clinical cure (2 studies, 702 participants, 28 weeks after start of treatment, RR 0.97, 95% CI 0.79 to 1.17). We found similar but more certain results for short-term improvement (4 studies, 850 participants, 12 weeks after start of treatment, RR 1.14, 95% CI 0.89 to 1.47; high-quality evidence). For the outcome ’any adverse effect’, we found high-quality evidence for little or no difference between topical 5% imiquimod and vehicle (3 studies, 827 participants, RR 0.97, 95% CI 0.88 to 1.07), but application site reactions were more frequent in the groups treated with imiquimod (moderate-quality evidence): any application site reaction (3 studies, 827 participants, RR 1.41, 95% CI 1.13 to 1.77, the number needed to treat for an additional harmful outcome (NNTH) was 11); severe application site reaction (3 studies, 827 participants, RR 4.33, 95% CI 1.16 to 16.19, NNTH over 40). For the following 11 comparisons, there was limited evidence to show which treatment was superior in achieving short-term clinical cure (low-quality evidence): 5% imiquimod less effective than cryospray (1 study, 74 participants, RR 0.60, 95% CI 0.46 to 0.78) and 10% potassium hydroxide (2 studies, 67 participants, RR 0.65, 95% CI 0.46 to 0.93); 10% Australian lemon myrtle oil more effective than olive oil (1 study, 31 participants, RR 17.88, 95% CI 1.13 to 282.72); 10% benzoyl peroxide cream more effective than 0.05% tretinoin (1 study, 30 participants, RR 2.20, 95% CI 1.01 to 4.79); 5% sodium nitrite co-applied with 5% salicylic acid more effective than 5% salicylic acid alone (1 study, 30 participants, RR 3.50, 95% CI 1.23 to 9.92); and iodine plus tea tree oil more effective than tea tree oil (1 study, 37 participants, RR 0.20, 95% CI 0.07 to 0.57) or iodine alone (1 study, 37 participants, RR 0.07, 95% CI 0.01 to 0.50). Although there is some uncertainty, 10% potassium hydroxide appears to be more effective than saline (1 study, 20 participants, RR 3.50, 95% CI 0.95 to 12.90); homeopathic calcarea carbonica appears to be more effective than placebo (1 study, 20 participants, RR 5.57, 95% CI 0.93 to 33.54); 2.5% appears to be less effective than 5% solution of potassium hydroxide (1 study, 25 participants, RR 0.35, 95% CI 0.12 to 1.01); and 10% povidone iodine solution plus 50% salicylic acid plaster appears to be more effective than salicylic acid plaster alone (1 study, 30 participants, RR 1.43, 95% CI 0.95 to 2.16). We found no statistically significant differences for other comparisons (most of which addressed two different topical treatments). We found no randomised controlled trial evidence for expressing lesions or topical hydrogen peroxide. Study limitations included no blinding, many dropouts, and no intention-to-treat analysis. Except for the severe application site reactions of imiquimod, none of the evaluated treatments described above were associated with serious adverse effects (low-quality evidence). Among the most common adverse events were pain during application, erythema, and itching. Included studies of the following comparisons did not report adverse effects: calcarea carbonica versus placebo, 10% povidone iodine plus 50% salicylic acid plaster versus salicylic acid plaster, and 10% benzoyl peroxide versus 0.05% tretinoin. We were unable to judge the risk of bias in most studies due to insufficient information, especially regarding concealment of allocation and possible selective reporting. We considered five studies to be at low risk of bias. Authors’ conclusions No single intervention has been shown to be convincingly effective in the treatment of molluscum contagiosum. We found moderatequality evidence that topical 5% imiquimod was no more effective than vehicle in terms of clinical cure, but led to more application site reactions, and high-quality evidence that there was no difference between the treatments in terms of short-term improvement. However, high-quality evidence showed a similar number of general side effects in both groups. As the evidence found did not favour any one treatment, the natural resolution of molluscum contagiosum remains a strong method for dealing with the condition. P L A I N L A N G U A G E S U M M A R Y Treatments for molluscum contagiosum, a common viral skin infection in children Review question 2 Interventions for cutaneous molluscum contagiosum (Review) Copyright © 2017 The Cochrane Collaboration. Published by John Wiley & Sons, Ltd. We reviewed the evidence for the effect of any treatment on the common viral skin infection molluscum contagiosum. We excluded people with a repressed immune system or sexually transmitted molluscum contagiosum. Background Molluscum contagiosum in healthy people is a self limiting, relatively harmless viral skin infection. It mainly affects children and adolescents and is rare in adults. It occurs worldwide, but seems much more frequent in geographic areas with warm climates. Molluscum contagiosum usually presents as single or multiple pimples filled with an oily substance. People may seek treatment for social and cosmetic reasons and because of concerns about spreading the disease to others. Treatment is intended to speed up the healing process. Study characteristics We searched the literature to July 2016. We included 22 trials (total of 1650 participants). Twenty of the studies evaluated topical treatment, and two studies evaluated treatment taken by mouth (oral). Comparisons included physical therapies, as well as topical and oral treatments. Most studies were set in hospital outpatient or emergency departments, and were performed in North America, the UK, Asia, or South America. Participants were of both sexes and were mainly children or young adults. Follow-up duration varied from 3 to 28 weeks after randomisation. Only five studies had longer than 3 months’ follow-up. Five studies reported commercial funding, three studies obtained medication for free from pharmaceutical companies, 12 studies did not mention the source of funding, one study reported charity funding, and one study reported they had had no financial support. Key results We found that many common treatments for molluscum, such as physical destruction, have not been adequately evaluated. Some of the included treatments are not part of standard practice. We found moderate-quality evidence that topical 5% imiquimod is probably no more effective than vehicle (i.e. the same cream but without imiquimod) in achieving short-, medium-, and long-term clinical cure. High-quality (and thus more certain) evidence showed that topical 5% imiquimod is no better than placebo at improving molluscum up to three months after the start of treatment. High-quality evidence showed that 5% imiquimod differed little or not at all in the number of side effects compared to vehicle. However, moderate-quality evidence suggests that there are probably more application site reactions when using topical 5% imiquimod compared with vehicle. Low-quality evidence, based on one or two mostly small studies, revealed the following results for the outcome short-term clinical cure: 5% imiquimod less effective than cryospray or 10% potassium hydroxide; 10% Australian lemon myrtle oil more effective than olive oil; 10% benzoyl peroxide cream more effective than 0.05% tretinoin; 5% sodium nitrite co-applied with 5% salicylic acid more effective than 5% salicylic acid alone; and iodine plus tea tree oil more effective than tea tree oil or iodine alone. We found more uncertain (low-quality) evidence to suggest that 10% potassium hydroxide is more effective than saline; homeopathic calcarea carbonica is more effective than placebo; 2.5% solution of potassium hydroxide is less effective than 5% solution of potassium hydroxide; and 10% povidone iodine solution and 50% salicylic acid plaster are more effective than salicylic acid plaster alone. Except for the severe application site reactions of imiquimod, none of these treatments led to serious adverse effects (low-quality evidence). Pain during treatment application, redness, and itching were among the most reported adverse effects. We found no differences between the treatments assessed in the other comparisons. We found no randomised trials for several commonly used treatments, such as expressing lesions with an orange stick or topical hydrogen peroxide. Since most lesions resolve within months, unless better evidence for the superiority of active treatments emerges, molluscum contagiosum can be left to heal naturally. Quality of the evidence For topical imiquimod, the quality of the evidence for clinical cure, short-term improvement, and adverse effects was moderate to high. For all other comparisons, the quality of the evidence for short-term clinical cure and adverse effects was low. Common limitations of the included studies were that the numbers of participants were small, the investigators were not blinded, and participants who did not complete the study (numerous in some studies) were not included in the analyses. 3 Interventions for cutaneous molluscum contagiosum (Rev",poster,cp46
p2418,fd6c52fe253f972a54102f43f7d6ee9827eeafd0,c110,IEEE International Conference on Automatic Face & Gesture Recognition,A high-resolution 3D dynamic facial expression database,"Face information processing relies on the quality of data resource. From the data modality point of view, a face database can be 2D or 3D, and static or dynamic. From the task point of view, the data can be used for research of computer based automatic face recognition, face expression recognition, face detection, or cognitive and psychological investigation. With the advancement of 3D imaging technologies, 3D dynamic facial sequences (called 4D data) have been used for face information analysis. In this paper, we focus on the modality of 3D dynamic data for the task of facial expression recognition. We present a newly created high-resolution 3D dynamic facial expression database, which is made available to the scientific research community. The database contains 606 3D facial expression sequences captured from 101 subjects of various ethnic backgrounds. The database has been validated through our facial expression recognition experiment using an HMM based 3D spatio-temporal facial descriptor. It is expected that such a database shall be used to facilitate the facial expression analysis from a static 3D space to a dynamic 3D space, with a goal of scrutinizing facial behavior at a higher level of detail in a real 3D spatio-temporal domain.",fullPaper,cp110
p2419,711d5205c29fd772e22521cc4f9150db2f338d8e,c99,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,NIST Atomic Spectra Database,"Accurate atomic data have great importance in astrophysics, plasma research, and other fields of physics. For more than 10 years, the Atomic Spectra Database (ASD) at the National Institute of Standards and Technology has served as a convenient and robust source of critically evaluated data on tens of thousands of spectral lines and energy levels. The recent upgrade of the ASD represents a significant new step in the development of dynamic databases providing powerful tools for data analysis and manipulation. We present a detailed description of ASD 3.0 emphasizing numerous advanced features and options for data search and presentation.",poster,cp99
p2420,4a30343f3230dddd96fd6f79547fef9407262dbf,c16,Knowledge Discovery and Data Mining,A comparison of a graph database and a relational database: a data provenance perspective,"Relational databases have been around for many decades and are the database technology of choice for most traditional data-intensive storage and retrieval applications. Retrievals are usually accomplished using SQL, a declarative query language. Relational database systems are generally efficient unless the data contains many relationships requiring joins of large tables. Recently there has been much interest in data stores that do not use SQL exclusively, the so-called NoSQL movement. Examples are Google's BigTable and Facebook's Cassandra. This paper reports on a comparison of one such NoSQL graph database called Neo4j with a common relational database system, MySQL, for use as the underlying technology in the development of a software system to record and query data provenance information.",poster,cp16
p2421,ad8b036c09fd87ab79dc087244fb93e40a5731c0,c91,Workshop on Algorithms and Models for the Web-Graph,GeneDB—an annotation database for pathogens,"GeneDB (http://www.genedb.org) is a genome database for prokaryotic and eukaryotic pathogens and closely related organisms. The resource provides a portal to genome sequence and annotation data, which is primarily generated by the Pathogen Genomics group at the Wellcome Trust Sanger Institute. It combines data from completed and ongoing genome projects with curated annotation, which is readily accessible from a web based resource. The development of the database in recent years has focused on providing database-driven annotation tools and pipelines, as well as catering for increasingly frequent assembly updates. The website has been significantly redesigned to take advantage of current web technologies, and improve usability. The current release stores 41 data sets, of which 17 are manually curated and maintained by biologists, who review and incorporate data from the scientific literature, as well as other sources. GeneDB is primarily a production and annotation database for the genomes of predominantly pathogenic organisms.",poster,cp91
p2422,353988f8d56224b9d46aa34059e499c638dcbc2e,j214,Journal of chemical information and computer sciences,The United Kingdom Chemical Database Service,"The Chemical Database Service (CDS) is a national service, funded by the Chemistry Programme of the United Kingdom Engineering and Physical Sciences Research Council (EPSRC). It provides access for UK academics to a range of chemistry databases in the areas of crystallography, synthetic organic chemistry, spectroscopy, and physical chemistry. Three post-doctoral chemists are available to assist users with problems, run training courses, and also give advice to the community on accessing other sources of chemical data and software.",fullPaper,jv214
p2423,3e83d54c5e8dfba82638b4f75ace31505ea60ff0,c59,British Computer Society Conference on Human-Computer Interaction,The IntAct molecular interaction database in 2010,"IntAct is an open-source, open data molecular interaction database and toolkit. Data is abstracted from the literature or from direct data depositions by expert curators following a deep annotation model providing a high level of detail. As of September 2009, IntAct contains over 200.000 curated binary interaction evidences. In response to the growing data volume and user requests, IntAct now provides a two-tiered view of the interaction data. The search interface allows the user to iteratively develop complex queries, exploiting the detailed annotation with hierarchical controlled vocabularies. Results are provided at any stage in a simplified, tabular view. Specialized views then allows ‘zooming in’ on the full annotation of interactions, interactors and their properties. IntAct source code and data are freely available at http://www.ebi.ac.uk/intact.",poster,cp59
p2424,e41aaa1dd224409dc0d87d08afec4a7dda17660d,c113,International Conference on Image Analysis and Processing,"Database Systems: A Practical Approach to Design, Implementation and Management","This best-selling text introduces the theory behind databases in a concise yet comprehensive manner, providing database design methodology that can be used by both technical and non-technical readers. The methodology for relational Database Management Systems is presented in simple, step-by-step instructions in conjunction with a realistic worked example using three explicit phasesconceptual, logical, and physical database design. Background: Introduction to Databases; Database Environment; Database Architectures and the Web. The Relational Model and Languages: The Relational model; Relational Algebra and Relational Calculus; SQL: Data Manipulation; SQL: Data Definition; Query-By-Example (QBE). Database Analysis and Design: Database System Lifecycle; Database Analysis and the DreamHome Case Study; EntityRelationship Modeling; Enhanced EntityRelationship Modeling; Normalization; Advanced Normalization. Methodology: MethodologyConceptual Database Design; MethodologyLogical Database Design for Relational Model; MethodologyPhysical Database Design for Relational Databases; MethodologyMonitoring and Tuning the Operational System. Selected Database Issues: Security and Administration; Professional, Legal, and Ethical Issues; Transaction Management; Query Processing. Distributed DBMSs and Replication: Distributed DBMSsConcepts and Design; Distributed DBMSsAdvanced Concepts; Replication and Mobile Databases. Object DBMSs: Object-Oriented DBMSsConcepts and Design; Object-Oriented DBMSsStandards and Languages; Object-Relational DBMSs. Web and DBMSs: Web Technology and DBMSs; Semistructured Data and XML. Business Intelligence Technologies: Data Warehousing Concepts; Data Warehousing Design; OLAP; Data Mining. Appendices: Users' Requirements Specification for DreamHome Case Study; Other Case Studies; Alternative Data Modeling Notations; Summary of the Database Design Methodology for Relational Databases; Introduction to PyrrhoA Liteweight RDBMS. Web Appendices: File Organization and Storage Structures; When Is a DBMS Relational?; Commercial DBMSs: Access and Oracle; Programmatic SQL; Estimating Disk Space Requirements; Introduction to Object-Orientation; Example Web Scripts. This book is ideal for readers interested in database management or database design.",poster,cp113
p2425,3bee5cc2d0b6bfcd564158cd0cf1bd311dae68b7,c42,Joint Conference of International Workshop on Software Measurement and International Conference on Software Process and Product Measurement,PHOSIDA 2011: the posttranslational modification database,"The primary purpose of PHOSIDA (http://www.phosida.com) is to manage posttranslational modification sites of various species ranging from bacteria to human. Since its last report, PHOSIDA has grown significantly in size and evolved in scope. It comprises more than 80 000 phosphorylated, N-glycosylated or acetylated sites from nine different species. All sites are obtained from high-resolution mass spectrometric data using the same stringent quality criteria. One of the main distinguishing features of PHOSIDA is the provision of a wide range of analysis tools. PHOSIDA is comprised of three main components: the database environment, the prediction platform and the toolkit section. The database environment integrates and combines high-resolution proteomic data with multiple annotations. High-accuracy species-specific phosphorylation and acetylation site predictors, trained on the modification sites contained in PHOSIDA, allow the in silico determination of modified sites on any protein on the basis of the primary sequence. The toolkit section contains methods that search for sequence motif matches or identify de novo consensus, sequences from large scale data sets.",poster,cp42
p2426,17891bdbfec1950f7c361db96dca043cfbf54769,c81,IEEE Annual Symposium on Foundations of Computer Science,The TIGRFAMs database of protein families,"TIGRFAMs is a collection of manually curated protein families consisting of hidden Markov models (HMMs), multiple sequence alignments, commentary, Gene Ontology (GO) assignments, literature references and pointers to related TIGRFAMs, Pfam and InterPro models. These models are designed to support both automated and manually curated annotation of genomes. TIGRFAMs contains models of full-length proteins and shorter regions at the levels of superfamilies, subfamilies and equivalogs, where equivalogs are sets of homologous proteins conserved with respect to function since their last common ancestor. The scope of each model is set by raising or lowering cutoff scores and choosing members of the seed alignment to group proteins sharing specific function (equivalog) or more general properties. The overall goal is to provide information with maximum utility for the annotation process. TIGRFAMs is thus complementary to Pfam, whose models typically achieve broad coverage across distant homologs but end at the boundaries of conserved structural domains. The database currently contains over 1600 protein families. TIGRFAMs is available for searching or downloading at www.tigr.org/TIGRFAMs.",poster,cp81
p2427,5db8051dd2ee99996484bf1c48795c5ca13e04c5,c87,European Conference on Computer Vision,XCOM: Photon Cross Section Database (version 1.2),Abstract content,poster,cp87
p2428,a047888622c576ccf06a7708ae18a5d9ec5f09fd,j372,"Behavoir research methods, instruments & computers",Lexique 2 : A new French lexical database,Abstract content,fullPaper,jv372
p2429,e2a5593f587970018b9e826e5d876125b374b801,c110,IEEE International Conference on Automatic Face & Gesture Recognition,Integrating compression and execution in column-oriented database systems,"Column-oriented database system architectures invite a re-evaluation of how and when data in databases is compressed. Storing data in a column-oriented fashion greatly increases the similarity of adjacent records on disk and thus opportunities for compression. The ability to compress many adjacent tuples at once lowers the per-tuple cost of compression, both in terms of CPU and space overheads.In this paper, we discuss how we extended C-Store (a column-oriented DBMS) with a compression sub-system. We show how compression schemes not traditionally used in row-oriented DBMSs can be applied to column-oriented systems. We then evaluate a set of compression schemes and show that the best scheme depends not only on the properties of the data but also on the nature of the query workload.",poster,cp110
p2430,e13a71f639bed2c1c739b206a6a053e8f18651a6,c13,International Conference on Data Science and Advanced Analytics,"BioModels Database: a free, centralized database of curated, published, quantitative kinetic models of biochemical and cellular systems","BioModels Database (), part of the international initiative BioModels.net, provides access to published, peer-reviewed, quantitative models of biochemical and cellular systems. Each model is carefully curated to verify that it corresponds to the reference publication and gives the proper numerical results. Curators also annotate the components of the models with terms from controlled vocabularies and links to other relevant data resources. This allows the users to search accurately for the models they need. The models can currently be retrieved in the SBML format, and import/export facilities are being developed to extend the spectrum of formats supported by the resource.",poster,cp13
p2431,9aa8a679e3401f1bbf805b738095bf2ed52e08e7,j380,PLoS Medicine,Tuberculosis Drug Resistance Mutation Database,Andreas Sandgren and colleagues describe a new comprehensive resource on drug resistance mutations inM. tuberculosis.,fullPaper,jv380
p2432,97232c7bba5bef3ac970bf82966a8ea97cb4fa14,c55,Annual Workshop of the Psychology of Programming Interest Group,WHO global database on child growth and malnutrition,"ii The designations employed and the presentation of material do not imply the expression of any opinion whatsoever on the part of the World Health Organization concerning the legal status of any country, territory or area, its authorities, its current or former official name or the delimitation of its frontiers or boundaries. We are guilty of many errors and many faults, but our worst crime is abandoning the children, neglecting the foundation of life. Many of the things we need can wait. The child cannot. Right now is the time his bones are being formed, his blood is being made and his senses are being developed. To him we cannot answer "" Tomorrow "". His name is "" Today "". We dedicate this work to the world's children in the hope that it will alert decision-makers to how much remains to be done to ensure children's healthy growth and development. "" "" WHO/NUT/97.4 iv Acknowledgements The Programme of Nutrition appreciates the strong support from numerous individuals, institutions, governments, and nongovernmental and international organizations, without whose continual collaboration this compilation would not have been possible. A special note of gratitude is due to all those who provided standardized information and reanalyses of original data sets to conform to the database requirements. Thanks to such international cooperation in keeping the Global Database up-to-date, the Programme of Nutrition is able to present this vast compilation of data on worldwide patterns and trends in child growth and malnutrition. SD Standard deviation WHO World Health Organization Z-score (or SD-score) The deviation of an individual's value from the median value of a reference population, divided by the standard deviation of the reference population.",poster,cp55
p2433,a5881da1c592ea11d24f90992f5b210beaa3ea73,c100,ACM SIGMOD Conference,Gigascope: a stream database for network applications,"We have developed Gigascope, a stream database for network applications including traffic analysis, intrusion detection, router configuration analysis, network research, network monitoring, and performance monitoring and debugging. Gigascope is undergoing installation at many sites within the AT&T network, including at OC48 routers, for detailed monitoring. In this paper we describe our motivation for and constraints in developing Gigascope, the Gigascope architecture and query language, and performance issues. We conclude with a discussion of stream database research problems we have found in our application.",fullPaper,cp100
p2434,aa163167f51e580c3dcb1aaef00f60b08b6f64a8,c82,Workshop on Interdisciplinary Software Engineering Research,EcoCyc: a comprehensive database resource for Escherichia coli,"The EcoCyc database (http://EcoCyc.org/) is a comprehensive source of information on the biology of the prototypical model organism Escherichia coli K12. The mission for EcoCyc is to contain both computable descriptions of, and detailed comments describing, all genes, proteins, pathways and molecular interactions in E.coli. Through ongoing manual curation, extensive information such as summary comments, regulatory information, literature citations and evidence types has been extracted from 8862 publications and added to Version 8.5 of the EcoCyc database. The EcoCyc database can be accessed through a World Wide Web interface, while the downloadable Pathway Tools software and data files enable computational exploration of the data and provide enhanced querying capabilities that web interfaces cannot support. For example, EcoCyc contains carefully curated information that can be used as training sets for bioinformatics prediction of entities such as promoters, operons, genetic networks, transcription factor binding sites, metabolic pathways, functionally related genes, protein complexes and protein–ligand interactions.",poster,cp82
p2435,369e98d934881e9cfd464a73e56011cb807ab104,c70,International Conference on Intelligent Robotics and Applications,"THE COLOGNE DATABASE FOR MOLECULAR SPECTROSCOPY, CDMS",Abstract content,poster,cp70
p2436,f1af714b92372c8e606485a3982eab2f16772ad8,c33,International Conference on Agile Software Development,The MUG facial expression database,This paper presents a new extended collection of posed and induced facial expression image sequences. All sequences were captured in a controlled laboratory environment with high resolution and no occlusions. The collection consists of two parts: The first part depicts eighty six subjects performing the six basic expressions according to the “emotion prototypes” as defined in the Investigator's Guide in the FACS manual. The second part contains the same subjects recorded while they were watching an emotion inducing video. Most of the database recordings are available to the scientific community. Beyond the emotion related annotation the database contains also manual and automatic annotation of 80 facial landmark points for a significant number of frames. The database contains sufficient material for the development and the statistical evaluation of facial expression recognition systems using posed and induced expressions.,poster,cp33
p2437,a5375b684c8e6640246df2eaec5f59b2ef94242b,c29,International Conference on Software Engineering,CDD: a curated Entrez database of conserved domain alignments,"The Conserved Domain Database (CDD) is now indexed as a separate database within the Entrez system and linked to other Entrez databases such as MEDLINE(R). This allows users to search for domain types by name, for example, or to view the domain architecture of any protein in Entrez's sequence database. CDD can be accessed on the WorldWideWeb at http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=cdd. Users may also employ the CD-Search service to identify conserved domains in new sequences, at http://www.ncbi.nlm.nih.gov/Structure/cdd/wrpsb.cgi. CD-Search results, and pre-computed links from Entrez's protein database, are calculated using the RPS-BLAST algorithm and Position Specific Score Matrices (PSSMs) derived from CDD alignments. CD-Searches are also run by default for protein-protein queries submitted to BLAST(R) at http://www.ncbi.nlm.nih.gov/BLAST. CDD mirrors the publicly available domain alignment collections SMART and PFAM, and now also contains alignment models curated at NCBI. Structure information is used to identify the core substructure likely to be present in all family members, and to produce sequence alignments consistent with structure conservation. This alignment model allows NCBI curators to annotate 'columns' corresponding to functional sites conserved among family members.",poster,cp29
p2438,794c048acd3d2d35e3248161729fcf142b4966c6,c54,International Workshop on Agent-Oriented Software Engineering,Phospho.ELM: a database of phosphorylation sites—update 2008,"Phospho.ELM is a manually curated database of eukaryotic phosphorylation sites. The resource includes data collected from published literature as well as high-throughput data sets. The current release of Phospho.ELM (version 7.0, July 2007) contains 4078 phospho-protein sequences covering 12 025 phospho-serine, 2362 phospho-threonine and 2083 phospho-tyrosine sites. The entries provide information about the phosphorylated proteins and the exact position of known phosphorylated instances, the kinases responsible for the modification (where known) and links to bibliographic references. The database entries have hyperlinks to easily access further information from UniProt, PubMed, SMART, ELM, MSD as well as links to the protein interaction databases MINT and STRING. A new BLAST search tool, complementary to retrieval by keyword and UniProt accession number, allows users to submit a protein query (by sequence or UniProt accession) to search against the curated data set of phosphorylated peptides. Phospho.ELM is available on line at: http://phospho.elm.eu.org",poster,cp54
p2439,48fa4530c0eddf525b273e222753c978606243f7,c79,International Conference on Medical Image Computing and Computer-Assisted Intervention,APD: the Antimicrobial Peptide Database,"An antimicrobial peptide database (APD) has been established based on an extensive literature search. It contains detailed information for 525 peptides (498 antibacterial, 155 antifungal, 28 antiviral and 18 antitumor). APD provides interactive interfaces for peptide query, prediction and design. It also provides statistical data for a select group of or all the peptides in the database. Peptide information can be searched using keywords such as peptide name, ID, length, net charge, hydrophobic percentage, key residue, unique sequence motif, structure and activity. APD is a useful tool for studying the structure-function relationship of antimicrobial peptides. The database can be accessed via a web-based browser at the URL: http://aps.unmc.edu/AP/main.html.",poster,cp79
p2440,b4ea6e57966ffdab58ec410e085acc1232064303,c89,Conference on Uncertainty in Artificial Intelligence,"The PANTHER database of protein families, subfamilies, functions and pathways","PANTHER is a large collection of protein families that have been subdivided into functionally related subfamilies, using human expertise. These subfamilies model the divergence of specific functions within protein families, allowing more accurate association with function (ontology terms and pathways), as well as inference of amino acids important for functional specificity. Hidden Markov models (HMMs) are built for each family and subfamily for classifying additional protein sequences. The latest version, 5.0, contains 6683 protein families, divided into 31 705 subfamilies, covering ∼90% of mammalian protein-coding genes. PANTHER 5.0 includes a number of significant improvements over previous versions, most notably (i) representation of pathways (primarily signaling pathways) and association with subfamilies and individual protein sequences; (ii) an improved methodology for defining the PANTHER families and subfamilies, and for building the HMMs; (iii) resources for scoring sequences against PANTHER HMMs both over the web and locally; and (iv) a number of new web resources to facilitate analysis of large gene lists, including data generated from high-throughput expression experiments. Efforts are underway to add PANTHER to the InterPro suite of databases, and to make PANTHER consistent with the PIRSF database. PANTHER is now publicly available without restriction at http://panther.appliedbiosystems.com.",poster,cp89
p2441,67f8831944ecc502ce74c761bd7ae0d929b5e2f8,c73,Workshop on Algorithms in Bioinformatics,An Electronic Lexical Database,Abstract content,poster,cp73
p2442,8bbb5e29bc76675ae3b73185c17e9077742742ee,j40,Social Science Research Network,A Historical Public Debt Database,"This paper describes the compilation of the first truly comprehensive database on gross government debt-to-GDP ratios, covering nearly the entire IMF membership (174 countries) and spanning an exceptionally long time period. The database was constructed by bringing together a number of other datasets and information from original sources. For the most recent years, the data are linked to the IMF World Economic Outlook (WEO) database to facilitate regular updates. The paper discusses the evolution of debt-to-GDP ratios across country groups for several decades, episodes of debt spikes and reversals, and a pattern of negative correlation between debt and growth.",fullPaper,jv40
p2443,978a4276e0874a590d34aef84e6238ce21f0539e,c68,International Workshop on Software Engineering for Large-Scale Multi-Agent Systems,Inparanoid: a comprehensive database of eukaryotic orthologs,"The Inparanoid eukaryotic ortholog database (http://inparanoid.cgb.ki.se/) is a collection of pairwise ortholog groups between 17 whole genomes; Anopheles gambiae, Caenorhabditis briggsae, Caenorhabditis elegans, Drosophila melanogaster, Danio rerio, Takifugu rubripes, Gallus gallus, Homo sapiens, Mus musculus, Pan troglodytes, Rattus norvegicus, Oryza sativa, Plasmodium falciparum, Arabidopsis thaliana, Escherichia coli, Saccharomyces cerevisiae and Schizosaccharomyces pombe. Complete proteomes for these genomes were derived from Ensembl and UniProt and compared pairwise using Blast, followed by a clustering step using the Inparanoid program. An Inparanoid cluster is seeded by a reciprocally best-matching ortholog pair, around which inparalogs (should they exist) are gathered independently, while outparalogs are excluded. The ortholog clusters can be searched on the website using Ensembl gene/protein or UniProt identifiers, annotation text or by Blast alignment against our protein datasets. The entire dataset can be downloaded, as can the Inparanoid program itself.",poster,cp68
p2444,86dc58305d4cb2766eb2746bbe578e9a58a6f238,c71,IEEE International Conference on Information Reuse and Integration,Carbohydrate-active enzymes : an integrated database approach,Abstract content,poster,cp71
p2445,523a87607f06f7ed56a0506bdb4671f76244264a,c30,IEEE Aerospace Conference,Introducing the Global Terrorism Database,"Compared to most types of criminal violence, terrorism poses special data collection challenges. In response, there has been growing interest in open source terrorist event data bases. One of the major problems with these data bases in the past is that they have been limited to international events—those involving a national or group of nationals from one country attacking targets physically located in another country. Past research shows that domestic incidents greatly outnumber international incidents. In this paper we describe a previously unavailable open source data base that includes some 70,000 domestic and international incidents since 1970. We began the Global Terrorism Database (GTD) by computerizing data originally collected by the Pinkerton Global Intelligence Service (PGIS). Following computerization, our research team has been working for the past two years to validate and extend the data to real time. In this paper, we describe our data collection efforts, the strengths and weaknesses of open source data in general and the GTD in particular, and provide descriptive statistics on the contents of this new resource.",poster,cp30
p2446,f993a6bfb7f998c7b68547772ed73145db4dc0a8,c81,IEEE Annual Symposium on Foundations of Computer Science,The Object Database Standard: ODMG-93,Abstract content,poster,cp81
p2447,b247bb08f116a181faa34e0e5806935a08b59dc5,c22,International Conference on Data Technologies and Applications,MINT: the Molecular INTeraction database,"Protein interaction databases represent unique tools to store, in a computer readable form, the protein interaction information disseminated in the scientific literature. Well organized and easily accessible databases permit the easy retrieval and analysis of large interaction data sets. Here we present MINT, a database (http://cbm.bio.uniroma2.it/mint/index.html) designed to store data on functional interactions between proteins. Beyond cataloguing binary complexes, MINT was conceived to store other types of functional interactions, including enzymatic modifications of one of the partners. Release 1.0 of MINT focuses on experimentally verified protein-protein interactions. Both direct and indirect relationships are considered. Furthermore, MINT aims at being exhaustive in the description of the interaction and, whenever available, information about kinetic and binding constants and about the domains participating in the interaction is included in the entry. MINT consists of entries extracted from the scientific literature by expert curators assisted by 'MINT Assistant', a software that targets abstracts containing interaction information and presents them to the curator in a user-friendly format. The interaction data can be easily extracted and viewed graphically through 'MINT Viewer'. Presently MINT contains 4568 interactions, 782 of which are indirect or genetic interactions.",poster,cp22
p2448,6ecb2f55ae787363712adf6e7ba6c2812d3f0b32,c93,Human Language Technology - The Baltic Perspectiv,miRBase: the microRNA sequence database.,Abstract content,poster,cp93
p2449,137832dd10d669a300b7751e0ed3e3b91172372a,c44,International Workshop on Green and Sustainable Software,Human protein reference database—2006 update,"Human Protein Reference Database (HPRD) () was developed to serve as a comprehensive collection of protein features, post-translational modifications (PTMs) and protein–protein interactions. Since the original report, this database has increased to >20 000 proteins entries and has become the largest database for literature-derived protein–protein interactions (>30 000) and PTMs (>8000) for human proteins. We have also introduced several new features in HPRD including: (i) protein isoforms, (ii) enhanced search options, (iii) linking of pathway annotations and (iv) integration of a novel browser, GenProt Viewer (), developed by us that allows integration of genomic and proteomic information. With the continued support and active participation by the biomedical community, we expect HPRD to become a unique source of curated information for the human proteome and spur biomedical discoveries based on integration of genomic, transcriptomic and proteomic data.",poster,cp44
p2450,dea76435761e7fa1fea68aa5224c3ed0ca1e0e4e,c39,International Conference on Global Software Engineering,Third millenium ideal gas and condensed phase thermochemical database for combustion (with update from active thermochemical tables).,"The thermochemical database of species involved in combustion processes is and has been available for free use for over 25 years. It was first published in print in 1984, approximately 8 years after it was first assembled, and contained 215 species at the time. This is the 7th printed edition and most likely will be the last one in print in the present format, which involves substantial manual labor. The database currently contains more than 1300 species, specifically organic molecules and radicals, but also inorganic species connected to combustion and air pollution. Since 1991 this database is freely available on the internet, at the Technion-IIT ftp server, and it is continuously expanded and corrected. The database is mirrored daily at an official mirror site, and at random at about a dozen unofficial mirror and 'finger' sites. The present edition contains numerous corrections and many recalculations of data of provisory type by the G3//B3LYP method, a high-accuracy composite ab initio calculation. About 300 species are newly calculated and are not yet published elsewhere. In anticipation of the full coupling, which is under development, the database started incorporating the available (as yet unpublished) values from Active Thermochemical Tables. The electronic version nowmore » also contains an XML file of the main database to allow transfer to other formats and ease finding specific information of interest. The database is used by scientists, educators, engineers and students at all levels, dealing primarily with combustion and air pollution, jet engines, rocket propulsion, fireworks, but also by researchers involved in upper atmosphere kinetics, astrophysics, abrasion metallurgy, etc. This introductory article contains explanations of the database and the means to use it, its sources, ways of calculation, and assessments of the accuracy of data.« less",poster,cp39
p2451,1337f14678d80f22df094a3a9ad09a695d5f86ee,c16,Knowledge Discovery and Data Mining,THE EXTRAGALACTIC DISTANCE DATABASE,"A database can be accessed on the Web at http://edd.ifa.hawaii.edu that was developed to promote access to information related to galaxy distances. The database has three functional components. First, tables from many literature sources have been gathered and enhanced with links through a distinct galaxy naming convention. Second, comparisons of results both at the levels of parameters and of techniques have begun and are continuing, leading to increasing homogeneity and consistency of distance measurements. Third, new material is presented arising from ongoing observational programs at the University of Hawaii 2.2 m telescope, radio telescopes at Green Bank, Arecibo, and Parkes and with the Hubble Space Telescope. This new observational material is made available in tandem with related material drawn from archives and passed through common analysis pipelines.",poster,cp16
p2452,f836da820f53f5bbc890647ecbf00e1031f200c7,j235,Proteomics,The International Protein Index: An integrated database for proteomics experiments,"Despite the complete determination of the genome sequence of several higher eukaryotes, their proteomes remain relatively poorly defined. Information about proteins identified by different experimental and computational methods is stored in different databases, meaning that no single resource offers full coverage of known and predicted proteins. IPI (the International Protein Index) has been developed to address these issues and offers complete nonredundant data sets representing the human, mouse and rat proteomes, built from the Swiss‐Prot, TrEMBL, Ensembl and RefSeq databases.",fullPaper,jv235
p2453,58efda5a28e5791adfde9ef6e330caf7b89349c6,c54,International Workshop on Agent-Oriented Software Engineering,Providing database as a service,"We explore a novel paradigm for data management in which a third party service provider hosts ""database as a service"", providing its customers with seamless mechanisms to create, store, and access their databases at the host site. Such a model alleviates the need for organizations to purchase expensive hardware and software, deal with software upgrades, and hire professionals for administrative and maintenance tasks which are taken over by the service provider. We have developed and deployed a database service on the Internet, called NetDB2, which is in constant use. In a sense, a data management model supported by NetDB2 provides an effective mechanism for organizations to purchase data management as a service, thereby freeing them to concentrate on their core businesses. Among the primary challenges introduced by ""database as a service"" are the additional overhead of remote access to data, an infrastructure to guarantee data privacy, and user interface design for such a service. These issues are investigated. We identify data privacy as a particularly vital problem and propose alternative solutions based on data encryption. The paper is meant as a challenge for the database community to explore a rich set of research issues that arise in developing such a service.",poster,cp54
p2454,dacc0018d0a0c45d93599751e53c91f88fcd45b8,c75,International Conference on Machine Learning,"The InterPro Database, 2003 brings increased coverage and new features","InterPro, an integrated documentation resource of protein families, domains and functional sites, was created in 1999 as a means of amalgamating the major protein signature databases into one comprehensive resource. PROSITE, Pfam, PRINTS, ProDom, SMART and TIGRFAMs have been manually integrated and curated and are available in InterPro for text- and sequence-based searching. The results are provided in a single format that rationalises the results that would be obtained by searching the member databases individually. The latest release of InterPro contains 5629 entries describing 4280 families, 1239 domains, 95 repeats and 15 post-translational modifications. Currently, the combined signatures in InterPro cover more than 74% of all proteins in SWISS-PROT and TrEMBL, an increase of nearly 15% since the inception of InterPro. New features of the database include improved searching capabilities and enhanced graphical user interfaces for visualisation of the data. The database is available via a webserver (http://www.ebi.ac.uk/interpro) and anonymous FTP (ftp://ftp.ebi.ac.uk/pub/databases/interpro).",poster,cp75
p2455,c2eb8cbfa71ead3e30d08fa5f2712a51950c6a40,c17,International Conference on Enterprise Information Systems,"Principles of database and knowledge-base systems, Vol. I",Abstract content,poster,cp17
p2456,88b98d7b20ede342fe471b2889ace70d082e4db7,c16,Knowledge Discovery and Data Mining,Query by humming: musical information retrieval in an audio database,"The emergence of audio and video data types in databases will require new information retrieval methods adapted to the specific characteristics and needs of these data types. An effective and natural way of querying a musical audio database is by humming the tune of a song. In this paper, a system for querying an audio database by humming is described along with a scheme for representing the melodic information in a song as relative pitch changes. Relevant difficulties involved with tracking pitch are enumerated, along with the approach we followed, and the performance results of system indicating its effectiveness are presented.",poster,cp16
p2457,b71ac5caa3a4335c311122cbacada6b17a199060,j275,Methods in Enzymology,Saccharomyces Genome Database.,Abstract content,fullPaper,jv275
p2458,e7ab23d011e5183db78cfea48e303210f6e57e2e,c53,International Conference on Software Engineering and Knowledge Engineering,The serializability of concurrent database updates,"A sequence of interleaved user transactions in a database system may not be ser:ahzable, t e, equivalent to some sequential execution of the individual transactions Using a simple transaction model, it ~s shown that recognizing the transaction histories that are serlahzable is an NP-complete problem. Several efficiently recognizable subclasses of the class of senahzable histories are therefore introduced; most of these subclasses correspond to senahzabdity principles existing in the hterature and used in practice Two new principles that subsume all previously known ones are also proposed Necessary and sufficient conditions are given for a class of histories to be the output of an efficient history scheduler, these conditions imply that there can be no efficient scheduler that outputs all of senahzable histories, and also that all subclasses of senalizable histories studied above have an efficient scheduler Finally, it is shown how these results can be extended to far more general transaction models, to transactions with partly interpreted functions, and to distributed database systems",poster,cp53
p2459,78b20577738126cb80ed80ad9d1ef96ed813a14a,c111,International Society for Music Information Retrieval Conference,System R: relational approach to database management,"System R is a database management system which provides a high level relational data interface. The systems provides a high level of data independence by isolating the end user as much as possible from underlying storage structures. The system permits definition of a variety of relational views on common underlying data. Data control features are provided, including authorization, integrity assertions, triggered transactions, a logging and recovery subsystem, and facilities for maintaining data consistency in a shared-update environment.
This paper contains a description of the overall architecture and design of the system. At the present time the system is being implemented and the design evaluated. We emphasize that System R is a vehicle for research in database architecture, and is not planned as a product.",poster,cp111
p2460,a8c72e6f18e6733c5981138ee7eb06632ba75de1,c18,Conference on Innovative Data Systems Research,Online Predicted Human Interaction Database,"MOTIVATION
High-throughput experiments are being performed at an ever-increasing rate to systematically elucidate protein-protein interaction (PPI) networks for model organisms, while the complexities of higher eukaryotes have prevented these experiments for humans.


RESULTS
The Online Predicted Human Interaction Database (OPHID) is a web-based database of predicted interactions between human proteins. It combines the literature-derived human PPI from BIND, HPRD and MINT, with predictions made from Saccharomyces cerevisiae, Caenorhabditis elegans, Drosophila melanogaster and Mus musculus. The 23,889 predicted interactions currently listed in OPHID are evaluated using protein domains, gene co-expression and Gene Ontology terms. OPHID can be queried using single or multiple IDs and results can be visualized using our custom graph visualization program.


AVAILABILITY
Freely available to academic users at http://ophid.utoronto.ca, both in tab-delimited and PSI-MI formats. Commercial users, please contact I.J.


CONTACT
juris@ai.utoronto.ca


SUPPLEMENTARY INFORMATION
http://ophid.utoronto.ca/supplInfo.pdf.",poster,cp18
p2461,b069e125442995787119db3bfa71dff5d965f3aa,c9,Pacific Symposium on Biocomputing,MCYT baseline corpus: a bimodal biometric database,"The current need for large multimodal databases to evaluate automatic biometric recognition systems has motivated the development of the MCYT bimodal database. The main purpose has been to consider a large scale population, with statistical significance, in a real multimodal procedure, and including several sources of variability that can be found in real environments. The acquisition process, contents and availability of the single-session baseline corpus are fully described. Some experiments showing consistency of data through the different acquisition sites and assessing data quality are also presented.",poster,cp9
p2462,d4fe4d4f62b8f14a0475bc8f8028a0d3565650f9,c3,Frontiers in Education Conference,The Harmonized World Soil Database,"For more than 30 years the FAO/Unesco Soil map of the World has been the only harmonized source of global soil information. Recent updates and release of new soil information in all regions of the globe was an incentive to tackle the harmonization and integration of the new soil data. The task was undertaken by a consortium of institutes and organizations and resulted in a product with 30 arc second resolution that includes for each soil unit estimates for fifteen top- and subsoil properties. The data come with a viewer, are GIS compatible and are freely available on-line.",poster,cp3
p2463,ef47742e72bd64fb1ae5359cd6d5dd6dfad34dc8,c100,ACM SIGMOD Conference,Implementation techniques for main memory database systems,"With the availability of very large, relatively inexpensive main memories, it is becoming possible keep large databases resident in main memory In this paper we consider the changes necessary to permit a relational database system to take advantage of large amounts of main memory We evaluate AVL vs B+-tree access methods for main memory databases, hash-based query processing strategies vs sort-merge, and study recovery issues when most or all of the database fits in main memory As expected, B+-trees are the preferred storage mechanism unless more than 80--90% of the database fits in main memory A somewhat surprising result is that hash based query processing strategies are advantageous for large memory situations",fullPaper,cp100
p2464,2033531aeaf7d0da158cdaacae9b208407bd4a1c,c5,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,AAindex: Amino Acid Index Database,"AAindex is a database of numerical indices representing various physicochemical and biochemical properties of amino acids and pairs of amino acids. It consists of two sections: AAindex1 for the amino acid index of 20 numerical values and AAindex2 for the amino acid mutation matrix of 210 numerical values. Each entry of either AAindex1 or AAindex2 consists of the definition, the reference information, a list of related entries in terms of the correlation coefficient, and the actual data. The database may be accessed through the DBGET/LinkDB system at GenomeNet (http://www.genome.ad. jp/dbget/) or may be downloaded by anonymous FTP (ftp://ftp.genome. ad.jp/db/genomenet/aaindex/).",poster,cp5
p2465,c7a77164a8ede4f536c2779f32aeb6bf98eff766,j381,Biophysical Journal,The nucleic acid database. A comprehensive relational database of three-dimensional structures of nucleic acids.,Abstract content,fullPaper,jv381
p2466,2a92c98a943ae21360d52730f1f2117c7edb2ecb,c83,International Conference on Computer Graphics and Interactive Techniques,The Regulation and Supervision of Banks around the World: A New Database,"International consultants on bank regulation, and supervision for developing countries, often base their advice on how their home country does things, for lack of information on practice in other countries. Recommendations for reform have tended to be shaped by bias rather than facts. To better inform advice about bank regulation, and supervision, and to lower the marginal cost of empirical research, the authors present, and discuss a new, and comprehensive database on the regulation, and supervision of banks in a hundred and seven countries. The data, based on surveys sent to national bank regulatory, supervisory authorities, are now available to researchers, and policymakers around the world. The data cover such aspects of banking as entry requirements, ownership restrictions, capital requirements, activity restrictions, external auditing requirements, characteristics of deposit insurance schemes, loan classification and provisioning requirements, accounting and disclosure requirements, troubled bank resolution actions, and (uniquely) the quality of supervisory personnel, and their actions. The database permits users to learn how banks are currently regulated, and supervised, and about bank structures, and deposit insurance schemes, for a broad cross-section of countries. In addition to describing the data, the authors show how variables ay be grouped, and aggregated. They also show some simple correlations among selected variables. In a comparison paper (""Bank regulation and supervision: What works best"") studying the relationship between differences in bank regulation and supervision, and bank performance and stability, they conclude that: 1) Countries with policies that promote private monitoring of banks, have better bank performance, and more stability. Countries with more generous deposit insurance schemes tend to have poorer bank performance, and more bank fragility. 2) Diversification of income streams, and loan portfolios - by not restricting bank activities - also tends to improve performance, and stability. (This works best when an active securities market exists). Countries in which banks are encouraged to diversify their portfolios, domestically and internationally, suffer fewer crisis.",poster,cp83
p2467,e9a1699735aff36cdd1fa385165426dd18b0d9ec,c59,British Computer Society Conference on Human-Computer Interaction,New developments in the InterPro database,"InterPro is an integrated resource for protein families, domains and functional sites, which integrates the following protein signature databases: PROSITE, PRINTS, ProDom, Pfam, SMART, TIGRFAMs, PIRSF, SUPERFAMILY, Gene3D and PANTHER. The latter two new member databases have been integrated since the last publication in this journal. There have been several new developments in InterPro, including an additional reading field, new database links, extensions to the web interface and additional match XML files. InterPro has always provided matches to UniProtKB proteins on the website and in the match XML file on the FTP site. Additional matches to proteins in UniParc (UniProt archive) are now available for download in the new match XML files only. The latest InterPro release (13.0) contains more than 13 000 entries, covering over 78% of all proteins in UniProtKB. The database is available for text- and sequence-based searches via a webserver (), and for download by anonymous FTP (). The InterProScan search tool is now also available via a web service at .",poster,cp59
p2468,28aa1b3725bc0cbcb29233d30c5effff93195d4e,j224,Plant Physiology,"ARAMEMNON, a Novel Database for Arabidopsis Integral Membrane Proteins1","A specialized database (DB) for Arabidopsis membrane proteins, ARAMEMNON, was designed that facilitates the interpretation of gene and protein sequence data by integrating features that are presently only available from individual sources. Using several publicly available prediction programs, putative integral membrane proteins were identified among the approximately 25,500 proteins in the Arabidopsis genome DBs. By averaging the predictions from seven programs, approximately 6,500 proteins were classified as transmembrane (TM) candidate proteins. Some 1,800 of these contain at least four TM spans and are possibly linked to transport functions. The ARAMEMNON DB enables direct comparison of the predictions of seven different TM span computation programs and the predictions of subcellular localization by eight signal peptide recognition programs. A special function displays the proteins related to the query and dynamically generates a protein family structure. As a first set of proteins from other organisms, all of the approximately 700 putative membrane proteins were extracted from the genome of the cyanobacterium Synechocystis sp. and incorporated in the ARAMEMNON DB. The ARAMEMNON DB is accessible at the URL http://aramemnon.botanik.uni-koeln.de.",fullPaper,jv224
p2469,fa3c3fb3db6d54105c7990b6fd3ef41f3aff439d,c75,International Conference on Machine Learning,Human Immunodeficiency Virus Reverse Transcriptase and Protease Sequence Database,"The HIV RT and Protease Sequence Database is an on-line relational database that catalogues evolutionary and drug-related human immunodeficiency virus reverse transcriptase (RT) and protease sequence variation (http://hivdb.stanford.edu). The database contains a compilation of nearly all published HIV RT and protease sequences including International Collaboration database submissions (e.g., GenBank) and sequences published in journal articles. Sequences are linked to data about the source of the sequence sample and the anti-HIV drug treatment history of the individual from whom the isolate was obtained. The database is curated and sequences are annotated with data from 180 literature references. Users can retrieve additional data and view alignments of sequences sets meeting specific criteria (e.g., treatment history, subtype, presence of a particular mutation).",poster,cp75
p2470,89e3fae32bf72b61834fc2ae60b1f8508e714e38,c61,Jahrestagung der Gesellschaft für Informatik,World Ocean Database,"The U.S. National Oceanic and Atmospheric Administration's (NOAA) World Ocean Database 2009, released in November as an update to the 2005 version, provides about 9.1 million temperature profiles and 3.5 million salinity reports, with some information dating as far back as 1800. The updated database includes scientific information about the oceans that can be sorted in various ways, including geographically or by year. 
 
“There is now more data about the global oceans than ever before,” according to Sydney Levitus, director of the World Data Center for Oceanography, part of NOAA's National Oceanographic Data Center. “Previous databases have shown the world ocean has warmed during the last 53 years, and it's crucial we have reliable, accurate monitoring of our oceans into the future,” he said. The database is a part of the Integrated Ocean Observing System and the Global Earth Observation System of Systems.",poster,cp61
p2471,a6089c7eca1d77dc199e462763ab13f99f85c663,c114,IEEE International Conference on Robotics and Automation,Global wood density database,Abstract content,poster,cp114
p2472,0b6fb2fceea248bcf8ad8c05c7d56abda763e2db,c63,IEEE International Software Metrics Symposium,PMRD: plant microRNA database,"MicroRNAs (miRNA) are ∼21 nucleotide-long non-coding small RNAs, which function as post-transcriptional regulators in eukaryotes. miRNAs play essential roles in regulating plant growth and development. In recent years, research into the mechanism and consequences of miRNA action has made great progress. With whole genome sequence available in such plants as Arabidopsis thaliana, Oryza sativa, Populus trichocarpa, Glycine max, etc., it is desirable to develop a plant miRNA database through the integration of large amounts of information about publicly deposited miRNA data. The plant miRNA database (PMRD) integrates available plant miRNA data deposited in public databases, gleaned from the recent literature, and data generated in-house. This database contains sequence information, secondary structure, target genes, expression profiles and a genome browser. In total, there are 8433 miRNAs collected from 121 plant species in PMRD, including model plants and major crops such as Arabidopsis, rice, wheat, soybean, maize, sorghum, barley, etc. For Arabidopsis, rice, poplar, soybean, cotton, medicago and maize, we included the possible target genes for each miRNA with a predicted interaction site in the database. Furthermore, we provided miRNA expression profiles in the PMRD, including our local rice oxidative stress related microarray data (LC Sciences miRPlants_10.1) and the recently published microarray data for poplar, Arabidopsis, tomato, maize and rice. The PMRD database was constructed by open source technology utilizing a user-friendly web interface, and multiple search tools. The PMRD is freely available at http://bioinformatics.cau.edu.cn/PMRD. We expect PMRD to be a useful tool for scientists in the miRNA field in order to study the function of miRNAs and their target genes, especially in model plants and major crops.",poster,cp63
p2473,174148018456e391ee06adc21ea0535c825e8df3,c46,Brazilian Symposium on Software Engineering,A common database approach for OLTP and OLAP using an in-memory column database,"When SQL and the relational data model were introduced 25 years ago as a general data management concept, enterprise software migrated quickly to this new technology. It is fair to say that SQL and the various implementations of RDBMSs became the backbone of enterprise systems. In those days. we believed that business planning, transaction processing and analytics should reside in one single system. Despite the incredible improvements in computer hardware, high-speed networks, display devices and the associated software, speed and flexibility remained an issue. The nature of RDBMSs, being organized along rows, prohibited us from providing instant analytical insight and finally led to the introduction of so-called data warehouses. This paper will question some of the fundamentals of the OLAP and OLTP separation. Based on the analysis of real customer environments and experience in some prototype implementations, a new proposal for an enterprise data management concept will be presented. In our proposal, the participants in enterprise applications, customers, orders, accounting documents, products, employees etc. will be modeled as objects and also stored and maintained as such. Despite that, the vast majority of business functions will operate on an in memory representation of their objects. Using the relational algebra and a column-based organization of data storage will allow us to revolutionize transactional applications while providing an optimal platform for analytical data processing. The unification of OLTP and OLAP workloads on a shared architecture and the reintegration of planning activities promise significant gains in application development while simplifying enterprise systems drastically. The latest trends in computer technology -- e.g. blade architecture, multiple CPUs per blade with multiple cores per CPU allow for a significant parallelization of application processes. The organization of data in columns supports the parallel use of cores for filtering and aggregation. Elements of application logic can be implemented as highly efficient stored procedures operating on columns. The vast increase in main memory combined with improvements in L1--, L2--, L3--caching, together with the high data compression rate column storage will allow us to support substantial data volumes on one single blade. Distributing data across multiple blades using a shared nothing approach provides further scalability.",poster,cp46
p2474,5e9c3103afaeed5adb209c2b03cf3622f13a3323,j382,Applied Optics,The HITRAN database: 1986 edition.,"A description and summary of the latest edition of the AFGL HITRAN molecular absorption parameters database are presented. This new database combines the information for the seven principal atmospheric absorbers and twenty-one additional molecular species previously contained on the AFGL atmospheric absorption line parameter compilation and on the trace gas compilation. In addition to updating the parameters on earlier editions of the compilation, new parameters have been added to this edition such as the self-broadened halfwidth, the temperature dependence of the air-broadened halfwidth, and the transition probability. The database contains 348043 entries between 0 and 17,900 cm(-1). A FORTRAN program is now furnished to allow rapid access to the molecular transitions and for the creation of customized output. A separate file of molecular cross sections of eleven heavy molecular species, applicable for qualitative simulation of transmission and emission in the atmosphere, has also been provided.",fullPaper,jv382
p2475,c17ee327e563536f8adaf214eb6d3bde33b73dd6,j79,Computer,Chabot: Retrieval from a Relational Database of Images,"Selecting from a large, expanding collection of images requires carefully chosen search criteria. We present an approach that integrates a relational database retrieval system with a color analysis technique. The Chabot project was initiated at our university to study storage and retrieval of a vast collection of digitized images. These images are from the State of California Department of Water Resources. The goal was to integrate a relational database retrieval system with content analysis techniques that would give our querying system a better method for handling images. Our simple color analysis method, if used in conjunction with other search criteria, improves our ability to retrieve images efficiently. The best result is obtained when text-based search criteria are combined with content-based criteria and when a coarse granularity is used for content analysis. >",fullPaper,jv79
p2476,de31aa8e914c60189a425e174af223967e2722cc,c111,International Society for Music Information Retrieval Conference,"RWC Music Database: Popular, Classical and Jazz Music Databases","paper describes the design policy and specifications of the RWC Music Database , a music database (DB) that is available to researchers for common use and research purposes. Various com- monly available DBs have been built in other research fields and have made a significant contribution to the research in those fields. The field of musical information processing, however, has lacked a commonly available music DB. We therefore built the RWC Mu- sic Database which contains four original DBs: the Popular Music Database (100 pieces), Royalty-Free Music Database(15 pieces), Classical Music Database(50 pieces), and Jazz Music Database (50 pieces). Each consists of originally-recorded music compact discs, standard MIDI files, and text files of lyrics. These DBs are now available in Japan at a cost equal to only duplication, shipping, and handling charges (virtually for free), and we plan to make them available outside Japan. We hope that our DB will encourage further advances in musical information processing research.",fullPaper,cp111
p2477,5160fb34a6719bbf8d60743d0d27db0ed5df3d2a,c112,Very Large Data Bases Conference,Foundations of Preferences in Database Systems,Abstract content,fullPaper,cp112
p2478,a5c4c2b5719eff7160334259b018809dc9c4ab4b,j97,Science,Exhaustive matching of the entire protein sequence database.,"The entire protein sequence database has been exhaustively matched. Definitive mutation matrices and models for scoring gaps were obtained from the matching and used to organize the sequence database as sets of evolutionarily connected components. The methods developed are general and can be used to manage sequence data generated by major genome sequencing projects. The alignments made possible by the exhaustive matching are the starting point for successful de novo prediction of the folded structures of proteins, for reconstructing sequences of ancient proteins and metabolisms in ancient organisms, and for obtaining new perspectives in structural biochemistry.",fullPaper,jv97
p2479,64acb315b6129061c62bfabef2ac06d1a6fff95b,c46,Brazilian Symposium on Software Engineering,Human protein reference database as a discovery resource for proteomics,"The rapid pace at which genomic and proteomic data is being generated necessitates the development of tools and resources for managing data that allow integration of information from disparate sources. The Human Protein Reference Database (http://www.hprd.org) is a web-based resource based on open source technologies for protein information about several aspects of human proteins including protein-protein interactions, post-translational modifications, enzyme-substrate relationships and disease associations. This information was derived manually by a critical reading of the published literature by expert biologists and through bioinformatics analyses of the protein sequence. This database will assist in biomedical discoveries by serving as a resource of genomic and proteomic information and providing an integrated view of sequence, structure, function and protein networks in health and disease.",poster,cp46
p2480,be9124db35e5f451f508e095fdc25727c067a9ef,c113,International Conference on Image Analysis and Processing,UBIRIS: A Noisy Iris Image Database,Abstract content,fullPaper,cp113
p2481,cbd499a34475b9fb493303b4ea3ab391e3aba7f2,c107,British Machine Vision Conference,The Transporter Classification Database: recent advances,"The Transporter Classification Database (TCDB), freely accessible at http://www.tcdb.org, is a relational database containing sequence, structural, functional and evolutionary information about transport systems from a variety of living organisms, based on the International Union of Biochemistry and Molecular Biology-approved transporter classification (TC) system. It is a curated repository for factual information compiled largely from published references. It uses a functional/phylogenetic system of classification, and currently encompasses about 5000 representative transporters and putative transporters in more than 500 families. We here describe novel software designed to support and extend the usefulness of TCDB. Our recent efforts render it more user friendly, incorporate machine learning to input novel data in a semiautomatic fashion, and allow analyses that are more accurate and less time consuming. The availability of these tools has resulted in recognition of distant phylogenetic relationships and tremendous expansion of the information available to TCDB users.",poster,cp107
p2482,825ca73a2be2ccdf3fdb8a0dd19e1fbf65871547,c41,Software Product Lines Conference,"PPDB, the Plant Proteomics Database at Cornell","The Plant Proteomics Database (PPDB; http://ppdb.tc.cornell.edu), launched in 2004, provides an integrated resource for experimentally identified proteins in Arabidopsis and maize (Zea mays). Internal BLAST alignments link maize and Arabidopsis information. Experimental identification is based on in-house mass spectrometry (MS) of cell type-specific proteomes (maize), or specific subcellular proteomes (e.g. chloroplasts, thylakoids, nucleoids) and total leaf proteome samples (maize and Arabidopsis). So far more than 5000 accessions both in maize and Arabidopsis have been identified. In addition, more than 80 published Arabidopsis proteome datasets from subcellular compartments or organs are stored in PPDB and linked to each locus. Using MS-derived information and literature, more than 1500 Arabidopsis proteins have a manually assigned subcellular location, with a strong emphasis on plastid proteins. Additional new features of PPDB include searchable posttranslational modifications and searchable experimental proteotypic peptides and spectral count information for each identified accession based on in-house experiments. Various search methods are provided to extract more than 40 data types for each accession and to extract accessions for different functional categories or curated subcellular localizations. Protein report pages for each accession provide comprehensive overviews, including predicted protein properties, with hyperlinks to the most relevant databases.",poster,cp41
p2483,3a1cb22aff87e990740bce7d4b60ce069651fe6c,c89,Conference on Uncertainty in Artificial Intelligence,MyLifeBits: a personal database for everything,"Developing a platform for recording, storing, and accessing a personal lifetime archive.",poster,cp89
p2484,a0d251f893e043175d0a6d0e12e6e166ff523255,c40,IEEE International Conference on Software Maintenance and Evolution,Documentation Mocap Database HDM05,"Preface In the past two decades, motion capture (mocap) systems have been developed that allow to track and record human motions at high spatial and temporal resolutions. The resulting motion capture data is used to analyze human motions in fields such as sports sciences and biometrics (person identification), and to synthesize realistic motion sequences in data-driven computer animation. Such applications require efficient methods and tools for the automatic analysis, synthesis and classification of motion capture data, which constitutes an active research area with many yet unsolved problems. Even though there is a rapidly growing corpus of motion capture data, the academic research community still lacks publicly available motion data, as supplied by [4], that can be freely used for systematic research on motion analysis, synthesis, and classification. Furthermore, a common dataset of annotated and well-documented motion capture data would be extremely valuable to the research community in view of an objective comparison and evaluation of the achieved research results. It is the objective of our motion capture database HDM05 1 to supply free motion capture data for research purposes. HDM05 contains more than tree hours of systematically recorded and well-documented motion capture data in the C3D as well as in the ASF/AMC data format. Furthermore, HDM05 contains for each of roughly 70 motion classes 10 to 50 realizations executed by various actors amounting to roughly 1, 500 motion clips. In this documentation, we give a detailed description of our mocap database HDM05. In Sect. 1, we provide some general information on motion capture data including references to various application fields. A detailed description of the database structure of HDM05 as well as of the content of each mocap file can be found in Sect. 2. We also provide several MATLAB tools comprising a parser for ASF/AMC and C3D as well as visualization, renaming and cutting tools, which are described in Sect. 3. Finally, Sect. 4 summarizes some facts on the mocap file formats ASF/AMC and C3D as used in our database. We appreciate any comments and suggestions for improvement. 1 The motion capture data has been recorded at the Hochschule der Medien (HDM) in the year 2005 under the supervision of Bernhard Eberhardt.",poster,cp40
p2485,c8ac11b447445785a891d0ad8aa5b79bbc29e18a,c43,ACM Symposium on Applied Computing,Active Database Systems: Triggers and Rules For Advanced Database Processing,"From the Publisher: 
Active database systems enhance traditional database functionality with powerful rule-processing capabilities, providing a uniform and efficient mechanism for many database system applications. Among these applications are integrity constraints, views, authorization, statistics gathering, monitoring and alerting, knowledge-based systems, expert systems, and workflow management. This significant collection focuses on the most prominent research projects in active database systems. The project leaders for each prototype system provide detailed discussions of their projects and the relevance of their results to the future of active database systems. 
Features: 
A broad overview of current active database systems and how they can be extended and improved A comprehensive introduction to the core topics of the field, including its motivation and history Coverage of active database (trigger) capabilities in commercial products Discussion of forthcoming standards",poster,cp43
p2486,599cc88971d2f5b375ff23b6342f17855e01791c,c92,Advances in Soft Computing,The CMU Motion of Body (MoBo) Database,"In March 2001 we started to collect the CMU Motion of Body (MoBo) database. To date the database contains 25 individuals walking on a treadmill in the CMU 3D room. The subjects perform four different walk patterns: slow walk, fast walk, incline walk and walking with a ball. All subjects are captured using six high resolution color cameras distributed evenly around the treadmill. In this technical report we describe the capture setup, the collection procedure and the organization of the database.",poster,cp92
p2487,ae89baa469ee08fccc6202c8e4a685ed54ddbc50,c114,IEEE International Conference on Robotics and Automation,The Columbia grasp database,"Collecting grasp data for learning and benchmarking purposes is very expensive. It would be helpful to have a standard database of graspable objects, along with a set of stable grasps for each object, but no such database exists. In this work we show how to automate the construction of a database consisting of several hands, thousands of objects, and hundreds of thousands of grasps. Using this database, we demonstrate a novel grasp planning algorithm that exploits geometric similarity between a 3D model and the objects in the database to synthesize form closure grasps. Our contributions are this algorithm, and the database itself, which we are releasing to the community as a tool for both grasp planning and benchmarking.",fullPaper,cp114
p2488,93560f5dd5cbadfb229984902f608baa3358107e,j102,Nucleic Acids Research,The Pfam protein families database,"Pfam is a widely used database of protein families and domains. This article describes a set of major updates that we have implemented in the latest release (version 24.0). The most important change is that we now use HMMER3, the latest version of the popular profile hidden Markov model package. This software is ∼100 times faster than HMMER2 and is more sensitive due to the routine use of the forward algorithm. The move to HMMER3 has necessitated numerous changes to Pfam that are described in detail. Pfam release 24.0 contains 11 912 families, of which a large number have been significantly updated during the past two years. Pfam is available via servers in the UK (http://pfam.sanger.ac.uk/), the USA (http://pfam.janelia.org/) and Sweden (http://pfam.sbc.su.se/).",fullPaper,jv102
p2489,4aa6aaeb14e5f881100c97cd5d06306f16ab80d0,c97,Interspeech,Current Status of the Digital Database for Screening Mammography,Abstract content,poster,cp97
p2490,d4a8e93f004c86267eead89edecbd332518dbf21,c2,International Symposium on Intelligent Data Analysis,Database description with SDM: a semantic database model,"SDM is a high-level semantics-based database description and structuring formalism (database model) for databases. This database model is designed to capture more of the meaning of an application environment than is possible with contemporary database models. An SDM specification describes a database in terms of the kinds of entities that exist in the application environment, the classifications and groupings of those entities, and the structural interconnections among them. SDM provides a collection of high-level modeling primitives to capture the semantics of an application environment. By accommodating derived information in a database structural specification, SDM allows the same information to be viewed in several ways; this makes it possible to directly accommodate the variety of needs and processing requirements typically present in database applications. The design of the present SDM is based on our experience in using a preliminary version of it.
SDM is designed to enhance the effectiveness and usability of database systems. An SDM database description can serve as a formal specification and documentation tool for a database; it can provide a basis for supporting a variety of powerful user interface facilities, it can serve as a conceptual database model in the database design process; and, it can be used as the database model for a new kind of database management system.",poster,cp2
p2491,0c4eb9a17f6621994277220aed8c691b02c24526,c104,IEEE International Conference on Multimedia and Expo,bioDBnet: the biological database network,"SUMMARY
bioDBnet is an online web resource that provides interconnected access to many types of biological databases. It has integrated many of the most commonly used biological databases and in its current state has 153 database identifiers (nodes) covering all aspects of biology including genes, proteins, pathways and other biological concepts. bioDBnet offers various ways to work with these databases including conversions, extensive database reports, custom navigation and has various tools to enhance the quality of the results. Importantly, the access to bioDBnet is updated regularly, providing access to the most recent releases of each individual database.


AVAILABILITY
http://biodbnet.abcc.ncifcrf.gov.",poster,cp104
p2492,5737a1f6fd8d928b88726ada916d7874afdfe0d7,c112,Very Large Data Bases Conference,Approximate String Joins in a Database (Almost) for Free,"String data is ubiquitous, and its management has taken on particular importance in the past few years. Approximate queries are very important on string data especially for more complex queries involving joins. This is due, for example, to the prevalence of typographical errors in data, and multiple conventions for recording attributes such as name and address. Commercial databases do not support approximate string joins directly, and it is a challenge to implement this functionality efficiently with user-defined functions (UDFs). In this paper, we develop a technique for building approximate string join capabilities on top of commercial databases by exploiting facilities already available in them. At the core, our technique relies on matching short substrings of length , called -grams, and taking into account both positions of individual matches and the total number of such matches. Our approach applies to both approximate full string matching and approximate substring matching, with a variety of possible edit distance functions. The approximate string match predicate, with a suitable edit distance threshold, can be mapped into a vanilla relational expression and optimized by conventional relational optimizers. We demonstrate experimentally the benefits of our technique over the direct use of UDFs, using commercial database systems and real data. To study the I/O and CPU behavior of approximate string join algorithms with variations in edit distance and -gram length, we also describe detailed experiments based on a prototype implementation.",fullPaper,cp112
p2493,624155130a5a4cdc229bef3a5338cd887b53c9f8,c32,International Conference on Software Technology: Methods and Tools,World Porifera Database,Abstract content,poster,cp32
p2494,daeabbe2ac3aa90aabf10527090f548fc125e9e6,j383,Journal of Medicinal Chemistry,The PDBbind database: methodologies and updates.,"We have developed the PDBbind database to provide a comprehensive collection of binding affinities for the protein-ligand complexes in the Protein Data Bank (PDB). This paper gives a full description of the latest version, i.e., version 2003, which is an update to our recently reported work. Out of 23 790 entries in the PDB release No.107 (January 2004), 5897 entries were identified as protein-ligand complexes that meet our definition. Experimentally determined binding affinities (K(d), K(i), and IC(50)) for 1622 of these were retrieved from the references associated with these complexes. A total of 900 complexes were selected to form a ""refined set"", which is of particular value as a standard data set for docking and scoring studies. All of the final data, including binding affinity data, reference citations, and processed structural files, have been incorporated into the PDBbind database accessible on-line at http:// www.pdbbind.org/.",fullPaper,jv383
p2495,2c0ed016d2bc69854523aab6bcf86342692e31e2,c41,Software Product Lines Conference,The RDP (Ribosomal Database Project) continues,"The Ribosomal Database Project (RDP-II), previously described by Maidak et al., continued during the past year to add new rRNA sequences to the aligned data and to improve the analysis commands. Release 7.1 (September 17, 1999) included more than 10 700 small subunit rRNA sequences. More than 850 type strain sequences were identified and added to the prokaryotic alignment, bringing the total number of type sequences to 3324 representing 2460 different species. Availability of an RDP-II mirror site in Japan is also near completion. RDP-II provides aligned and annotated rRNA sequences, derived phylogenetic trees and taxonomic hierarchies, and analysis services through its WWW server (http://rdp.cme.msu.edu/ ). Analysis services include rRNA probe checking, approx-i-mate phylogenetic placement of user sequences, screening user sequences for possible chimeric rRNA sequences, automated alignment, production of similarity matrices and services to plan and analyze terminal restriction fragment length polymorphism (T-RFLP) experiments.",poster,cp41
p2496,c476b1838f10199cfb86fed375f2b755d2506a2a,c54,International Workshop on Agent-Oriented Software Engineering,DOOR: a database for prokaryotic operons,"We present a database DOOR (Database for prOkaryotic OpeRons) containing computationally predicted operons of all the sequenced prokaryotic genomes. All the operons in DOOR are predicted using our own prediction program, which was ranked to be the best among 14 operon prediction programs by a recent independent review. Currently, the DOOR database contains operons for 675 prokaryotic genomes, and supports a number of search capabilities to facilitate easy access and utilization of the information stored in it. Querying the database: the database provides a search capability for a user to find desired operons and associated information through multiple querying methods. Searching for similar operons: the database provides a search capability for a user to find operons that have similar composition and structure to a query operon. Prediction of cis-regulatory motifs: the database provides a capability for motif identification in the promoter regions of a user-specified group of possibly coregulated operons, using motif-finding tools. Operons for RNA genes: the database includes operons for RNA genes. OperonWiki: the database provides a wiki page (OperonWiki) to facilitate interactions between users and the developer of the database. We believe that DOOR provides a useful resource to many biologists working on bacteria and archaea, which can be accessed at http://csbl1.bmb.uga.edu/OperonDB.",poster,cp54
p2497,666dfa8258914bf17970b20d2f7247c7c1468307,c53,International Conference on Software Engineering and Knowledge Engineering,On the Desirability of Acyclic Database Schemes,"A class of database schemes, called acychc, was recently introduced. It is shown that this class has a number of desirable properties. In particular, several desirable properties that have been studied by other researchers m very different terms are all shown to be eqmvalent to acydicity. In addition, several equivalent charactenzauons of the class m terms of graphs and hypergraphs are given, and a smaple algorithm for determining acychclty is presented. Also given are several eqmvalent characterizations of those sets M of multivalued dependencies such that M is the set of muRlvalued dependencies that are the consequences of a given join dependency. Several characterizations for a conflict-free (in the sense of Lien) set of muluvalued dependencies are provided.",poster,cp53
p2498,099f9eccf15aed58801051148a5e937d05f4ede3,c107,British Machine Vision Conference,Lore: a database management system for semistructured data,"Lore (for Lightweight Object Repository) is a DBMS designed specifically for managing semistructured information. Implementing Lore has required rethinking all aspects of a DBMS, including storage management, indexing, query processing and optimization, and user interfaces. This paper provides an overview of these aspects of the Lore system, as well as other novel features such as dynamic structural summaries and seamless access to data from external sources.",poster,cp107
p2499,54a54371577545b529a8ab53b421d14a58b33ba6,j175,Nature reviews genetics,Genetic association database,Abstract content,fullPaper,jv175
