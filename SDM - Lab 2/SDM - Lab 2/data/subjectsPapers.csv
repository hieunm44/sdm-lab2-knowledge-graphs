domainName,paperId,domainId,paperUrl,conferenceJournalId,conferenceJournalTitle,paperTitle,paperAbstract,paperType,proceedingsVolumeIds
Engineering,p0,d0,133bcd7488a3c07cb0f493a87564c30e5433768c,j0,Nature Biotechnology,"Reproducible, interactive, scalable and extensible microbiome data science using QIIME 2",Abstract,fullPaper,jv0
Medicine,p0,d1,133bcd7488a3c07cb0f493a87564c30e5433768c,j0,Nature Biotechnology,"Reproducible, interactive, scalable and extensible microbiome data science using QIIME 2",Abstract,fullPaper,jv0
Engineering,p22,d0,2d6adb9636df5a8a5dbcbfaecd0c4d34d7c85034,c106,International Conference on Biometrics,Spectral Methods for Data Science: A Statistical Perspective,"Spectral methods have emerged as a simple yet surprisingly effective approach for extracting information from massive, noisy and incomplete data. In a nutshell, spectral methods refer to a collection of algorithms built upon the eigenvalues (resp. singular values) and eigenvectors (resp. singular vectors) of some properly designed matrices constructed from data. A diverse array of applications have been found in machine learning, data science, and signal processing. Due to their simplicity and effectiveness, spectral methods are not only used as a stand-alone estimator, but also frequently employed to initialize other more sophisticated algorithms to improve performance. 
While the studies of spectral methods can be traced back to classical matrix perturbation theory and methods of moments, the past decade has witnessed tremendous theoretical advances in demystifying their efficacy through the lens of statistical modeling, with the aid of non-asymptotic random matrix theory. This monograph aims to present a systematic, comprehensive, yet accessible introduction to spectral methods from a modern statistical perspective, highlighting their algorithmic implications in diverse large-scale applications. In particular, our exposition gravitates around several central questions that span various applications: how to characterize the sample efficiency of spectral methods in reaching a target level of statistical accuracy, and how to assess their stability in the face of random noise, missing data, and adversarial corruptions? In addition to conventional $\ell_2$ perturbation analysis, we present a systematic $\ell_{\infty}$ and $\ell_{2,\infty}$ perturbation theory for eigenspace and singular subspaces, which has only recently become available owing to a powerful ""leave-one-out"" analysis framework.",poster,cp106
Computer Science,p22,d3,2d6adb9636df5a8a5dbcbfaecd0c4d34d7c85034,c106,International Conference on Biometrics,Spectral Methods for Data Science: A Statistical Perspective,"Spectral methods have emerged as a simple yet surprisingly effective approach for extracting information from massive, noisy and incomplete data. In a nutshell, spectral methods refer to a collection of algorithms built upon the eigenvalues (resp. singular values) and eigenvectors (resp. singular vectors) of some properly designed matrices constructed from data. A diverse array of applications have been found in machine learning, data science, and signal processing. Due to their simplicity and effectiveness, spectral methods are not only used as a stand-alone estimator, but also frequently employed to initialize other more sophisticated algorithms to improve performance. 
While the studies of spectral methods can be traced back to classical matrix perturbation theory and methods of moments, the past decade has witnessed tremendous theoretical advances in demystifying their efficacy through the lens of statistical modeling, with the aid of non-asymptotic random matrix theory. This monograph aims to present a systematic, comprehensive, yet accessible introduction to spectral methods from a modern statistical perspective, highlighting their algorithmic implications in diverse large-scale applications. In particular, our exposition gravitates around several central questions that span various applications: how to characterize the sample efficiency of spectral methods in reaching a target level of statistical accuracy, and how to assess their stability in the face of random noise, missing data, and adversarial corruptions? In addition to conventional $\ell_2$ perturbation analysis, we present a systematic $\ell_{\infty}$ and $\ell_{2,\infty}$ perturbation theory for eigenspace and singular subspaces, which has only recently become available owing to a powerful ""leave-one-out"" analysis framework.",poster,cp106
Mathematics,p22,d6,2d6adb9636df5a8a5dbcbfaecd0c4d34d7c85034,c106,International Conference on Biometrics,Spectral Methods for Data Science: A Statistical Perspective,"Spectral methods have emerged as a simple yet surprisingly effective approach for extracting information from massive, noisy and incomplete data. In a nutshell, spectral methods refer to a collection of algorithms built upon the eigenvalues (resp. singular values) and eigenvectors (resp. singular vectors) of some properly designed matrices constructed from data. A diverse array of applications have been found in machine learning, data science, and signal processing. Due to their simplicity and effectiveness, spectral methods are not only used as a stand-alone estimator, but also frequently employed to initialize other more sophisticated algorithms to improve performance. 
While the studies of spectral methods can be traced back to classical matrix perturbation theory and methods of moments, the past decade has witnessed tremendous theoretical advances in demystifying their efficacy through the lens of statistical modeling, with the aid of non-asymptotic random matrix theory. This monograph aims to present a systematic, comprehensive, yet accessible introduction to spectral methods from a modern statistical perspective, highlighting their algorithmic implications in diverse large-scale applications. In particular, our exposition gravitates around several central questions that span various applications: how to characterize the sample efficiency of spectral methods in reaching a target level of statistical accuracy, and how to assess their stability in the face of random noise, missing data, and adversarial corruptions? In addition to conventional $\ell_2$ perturbation analysis, we present a systematic $\ell_{\infty}$ and $\ell_{2,\infty}$ perturbation theory for eigenspace and singular subspaces, which has only recently become available owing to a powerful ""leave-one-out"" analysis framework.",poster,cp106
Engineering,p37,d0,12f62537251cf8eb76fa11c59df68d2211008898,j16,International Journal of Digital Earth,Big Earth Data science: an information framework for a sustainable planet,"ABSTRACT The digital transformation of our society coupled with the increasing exploitation of natural resources makes sustainability challenges more complex and dynamic than ever before. These changes will unlikely stop or even decelerate in the near future. There is an urgent need for a new scientific approach and an advanced form of evidence-based decision-making towards the benefit of society, the economy, and the environment. To understand the impacts and interrelationships between humans as a society and natural Earth system processes, we propose a new engineering discipline, Big Earth Data science. This science is called to provide the methodologies and tools to generate knowledge from diverse, numerous, and complex data sources necessary to ensure a sustainable human society essential for the preservation of planet Earth. Big Earth Data science aims at utilizing data from Earth observation and social sensing and develop theories for understanding the mechanisms of how such a social-physical system operates and evolves. The manuscript introduces the universe of discourse characterizing this new science, its foundational paradigms and methodologies, and a possible technological framework to be implemented by applying an ecosystem approach. CASEarth and GEOSS are presented as examples of international implementation attempts. Conclusions discuss important challenges and collaboration opportunities.",fullPaper,jv16
Computer Science,p37,d3,12f62537251cf8eb76fa11c59df68d2211008898,j16,International Journal of Digital Earth,Big Earth Data science: an information framework for a sustainable planet,"ABSTRACT The digital transformation of our society coupled with the increasing exploitation of natural resources makes sustainability challenges more complex and dynamic than ever before. These changes will unlikely stop or even decelerate in the near future. There is an urgent need for a new scientific approach and an advanced form of evidence-based decision-making towards the benefit of society, the economy, and the environment. To understand the impacts and interrelationships between humans as a society and natural Earth system processes, we propose a new engineering discipline, Big Earth Data science. This science is called to provide the methodologies and tools to generate knowledge from diverse, numerous, and complex data sources necessary to ensure a sustainable human society essential for the preservation of planet Earth. Big Earth Data science aims at utilizing data from Earth observation and social sensing and develop theories for understanding the mechanisms of how such a social-physical system operates and evolves. The manuscript introduces the universe of discourse characterizing this new science, its foundational paradigms and methodologies, and a possible technological framework to be implemented by applying an ecosystem approach. CASEarth and GEOSS are presented as examples of international implementation attempts. Conclusions discuss important challenges and collaboration opportunities.",fullPaper,jv16
Engineering,p50,d0,41cf91ee13a1d15983ede066ddf6b67cc94a41f4,c11,European Conference on Modelling and Simulation,The Role of Academia in Data Science Education,"As the demand for data scientists continues to grow, universities are trying to figure out how to best contribute to the training of a workforce. However, there does not appear to be a consensus on the fundamental principles, expertise, skills, or knowledge-base needed to define an academic discipline. We argue that data science is not a discipline but rather an umbrella term used to describe a complex process involving not one data scientist possessing all the necessary expertise, but a team of data scientists with nonoverlapping complementary skills. We provide some recommendations for how to take this into account when designing data science academic programs.Keywords: applied statistics, data science, data science curriculum, data wrangling, machine learning, software engineering",poster,cp11
Engineering,p51,d0,840b60da93c2776230d3e6123d708e1c7e66ebc0,j21,Journal of Statistics and Data Science Education,Teaching Creative and Practical Data Science at Scale,"Abstract–Nolan and Temple Lang’s Computing in the Statistics Curricula (2010) advocated for a shift in statistical education to broadly include computing. In the time since, individuals with training in both computing and statistics have become increasingly employable in the burgeoning data science field. In response, universities have developed new courses and programs to meet the growing demand for data science education. To address this demand, we created Data Science in Practice, a large-enrollment undergraduate course. Here, we present our goals for teaching this course, including: (1) conceptualizing data science as creative problem solving, with a focus on project-based learning, (2) prioritizing practical application, teaching and using standardized tools and best practices, and (3) scaling education through coursework that enables hands-on and classroom learning in a large-enrollment course. Throughout this course we also emphasize social context and data ethics to best prepare students for the interdisciplinary and impactful nature of their work. We highlight creative problem solving and strategies for teaching automation-resilient skills, while providing students the opportunity to create a unique data science project that demonstrates their technical and creative capacities.",fullPaper,jv21
Engineering,p63,d0,38f0c0f2567e074c775017e0e8dd1a43b1f6fcdd,j21,Journal of Statistics and Data Science Education,"Data Science in 2020: Computing, Curricula, and Challenges for the Next 10 Years","Abstract In the past 10 years, new data science courses and programs have proliferated at the collegiate level. As faculty and administrators enter the race to provide data science training and attract new students, the road map for teaching data science remains elusive. In 2019, 69 college and university faculty teaching data science courses and developing data science curricula were surveyed to learn about their curricula, computing tools, and challenges they face in their classrooms. Faculty reported teaching a variety of computing skills in introductory data science (albeit fewer computing topics than statistics topics), and that one of the biggest challenges they face is teaching computing to a diverse audience with varying preparation. The ever-evolving nature of data science is a major hurdle for faculty teaching data science courses, and a call for more data science teaching resources was echoed in many responses.",fullPaper,jv21
Engineering,p68,d0,f56425ec56586dcfd2694ab83643e9e76f314e91,c13,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,50 Years of Data Science,"ABSTRACT More than 50 years ago, John Tukey called for a reformation of academic statistics. In “The Future of Data Analysis,” he pointed to the existence of an as-yet unrecognized science, whose subject of interest was learning from data, or “data analysis.” Ten to 20 years ago, John Chambers, Jeff Wu, Bill Cleveland, and Leo Breiman independently once again urged academic statistics to expand its boundaries beyond the classical domain of theoretical statistics; Chambers called for more emphasis on data preparation and presentation rather than statistical modeling; and Breiman called for emphasis on prediction rather than inference. Cleveland and Wu even suggested the catchy name “data science” for this envisioned field. A recent and growing phenomenon has been the emergence of “data science” programs at major universities, including UC Berkeley, NYU, MIT, and most prominently, the University of Michigan, which in September 2015 announced a $100M “Data Science Initiative” that aims to hire 35 new faculty. Teaching in these new programs has significant overlap in curricular subject matter with traditional statistics courses; yet many academic statisticians perceive the new programs as “cultural appropriation.” This article reviews some ingredients of the current “data science moment,” including recent commentary about data science in the popular media, and about how/whether data science is really different from statistics. The now-contemplated field of data science amounts to a superset of the fields of statistics and machine learning, which adds some technology for “scaling up” to “big data.” This chosen superset is motivated by commercial rather than intellectual developments. Choosing in this way is likely to miss out on the really important intellectual event of the next 50 years. Because all of science itself will soon become data that can be mined, the imminent revolution in data science is not about mere “scaling up,” but instead the emergence of scientific studies of data analysis science-wide. In the future, we will be able to predict how a proposal to change data analysis workflows would impact the validity of data analysis across all of science, even predicting the impacts field-by-field. Drawing on work by Tukey, Cleveland, Chambers, and Breiman, I present a vision of data science based on the activities of people who are “learning from data,” and I describe an academic field dedicated to improving that activity in an evidence-based manner. This new field is a better academic enlargement of statistics and machine learning than today’s data science initiatives, while being able to accommodate the same short-term goals. Based on a presentation at the Tukey Centennial Workshop, Princeton, NJ, September 18, 2015.",poster,cp13
Engineering,p75,d0,0e23ff1f915b6af32bf1a1107ee7e15ebe10efe8,j32,"Annual review of political science (Palo Alto, Calif. Print)",The Challenge of Big Data and Data Science,"Big data and data science are transforming the world in ways that spawn new concerns for social scientists, such as the impacts of the internet on citizens and the media, the repercussions of smart cities, the possibilities of cyber-warfare and cyber-terrorism, the implications of precision medicine, and the consequences of artificial intelligence and automation. Along with these changes in society, powerful new data science methods support research using administrative, internet, textual, and sensor-audio-video data. Burgeoning data and innovative methods facilitate answering previously hard-to-tackle questions about society by offering new ways to form concepts from data, to do descriptive inference, to make causal inferences, and to generate predictions. They also pose challenges as social scientists must grasp the meaning of concepts and predictions generated by convoluted algorithms, weigh the relative value of prediction versus causal inference, and cope with ethical challenges as their methods, such as algorithms for mobilizing voters or determining bail, are adopted by policy makers.",fullPaper,jv32
Engineering,p100,d0,fb566f2001e44a65433fb7cc2eb7bcf6513a7db8,c51,International Conference on Engineering Education,The 9 Pitfalls of Data Science,"Scientific rigor and critical thinking skills are indispensable in this age of big data because machine learning and artificial intelligence are often led astray by meaningless patterns. The 9 Pitfalls of Data Science is loaded with entertaining real-world examples of both successful and misguided approaches to interpreting data, both grand successes and epic failures. Anyone can learn to distinguish between good data science and nonsense. We are confident that readers will learn how to avoid being duped by data, and make better, more informed decisions. Whether they want to be effective creators, interpreters, or users of data, they need to know the nine pitfalls of data science.",poster,cp51
Engineering,p108,d0,c0b1eedfa2031a69fbdf02a4abc8a741faf6a912,c44,Italian National Conference on Sensors,"Introduction to Data Science - A Python Approach to Concepts, Techniques and Applications",Abstract,poster,cp44
Computer Science,p108,d3,c0b1eedfa2031a69fbdf02a4abc8a741faf6a912,c44,Italian National Conference on Sensors,"Introduction to Data Science - A Python Approach to Concepts, Techniques and Applications",Abstract,poster,cp44
Engineering,p132,d0,c9ed1ad1a3a08bf5ebebe8105805dd102546b8f3,c39,Online World Conference on Soft Computing in Industrial Applications,Process-Structure Linkages Using a Data Science Approach: Application to Simulated Additive Manufacturing Data,Abstract,poster,cp39
Medicine,p132,d1,c9ed1ad1a3a08bf5ebebe8105805dd102546b8f3,c39,Online World Conference on Soft Computing in Industrial Applications,Process-Structure Linkages Using a Data Science Approach: Application to Simulated Additive Manufacturing Data,Abstract,poster,cp39
Engineering,p134,d0,b34b9758b36c92c023c3c10f3a39aeb8f5c83927,c10,Americas Conference on Information Systems,Exploring Project Management Methodologies Used Within Data Science Teams,"There are many reasons data science teams should use a well-defined process to manage and coordinate their efforts, such as improved collaboration, efficiency and stakeholder communication. This paper explores the current methodology data science teams use to manage and coordinate their efforts. Unfortunately, based on our survey results, most data science teams currently use an ad hoc project management approach. In fact, 82% of the data scientists surveyed did not follow an explicit process. However, it is encouraging to note that 85% of the respondents thought that adopting an improved process methodology would improve the teams’ outcomes. Based on these results, we described six possible process methodologies teams could use. To conclude, we outlined plans to describe best practices for data science team processes and to develop a process evaluation framework.",fullPaper,cp10
Computer Science,p134,d3,b34b9758b36c92c023c3c10f3a39aeb8f5c83927,c10,Americas Conference on Information Systems,Exploring Project Management Methodologies Used Within Data Science Teams,"There are many reasons data science teams should use a well-defined process to manage and coordinate their efforts, such as improved collaboration, efficiency and stakeholder communication. This paper explores the current methodology data science teams use to manage and coordinate their efforts. Unfortunately, based on our survey results, most data science teams currently use an ad hoc project management approach. In fact, 82% of the data scientists surveyed did not follow an explicit process. However, it is encouraging to note that 85% of the respondents thought that adopting an improved process methodology would improve the teams’ outcomes. Based on these results, we described six possible process methodologies teams could use. To conclude, we outlined plans to describe best practices for data science team processes and to develop a process evaluation framework.",fullPaper,cp10
Engineering,p141,d0,e420259fd53d15c2b6cb8906027d5a100ca356d7,c96,Human Language Technology - The Baltic Perspectiv,Data science for building energy management: A review,Abstract,poster,cp96
Engineering,p157,d0,82feed9f0f8d077046b9b8be36e664483a66e33b,c71,International Joint Conference on Artificial Intelligence,Teaching Stats for Data Science,"ABSTRACT “Data science” is a useful catchword for methods and concepts original to the field of statistics, but typically being applied to large, multivariate, observational records. Such datasets call for techniques not often part of an introduction to statistics: modeling, consideration of covariates, sophisticated visualization, and causal reasoning. This article re-imagines introductory statistics as an introduction to data science and proposes a sequence of 10 blocks that together compose a suitable course for extracting information from contemporary data. Recent extensions to the mosaic packages for R together with tools from the “tidyverse” provide a concise and readable notation for wrangling, visualization, model-building, and model interpretation: the fundamental computational tasks of data science.",poster,cp71
Computer Science,p157,d3,82feed9f0f8d077046b9b8be36e664483a66e33b,c71,International Joint Conference on Artificial Intelligence,Teaching Stats for Data Science,"ABSTRACT “Data science” is a useful catchword for methods and concepts original to the field of statistics, but typically being applied to large, multivariate, observational records. Such datasets call for techniques not often part of an introduction to statistics: modeling, consideration of covariates, sophisticated visualization, and causal reasoning. This article re-imagines introductory statistics as an introduction to data science and proposes a sequence of 10 blocks that together compose a suitable course for extracting information from contemporary data. Recent extensions to the mosaic packages for R together with tools from the “tidyverse” provide a concise and readable notation for wrangling, visualization, model-building, and model interpretation: the fundamental computational tasks of data science.",poster,cp71
Engineering,p167,d0,19bb52bec8b5ced3175f4c3ef1b8fb7027cc5ff1,c14,Hawaii International Conference on System Sciences,Applications of Python to evaluate environmental data science problems,"There is a significant convergence of interests in the research community efforts to advance the development and application of software resources (capable of handling the relevant mathematical algorithms to provide scalable information) for solving data science problems. Anaconda is one of the many open source platforms that facilitate the use of open source programming languages (R, Python) for large‐scale data processing, predictive analytics, and scientific computing. The environmental research community may choose to adapt the use of either of the R or the Python programming languages for analyzing the data science problems on the Anaconda platform. This study demonstrated the applications of using Scikit‐learn (a Python machine learning library package) on Anaconda platform for analyzing the in‐bus carbon dioxide concentrations by (i) importing the data into Spyder (Python 3.6) in Anaconda, (ii) performing an exploratory data analysis, (iii) performing dimensionality reduction through RandomForestRegressor feature selection, (iv) developing statistical regression models, and (v) generating regression decision tree models with DecisionTreeRegressor feature. The readers may adopt the methods (inclusive of the Python coding) discussed in this article to successfully address their own data science problems. © 2017 American Institute of Chemical Engineers Environ Prog, 36: 1580–1586, 2017",poster,cp14
Engineering,p176,d0,12e17fa5dd5715c563aadf705427da84817f100f,c26,Decision Support Systems,Data science: Data science tutorials,Abstract,poster,cp26
Engineering,p188,d0,224eb3407b50533668b6c1caa55a720688b8b532,j65,International Journal of Information Management,"A review and future direction of agile, business intelligence, analytics and data science",Abstract,fullPaper,jv65
Computer Science,p188,d3,224eb3407b50533668b6c1caa55a720688b8b532,j65,International Journal of Information Management,"A review and future direction of agile, business intelligence, analytics and data science",Abstract,fullPaper,jv65
Engineering,p212,d0,12f3b97d76e2e07c3bf2914606d26bbfbbe85bd1,c95,Cyber ..,Role of materials data science and informatics in accelerated materials innovation,"The goal of the Materials Genome Initiative is to substantially reduce the time and cost of materials design and deployment. Achieving this goal requires taking advantage of the recent advances in data and information sciences. This critical need has impelled the emergence of a new discipline, called materials data science and informatics. This emerging new discipline not only has to address the core scientific/technological challenges related to datafication of materials science and engineering, but also, a number of equally important challenges around data-driven transformation of the current culture, practices, and workflows employed for materials innovation. A comprehensive effort that addresses both of these aspects in a synergistic manner is likely to succeed in realizing the vision of scaled-up materials innovation. Key toolsets needed for the successful adoption of materials data science and informatics in materials innovation are identified and discussed in this article. Prototypical examples of emerging novel toolsets and their functionality are described along with select case studies.",poster,cp95
Engineering,p219,d0,2ce0b954b5180fdc0834c3e4f0d14b5a0e668d53,c9,Big Data,Mining the Quantified Self: Personal Knowledge Discovery as a Challenge for Data Science,"The last several years have seen an explosion of interest in wearable computing, personal tracking devices, and the so-called quantified self (QS) movement. Quantified self involves ordinary people recording and analyzing numerous aspects of their lives to understand and improve themselves. This is now a mainstream phenomenon, attracting a great deal of attention, participation, and funding. As more people are attracted to the movement, companies are offering various new platforms (hardware and software) that allow ever more aspects of daily life to be tracked. Nearly every aspect of the QS ecosystem is advancing rapidly, except for analytic capabilities, which remain surprisingly primitive. With increasing numbers of qualified self participants collecting ever greater amounts and types of data, many people literally have more data than they know what to do with. This article reviews the opportunities and challenges posed by the QS movement. Data science provides well-tested techniques for knowledge discovery. But making these useful for the QS domain poses unique challenges that derive from the characteristics of the data collected as well as the specific types of actionable insights that people want from the data. Using a small sample of QS time series data containing information about personal health we provide a formulation of the QS problem that connects data to the decisions of interest to the user.",fullPaper,cp9
Medicine,p219,d1,2ce0b954b5180fdc0834c3e4f0d14b5a0e668d53,c9,Big Data,Mining the Quantified Self: Personal Knowledge Discovery as a Challenge for Data Science,"The last several years have seen an explosion of interest in wearable computing, personal tracking devices, and the so-called quantified self (QS) movement. Quantified self involves ordinary people recording and analyzing numerous aspects of their lives to understand and improve themselves. This is now a mainstream phenomenon, attracting a great deal of attention, participation, and funding. As more people are attracted to the movement, companies are offering various new platforms (hardware and software) that allow ever more aspects of daily life to be tracked. Nearly every aspect of the QS ecosystem is advancing rapidly, except for analytic capabilities, which remain surprisingly primitive. With increasing numbers of qualified self participants collecting ever greater amounts and types of data, many people literally have more data than they know what to do with. This article reviews the opportunities and challenges posed by the QS movement. Data science provides well-tested techniques for knowledge discovery. But making these useful for the QS domain poses unique challenges that derive from the characteristics of the data collected as well as the specific types of actionable insights that people want from the data. Using a small sample of QS time series data containing information about personal health we provide a formulation of the QS problem that connects data to the decisions of interest to the user.",fullPaper,cp9
Computer Science,p219,d3,2ce0b954b5180fdc0834c3e4f0d14b5a0e668d53,c9,Big Data,Mining the Quantified Self: Personal Knowledge Discovery as a Challenge for Data Science,"The last several years have seen an explosion of interest in wearable computing, personal tracking devices, and the so-called quantified self (QS) movement. Quantified self involves ordinary people recording and analyzing numerous aspects of their lives to understand and improve themselves. This is now a mainstream phenomenon, attracting a great deal of attention, participation, and funding. As more people are attracted to the movement, companies are offering various new platforms (hardware and software) that allow ever more aspects of daily life to be tracked. Nearly every aspect of the QS ecosystem is advancing rapidly, except for analytic capabilities, which remain surprisingly primitive. With increasing numbers of qualified self participants collecting ever greater amounts and types of data, many people literally have more data than they know what to do with. This article reviews the opportunities and challenges posed by the QS movement. Data science provides well-tested techniques for knowledge discovery. But making these useful for the QS domain poses unique challenges that derive from the characteristics of the data collected as well as the specific types of actionable insights that people want from the data. Using a small sample of QS time series data containing information about personal health we provide a formulation of the QS problem that connects data to the decisions of interest to the user.",fullPaper,cp9
Engineering,p220,d0,659890e52fe234cde0e02a2305e213d3e8cb14b2,c88,International Conference on Big Data Computing and Communications,Data science and cyberinfrastructure: critical enablers for accelerated development of hierarchical materials,"The slow pace of new/improved materials development and deployment has been identified as the main bottleneck in the innovation cycles of most emerging technologies. Much of the continuing discussion in the materials development community is therefore focused on the creation of novel materials innovation ecosystems designed to dramatically accelerate materials development efforts, while lowering the overall cost involved. In this paper, it is argued that the recent advances in data science can be leveraged suitably to address this challenge by effectively mediating between the seemingly disparate, inherently uncertain, multiscale and multimodal measurements and computations involved in the current materials’ development efforts. Proper utilisation of modern data science in the materials’ development efforts can lead to a new generation of data-driven decision support tools for guiding effort investment (for both measurements and computations) at various stages of the materials development. It should also be recognised that the success of such ecosystems is predicated on the creation and utilisation of integration platforms for promoting intimate, synchronous collaborations between cross-disciplinary and distributed team members (i.e. cyberinfrastructure). Indeed, data sciences and cyberinfrastructure form the two main pillars of the emerging new discipline broadly referred to as materials informatics (MI). This paper provides a summary of current capabilities in this emerging new field as they relate to the accelerated development of advanced hierarchical materials (the internal structure plays a dominant role in controlling overall properties/performance in these materials) and identifies specific directions of research that offer the most promising avenues.",poster,cp88
Engineering,p228,d0,469fb7c7178c8370a93fb27dadc9c5c839a9b8ec,j59,Journal of Data and Information Science,Information Science Roles in the Emerging Field of Data Science,"There has long been discussion about the distinctions of library science, information science, and informatics, and how these areas differ and overlap with computer science. Today the term data science is emerging that generates excitement and questions about how it relates to and differs from these other areas of study. For our purposes here, I consider information science to be the general term that subsumes library science and informatics and focuses on distinctions and similarities among these disciplines that each informs data science. At the most general levels, information science deals with the genesis, flow, use, and preservation of information; computer science deals with algorithms and techniques for computational processes. Data science as a concept emerges from the applications of existing studies of measurement, representation, interpretation, and management to problems in Citation: Gary Marchionini (2016). Information Science Roles in the Emerging Field of Data Science. Received: Mar. 10, 2016 Accepted: Mar. 22, 2016",fullPaper,jv59
Computer Science,p228,d3,469fb7c7178c8370a93fb27dadc9c5c839a9b8ec,j59,Journal of Data and Information Science,Information Science Roles in the Emerging Field of Data Science,"There has long been discussion about the distinctions of library science, information science, and informatics, and how these areas differ and overlap with computer science. Today the term data science is emerging that generates excitement and questions about how it relates to and differs from these other areas of study. For our purposes here, I consider information science to be the general term that subsumes library science and informatics and focuses on distinctions and similarities among these disciplines that each informs data science. At the most general levels, information science deals with the genesis, flow, use, and preservation of information; computer science deals with algorithms and techniques for computational processes. Data science as a concept emerges from the applications of existing studies of measurement, representation, interpretation, and management to problems in Citation: Gary Marchionini (2016). Information Science Roles in the Emerging Field of Data Science. Received: Mar. 10, 2016 Accepted: Mar. 22, 2016",fullPaper,jv59
Engineering,p236,d0,d4dd5eb7a6081c0857ddae0caca1fdcf288553e1,j73,Journal of Biomechanics,Gait biomechanics in the era of data science.,Abstract,fullPaper,jv73
Medicine,p236,d1,d4dd5eb7a6081c0857ddae0caca1fdcf288553e1,j73,Journal of Biomechanics,Gait biomechanics in the era of data science.,Abstract,fullPaper,jv73
Engineering,p249,d0,1a95f1e8ff32488f228a25764af64531cb758ff0,c115,International Conference on Information Integration and Web-based Applications & Services,Exploration of data science techniques to predict fatigue strength of steel from composition and processing parameters,Abstract,poster,cp115
Engineering,p274,d0,379e9576dea9690cf88d9132287edbefb7626232,c116,International Society for Music Information Retrieval Conference,Data Smart: Using Data Science to Transform Information into Insight,"Data Science gets thrown around in the press like it's magic. Major retailers are predicting everything from when their customers are pregnant to when they want a new pair of Chuck Taylors. It's a brave new world where seemingly meaningless data can be transformed into valuable insight to drive smart business decisions.But how does one exactly do data science? Do you have to hire one of these priests of the dark arts, the ""data scientist,"" to extract this gold from your data? Nope.Data science is little more than using straight-forward steps to process raw data into actionable insight. And inData Smart, author and data scientist John Foreman will show you how that's done within the familiar environment of a spreadsheet.",poster,cp116
Engineering,p276,d0,010a8ed71c6a80c2c02c7f55e1718151f91ff35a,j81,Quantitative Science Studies,Web of Science as a data source for research on scientific and scholarly activity,"Abstract Web of Science (WoS) is the world’s oldest, most widely used and authoritative database of research publications and citations. Based on the Science Citation Index, founded by Eugene Garfield in 1964, it has expanded its selective, balanced, and complete coverage of the world’s leading research to cover around 34,000 journals today. A wide range of use cases are supported by WoS from daily search and discovery by researchers worldwide through to the supply of analytical data sets and the provision of specialized access to raw data for bibliometric partners. A long- and well-established network of such partners enables the Institute for Scientific Information (ISI) to continue to work closely with bibliometric groups around the world to the benefit of both the community and the services that the company provides to researchers and analysts.",fullPaper,jv81
Computer Science,p276,d3,010a8ed71c6a80c2c02c7f55e1718151f91ff35a,j81,Quantitative Science Studies,Web of Science as a data source for research on scientific and scholarly activity,"Abstract Web of Science (WoS) is the world’s oldest, most widely used and authoritative database of research publications and citations. Based on the Science Citation Index, founded by Eugene Garfield in 1964, it has expanded its selective, balanced, and complete coverage of the world’s leading research to cover around 34,000 journals today. A wide range of use cases are supported by WoS from daily search and discovery by researchers worldwide through to the supply of analytical data sets and the provision of specialized access to raw data for bibliometric partners. A long- and well-established network of such partners enables the Institute for Scientific Information (ISI) to continue to work closely with bibliometric groups around the world to the benefit of both the community and the services that the company provides to researchers and analysts.",fullPaper,jv81
Engineering,p314,d0,d1db1c83c64f556fd4005cc12bddf7963f82a77f,j19,Scientific Data,Open science resources for the discovery and analysis of Tara Oceans data,Abstract,fullPaper,jv19
Medicine,p314,d1,d1db1c83c64f556fd4005cc12bddf7963f82a77f,j19,Scientific Data,Open science resources for the discovery and analysis of Tara Oceans data,Abstract,fullPaper,jv19
Biology,p314,d5,d1db1c83c64f556fd4005cc12bddf7963f82a77f,j19,Scientific Data,Open science resources for the discovery and analysis of Tara Oceans data,Abstract,fullPaper,jv19
Geography,p314,d13,d1db1c83c64f556fd4005cc12bddf7963f82a77f,j19,Scientific Data,Open science resources for the discovery and analysis of Tara Oceans data,Abstract,fullPaper,jv19
Engineering,p320,d0,51995dc568874ea34911833355234b1f696dacfc,j59,Journal of Data and Information Science,Science Mapping: A Systematic Review of the Literature,"Abstract Purpose We present a systematic review of the literature concerning major aspects of science mapping to serve two primary purposes: First, to demonstrate the use of a science mapping approach to perform the review so that researchers may apply the procedure to the review of a scientific domain of their own interest, and second, to identify major areas of research activities concerning science mapping, intellectual milestones in the development of key specialties, evolutionary stages of major specialties involved, and the dynamics of transitions from one specialty to another. Design/methodology/approach We first introduce a theoretical framework of the evolution of a scientific specialty. Then we demonstrate a generic search strategy that can be used to construct a representative dataset of bibliographic records of a domain of research. Next, progressively synthesized co-citation networks are constructed and visualized to aid visual analytic studies of the domain’s structural and dynamic patterns and trends. Finally, trajectories of citations made by particular types of authors and articles are presented to illustrate the predictive potential of the analytic approach. Findings The evolution of the science mapping research involves the development of a number of interrelated specialties. Four major specialties are discussed in detail in terms of four evolutionary stages: conceptualization, tool construction, application, and codification. Underlying connections between major specialties are also explored. The predictive analysis demonstrates citations trajectories of potentially transformative contributions. Research limitations The systematic review is primarily guided by citation patterns in the dataset retrieved from the literature. The scope of the data is limited by the source of the retrieval, i.e. the Web of Science, and the composite query used. An iterative query refinement is possible if one would like to improve the data quality, although the current approach serves our purpose adequately. More in-depth analyses of each specialty would be more revealing by incorporating additional methods such as citation context analysis and studies of other aspects of scholarly publications. Practical implications The underlying analytic process of science mapping serves many practical needs, notably bibliometric mapping, knowledge domain visualization, and visualization of scientific literature. In order to master such a complex process of science mapping, researchers often need to develop a diverse set of skills and knowledge that may span multiple disciplines. The approach demonstrated in this article provides a generic method for conducting a systematic review. Originality/value Incorporating the evolutionary stages of a specialty into the visual analytic study of a research domain is innovative. It provides a systematic methodology for researchers to achieve a good understanding of how scientific fields evolve, to recognize potentially insightful patterns from visually encoded signs, and to synthesize various information so as to capture the state of the art of the domain.",fullPaper,jv59
Computer Science,p320,d3,51995dc568874ea34911833355234b1f696dacfc,j59,Journal of Data and Information Science,Science Mapping: A Systematic Review of the Literature,"Abstract Purpose We present a systematic review of the literature concerning major aspects of science mapping to serve two primary purposes: First, to demonstrate the use of a science mapping approach to perform the review so that researchers may apply the procedure to the review of a scientific domain of their own interest, and second, to identify major areas of research activities concerning science mapping, intellectual milestones in the development of key specialties, evolutionary stages of major specialties involved, and the dynamics of transitions from one specialty to another. Design/methodology/approach We first introduce a theoretical framework of the evolution of a scientific specialty. Then we demonstrate a generic search strategy that can be used to construct a representative dataset of bibliographic records of a domain of research. Next, progressively synthesized co-citation networks are constructed and visualized to aid visual analytic studies of the domain’s structural and dynamic patterns and trends. Finally, trajectories of citations made by particular types of authors and articles are presented to illustrate the predictive potential of the analytic approach. Findings The evolution of the science mapping research involves the development of a number of interrelated specialties. Four major specialties are discussed in detail in terms of four evolutionary stages: conceptualization, tool construction, application, and codification. Underlying connections between major specialties are also explored. The predictive analysis demonstrates citations trajectories of potentially transformative contributions. Research limitations The systematic review is primarily guided by citation patterns in the dataset retrieved from the literature. The scope of the data is limited by the source of the retrieval, i.e. the Web of Science, and the composite query used. An iterative query refinement is possible if one would like to improve the data quality, although the current approach serves our purpose adequately. More in-depth analyses of each specialty would be more revealing by incorporating additional methods such as citation context analysis and studies of other aspects of scholarly publications. Practical implications The underlying analytic process of science mapping serves many practical needs, notably bibliometric mapping, knowledge domain visualization, and visualization of scientific literature. In order to master such a complex process of science mapping, researchers often need to develop a diverse set of skills and knowledge that may span multiple disciplines. The approach demonstrated in this article provides a generic method for conducting a systematic review. Originality/value Incorporating the evolutionary stages of a specialty into the visual analytic study of a research domain is innovative. It provides a systematic methodology for researchers to achieve a good understanding of how scientific fields evolve, to recognize potentially insightful patterns from visually encoded signs, and to synthesize various information so as to capture the state of the art of the domain.",fullPaper,jv59
Engineering,p334,d0,30d6f200f8b4bae78dbb4f69f1730bcad131d523,c41,IEEE International Conference on Data Engineering,The Materials Data Facility: Data Services to Advance Materials Science Research,Abstract,poster,cp41
Computer Science,p334,d3,30d6f200f8b4bae78dbb4f69f1730bcad131d523,c41,IEEE International Conference on Data Engineering,The Materials Data Facility: Data Services to Advance Materials Science Research,Abstract,poster,cp41
Materials Science,p334,d7,30d6f200f8b4bae78dbb4f69f1730bcad131d523,c41,IEEE International Conference on Data Engineering,The Materials Data Facility: Data Services to Advance Materials Science Research,Abstract,poster,cp41
Engineering,p422,d0,09ee0ba924ffd21fc7e14ad3147284133cf2f576,c108,IEEE International Conference on Multimedia and Expo,"Color Science, Concepts and Methods. Quantitative Data and Formulas","G. Wyszecki and W. S. Stiles London: John Wiley. 1967. Pp. xiv + 628. Price £11. This remarkable and unusual book is by two outstanding authorities on the science of colour: Dr. Stiles, for many years a senior member of the Light Division at the National Physical Laboratory, and Dr. Wyszecki, currently in charge of the Radiation Optics Section of the Canadian National Research Council. The authors' aim has been to provide a comprehensive source book of data required by the practical and theoretical worker in the field of colour and they have achieved this aim so successfully that their book is likely to become the standard work on the subject and to remain so for a good many years.",poster,cp108
Engineering,p437,d0,0131258a516da6f9d86795fc6ed4968206dba005,j59,Journal of Data and Information Science,A Criteria-based Assessment of the Coverage of Scopus and Web of Science,"Abstract Purpose The purpose of this study is to assess the coverage of the scientific literature in Scopus and Web of Science from the perspective of research evaluation. Design/methodology/approach The academic communities of Norway have agreed on certain criteria for what should be included as original research publications in research evaluation and funding contexts. These criteria have been applied since 2004 in a comprehensive bibliographic database called the Norwegian Science Index (NSI). The relative coverages of Scopus and Web of Science are compared with regard to publication type, field of research and language. Findings Our results show that Scopus covers 72 percent of the total Norwegian scientific and scholarly publication output in 2015 and 2016, while the corresponding figure for Web of Science Core Collection is 69 percent. The coverages are most comprehensive in medicine and health (89 and 87 percent) and in the natural sciences and technology (85 and 84 percent). The social sciences (48 percent in Scopus and 40 percent in Web of Science Core Collection) and particularly the humanities (27 and 23 percent) are much less covered in the two international data sources. Research limitation Comparing with data from only one country is a limitation of the study, but the criteria used to define a country’s scientific output as well as the identification of patterns of field-dependent partial representations in Scopus and Web of Science should be recognizable and useful also for other countries. Originality/value The novelty of this study is the criteria-based approach to studying coverage problems in the two data sources.",fullPaper,jv59
Computer Science,p437,d3,0131258a516da6f9d86795fc6ed4968206dba005,j59,Journal of Data and Information Science,A Criteria-based Assessment of the Coverage of Scopus and Web of Science,"Abstract Purpose The purpose of this study is to assess the coverage of the scientific literature in Scopus and Web of Science from the perspective of research evaluation. Design/methodology/approach The academic communities of Norway have agreed on certain criteria for what should be included as original research publications in research evaluation and funding contexts. These criteria have been applied since 2004 in a comprehensive bibliographic database called the Norwegian Science Index (NSI). The relative coverages of Scopus and Web of Science are compared with regard to publication type, field of research and language. Findings Our results show that Scopus covers 72 percent of the total Norwegian scientific and scholarly publication output in 2015 and 2016, while the corresponding figure for Web of Science Core Collection is 69 percent. The coverages are most comprehensive in medicine and health (89 and 87 percent) and in the natural sciences and technology (85 and 84 percent). The social sciences (48 percent in Scopus and 40 percent in Web of Science Core Collection) and particularly the humanities (27 and 23 percent) are much less covered in the two international data sources. Research limitation Comparing with data from only one country is a limitation of the study, but the criteria used to define a country’s scientific output as well as the identification of patterns of field-dependent partial representations in Scopus and Web of Science should be recognizable and useful also for other countries. Originality/value The novelty of this study is the criteria-based approach to studying coverage problems in the two data sources.",fullPaper,jv59
Engineering,p449,d0,0bc97adfb3c77f27397d19395af2fdff9f04aaa0,c6,Annual Conference on Genetic and Evolutionary Computation,The TESS science processing operations center,"The Transiting Exoplanet Survey Satellite (TESS) will conduct a search for Earth's closest cousins starting in early 2018 and is expected to discover ∼1,000 small planets with Rp < 4 R⊕ and measure the masses of at least 50 of these small worlds. The Science Processing Operations Center (SPOC) is being developed at NASA Ames Research Center based on the Kepler science pipeline and will generate calibrated pixels and light curves on the NASA Advanced Supercomputing Division's Pleiades supercomputer. The SPOC will also search for periodic transit events and generate validation products for the transit-like features in the light curves. All TESS SPOC data products will be archived to the Mikulski Archive for Space Telescopes (MAST).",poster,cp6
Physics,p449,d2,0bc97adfb3c77f27397d19395af2fdff9f04aaa0,c6,Annual Conference on Genetic and Evolutionary Computation,The TESS science processing operations center,"The Transiting Exoplanet Survey Satellite (TESS) will conduct a search for Earth's closest cousins starting in early 2018 and is expected to discover ∼1,000 small planets with Rp < 4 R⊕ and measure the masses of at least 50 of these small worlds. The Science Processing Operations Center (SPOC) is being developed at NASA Ames Research Center based on the Kepler science pipeline and will generate calibrated pixels and light curves on the NASA Advanced Supercomputing Division's Pleiades supercomputer. The SPOC will also search for periodic transit events and generate validation products for the transit-like features in the light curves. All TESS SPOC data products will be archived to the Mikulski Archive for Space Telescopes (MAST).",poster,cp6
Engineering,p450,d0,929607741b2a12656ff8d3360ca96fe76a6557a4,c84,EUROCON Conference,Next Generation Science Standards,"Science and Engineering Practices that connect to garden-based education (all 8): • Asking questions (for science) and defining problems (for engineering) • Developing and using models • Planning and carrying out investigations • Analyzing and interpreting data • Using mathematics and computational thinking • Constructing explanations (for science) and designing solutions (for engineering) • Engaging in argument from evidence • Obtaining, evaluating, and communicating information",poster,cp84
Psychology,p450,d10,929607741b2a12656ff8d3360ca96fe76a6557a4,c84,EUROCON Conference,Next Generation Science Standards,"Science and Engineering Practices that connect to garden-based education (all 8): • Asking questions (for science) and defining problems (for engineering) • Developing and using models • Planning and carrying out investigations • Analyzing and interpreting data • Using mathematics and computational thinking • Constructing explanations (for science) and designing solutions (for engineering) • Engaging in argument from evidence • Obtaining, evaluating, and communicating information",poster,cp84
Engineering,p473,d0,a3324c0dcb1efaf5d88003b3fe22a3351b4c16da,c21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,"""Big Data"" : big gaps of knowledge in the field of internet science","Research on so-called ‘Big Data’ has received a considerable momentum and is expected to grow in the future. One very interesting stream of research on Big Data analyzes online networks. Many online networks are known to have some typical macro-characteristics, such as ‘small world’ properties. Much less is known about underlying micro-processes leading to these properties. The models used by Big Data researchers usually are inspired by mathematical ease of exposition. We propose to follow in addition a different strategy that leads to knowledge about micro-processes that match with actual online behavior. This knowledge can then be used for the selection of mathematically-tractable models of online network formation and evolution. Insight from social and behavioral research is needed for pursuing this strategy of knowledge generation about micro-processes. Accordingly, our proposal points to a unique role that social scientists could play in Big Data research.",poster,cp21
Engineering,p478,d0,a6e594b11bd8195e96a1826f591fcec9a20fdcf3,c77,Visualization for Computer Security,"Frascati manual 2015 : guidelines for collecting and reporting data in research and experimental development: the measurement of scientific, technological and innovation activities.","The Frascati Manual is firmly based on experience gained from collecting R&D 
statistics in both OECD and non-member countries. It is a result of the collective work 
of national experts in NESTI, the OECD Working Party of National Experts on Science 
and Technology Indicators. This group, with support from the OECD Secretariat, has 
worked over now more than 50 years as an effective community of practitioners to 
implement measurement approaches for the concepts of science, technology and 
innovation. This effort has resulted in a series of methodological manuals known as the 
“Frascati Family”, which in addition to this manual includes guidance documents on 
the measurement of innovation (the Oslo Manual), human resources devoted to science 
and technology, patents, and technological balance of payments, but most importantly, 
it has provided the basis for the main statistics and indicators on science and technology 
that are currently used.",poster,cp77
Engineering,p524,d0,133bcd7488a3c07cb0f493a87564c30e5433768c,j0,Nature Biotechnology,"Reproducible, interactive, scalable and extensible microbiome data science using QIIME 2",Abstract,fullPaper,jv0
Medicine,p524,d1,133bcd7488a3c07cb0f493a87564c30e5433768c,j0,Nature Biotechnology,"Reproducible, interactive, scalable and extensible microbiome data science using QIIME 2",Abstract,fullPaper,jv0
Engineering,p532,d0,f80b4316419482031c0ee28ab96ee2051c9863b8,j157,Journal of Industrial and Production Engineering,Sustainable industrial and operation engineering trends and challenges Toward Industry 4.0: a data driven analysis,"ABSTRACT This study supplies contributions to the existing literature with a state-of-the-art bibliometric review of sustainable industrial and operation engineering as the field moves toward Industry 4.0, and guidance for future studies and practical achievements. Although industrial and operation engineering is being promoted forward to sustainability, the systematization of the knowledge that forms firms’ manufacturing and operations and encompasses their wide concepts and abundant complementary elements is still absent. This study aims to analyze contemporary sustainable industrial and operations engineering in Industry 4.0 context. The bibliometric analysis and fuzzy Delphi method are proposed. Resulting in a total of 30 indicators that are criticized and clustered into eight study groups, including lean manufacturing in Industry 4.0, cyber-physical production system, big data-driven and smart communications, safety and security, artificial intelligence for sustainability, the circular economy in a digital environment, business intelligence and virtual reality, and environmental sustainability. Graphical Abstract",fullPaper,jv157
Engineering,p563,d0,b890447611d11dcd8b835183a35afef16c096eb9,j164,Annual Review of Chemical and Biomolecular Engineering,Big Data Analytics in Chemical Engineering.,"Big data analytics is the journey to turn data into insights for more informed business and operational decisions. As the chemical engineering community is collecting more data (volume) from different sources (variety), this journey becomes more challenging in terms of using the right data and the right tools (analytics) to make the right decisions in real time (velocity). This article highlights recent big data advancements in five industries, including chemicals, energy, semiconductors, pharmaceuticals, and food, and then discusses technical, platform, and culture challenges. To reach the next milestone in multiplying successes to the enterprise level, government, academia, and industry need to collaboratively focus on workforce development and innovation.",fullPaper,jv164
Medicine,p563,d1,b890447611d11dcd8b835183a35afef16c096eb9,j164,Annual Review of Chemical and Biomolecular Engineering,Big Data Analytics in Chemical Engineering.,"Big data analytics is the journey to turn data into insights for more informed business and operational decisions. As the chemical engineering community is collecting more data (volume) from different sources (variety), this journey becomes more challenging in terms of using the right data and the right tools (analytics) to make the right decisions in real time (velocity). This article highlights recent big data advancements in five industries, including chemicals, energy, semiconductors, pharmaceuticals, and food, and then discusses technical, platform, and culture challenges. To reach the next milestone in multiplying successes to the enterprise level, government, academia, and industry need to collaboratively focus on workforce development and innovation.",fullPaper,jv164
Engineering,p566,d0,538229ccc6b3cdd3f4162fe13d03791234b991e6,c41,IEEE International Conference on Data Engineering,"Proceedings of the 25th International Conference on Data Engineering, ICDE 2009, March 29 2009 - April 2 2009, Shanghai, China",Abstract,fullPaper,cp41
Computer Science,p566,d3,538229ccc6b3cdd3f4162fe13d03791234b991e6,c41,IEEE International Conference on Data Engineering,"Proceedings of the 25th International Conference on Data Engineering, ICDE 2009, March 29 2009 - April 2 2009, Shanghai, China",Abstract,fullPaper,cp41
Engineering,p567,d0,4d01fc57d4a00274acacfe1aac3414434476e73c,c105,International Conference on Automatic Face and Gesture Recognition,"Data Engineering: Mining, Information and Intelligence","It is quite clear that the world is awash in all kinds of data. In fact, the sheer volume of data adds little value to human activity and choices. The key to the vast amounts of data and information available to us is to distill and organize the data into information that we can productively use. Therefore, the most desirable information is that information which can be used to provide insight and intelligence for actionable strategies. DATA ENGINEERING: Mining, Information and Intelligence focuses specifically on applied information-warehousing and data-mining research that is being used or can be used by both academic researchers and industry enterprises. Moreover, the book will be the first to categorize and synthesize the diverse methodologies that are used in these interrelated fields into a structured approach entitled, Data Engineering.",poster,cp105
Engineering,p570,d0,967c6ed8bbc9ae86802f9025fd47d91f7e83a6e4,c105,International Conference on Automatic Face and Gesture Recognition,"Multi-Disciplinary Engineering for Cyber-Physical Production Systems, Data Models and Software Solutions for Handling Complex Engineering Projects",Abstract,poster,cp105
Computer Science,p570,d3,967c6ed8bbc9ae86802f9025fd47d91f7e83a6e4,c105,International Conference on Automatic Face and Gesture Recognition,"Multi-Disciplinary Engineering for Cyber-Physical Production Systems, Data Models and Software Solutions for Handling Complex Engineering Projects",Abstract,poster,cp105
Engineering,p572,d0,f7a217f58c99fcfb41f16f9f32159a3ffb262b8c,c105,International Conference on Automatic Face and Gesture Recognition,XBRL for Interactive Data: Engineering the Information Value Chain,"Interactive data supports organizations to communicate effectively with their stakeholders and partners on the Internet and the World Wide Web. XBRL (eXtensible Business Reporting Language) is a key enabling technology for interactive data. XBRL links organizations and knowledge consumers in a variety of information value chains. XBRL is now in use in many countries and important settings. These include the mandate by the Securities and Exchange Commission (SEC) in the USA for corporations and mutual funds to report in XBRL. The publication includes an in-depth analysis of XBRL and up-to-date explanation of the most popular constructs in XML, on which XBRL builds. The book provides business and policy makers, technologists and information engineers with an essential toolkit to understand the complete implementation of XBRL. The chapters include introduction to XBRL, design and construction topics as well as advanced dimensional and extensibility aspects. The book also provides detailed analysis of the interaction of instance documents and taxonomies and a synopsis of the most current XBRL technologies. Features: * Written by XBRL experts with long experience in XBRL projects. * With foreword by Olivier Servais, Director - XBRL Activities of the International Accounting Standards Committee Foundation (IASCF). * With personal letter ""How it all began..."" from Charles Hoffman - father of XBRL. * First and only book on the market with two in depth and comprehensive chapters on dimensions in XBRL. * Supporting the content of the official XBRL International Taxonomy Development training. * With over 70 illustrations and over 60 code examples. * Covering aspects of XBRL taxonomy engineering in general and dimensional taxonomy engineering in particular. * With a number of real-world practical examples and eight comprehensive case studies. * First book on the market to cover XBRL formulas, XBRL versioning and XBRL rendering.",poster,cp105
Engineering,p590,d0,ec59569fdee17844ae071be1536a08f937f08c57,j162,IEEE Software,"Speed, Data, and Ecosystems: The Future of Software Engineering","An evaluation of recent industrial and societal trends revealed three key factors driving software engineering's future: speed, data, and ecosystems. These factors' implications have led to guidelines for companies to evolve their software engineering practices. This article is part of a special issue on the Future of Software Engineering.",fullPaper,jv162
Computer Science,p590,d3,ec59569fdee17844ae071be1536a08f937f08c57,j162,IEEE Software,"Speed, Data, and Ecosystems: The Future of Software Engineering","An evaluation of recent industrial and societal trends revealed three key factors driving software engineering's future: speed, data, and ecosystems. These factors' implications have led to guidelines for companies to evolve their software engineering practices. This article is part of a special issue on the Future of Software Engineering.",fullPaper,jv162
Engineering,p591,d0,5de7babaeb9c67c8e79a19f3335b651700719f35,c82,Symposium on Networked Systems Design and Implementation,The Engineering Strong‐Motion Database: A Platform to Access Pan‐European Accelerometric Data,"This article describes the Engineering Strong‐Motion Database (ESM), developed in the framework of the European project Network of European Research Infrastructures for Earthquake Risk Assessment and Mitigation (NERA, see [Data and Resources][1]). ESM is specifically designed to provide end users only with quality‐checked, uniformly processed strong‐motion data and relevant parameters and has done so since 1969 in the Euro‐Mediterranean region. The database was designed for a large variety of stakeholders (expert seismologists, earthquake engineers, students, and professionals) with a user‐friendly and straightforward web interface.

Users can access earthquake and station information and download waveforms of events with magnitude≥4.0 (unprocessed and processed acceleration, velocity, and displacement, and acceleration and displacement response spectra at 5% damping). Specific tools are also available to users to process strong‐motion data and select ground‐motion suites for code‐based seismic structural analyses.

 [1]: #sec-13",poster,cp82
Engineering,p618,d0,7ab84ca9b72ae064e1c1a23ed439bb1a717c5185,c46,Ideal,Unit Operations Of Chemical Engineering,"chemical engineering chemical engineering essentials for, home american journal of chemical engineering, bachelor of science in chemical engineering american, specialty polymers high performance polymers solvay, chemical engineering degrees top universities, chemical process wikipedia, perry s chemical engineers handbook eighth edition, wolfram and mathematica solutions for chemical engineering, aquatherm engineering consultants india pvt ltd edit, naval reserve officers training corps scholarship, college of engineering california state university long, unit and door heaters armstrong international, chemical engineering cput, water treatment products and services h2o engineering, journal of chemical amp engineering data acs publications, wbdg wbdg whole building design guide, chemical engineering for non chemical engineers aiche, proposed syllabus for b tech program in chemical engineering, phase out of the national diploma in chemical engineering, chemical engineering free books at ebd, operations council sgs, chemical plants india caustic soda plants chemical plants, martindale s calculators on line center mathematics, kraft recovery operations course tappi org, patent technology centers management uspto, chemical recycling makes waste plastic a resource, faculty of engineering amp technology vaal university of, chemical engineer wikipedia, index chemical engineering conferences asia events, european training network for chemical engineering, diploma of engineering curtin college, search unit standards south african qualifications authority, bcit chemical and environmental technology process, perry s chemical engineers handbook 9th edition, electrical amp systems engineering washington university, csulb chemical engineering california state university, visual encyclopedia of chemical engineeringabout gold membership gold level membership allows you full access to the chemical engineering archives dating back to 1986 quickly search and retrieve all articles and back issues, in chemical engineering process design is the design of processes for desired physical and or chemical transformation of materials process design is central to chemical engineering and it can be considered to be the summit of that field bringing together all of the fields components, at the department of chemical engineering we provide you with a challenging and contemporary chemical engineering degree program enhanced by the research accomplishments of our faculty members we value excellence in teaching quality research and service along with the intellectual development of students in a challenging rewarding academic environment, the largest selection of the highest performing polymers solvay is the industry leader in specialty polymers offering the broadest selection of high performance thermoplastic resins fluoroelastomers and fluorinated fluids, what is chemical engineering so what is chemical engineering chemical engineering is a multi disciplinary branch of engineering that combines natural and experimental sciences such as chemistry and physics along with life sciences such as biology microbiology and biochemistry plus mathematics and economics to design develop produce transform transport operate and manage the, in a scientific sense a chemical process is a method or means of somehow changing one or more chemicals or chemical compounds such a chemical process can occur by itself or be caused by an outside force and involves a chemical reaction",poster,cp46
Engineering,p621,d0,8702e8a17eed574c314d77c19a0ff2c2f26fdbb3,j176,Earthquake spectra,The promise of implementing machine learning in earthquake engineering: A state-of-the-art review,"Machine learning (ML) has evolved rapidly over recent years with the promise to substantially alter and enhance the role of data science in a variety of disciplines. Compared with traditional approaches, ML offers advantages to handle complex problems, provide computational efficiency, propagate and treat uncertainties, and facilitate decision making. Also, the maturing of ML has led to significant advances in not only the main-stream artificial intelligence (AI) research but also other science and engineering fields, such as material science, bioengineering, construction management, and transportation engineering. This study conducts a comprehensive review of the progress and challenges of implementing ML in the earthquake engineering domain. A hierarchical attribute matrix is adopted to categorize the existing literature based on four traits identified in the field, such as ML method, topic area, data resource, and scale of analysis. The state-of-the-art review indicates to what extent ML has been applied in four topic areas of earthquake engineering, including seismic hazard analysis, system identification and damage detection, seismic fragility assessment, and structural control for earthquake mitigation. Moreover, research challenges and the associated future research needs are discussed, which include embracing the next generation of data sharing and sensor technologies, implementing more advanced ML techniques, and developing physics-guided ML models.",fullPaper,jv176
Engineering,p629,d0,5f1abea9244e76cab265dfc83cb68226ea381df6,c39,Online World Conference on Soft Computing in Industrial Applications,Bulletin of the Technical Committee on Data Engineering,Abstract,poster,cp39
Engineering,p636,d0,aede98ccae969f508bcbb4ce57b8eee5cf69f132,c6,Annual Conference on Genetic and Evolutionary Computation,Realising the potential of product data engineering,"The increased pressures of competing in the global marketplace have compelled industries to seek innovative ways to increase their competitive edge. Immediate access to information has rapidly become a key to success. The use of information technology in information management makes such access to computer-based information a real possibility. Product data technology provides mechanisms for the management of product-related information. It offers approaches for business enterprises to design and implement their product information systems. Research results lead to the hypothesis that the benefits of product data technology can only be fully realised through its planned and controlled introduction, namely, product data engineering, and through the implementation of changes to the business processes that this leads to. The European Computer Manufacturers' Association's Reference Model for Open Distributed Processing (RM-ODP) provides five viewpoints from which an information system may be viewed. A product data engineering methodology based upon RM-ODP was established and used by a research project, MOSES, in collaboration with industry.",poster,cp6
Engineering,p640,d0,f7c3b5a070e3cb73708964d4d2199c1c146a5527,c49,ACM/SIGCOMM Internet Measurement Conference,Network traffic characteristics of data centers in the wild,"Although there is tremendous interest in designing improved networks for data centers, very little is known about the network-level traffic characteristics of data centers today. In this paper, we conduct an empirical study of the network traffic in 10 data centers belonging to three different categories, including university, enterprise campus, and cloud data centers. Our definition of cloud data centers includes not only data centers employed by large online service providers offering Internet-facing applications but also data centers used to host data-intensive (MapReduce style) applications). We collect and analyze SNMP statistics, topology and packet-level traces. We examine the range of applications deployed in these data centers and their placement, the flow-level and packet-level transmission properties of these applications, and their impact on network and link utilizations, congestion and packet drops. We describe the implications of the observed traffic patterns for data center internal traffic engineering as well as for recently proposed architectures for data center networks.",fullPaper,cp49
Computer Science,p640,d3,f7c3b5a070e3cb73708964d4d2199c1c146a5527,c49,ACM/SIGCOMM Internet Measurement Conference,Network traffic characteristics of data centers in the wild,"Although there is tremendous interest in designing improved networks for data centers, very little is known about the network-level traffic characteristics of data centers today. In this paper, we conduct an empirical study of the network traffic in 10 data centers belonging to three different categories, including university, enterprise campus, and cloud data centers. Our definition of cloud data centers includes not only data centers employed by large online service providers offering Internet-facing applications but also data centers used to host data-intensive (MapReduce style) applications). We collect and analyze SNMP statistics, topology and packet-level traces. We examine the range of applications deployed in these data centers and their placement, the flow-level and packet-level transmission properties of these applications, and their impact on network and link utilizations, congestion and packet drops. We describe the implications of the observed traffic patterns for data center internal traffic engineering as well as for recently proposed architectures for data center networks.",fullPaper,cp49
Engineering,p648,d0,d481a6219f4cd1c337e0ec31086a2f527bab9109,c81,ACM Symposium on Applied Computing,Data science: Accelerating innovation and discovery in chemical engineering,Abstract,poster,cp81
Engineering,p661,d0,45752d6bbca650b182964d388b431293fbd2d41b,j183,Innovations in Systems and Software Engineering,State of practice in requirements engineering: contemporary data,Abstract,fullPaper,jv183
Computer Science,p661,d3,45752d6bbca650b182964d388b431293fbd2d41b,j183,Innovations in Systems and Software Engineering,State of practice in requirements engineering: contemporary data,Abstract,fullPaper,jv183
Engineering,p662,d0,191b9759decb2700c3d4b072f67cdcdf9f57cc8f,j184,Journal of computing in civil engineering,"Civil Engineering Grand Challenges: Opportunities for Data Sensing, Information Analysis, and Knowledge Discovery","AbstractThis paper presents an exploratory analysis to identify civil engineering challenges that can be addressed with further data sensing and analysis (DSA) research. An initial literature review was followed by a web-based survey to solicit expert opinions in each civil engineering subdiscipline to select challenges that can be addressed by civil engineering DSA research. A total of 10 challenges were identified and evidence of economic, environmental, and societal impacts of these challenges is presented through a review of the literature. The challenges presented in this paper are high building energy consumption, crude estimation of sea level, increased soil and coastal erosion, inadequate water quality, untapped and depleting groundwater, increasing traffic congestion, poor infrastructure resilience to disasters, poor and degrading infrastructure, need for better mining and coal ash waste disposal, and low construction site safety. The paper aims to assist the civil engineering research community ...",fullPaper,jv184
Computer Science,p662,d3,191b9759decb2700c3d4b072f67cdcdf9f57cc8f,j184,Journal of computing in civil engineering,"Civil Engineering Grand Challenges: Opportunities for Data Sensing, Information Analysis, and Knowledge Discovery","AbstractThis paper presents an exploratory analysis to identify civil engineering challenges that can be addressed with further data sensing and analysis (DSA) research. An initial literature review was followed by a web-based survey to solicit expert opinions in each civil engineering subdiscipline to select challenges that can be addressed by civil engineering DSA research. A total of 10 challenges were identified and evidence of economic, environmental, and societal impacts of these challenges is presented through a review of the literature. The challenges presented in this paper are high building energy consumption, crude estimation of sea level, increased soil and coastal erosion, inadequate water quality, untapped and depleting groundwater, increasing traffic congestion, poor infrastructure resilience to disasters, poor and degrading infrastructure, need for better mining and coal ash waste disposal, and low construction site safety. The paper aims to assist the civil engineering research community ...",fullPaper,jv184
Engineering,p664,d0,0ed846e87ce0961d162e9115b4e9837537138e3a,c2,International Conference on Software Engineering,Analyze this! 145 questions for data scientists in software engineering,"In this paper, we present the results from two surveys related to data science applied to software engineering. The first survey solicited questions that software engineers would like data scientists to investigate about software, about software processes and practices, and about software engineers. Our analyses resulted in a list of 145 questions grouped into 12 categories. The second survey asked a different pool of software engineers to rate these 145 questions and identify the most important ones to work on first. Respondents favored questions that focus on how customers typically use their applications. We also saw opposition to questions that assess the performance of individual employees or compare them with one another. Our categorization and catalog of 145 questions can help researchers, practitioners, and educators to more easily focus their efforts on topics that are important to the software industry.",fullPaper,cp2
Computer Science,p664,d3,0ed846e87ce0961d162e9115b4e9837537138e3a,c2,International Conference on Software Engineering,Analyze this! 145 questions for data scientists in software engineering,"In this paper, we present the results from two surveys related to data science applied to software engineering. The first survey solicited questions that software engineers would like data scientists to investigate about software, about software processes and practices, and about software engineers. Our analyses resulted in a list of 145 questions grouped into 12 categories. The second survey asked a different pool of software engineers to rate these 145 questions and identify the most important ones to work on first. Respondents favored questions that focus on how customers typically use their applications. We also saw opposition to questions that assess the performance of individual employees or compare them with one another. Our categorization and catalog of 145 questions can help researchers, practitioners, and educators to more easily focus their efforts on topics that are important to the software industry.",fullPaper,cp2
Engineering,p692,d0,68d12aeab53834e9e851fa6751b0d3122bf18d51,c55,Design Automation Conference,Review of Metamodeling Techniques in Support of Engineering Design Optimization,"Computation-intensive design problems are becoming increasingly common in manufacturing industries. The computation burden is often caused by expensive analysis and simulation processes in order to reach a comparable level of accuracy as physical testing data. To address such a challenge, approximation or metamodeling techniques are often used. Metamodeling techniques have been developed from many different disciplines including statistics, mathematics, computer science, and various engineering disciplines. These metamodels are initially developed as “surrogates” of the expensive simulation process in order to improve the overall computation efficiency. They are then found to be a valuable tool to support a wide scope of activities in modern engineering design, especially design optimization. This work reviews the state-of-the-art metamodel-based techniques from a practitioner’s perspective according to the role of metamodeling in supporting design optimization, including model approximation, design space exploration, problem formulation, and solving various types of optimization problems. Challenges and future development of metamodeling in support of engineering design is also analyzed and discussed.Copyright © 2006 by ASME",fullPaper,cp55
Engineering,p700,d0,ee75981d55610ea82c73ec73c3ba7fb9e5f4d9cc,j188,Journal of Tissue Engineering and Regenerative Medicine,Additive manufacturing techniques for the production of tissue engineering constructs,"‘Additive manufacturing’ (AM) refers to a class of manufacturing processes based on the building of a solid object from three‐dimensional (3D) model data by joining materials, usually layer upon layer. Among the vast array of techniques developed for the production of tissue‐engineering (TE) scaffolds, AM techniques are gaining great interest for their suitability in achieving complex shapes and microstructures with a high degree of automation, good accuracy and reproducibility. In addition, the possibility of rapidly producing tissue‐engineered constructs meeting patient's specific requirements, in terms of tissue defect size and geometry as well as autologous biological features, makes them a powerful way of enhancing clinical routine procedures. This paper gives an extensive overview of different AM techniques classes (i.e. stereolithography, selective laser sintering, 3D printing, melt–extrusion‐based techniques, solution/slurry extrusion‐based techniques, and tissue and organ printing) employed for the development of tissue‐engineered constructs made of different materials (i.e. polymeric, ceramic and composite, alone or in combination with bioactive agents), by highlighting their principles and technological solutions. Copyright © 2012 John Wiley & Sons, Ltd.",fullPaper,jv188
Medicine,p700,d1,ee75981d55610ea82c73ec73c3ba7fb9e5f4d9cc,j188,Journal of Tissue Engineering and Regenerative Medicine,Additive manufacturing techniques for the production of tissue engineering constructs,"‘Additive manufacturing’ (AM) refers to a class of manufacturing processes based on the building of a solid object from three‐dimensional (3D) model data by joining materials, usually layer upon layer. Among the vast array of techniques developed for the production of tissue‐engineering (TE) scaffolds, AM techniques are gaining great interest for their suitability in achieving complex shapes and microstructures with a high degree of automation, good accuracy and reproducibility. In addition, the possibility of rapidly producing tissue‐engineered constructs meeting patient's specific requirements, in terms of tissue defect size and geometry as well as autologous biological features, makes them a powerful way of enhancing clinical routine procedures. This paper gives an extensive overview of different AM techniques classes (i.e. stereolithography, selective laser sintering, 3D printing, melt–extrusion‐based techniques, solution/slurry extrusion‐based techniques, and tissue and organ printing) employed for the development of tissue‐engineered constructs made of different materials (i.e. polymeric, ceramic and composite, alone or in combination with bioactive agents), by highlighting their principles and technological solutions. Copyright © 2012 John Wiley & Sons, Ltd.",fullPaper,jv188
Engineering,p707,d0,a8d3dc7d66a242973dcd9ff0426c6b4369d7a90b,c22,Grid Computing Environments,An Introduction to Reliability and Maintainability Engineering,Part 1 Basic reliability models: the failure distribution constant failure rate model time-dependent failure models reliability of systems state dependent systems physical reliability models design for reliability maintainability design for maintainability availability. Part 2 The analysis of failure data: data collection and empirical methods reliabilty testing reliability growth testing identifying failure and repair distribution goodness-of-fit tests. Part 3 Application: reliability estimation and application implementation.,poster,cp22
Engineering,p724,d0,1f53b2c6428ca6b4484b201e14b56d41db5c5ce1,c113,International Conference on Mobile Data Management,Prognostics and Health Management: A Review on Data Driven Approaches,"Prognostics and health management (PHM) is a framework that offers comprehensive yet individualized solutions for managing system health. In recent years, PHM has emerged as an essential approach for achieving competitive advantages in the global market by improving reliability, maintainability, safety, and affordability. Concepts and components in PHM have been developed separately in many areas such as mechanical engineering, electrical engineering, and statistical science, under varied names. In this paper, we provide a concise review of mainstream methods in major aspects of the PHM framework, including the updated research from both statistical science and engineering, with a focus on data-driven approaches. Real world examples have been provided to illustrate the implementation of PHM in practice.",poster,cp113
Engineering,p725,d0,9e1e273ed7b6f3210f70576a36312670441d35f6,j160,Archives of Computational Methods in Engineering,"A Review of Trimming in Isogeometric Analysis: Challenges, Data Exchange and Simulation Aspects",Abstract,fullPaper,jv160
Medicine,p725,d1,9e1e273ed7b6f3210f70576a36312670441d35f6,j160,Archives of Computational Methods in Engineering,"A Review of Trimming in Isogeometric Analysis: Challenges, Data Exchange and Simulation Aspects",Abstract,fullPaper,jv160
Engineering,p732,d0,314ebb14030eb6a321dd86440ffe56887541b07c,c50,Conference on Emerging Network Experiment and Technology,Identifying 21st Century STEM Competencies Using Workplace Data,Abstract,poster,cp50
Physics,p732,d2,314ebb14030eb6a321dd86440ffe56887541b07c,c50,Conference on Emerging Network Experiment and Technology,Identifying 21st Century STEM Competencies Using Workplace Data,Abstract,poster,cp50
Engineering,p739,d0,a9911c08c685815f4dbce2a09eacfc5bdc00e6ca,j194,Tissue Engineering. Part A,An Overview of the Tissue Engineering Market in the United States from 2011 to 2018.,"IMPACT STATEMENT
This report seeks to provide an update of the current landscape of the tissue engineering market in the United States from an unbiased point of view by analyzing the financial reports provided by tissue engineering companies, as well as data from publicly available clinical trials with relevant tissue engineering applications.",fullPaper,jv194
Medicine,p739,d1,a9911c08c685815f4dbce2a09eacfc5bdc00e6ca,j194,Tissue Engineering. Part A,An Overview of the Tissue Engineering Market in the United States from 2011 to 2018.,"IMPACT STATEMENT
This report seeks to provide an update of the current landscape of the tissue engineering market in the United States from an unbiased point of view by analyzing the financial reports provided by tissue engineering companies, as well as data from publicly available clinical trials with relevant tissue engineering applications.",fullPaper,jv194
Engineering,p756,d0,0fd8e04c9717409fa23e0d2fc995920a23c933be,j197,Biomedical Engineering Letters,Machine learning in biomedical engineering,Abstract,fullPaper,jv197
Medicine,p756,d1,0fd8e04c9717409fa23e0d2fc995920a23c933be,j197,Biomedical Engineering Letters,Machine learning in biomedical engineering,Abstract,fullPaper,jv197
Engineering,p757,d0,902a8d4496020759dffd14265775d8e95381fdd9,c68,Symposium on Advances in Databases and Information Systems,Microseismic Imaging Of Hydraulic Fracturing : improved engineering of unconventional shale reservoirs,"Microseismic Imaging of Hydraulic Fracturing: Improved Engineering of Unconventional Shale Reservoirs (SEG Distinguished Instructor Series No. 17) covers the use of microseismic data to enhance engineering design of hydraulic fracturing and well completion. The book, which accompanies the 2014 SEG Distinguished Instructor Short Course, describes the design, acquisition, processing, and interpretation of an effective microseismic project. The text includes a tutorial of the basics of hydraulic fracturing, including the geologic and geomechanical factors that control fracture growth. In addition to practical issues associated with collecting and interpreting microseismic data, potential pitfalls and quality-control steps are discussed. Actual case studies are used to demonstrate engineering benefits and improved production through the use of microseismic monitoring. Providing a practical user guide for survey design, quality control, interpretation, and application of microseismic hydraulic fracture monitoring, this book will be of interest to geoscientists and engineers involved in development of unconventional reservoirs.",poster,cp68
Engineering,p764,d0,c5c00002b3adf106219bc6b31a7a5cd9f998168c,c83,"International Convention on Information and Communication Technology, Electronics and Microelectronics",Intelligent Fault Diagnosis and Prognosis for Engineering Systems,"PREFACE. ACKNOWLEDGMENTS. PROLOGUE. 1 INTRODUCTION. 1.1 Historical Perspective. 1.2 Diagnostic and Prognostic System Requirements. 1.3 Designing in Fault Diagnostic and Prognostic Systems. 1.4 Diagnostic and Prognostic Functional Layers. 1.5 Preface to Book Chapters. 1.6 References. 2 SYSTEMS APPROACH TO CBM/PHM. 2.1 Introduction. 2.2 Trade Studies. 2.3 Failure Modes and Effects Criticality Analysis (FMECA). 2.4 System CBM Test-Plan Design. 2.5 Performance Assessment. 2.6 CBM/PHM Impact on Maintenance and Operations: Case Studies. 2.7 CBM/PHM in Control and Contingency Management. 2.8 References. 3 SENSORS AND SENSING STRATEGIES. 3.1 Introduction. 3.2 Sensors. 3.3 Sensor Placement. 3.4 Wireless Sensor Networks. 3.5 Smart Sensors. 3.6 References. 4 SIGNAL PROCESSING AND DATABASE MANAGEMENT SYSTEMS. 4.1 Introduction. 4.2 Signal Processing in CBM/PHM. 4.3 Signal Preprocessing. 4.4 Signal Processing. 4.5 Vibration Monitoring and Data Analysis. 4.6 Real-Time Image Feature Extraction and Defect/Fault Classification. 4.7 The Virtual Sensor. 4.8 Fusion or Integration Technologies. 4.9 Usage-Pattern Tracking. 4.10 Database Management Methods. 4.11 References. 5 FAULT DIAGNOSIS. 5.1 Introduction. 5.2 The Diagnostic Framework. 5.3 Historical Data Diagnostic Methods. 5.4 Data-Driven Fault Classification and Decision Making. 5.5 Dynamic Systems Modeling. 5.6 Physical Model-Based Methods. 5.7 Model-Based Reasoning. 5.8 Case-Based Reasoning (CBR). 5.9 Other Methods for Fault Diagnosis. 5.10 A Diagnostic Framework for Electrical/Electronic Systems. 5.11 Case Study: Vibration-Based Fault Detection and Diagnosis for Engine Bearings. 5.12 References. 6 FAULT PROGNOSIS. 6.1 Introduction. 6.2 Model-Based Prognosis Techniques. 6.3 Probability-Based Prognosis Techniques. 6.4 Data-Driven Prediction Techniques. 6.5 Case Studies. 6.6 References. 7 FAULT DIAGNOSIS AND PROGNOSIS PERFORMANCE METRICS. 7.1 Introduction. 7.2 CBM/PHM Requirements Definition. 7.3 Feature-Evaluation Metrics. 7.4 Fault Diagnosis Performance Metrics. 7.5 Prognosis Performance Metrics. 7.6 Diagnosis and Prognosis Effectiveness Metrics. 7.7 Complexity/Cost-Benefit Analysis of CBM/PHM Systems. 7.8 References. 8 LOGISTICS: SUPPORT OF THE SYSTEM IN OPERATION. 8.1 Introduction. 8.2 Product-Support Architecture, Knowledge Base, and Methods for CBM. 8.3 Product Support without CBM. 8.4 Product Support with CBM. 8.5 Maintenance Scheduling Strategies. 8.6 A Simple Example. 8.7 References. APPENDIX. INDEX.",poster,cp83
Engineering,p768,d0,70f0bdfad8d671b95a1840e3954e0194989806e6,j200,International Journal of Digital Curation,The Informatics Transform: Re-Engineering Libraries for the Data Decade,"In this paper, Liz Lyon explores how libraries can re-shape to better reflect the requirements and challenges of today’s data-centric research landscape. The Informatics Transform presents five assertions as potential pathways to change, which will help libraries to re-position, re-profile, and re-structure to better address research data management challenges. The paper deconstructs the institutional research lifecycle and describes a portfolio of ten data support services which libraries can deliver to support the research lifecycle phases. Institutional roles and responsibilities for research data management are also unpacked, building on the framework from the earlier Dealing with Data Report. Finally, the paper examines critical capacity and capability challenges and proposes some innovative steps to addressing the significant skills gaps.",fullPaper,jv200
Computer Science,p768,d3,70f0bdfad8d671b95a1840e3954e0194989806e6,j200,International Journal of Digital Curation,The Informatics Transform: Re-Engineering Libraries for the Data Decade,"In this paper, Liz Lyon explores how libraries can re-shape to better reflect the requirements and challenges of today’s data-centric research landscape. The Informatics Transform presents five assertions as potential pathways to change, which will help libraries to re-position, re-profile, and re-structure to better address research data management challenges. The paper deconstructs the institutional research lifecycle and describes a portfolio of ten data support services which libraries can deliver to support the research lifecycle phases. Institutional roles and responsibilities for research data management are also unpacked, building on the framework from the earlier Dealing with Data Report. Finally, the paper examines critical capacity and capability challenges and proposes some innovative steps to addressing the significant skills gaps.",fullPaper,jv200
Engineering,p771,d0,0353cc70cbb1da71e921137463ca0f02094d67c6,c38,IEEE Global Engineering Education Conference,Virtual and augmented reality game-based applications to civil engineering education,"Gaming scenarios and virtual environments have shown beneficial results in Engineering Education. Various activities conducted in different fields demonstrate that students reveal appraisal for the integration of innovative technologies such as Virtual or Augmented Reality in the learning process. In this paper, Virtual Reality (VR) applications developed by first year students during an introductory class of the Integrated Masters in Civil Engineering are described. Additionally, two trials concerning the application of VR and Augmented Reality (AR) to Civil Engineering held at a local school (K-12 students) are also detailed. After the tests, students were surveyed and data was collected. Results and considerations are revealed in the final sections of this paper.",fullPaper,cp38
Computer Science,p771,d3,0353cc70cbb1da71e921137463ca0f02094d67c6,c38,IEEE Global Engineering Education Conference,Virtual and augmented reality game-based applications to civil engineering education,"Gaming scenarios and virtual environments have shown beneficial results in Engineering Education. Various activities conducted in different fields demonstrate that students reveal appraisal for the integration of innovative technologies such as Virtual or Augmented Reality in the learning process. In this paper, Virtual Reality (VR) applications developed by first year students during an introductory class of the Integrated Masters in Civil Engineering are described. Additionally, two trials concerning the application of VR and Augmented Reality (AR) to Civil Engineering held at a local school (K-12 students) are also detailed. After the tests, students were surveyed and data was collected. Results and considerations are revealed in the final sections of this paper.",fullPaper,cp38
Engineering,p772,d0,b6e9921477cf2e8ad95e5362654246245f082ab3,c61,"International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems",Multisensor Data Fusion,Abstract,fullPaper,cp61
Computer Science,p772,d3,b6e9921477cf2e8ad95e5362654246245f082ab3,c61,"International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems",Multisensor Data Fusion,Abstract,fullPaper,cp61
Engineering,p773,d0,536c08a5dbcbf75c14541fb15cbde4db8385ec7e,c44,Italian National Conference on Sensors,"Flexible Sketches and Inflexible Data Bases: Visual Communication, Conscription Devices, and Boundary Objects in Design Engineering","Engineering sketches and drawings are the building blocks of technological design and production. These visual representations act as the means for organizing the design to production process, hence serving as a ""social glue"" both between individuals and between groups. The author discusses two main capacities such visual representations serve in facilitating distributed cognition in team design work As conscription devices, they enlist and organize group participation. As boundary objects, they facilitate the reading of alternative meanings by various groups involved in the design process. The introduction of computer-aided design into this visual culture of engineering restructures relationships between workers in ways that can hamper the flexibility necessary for these crucial capacities to take place. The data are drawn from a study of the daily practices of engineers engaged in redesigning a turbine engine package. The method is participant observation.",poster,cp44
Engineering,p787,d0,eed2d7c523eaa5c3758ac2ff8697d7fc8173e926,c9,Big Data,The Uses of Big Data in Cities,"There is much enthusiasm currently about the possibilities created by new and more extensive sources of data to better understand and manage cities. Here, I explore how big data can be useful in urban planning by formalizing the planning process as a general computational problem. I show that, under general conditions, new sources of data coordinated with urban policy can be applied following fundamental principles of engineering to achieve new solutions to important age-old urban problems. I also show that comprehensive urban planning is computationally intractable (i.e., practically impossible) in large cities, regardless of the amounts of data available. This dilemma between the need for planning and coordination and its impossibility in detail is resolved by the recognition that cities are first and foremost self-organizing social networks embedded in space and enabled by urban infrastructure and services. As such, the primary role of big data in cities is to facilitate information flows and mechanisms of learning and coordination by heterogeneous individuals. However, processes of self-organization in cities, as well as of service improvement and expansion, must rely on general principles that enforce necessary conditions for cities to operate and evolve. Such ideas are the core of a developing scientific theory of cities, which is itself enabled by the growing availability of quantitative data on thousands of cities worldwide, across different geographies and levels of development. These three uses of data and information technologies in cities constitute then the necessary pillars for more successful urban policy and management that encourages, and does not stifle, the fundamental role of cities as engines of development and innovation in human societies.",fullPaper,cp9
Medicine,p787,d1,eed2d7c523eaa5c3758ac2ff8697d7fc8173e926,c9,Big Data,The Uses of Big Data in Cities,"There is much enthusiasm currently about the possibilities created by new and more extensive sources of data to better understand and manage cities. Here, I explore how big data can be useful in urban planning by formalizing the planning process as a general computational problem. I show that, under general conditions, new sources of data coordinated with urban policy can be applied following fundamental principles of engineering to achieve new solutions to important age-old urban problems. I also show that comprehensive urban planning is computationally intractable (i.e., practically impossible) in large cities, regardless of the amounts of data available. This dilemma between the need for planning and coordination and its impossibility in detail is resolved by the recognition that cities are first and foremost self-organizing social networks embedded in space and enabled by urban infrastructure and services. As such, the primary role of big data in cities is to facilitate information flows and mechanisms of learning and coordination by heterogeneous individuals. However, processes of self-organization in cities, as well as of service improvement and expansion, must rely on general principles that enforce necessary conditions for cities to operate and evolve. Such ideas are the core of a developing scientific theory of cities, which is itself enabled by the growing availability of quantitative data on thousands of cities worldwide, across different geographies and levels of development. These three uses of data and information technologies in cities constitute then the necessary pillars for more successful urban policy and management that encourages, and does not stifle, the fundamental role of cities as engines of development and innovation in human societies.",fullPaper,cp9
Computer Science,p787,d3,eed2d7c523eaa5c3758ac2ff8697d7fc8173e926,c9,Big Data,The Uses of Big Data in Cities,"There is much enthusiasm currently about the possibilities created by new and more extensive sources of data to better understand and manage cities. Here, I explore how big data can be useful in urban planning by formalizing the planning process as a general computational problem. I show that, under general conditions, new sources of data coordinated with urban policy can be applied following fundamental principles of engineering to achieve new solutions to important age-old urban problems. I also show that comprehensive urban planning is computationally intractable (i.e., practically impossible) in large cities, regardless of the amounts of data available. This dilemma between the need for planning and coordination and its impossibility in detail is resolved by the recognition that cities are first and foremost self-organizing social networks embedded in space and enabled by urban infrastructure and services. As such, the primary role of big data in cities is to facilitate information flows and mechanisms of learning and coordination by heterogeneous individuals. However, processes of self-organization in cities, as well as of service improvement and expansion, must rely on general principles that enforce necessary conditions for cities to operate and evolve. Such ideas are the core of a developing scientific theory of cities, which is itself enabled by the growing availability of quantitative data on thousands of cities worldwide, across different geographies and levels of development. These three uses of data and information technologies in cities constitute then the necessary pillars for more successful urban policy and management that encourages, and does not stifle, the fundamental role of cities as engines of development and innovation in human societies.",fullPaper,cp9
Engineering,p789,d0,72c7dce52ef232122cc62535f7bc81804fa3b417,c31,Information Security Solutions Europe,Factors of Safety and Reliability in Geotechnical Engineering,"Simple reliability analyses, involving neither complex theory nor unfamiliar terms, can be used in routine geotechnical engineering practice. These simple reliability analyses require little effort beyond that involved in conventional geotechnical analyses. They provide a means of evaluating the combined effects of uncertainties in the parameters involved in the calculations, and they offer a useful supplement to conventional analyses. The additional parameters needed for the reliability analyses—standard deviations of the parameters—can be evaluated using the same amount of data and types of correlations that are widely used in geotechnical engineering practice. Example applications to stability and settlement problems illustrate the simplicity and practical usefulness of the method.",poster,cp31
Engineering,p792,d0,c26fa4322ad54671e13917cfb8cca289a4ebafd9,c37,International Workshop on the Semantic Web,Introduction to the Multi-Disciplinary Engineering for Cyber-Physical Production Systems,Abstract,poster,cp37
Computer Science,p792,d3,c26fa4322ad54671e13917cfb8cca289a4ebafd9,c37,International Workshop on the Semantic Web,Introduction to the Multi-Disciplinary Engineering for Cyber-Physical Production Systems,Abstract,poster,cp37
Engineering,p796,d0,c4518592ff763d6746a197c2c5f3df2c4044d13d,c85,IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering,Guide to the Software Engineering Body of Knowledge,data types Sorting and searching parallel and distributed algorithms 3. [AR] Computer Architecture,poster,cp85
Engineering,p802,d0,f33aee10eb09666fdca5c0516143b1fbcf85bf22,c88,International Conference on Big Data Computing and Communications,The changing landscape of requirements engineering practices over the past decade,"Even though there is ample information available on solid requirements engineering practices, anecdotal evidence still indicates poor practices in industry. The key issue in implementing an improvement is to first identify the areas that need most improvement. Three surveys were conducted in 2003, 2008 and 2013 on the state of practice of requirements engineering. Surveys data obtained includes characteristics of projects, practices, organizations, and practitioners related to requirements engineering. In this paper we present a comparison and analysis of the responses from the three surveys in order to understand the changing land-scape of requirements engineering industrial practices over the past years.",poster,cp88
Computer Science,p802,d3,f33aee10eb09666fdca5c0516143b1fbcf85bf22,c88,International Conference on Big Data Computing and Communications,The changing landscape of requirements engineering practices over the past decade,"Even though there is ample information available on solid requirements engineering practices, anecdotal evidence still indicates poor practices in industry. The key issue in implementing an improvement is to first identify the areas that need most improvement. Three surveys were conducted in 2003, 2008 and 2013 on the state of practice of requirements engineering. Surveys data obtained includes characteristics of projects, practices, organizations, and practitioners related to requirements engineering. In this paper we present a comparison and analysis of the responses from the three surveys in order to understand the changing land-scape of requirements engineering industrial practices over the past years.",poster,cp88
Engineering,p824,d0,bd07ddc6111f2b3ee9bd1f6529ee9b9b32f7f82b,c36,International Conference on Information Technology Based Higher Education and Training,Engineering Design Processes: A Comparison of Students and Expert Practitioners,"In this paper we report on an in‐depth study of engineering design processes. Specifically, we extend our previous research on engineering student design processes to compare the design behavior of students and expert engineers. Nineteen experts from a variety of engineering disciplines and industries each designed a playground in a lab setting, and gave verbal reports of their thoughts during the design task. Measures of their design processes and solution quality were compared to pre‐existing data from 26 freshmen and 24 seniors. The experts spent significantly more time on the task overall and in each stage of engineering design, including significantly more time problem scoping. The experts also gathered significantly more information covering more categories. Results support the argument that problem scoping and information gathering are major differences between advanced engineers and students, and important competencies for engineering students to develop. Timeline representations of the expert designers' processes illustrate characteristic distinctions we found and may help students gain insights into their own design processes.",poster,cp36
Engineering,p848,d0,2c5c6a334cc2f2a2143e9dbcac3ab5a43720292d,c55,Design Automation Conference,Damage identification and health monitoring of structural and mechanical systems from changes in their vibration characteristics: A literature review,"This report contains a review of the technical literature concerning the detection, location, and characterization of structural damage via techniques that examine changes in measured structural vibration response. The report first categorizes the methods according to required measured data and analysis technique. The analysis categories include changes in modal frequencies, changes in measured mode shapes (and their derivatives), and changes in measured flexibility coefficients. Methods that use property (stiffness, mass, damping) matrix updating, detection of nonlinear response, and damage detection via neural networks are also summarized. The applications of the various methods to different types of engineering problems are categorized by type of structure and are summarized. The types of structures include beams, trusses, plates, shells, bridges, offshore platforms, other large civil structures, aerospace structures, and composite structures. The report describes the development of the damage-identification methods and applications and summarizes the current state-of-the-art of the technology. The critical issues for future research in the area of damage identification are also discussed.",poster,cp55
Engineering,p858,d0,1e513c7e99197dc30f9c97feabe0223afc786068,c6,Annual Conference on Genetic and Evolutionary Computation,Revisions to the JDL data fusion model,"The Data Fusion Model maintained by the Joint Directors of Laboratories (JDL) Data Fusion Group is the most widely-used method for categorizing data fusion-related functions. This paper discusses the current effort to revise the expand this model to facilitate the cost-effective development, acquisition, integration and operation of multi- sensor/multi-source systems. Data fusion involves combining information - in the broadest sense - to estimate or predict the state of some aspect of the universe. These may be represented in terms of attributive and relational states. If the job is to estimate the state of a people, it can be useful to include consideration of informational and perceptual states in addition to the physical state. Developing cost-effective multi-source information systems requires a method for specifying data fusion processing and control functions, interfaces, and associate databases. The lack of common engineering standards for data fusion systems has been a major impediment to integration and re-use of available technology: current developments do not lend themselves to objective evaluation, comparison or re-use. This paper reports on proposed revisions and expansions of the JDL Data FUsion model to remedy some of these deficiencies. This involves broadening the functional model and related taxonomy beyond the original military focus, and integrating the Data Fusion Tree Architecture model for system description, design and development.",poster,cp6
Computer Science,p858,d3,1e513c7e99197dc30f9c97feabe0223afc786068,c6,Annual Conference on Genetic and Evolutionary Computation,Revisions to the JDL data fusion model,"The Data Fusion Model maintained by the Joint Directors of Laboratories (JDL) Data Fusion Group is the most widely-used method for categorizing data fusion-related functions. This paper discusses the current effort to revise the expand this model to facilitate the cost-effective development, acquisition, integration and operation of multi- sensor/multi-source systems. Data fusion involves combining information - in the broadest sense - to estimate or predict the state of some aspect of the universe. These may be represented in terms of attributive and relational states. If the job is to estimate the state of a people, it can be useful to include consideration of informational and perceptual states in addition to the physical state. Developing cost-effective multi-source information systems requires a method for specifying data fusion processing and control functions, interfaces, and associate databases. The lack of common engineering standards for data fusion systems has been a major impediment to integration and re-use of available technology: current developments do not lend themselves to objective evaluation, comparison or re-use. This paper reports on proposed revisions and expansions of the JDL Data FUsion model to remedy some of these deficiencies. This involves broadening the functional model and related taxonomy beyond the original military focus, and integrating the Data Fusion Tree Architecture model for system description, design and development.",poster,cp6
Engineering,p859,d0,bc2b892b27ae9a9805e37d79d1986291c5b589d2,c45,IEEE Symposium on Security and Privacy,Data Fusion Approaches and Applications for Construction Engineering,"Data fusion can be defined as the process of combining data or information for estimating the state of an entity. Data fusion is a multidisciplinary field that has several benefits, such as enhancing the confidence, improving reliability, and reducing ambiguity of measurements for estimating the state of entities in engineering systems. It can also enhance completeness of fused data that may be required for estimating the state of engineering systems. Data fusion has been applied to different fields, such as robotics, automation, and intelligent systems. This paper reviews some examples of recent applications of data fusion in civil engineering and presents some of the potential benefits of using data fusion in civil engineering.",poster,cp45
Engineering,p864,d0,690c04954ca8752c72e5a2a06f10c7ffca3d299c,c100,IEEE International Conference on Computer Vision,"The Complete ISRM Suggested Methods for Rock Characterization, Testing and Monitoring; 1974–2006","Many engineering geologists and geological engineers have a vested interest in the broad spectrum of professional activities that constitute “rock engineering.” Important contributions to this greater body of knowledge have been provided by the fine, worldwide efforts of the International Society for Rock Mechanics (ISRM) over the past 34 years. While much of the published literature in the field is and has been theoretical, perhaps the larger part is of direct interest and use to practitioners. This long-needed revised compendium of the suggested methods for gathering site information of use to engineers designing structures on and in rock stands at the top of the list of useful technical literature in rock engineering.

The history of this effort is remarkable in itself. ISRM work products have historically been generated by its internal “commissions” as appointed by the leadership directorate, which are designed to bring forth practical solutions to recognized rock engineering data and methods needs. ISRM was founded in 1962, by Prof. Dr. Leopold Mueller of Karlsruhe University, then West Germany, who chose to release the Rock Testing Commission findings as separate papers (1974–1981) that have appeared in the International …",poster,cp100
Engineering,p866,d0,929607741b2a12656ff8d3360ca96fe76a6557a4,c2,International Conference on Software Engineering,Next Generation Science Standards,"Science and Engineering Practices that connect to garden-based education (all 8): • Asking questions (for science) and defining problems (for engineering) • Developing and using models • Planning and carrying out investigations • Analyzing and interpreting data • Using mathematics and computational thinking • Constructing explanations (for science) and designing solutions (for engineering) • Engaging in argument from evidence • Obtaining, evaluating, and communicating information",poster,cp2
Psychology,p866,d10,929607741b2a12656ff8d3360ca96fe76a6557a4,c2,International Conference on Software Engineering,Next Generation Science Standards,"Science and Engineering Practices that connect to garden-based education (all 8): • Asking questions (for science) and defining problems (for engineering) • Developing and using models • Planning and carrying out investigations • Analyzing and interpreting data • Using mathematics and computational thinking • Constructing explanations (for science) and designing solutions (for engineering) • Engaging in argument from evidence • Obtaining, evaluating, and communicating information",poster,cp2
Engineering,p871,d0,f5305cd90970a8917392f1a0e65c2ecf69325374,c16,International Conference on Data Science and Advanced Analytics,A summary review of wireless sensors and sensor networks for structural health monitoring,"In recent years, there has been an increasing interest in the adoption of emerging sensing technologies for instrumentation within a variety of structural systems. Wireless sensors and sensor networks are emerging as sensing paradigms that the structural engineering field has begun to consider as substitutes for traditional tethered monitoring systems. A benefit of wireless structural monitoring systems is that they are inexpensive to install because extensive wiring is no longer required between sensors and the data acquisition system. Researchers are discovering that wireless sensors are an exciting technology that should not be viewed as simply a substitute for traditional tethered monitoring systems. Rather, wireless sensors can play greater roles in the processing of structural response data; this feature can be utilized to screen data for signs of structural damage. Also, wireless sensors have limitations that require novel system architectures and modes of operation. This paper is intended to serve as a summary review of the collective experience the structural engineering community has gained from the use of wireless sensors and sensor networks for monitoring structural performance and health.",poster,cp16
Engineering,p873,d0,969bae558fa9a8d2c1daa7c8575852eaf82815f6,c37,International Workshop on the Semantic Web,DATA DIVISION FOR DEVELOPING NEURAL NETWORKS APPLIED TO GEOTECHNICAL ENGINEERING,"In recent years, artificial neural networks (ANNs) have been applied to many geotechnical engineering problems with some degree of success. In the majority of these applications, data division is carried out on an arbitrary basis. However, the way the data are divided can have a significant effect on model performance. In this paper, the issue of data division and its impact on ANN model performance is investigated for a case study of predicting the settlement of shallow foundations on granular soils. Four data division methods are investigated: (1) random data division; (2) data division to ensure statistical consistency of the subsets needed for ANN model development; (3) data division using self-organizing maps (SOMs); and (4) a new data division method using fuzzy clustering. The results indicate that the statistical properties of the data in the training, testing, and validation sets need to be taken into account to ensure that optimal model performance is achieved. It is also apparent from the results that the SOM and fuzzy clustering methods are suitable approaches for data division.",poster,cp37
Engineering,p874,d0,e23e287baf50b5b9a19774de6d6fb356b6bac212,c105,International Conference on Automatic Face and Gesture Recognition,Software intelligence: the future of mining software engineering data,"Mining software engineering data has emerged as a successful research direction over the past decade. In this position paper, we advocate Software Intelligence (SI) as the future of mining software engineering data, within modern software engineering research, practice, and education. We coin the name SI as an inspiration from the Business Intelligence (BI) field, which offers concepts and techniques to improve business decision making by using fact-based support systems. Similarly, SI offers software practitioners (not just developers) up-to-date and pertinent information to support their daily decision-making processes. SI should support decision-making processes throughout the lifetime of a software system not just during its development phase.
 The vision of SI has yet to become a reality that would enable software engineering research to have a strong impact on modern software practice. Nevertheless, recent advances in the Mining Software Repositories (MSR) field show great promise and provide strong support for realizing SI in the near future. This position paper summarizes the state of practice and research of SI, and lays out future research directions for mining software engineering data to enable SI.",poster,cp105
Computer Science,p874,d3,e23e287baf50b5b9a19774de6d6fb356b6bac212,c105,International Conference on Automatic Face and Gesture Recognition,Software intelligence: the future of mining software engineering data,"Mining software engineering data has emerged as a successful research direction over the past decade. In this position paper, we advocate Software Intelligence (SI) as the future of mining software engineering data, within modern software engineering research, practice, and education. We coin the name SI as an inspiration from the Business Intelligence (BI) field, which offers concepts and techniques to improve business decision making by using fact-based support systems. Similarly, SI offers software practitioners (not just developers) up-to-date and pertinent information to support their daily decision-making processes. SI should support decision-making processes throughout the lifetime of a software system not just during its development phase.
 The vision of SI has yet to become a reality that would enable software engineering research to have a strong impact on modern software practice. Nevertheless, recent advances in the Mining Software Repositories (MSR) field show great promise and provide strong support for realizing SI in the near future. This position paper summarizes the state of practice and research of SI, and lays out future research directions for mining software engineering data to enable SI.",poster,cp105
Engineering,p881,d0,4e0b35201060193709610bbe4a20e4a7cd597edb,c31,Information Security Solutions Europe,Methodologies for model-free data interpretation of civil engineering structures,Abstract,poster,cp31
Engineering,p883,d0,ca4d2075b3fc8bf7edd1d6a8041ead8400ba266d,c49,ACM/SIGCOMM Internet Measurement Conference,Quantitative Classification of Near-Fault Ground Motions Using Wavelet Analysis,"A method is described for quantitatively identifying ground motions containing strong velocity pulses, such as those caused by near-fault directivity. The approach uses wavelet analysis to extract the largest velocity pulse from a given ground motion. The size of the extracted pulse relative to the original ground motion is used to develop a quantitative criterion for classifying a ground motion as “pulselike.” The criterion is calibrated by using a training data set of manually classified ground motions. To identify the subset of these pulselike records of greatest engineering interest, two additional criteria are applied: the pulse arrives early in the ground motion and the absolute amplitude of the velocity pulse is large. The period of the velocity pulse (a quantity of interest to engineers) is easily determined as part of the procedure, using the pseudoperiods of the basis wavelets. This classification approach is useful for a variety of seismology and engineering topics where pulselike ground motions are of interest, such as probabilistic seismic hazard analysis, ground- motion prediction (“attenuation”) models, and nonlinear dynamic analysis of structures. The Next Generation Attenuation (nga) project ground motion library was processed using this approach, and 91 large-velocity pulses were found in the fault- normal components of the approximately 3500 strong ground motion recordings considered. It is believed that many of the identified pulses are caused by near-fault directivity effects. The procedure can be used as a stand-alone classification criterion or as a filter to identify ground motions deserving more careful study.",poster,cp49
Engineering,p887,d0,f874dc10896be9022cf93f63f4507c102ded2b74,j211,IEEE Transactions on Automation Science and Engineering,A Data-Level Fusion Model for Developing Composite Health Indices for Degradation Modeling and Prognostic Analysis,"Prognostics involves the effective utilization of condition or performance-based sensor signals to accurately estimate the remaining lifetime of partially degraded systems and components. The rapid development of sensor technology, has led to the use of multiple sensors to monitor the condition of an engineering system. It is therefore important to develop methodologies capable of integrating data from multiple sensors with the goal of improving the accuracy of predicting remaining lifetime. Although numerous efforts have focused on developing feature-level and decision-level fusion methodologies for prognostics, little research has targeted the development of “data-level” fusion models. In this paper, we present a methodology for constructing a composite health index for characterizing the performance of a system through the fusion of multiple degradation-based sensor data. This methodology includes data selection, data processing, and data fusion steps that lead to an improved degradation-based prognostic model. Our goal is that the composite health index provides a much better characterization of the condition of a system compared to relying solely on data from an individual sensor. Our methodology was evaluated through a case study involving a degradation dataset of an aircraft gas turbine engine that was generated by the Commercial Modular Aero-Propulsion System Simulation (C-MAPSS).",fullPaper,jv211
Computer Science,p887,d3,f874dc10896be9022cf93f63f4507c102ded2b74,j211,IEEE Transactions on Automation Science and Engineering,A Data-Level Fusion Model for Developing Composite Health Indices for Degradation Modeling and Prognostic Analysis,"Prognostics involves the effective utilization of condition or performance-based sensor signals to accurately estimate the remaining lifetime of partially degraded systems and components. The rapid development of sensor technology, has led to the use of multiple sensors to monitor the condition of an engineering system. It is therefore important to develop methodologies capable of integrating data from multiple sensors with the goal of improving the accuracy of predicting remaining lifetime. Although numerous efforts have focused on developing feature-level and decision-level fusion methodologies for prognostics, little research has targeted the development of “data-level” fusion models. In this paper, we present a methodology for constructing a composite health index for characterizing the performance of a system through the fusion of multiple degradation-based sensor data. This methodology includes data selection, data processing, and data fusion steps that lead to an improved degradation-based prognostic model. Our goal is that the composite health index provides a much better characterization of the condition of a system compared to relying solely on data from an individual sensor. Our methodology was evaluated through a case study involving a degradation dataset of an aircraft gas turbine engine that was generated by the Commercial Modular Aero-Propulsion System Simulation (C-MAPSS).",fullPaper,jv211
Engineering,p894,d0,0b70b3424193fa9a576f58fee7d9a6cc262f383f,c53,International Conference on Learning Representations,The Case for Fine-Grained Traffic Engineering in Data Centers,"In recent years, several techniques have been suggested for routing and traffic engineering in data centers. However, not much is known about how these techniques per-formrelative to each other under realistic data center traffic patterns. Our preliminary study reveals that existing techniques can only achieve 80% to 85% of the ideal solution in terms of the number of bytes delivered. We find that these techniques suffer due to their inability to utilize global knowledge of the properties of traffic flows and their inability to make coordinated decision for scheduling flows at fine time-scales. Even recent traffic engineering techniques such as COPE fail in data centers despite their proven ability to adapt to dynamic variations, because they are designed to operate at longer time scales (on the order of hours, at least). In contrast, data centers, due to the bursty nature inherent to their traffic, require adaptation at much finer times scales. To this end, we define a set of requirements that a data center-oriented traffic engineering technique must possess in order to successfully mitigate congestion. In this paper, we present the design for a strawman framework that fulfills these requirements.",poster,cp53
Computer Science,p894,d3,0b70b3424193fa9a576f58fee7d9a6cc262f383f,c53,International Conference on Learning Representations,The Case for Fine-Grained Traffic Engineering in Data Centers,"In recent years, several techniques have been suggested for routing and traffic engineering in data centers. However, not much is known about how these techniques per-formrelative to each other under realistic data center traffic patterns. Our preliminary study reveals that existing techniques can only achieve 80% to 85% of the ideal solution in terms of the number of bytes delivered. We find that these techniques suffer due to their inability to utilize global knowledge of the properties of traffic flows and their inability to make coordinated decision for scheduling flows at fine time-scales. Even recent traffic engineering techniques such as COPE fail in data centers despite their proven ability to adapt to dynamic variations, because they are designed to operate at longer time scales (on the order of hours, at least). In contrast, data centers, due to the bursty nature inherent to their traffic, require adaptation at much finer times scales. To this end, we define a set of requirements that a data center-oriented traffic engineering technique must possess in order to successfully mitigate congestion. In this paper, we present the design for a strawman framework that fulfills these requirements.",poster,cp53
Engineering,p900,d0,c3469ac1064a4045c1bb519a1247bb60c464ab8c,j217,Journal of Biological Engineering,Recent advances in 3D printing of biomaterials,Abstract,fullPaper,jv217
Medicine,p900,d1,c3469ac1064a4045c1bb519a1247bb60c464ab8c,j217,Journal of Biological Engineering,Recent advances in 3D printing of biomaterials,Abstract,fullPaper,jv217
Engineering,p906,d0,b18fb48c8004faa30e786b2d762d0a438a4573f7,c3,Knowledge Discovery and Data Mining,Clinical Biomechanics of the Spine,"Combining orthopedic surgery with biomechanical engineering, this reference and teaching text reviews and analyzes the clinical and scientific data on the mechanics of the human spine. This edition adds new material on vibration (i.e. road driving) and its effect on the spine; anatomy and kinematics",poster,cp3
Engineering,p910,d0,371c4477d6beab4efa5f8ce2ae7f4516a9f7889e,c3,Knowledge Discovery and Data Mining,Underground excavations in rock,"This book deals with the geotechnical aspects of the design of underground openings for mining and civil engineering purposes. It contains a number of worked examples to assist the reader in applying the techniques described to his or her own problems. The data are presented under the following chapter headings: (1) planning considerations; (2) classification of rock masses; (3) geological data collection; (4) graphical presentation of geological data; (5) stresses around underground excavations; (6) strength of rock and rock masses; (7) underground excavation failure mechanisms; (8) underground excavation support design; (9) rockbolts, shotcrete and mesh; (10) blasting in underground excavations; (11) instrumentation. Several appendices deal with: isometric drawing charts, stresses around single openings, two-dimensional boundary element stress analysis, determination of material constants, underground wedge analysis, and conversion factors. A very extensive bibliography is included.",poster,cp3
Engineering,p923,d0,e85acfee90971bf120ef7e8605855b01db08a833,c49,ACM/SIGCOMM Internet Measurement Conference,Multisensor Data Fusion,"Expanding the scope of the bestselling first edition, this new edition is now in two volumes. It contains nine new chapters and focuses on the most recent developments in the fusion of data in a variety of applications from military to automotive to medical. It provides information on mathematical techniques and computer methods employed to perform fusion. The set includes new material on target tracking and identification, situation refinement, consequence prediction, resource allocation and refinement, and human computer interaction and cognitive support. It provides new material on engineering issues such as fusion in distributed network systems, information security, and issues with service-oriented architectures.",poster,cp49
Computer Science,p923,d3,e85acfee90971bf120ef7e8605855b01db08a833,c49,ACM/SIGCOMM Internet Measurement Conference,Multisensor Data Fusion,"Expanding the scope of the bestselling first edition, this new edition is now in two volumes. It contains nine new chapters and focuses on the most recent developments in the fusion of data in a variety of applications from military to automotive to medical. It provides information on mathematical techniques and computer methods employed to perform fusion. The set includes new material on target tracking and identification, situation refinement, consequence prediction, resource allocation and refinement, and human computer interaction and cognitive support. It provides new material on engineering issues such as fusion in distributed network systems, information security, and issues with service-oriented architectures.",poster,cp49
Engineering,p924,d0,58ab642f01668d4d32ac1fb79d46acb6710a472f,c8,Frontiers in Education Conference,"Automation, Production Systems, and Computer-Integrated Manufacturing","This book provides the most advanced, comprehensive, and balanced coverage on the market of the technical and engineering aspects of automated production systems. It covers all the major cutting-edge technologies of production automation and material handling, and how these technologies are used to construct modern manufacturing systems. Manufacturing Operations; Industrial Control Systems; Sensors, Actuators, and Other Control System Components; Numerical Control; Industrial Robotics; Discrete Control Using Programmable Logic Controllers and Personal Computers; Material Transport Systems; Storage Systems; Automatic Data Capture; Single Station Manufacturing Cells; Group Technology and Cellular Manufacturing; Flexible Manufacturing Systems; Manual Assembly Lines; Transfer Lines and Similar Automated Manufacturing Systems; Automated Assembly Systems; Statistical Process Control; Inspection Principles and Practices; Inspection Technologies; Product Design and CAD/CAM in the Production System; Process Planning and Concurrent Engineering; Production Planning and Control Systems; and Lean Production and Agile Manufacturing. For anyone interested in Automation, Production Systems, and Computer-Integrated Manufacturing.",poster,cp8
Engineering,p927,d0,8d3e4d0ea9f81d045387bdea68ac38dbbcb54c6a,c1,International Conference on Human Factors in Computing Systems,Low-Speed Wind Tunnel Testing,"Wind Tunnels. Wind Tunnel Design. Pressure, Flow, and Shear Stress Measurements. Flow Visualization. Calibration of the Test Section. Forces and Moments from Balance Measurements. Use of Wind Tunnel Data: Scale Effects. Boundary Corrections I: Basics and Two- Dimensional Cases. Boundary Corrections II: Three-Dimensional Flow. Boundary Corrections III: Additional Applications. Additional Considerations for Aerodynamic Experiments. Aircraft and Aircraft Components. Ground Vehicles. Marine Vehicles. Wind Engineering. Small Wind Tunnels. Dynamic Tests. Appendices. Index.",poster,cp1
Engineering,p934,d0,4b8c870140a70aba606ebe320fd0be7ebc4abaa9,c84,EUROCON Conference,Genetic algorithms in engineering electromagnetics,"This paper presents a tutorial and overview of genetic algorithms for electromagnetic optimization. Genetic-algorithm (GA) optimizers are robust, stochastic search methods modeled on the concepts of natural selection and evolution. The relationship between traditional optimization techniques and the GA is discussed. Step-by-step implementation aspects of the GA are detailed, through an example with the objective of providing useful guidelines for the potential user. Extensive use is made of sidebars and graphical presentation to facilitate understanding. The tutorial is followed by a discussion of several electromagnetic applications in which the GA has proven useful. The applications discussed include the design of lightweight, broadband microwave absorbers, the reduction of array sidelobes in thinned arrays, the design of shaped-beam antenna arrays, the extraction of natural resonance modes of radar targets from backscattered response data, and the design of broadband patch antennas. Genetic-algorithm optimization is shown to be suitable for optimizing a broad class of problems of interest to the electromagnetic community. A comprehensive list of key references, organized by application category, is also provided.",poster,cp84
Engineering,p946,d0,651e43a8f88c15e13cb4b1567ec7828410bdcb34,c6,Annual Conference on Genetic and Evolutionary Computation,Mobile Communications Engineering,"From the Publisher: 
This authoritative work was written to be the bible for advanced commercial mobile phone systems and military mobile communications systems. Emphasizing how to study and analyze mobile communications problems,the book thoroughly explains how to design a mobile radio communication system. Introductory material describes existing systems,bridging the gap for students unfamiliar with the field. The characteristics of mobile radio signals are covered,applying statistical communication theory to propagation and received signal characteristics. Functional design is introduced,and a discussion of system performance included to explain how to evaluate a new system. The author uses both theory and experiments to support practical design and development of systems in the microwave frequency ranges configured for high-capacity mobile telephone and data communications.",poster,cp6
Engineering,p967,d0,558d8f66074cc791b5a6504354ac1280e3a2ebf1,c2,International Conference on Software Engineering,Mining Software Engineering Data,"Software engineering data (such as code bases, exe- cution traces, historical code changes, mailing lists, and bug databases) contains a wealth of information about a project¿s status, progress, and evolution. Using well- established data mining techniques, practitioners and re- searchers can explore the potential of this valuable data in order to better manage their projects and to produce higher-quality software systems that are delivered on time and within budget. This tutorial presents the latest research in mining Soft- ware Engineering (SE) data, discusses challenges associ- ated with mining SE data, highlights SE data mining suc- cess stories, and outlines future research directions. Partic- ipants will acquire knowledge and skills needed to perform research or conduct practice in the field and to integrate data mining techniques in their own research or practice.",fullPaper,cp2
Computer Science,p967,d3,558d8f66074cc791b5a6504354ac1280e3a2ebf1,c2,International Conference on Software Engineering,Mining Software Engineering Data,"Software engineering data (such as code bases, exe- cution traces, historical code changes, mailing lists, and bug databases) contains a wealth of information about a project¿s status, progress, and evolution. Using well- established data mining techniques, practitioners and re- searchers can explore the potential of this valuable data in order to better manage their projects and to produce higher-quality software systems that are delivered on time and within budget. This tutorial presents the latest research in mining Soft- ware Engineering (SE) data, discusses challenges associ- ated with mining SE data, highlights SE data mining suc- cess stories, and outlines future research directions. Partic- ipants will acquire knowledge and skills needed to perform research or conduct practice in the field and to integrate data mining techniques in their own research or practice.",fullPaper,cp2
Engineering,p970,d0,d6cce4139471cf88035748f03e1a6a1df1ab9140,j213,IEEE transactions on engineering management,Evaluating the Relative Performance of Engineering Design Projects: A Case Study Using Data Envelopment Analysis,"This paper presents a case study of how Data Envelopment Analysis (DEA) was applied to generate objective cross-project comparisons of project duration within an engineering department of the Belgian Armed Forces. To date, DEA has been applied to study projects within certain domains (e.g., software and R&D); however, DEA has not been proposed as a general project evaluation tool within the project management literature. In this case study, we demonstrate how DEA fills a gap not addressed by commonly applied project evaluation methods (such as earned value management) by allowing the objective comparison of projects on actual measures, such as duration and cost, by explicitly considering differences in key input characteristics across these projects. Thus, DEA can overcome the paradigm of project uniqueness and facilitate cross-project learning. We describe how DEA allowed the department to gain new insight about the impact of changes to its engineering design process (redesigned based on ISO 15288), creating a performance index that simultaneously considers project duration and key input variables that determine project duration. We conclude with directions for future research on the application of DEA as a project evaluation tool for project managers, program office managers, and other decision-makers in project-based organizations",fullPaper,jv213
Computer Science,p970,d3,d6cce4139471cf88035748f03e1a6a1df1ab9140,j213,IEEE transactions on engineering management,Evaluating the Relative Performance of Engineering Design Projects: A Case Study Using Data Envelopment Analysis,"This paper presents a case study of how Data Envelopment Analysis (DEA) was applied to generate objective cross-project comparisons of project duration within an engineering department of the Belgian Armed Forces. To date, DEA has been applied to study projects within certain domains (e.g., software and R&D); however, DEA has not been proposed as a general project evaluation tool within the project management literature. In this case study, we demonstrate how DEA fills a gap not addressed by commonly applied project evaluation methods (such as earned value management) by allowing the objective comparison of projects on actual measures, such as duration and cost, by explicitly considering differences in key input characteristics across these projects. Thus, DEA can overcome the paradigm of project uniqueness and facilitate cross-project learning. We describe how DEA allowed the department to gain new insight about the impact of changes to its engineering design process (redesigned based on ISO 15288), creating a performance index that simultaneously considers project duration and key input variables that determine project duration. We conclude with directions for future research on the application of DEA as a project evaluation tool for project managers, program office managers, and other decision-makers in project-based organizations",fullPaper,jv213
Engineering,p972,d0,c5b885b9b777933d351d621b1ecd3ccd44b6f93c,j234,IEEE Transactions on Information Forensics and Security,READ: Reverse Engineering of Automotive Data Frames,"Security analytics and forensics applied to in-vehicle networks are growing research areas that gained relevance after recent reports of cyber-attacks against unmodified licensed vehicles. However, the application of security analytics algorithms and tools to the automotive domain is hindered by the lack of public specifications about proprietary data exchanged over in-vehicle networks. Since the controller area network (CAN) bus is the de-facto standard for the interconnection of automotive electronic control units, the lack of public specifications for CAN messages is a key issue. This paper strives to solve this problem by proposing READ: a novel algorithm for the automatic Reverse Engineering of Automotive Data frames. READ has been designed to analyze traffic traces containing unknown CAN bus messages in order to automatically identify and label different types of signals encoded in the payload of their data frames. Experimental results based on CAN traffic gathered from a licensed unmodified vehicle and validated against its complete formal specifications demonstrate that the proposed algorithm can extract and classify more than twice the signals with respect to the previous related work. Moreover, the execution time of signal extraction and classification is reduced by two orders of magnitude. Applications of READ to CAN messages generated by real vehicles demonstrate its usefulness in the analysis of CAN traffic.",fullPaper,jv234
Computer Science,p972,d3,c5b885b9b777933d351d621b1ecd3ccd44b6f93c,j234,IEEE Transactions on Information Forensics and Security,READ: Reverse Engineering of Automotive Data Frames,"Security analytics and forensics applied to in-vehicle networks are growing research areas that gained relevance after recent reports of cyber-attacks against unmodified licensed vehicles. However, the application of security analytics algorithms and tools to the automotive domain is hindered by the lack of public specifications about proprietary data exchanged over in-vehicle networks. Since the controller area network (CAN) bus is the de-facto standard for the interconnection of automotive electronic control units, the lack of public specifications for CAN messages is a key issue. This paper strives to solve this problem by proposing READ: a novel algorithm for the automatic Reverse Engineering of Automotive Data frames. READ has been designed to analyze traffic traces containing unknown CAN bus messages in order to automatically identify and label different types of signals encoded in the payload of their data frames. Experimental results based on CAN traffic gathered from a licensed unmodified vehicle and validated against its complete formal specifications demonstrate that the proposed algorithm can extract and classify more than twice the signals with respect to the previous related work. Moreover, the execution time of signal extraction and classification is reduced by two orders of magnitude. Applications of READ to CAN messages generated by real vehicles demonstrate its usefulness in the analysis of CAN traffic.",fullPaper,jv234
Engineering,p974,d0,dbe217b10bdb05d0e6e218fb64519ea3df28d4ca,c51,International Conference on Engineering Education,"Parallel Control and Management for Intelligent Transportation Systems: Concepts, Architectures, and Applications","Parallel control and management have been proposed as a new mechanism for conducting operations of complex systems, especially those that involved complexity issues of both engineering and social dimensions, such as transportation systems. This paper presents an overview of the background, concepts, basic methods, major issues, and current applications of Parallel transportation Management Systems (PtMS). In essence, parallel control and management is a data-driven approach for modeling, analysis, and decision-making that considers both the engineering and social complexity in its processes. The developments and applications described here clearly indicate that PtMS is effective for use in networked complex traffic systems and is closely related to emerging technologies in cloud computing, social computing, and cyberphysical-social systems. A description of PtMS system architectures, processes, and components, including OTSt, Dyna CAS, aDAPTS, iTOP, and TransWorld is presented and discussed. Finally, the experiments and examples of real-world applications are illustrated and analyzed.",poster,cp51
Computer Science,p974,d3,dbe217b10bdb05d0e6e218fb64519ea3df28d4ca,c51,International Conference on Engineering Education,"Parallel Control and Management for Intelligent Transportation Systems: Concepts, Architectures, and Applications","Parallel control and management have been proposed as a new mechanism for conducting operations of complex systems, especially those that involved complexity issues of both engineering and social dimensions, such as transportation systems. This paper presents an overview of the background, concepts, basic methods, major issues, and current applications of Parallel transportation Management Systems (PtMS). In essence, parallel control and management is a data-driven approach for modeling, analysis, and decision-making that considers both the engineering and social complexity in its processes. The developments and applications described here clearly indicate that PtMS is effective for use in networked complex traffic systems and is closely related to emerging technologies in cloud computing, social computing, and cyberphysical-social systems. A description of PtMS system architectures, processes, and components, including OTSt, Dyna CAS, aDAPTS, iTOP, and TransWorld is presented and discussed. Finally, the experiments and examples of real-world applications are illustrated and analyzed.",poster,cp51
Engineering,p975,d0,2d7f06f53517dc30c49f59512bfbebd0ee653441,c80,International Symposium on Software Engineering for Adaptive and Self-Managing Systems,Engineering Sciences Data Unit (ESDU),"Aircraft tire performance on contaminated paved surface modeling by Engineering Sciences Data Unit (ESDU) is examined. Two elements naturally separate analytical work and the resulting mathematical model: retarding forces resulting from tire rolling and additional drag resulting from significant contaminant layer effects; and braking forces resulting from tire and surface interface. A case study of aircraft braking in loose snow is given. Limitations, including controllability and surfaces, are discussed. Inserts provide detail on state-of-the-art airfield friction measurement equipment and on ASFT, a family run business and leading surface friction measurement technology developer.",poster,cp80
Engineering,p983,d0,dab66642db0be57dba5359491601b8e968c39f6d,c91,International Symposium on High-Performance Computer Architecture,Creating Fragility Functions for Performance-Based Earthquake Engineering,"The Applied Technology Council is adapting PEER's performance-based earthquake engineering methodology to professional practice. The methodology's damage-analysis stage uses fragility functions to calculate the probability of damage to facility components given the force, deformation, or other engineering demand parameter (EDP) to which each is subjected. This paper introduces a set of procedures for creating fragility functions from various kinds of data: (A) actual EDP at which each specimen failed; (B) bounding EDP, in which some specimens failed and one knows the EDP to which each specimen was subjected; (C) capable EDP, where specimen EDPs are known but no specimens failed; (D) derived, where fragility functions are produced analytically; (E) expert opinion; and (U) updating, in which one improves an existing fragility function using new observations. Methods C, E, and U are all introduced here for the first time. A companion document offers additional procedures and more examples.",poster,cp91
Engineering,p991,d0,3eb3d244ed7e343a120ec35e03e282c196fefbdd,c33,Workshop on Python for High-Performance and Scientific Computing,Data Reduction Methods for Reverse Engineering,Abstract,poster,cp33
Engineering,p993,d0,0a55086018b3c3ee899359d6e2665255abac040e,c32,International Conference on Smart Data Services,Research Challenges for Data Mining in Science and Engineering,"With the rapid development of computer and information technology in the last several decades, an enormous amount of data in science and engineering has been and will continuously be generated in massive scale, either being stored in gigantic storage devices or flowing into and out of the system in the form of data streams. Moreover, such data has been made widely available, e.g., via the Internet. Such tremendous amount of data, in the order of terato petabytes, has fundamentally changed science and engineering, transforming many disciplines from data-poor to increasingly data-rich, and calling for new, data-intensive methods to conduct research in science and engineering. In this paper, we discuss the research challenges in science and engineering, from the data mining perspective, with a focus on the following issues: (1) information network analysis, (2) discovery, usage, and understanding of patterns and knowledge, (3) stream data mining, (4) mining moving object data, RFID data, and data from sensor networks, (5) spatiotemporal and multimedia data mining, (6) mining text, Web, and other unstructured data, (7) data cube-oriented multidimensional online analytical mining, (8) visual data mining, and (9) data mining by integration of sophisticated scientific and engineering domain knowledge.",poster,cp32
Computer Science,p993,d3,0a55086018b3c3ee899359d6e2665255abac040e,c32,International Conference on Smart Data Services,Research Challenges for Data Mining in Science and Engineering,"With the rapid development of computer and information technology in the last several decades, an enormous amount of data in science and engineering has been and will continuously be generated in massive scale, either being stored in gigantic storage devices or flowing into and out of the system in the form of data streams. Moreover, such data has been made widely available, e.g., via the Internet. Such tremendous amount of data, in the order of terato petabytes, has fundamentally changed science and engineering, transforming many disciplines from data-poor to increasingly data-rich, and calling for new, data-intensive methods to conduct research in science and engineering. In this paper, we discuss the research challenges in science and engineering, from the data mining perspective, with a focus on the following issues: (1) information network analysis, (2) discovery, usage, and understanding of patterns and knowledge, (3) stream data mining, (4) mining moving object data, RFID data, and data from sensor networks, (5) spatiotemporal and multimedia data mining, (6) mining text, Web, and other unstructured data, (7) data cube-oriented multidimensional online analytical mining, (8) visual data mining, and (9) data mining by integration of sophisticated scientific and engineering domain knowledge.",poster,cp32
Engineering,p994,d0,5b7112f3d38745ab81857e9a3f5ef3523e96a8cd,c22,Grid Computing Environments,Informal information for web-based engineering catalogues,"Success is highly dependent on the ability of a company to efficiently produce optimal designs. In order to achieve this companies must minimize time to market and possess the ability to make fully informed decisions at the early phase of the design process. Such decisions may include the choice of component and suppliers, as well as cost and maintenance considerations. Computer modeling and electronic catalogues are becoming the preferred medium for the selection and design of mechanical components. In utilizing these techniques, the designer demands the capability to identify, evaluate and select mechanical components both quantitatively and qualitatively. Quantitative decisions generally encompass performance data included in the formal catalogue representation. It is in the area of qualitative decisions that the use of what the authors call 'Informal Information' is of crucial importance. Thus, 'Informal Information' must often be incorporated into the selection process and selection systems. This would enable more informed decisions to be made quicker, without the need for information retrieval via discussion with colleagues in the design environment. This paper provides an overview of the use of electronic information in the design of mechanical systems, including a discussion of limitations of current technology. The importance of Informal Information is discussed and the requirements for association with web based electronic catalogues are developed. This system is based on a flexible XML schema and enables the storage, classification and recall of Informal Information packets. Furthermore, a strategy for the inclusion of Informal Information is proposed, and an example case is used to illustrate the benefits.",poster,cp22
Computer Science,p994,d3,5b7112f3d38745ab81857e9a3f5ef3523e96a8cd,c22,Grid Computing Environments,Informal information for web-based engineering catalogues,"Success is highly dependent on the ability of a company to efficiently produce optimal designs. In order to achieve this companies must minimize time to market and possess the ability to make fully informed decisions at the early phase of the design process. Such decisions may include the choice of component and suppliers, as well as cost and maintenance considerations. Computer modeling and electronic catalogues are becoming the preferred medium for the selection and design of mechanical components. In utilizing these techniques, the designer demands the capability to identify, evaluate and select mechanical components both quantitatively and qualitatively. Quantitative decisions generally encompass performance data included in the formal catalogue representation. It is in the area of qualitative decisions that the use of what the authors call 'Informal Information' is of crucial importance. Thus, 'Informal Information' must often be incorporated into the selection process and selection systems. This would enable more informed decisions to be made quicker, without the need for information retrieval via discussion with colleagues in the design environment. This paper provides an overview of the use of electronic information in the design of mechanical systems, including a discussion of limitations of current technology. The importance of Informal Information is discussed and the requirements for association with web based electronic catalogues are developed. This system is based on a flexible XML schema and enables the storage, classification and recall of Informal Information packets. Furthermore, a strategy for the inclusion of Informal Information is proposed, and an example case is used to illustrate the benefits.",poster,cp22
Engineering,p996,d0,c7bc5a5244f7e419ca2c8c33af1961bab9fc812d,c58,Extreme Science and Engineering Discovery Environment,Practical Applications of Time-lapse Seismic Data,"Time-lapse (4D) seismic technology is a key enabler for improved hydrocarbon recovery and more cost-effective field operations. Practical Applications of Time-lapse Seismic Data (SEG Distinguished Instructor Series No. 16) shows how 4D seismic data are used for reservoir surveillance, how they provide valuable insight on dynamic reservoir properties such as fluid saturation, pressure, and temperature, and how they add value to reservoir management. The material, based on the 2013 SEG Distinguished Instructor Short Course, includes discussions of reservoir-engineering concepts and rock physics critical to the understanding of 4D data, along with topics in 4D seismic acquisition and processing. A primary focus of the book is interpretation and data integration. Case-study examples are used to demonstrate key concepts and are drawn on to demonstrate the range of interpretation methods currently employed by industry and the diversity of geologic settings and production scenarios in which 4D is making a difference. Time-lapse seismic interpretation is inherently integrative, drawing on geophysical, geologic, and reservoir-engineering data and concepts. As a result, this book should be of interest to individuals from all subsurface disciplines.",poster,cp58
Engineering,p997,d0,d7e34d62bea98da5dda9450920886ff0d174fe28,c1,International Conference on Human Factors in Computing Systems,"Engineering Rock Mass Classifications: A Complete Manual for Engineers and Geologists in Mining, Civil, and Petroleum Engineering",Role of Rock Mass Classifications in Site Characterization and Engineering Design. Early Rock Mass Classifications. Geomechanics Classification (Rock Mass Rating System). Q--System. Other Classifications. Applications in Tunneling. Applications in Mining. Other Applications. Case Histories Data Base. Appendix. Bibliography. Index.,poster,cp1
Engineering,p1002,d0,f117c6f12d067bd66dad40996b3931c069daa2da,c116,International Society for Music Information Retrieval Conference,Business Intelligence and Analytics: From Big Data to Big Impact,"Business intelligence and analytics (BI&A) has emerged as an important area of study for both practitioners and researchers, reflecting the magnitude and impact of data-related problems to be solved in contemporary business organizations. This introduction to the MIS Quarterly Special Issue on Business Intelligence Research first provides a framework that identifies the evolution, applications, and emerging research areas of BI&A. BI&A 1.0, BI&A 2.0, and BI&A 3.0 are defined and described in terms of their key characteristics and capabilities. Current research in BI&A is analyzed and challenges and opportunities associated with BI&A research and education are identified. We also report a bibliometric study of critical BI&A publications, researchers, and research topics based on more than a decade of related academic and industry publications. Finally, the six articles that comprise this special issue are introduced and characterized in terms of the proposed BI&A research framework.",poster,cp116
Computer Science,p1002,d3,f117c6f12d067bd66dad40996b3931c069daa2da,c116,International Society for Music Information Retrieval Conference,Business Intelligence and Analytics: From Big Data to Big Impact,"Business intelligence and analytics (BI&A) has emerged as an important area of study for both practitioners and researchers, reflecting the magnitude and impact of data-related problems to be solved in contemporary business organizations. This introduction to the MIS Quarterly Special Issue on Business Intelligence Research first provides a framework that identifies the evolution, applications, and emerging research areas of BI&A. BI&A 1.0, BI&A 2.0, and BI&A 3.0 are defined and described in terms of their key characteristics and capabilities. Current research in BI&A is analyzed and challenges and opportunities associated with BI&A research and education are identified. We also report a bibliometric study of critical BI&A publications, researchers, and research topics based on more than a decade of related academic and industry publications. Finally, the six articles that comprise this special issue are introduced and characterized in terms of the proposed BI&A research framework.",poster,cp116
Engineering,p1011,d0,94deb62af3054c49e7d80bd7eb3ed5efe990fc0b,c18,International Conference on Exploring Services Science,Traffic Flow Prediction With Big Data: A Deep Learning Approach,"Accurate and timely traffic flow information is important for the successful deployment of intelligent transportation systems. Over the last few years, traffic data have been exploding, and we have truly entered the era of big data for transportation. Existing traffic flow prediction methods mainly use shallow traffic prediction models and are still unsatisfying for many real-world applications. This situation inspires us to rethink the traffic flow prediction problem based on deep architecture models with big traffic data. In this paper, a novel deep-learning-based traffic flow prediction method is proposed, which considers the spatial and temporal correlations inherently. A stacked autoencoder model is used to learn generic traffic flow features, and it is trained in a greedy layerwise fashion. To the best of our knowledge, this is the first time that a deep architecture model is applied using autoencoders as building blocks to represent traffic flow features for prediction. Moreover, experiments demonstrate that the proposed method for traffic flow prediction has superior performance.",poster,cp18
Computer Science,p1011,d3,94deb62af3054c49e7d80bd7eb3ed5efe990fc0b,c18,International Conference on Exploring Services Science,Traffic Flow Prediction With Big Data: A Deep Learning Approach,"Accurate and timely traffic flow information is important for the successful deployment of intelligent transportation systems. Over the last few years, traffic data have been exploding, and we have truly entered the era of big data for transportation. Existing traffic flow prediction methods mainly use shallow traffic prediction models and are still unsatisfying for many real-world applications. This situation inspires us to rethink the traffic flow prediction problem based on deep architecture models with big traffic data. In this paper, a novel deep-learning-based traffic flow prediction method is proposed, which considers the spatial and temporal correlations inherently. A stacked autoencoder model is used to learn generic traffic flow features, and it is trained in a greedy layerwise fashion. To the best of our knowledge, this is the first time that a deep architecture model is applied using autoencoders as building blocks to represent traffic flow features for prediction. Moreover, experiments demonstrate that the proposed method for traffic flow prediction has superior performance.",poster,cp18
Engineering,p1016,d0,1597449a7f64b6bd24639b4deab96c8a8c184177,j244,The International Journal of Advanced Manufacturing Technology,"Digital twin-driven product design, manufacturing and service with big data",Abstract,fullPaper,jv244
Engineering,p1064,d0,cc1e82125f7f8636b25ccdcdb63e8f812add7f87,j260,IEEE Transactions on Big Data,A Big Data Enabled Channel Model for 5G Wireless Communication Systems,"The standardization process of the fifth generation (5G) wireless communications has recently been accelerated and the first commercial 5G services would be provided as early as in 2018. The increasing of enormous smartphones, new complex scenarios, large frequency bands, massive antenna elements, and dense small cells will generate big datasets and bring 5G communications to the era of big data. This paper investigates various applications of big data analytics, especially machine learning algorithms in wireless communications and channel modeling. We propose a big data and machine learning enabled wireless channel model framework. The proposed channel model is based on artificial neural networks (ANNs), including feed-forward neural network (FNN) and radial basis function neural network (RBF-NN). The input parameters are transmitter (Tx) and receiver (Rx) coordinates, Tx–Rx distance, and carrier frequency, while the output parameters are channel statistical properties, including the received power, root mean square (RMS) delay spread (DS), and RMS angle spreads (ASs). Datasets used to train and test the ANNs are collected from both real channel measurements and a geometry based stochastic model (GBSM). Simulation results show good performance and indicate that machine learning algorithms can be powerful analytical tools for future measurement-based wireless channel modeling.",fullPaper,jv260
Computer Science,p1064,d3,cc1e82125f7f8636b25ccdcdb63e8f812add7f87,j260,IEEE Transactions on Big Data,A Big Data Enabled Channel Model for 5G Wireless Communication Systems,"The standardization process of the fifth generation (5G) wireless communications has recently been accelerated and the first commercial 5G services would be provided as early as in 2018. The increasing of enormous smartphones, new complex scenarios, large frequency bands, massive antenna elements, and dense small cells will generate big datasets and bring 5G communications to the era of big data. This paper investigates various applications of big data analytics, especially machine learning algorithms in wireless communications and channel modeling. We propose a big data and machine learning enabled wireless channel model framework. The proposed channel model is based on artificial neural networks (ANNs), including feed-forward neural network (FNN) and radial basis function neural network (RBF-NN). The input parameters are transmitter (Tx) and receiver (Rx) coordinates, Tx–Rx distance, and carrier frequency, while the output parameters are channel statistical properties, including the received power, root mean square (RMS) delay spread (DS), and RMS angle spreads (ASs). Datasets used to train and test the ANNs are collected from both real channel measurements and a geometry based stochastic model (GBSM). Simulation results show good performance and indicate that machine learning algorithms can be powerful analytical tools for future measurement-based wireless channel modeling.",fullPaper,jv260
Engineering,p1130,d0,a1352af5cdf57823a772efb81b93307db709c128,j280,IEEE Systems Journal,Health-CPS: Healthcare Cyber-Physical System Assisted by Cloud and Big Data,"The advances in information technology have witnessed great progress on healthcare technologies in various domains nowadays. However, these new technologies have also made healthcare data not only much bigger but also much more difficult to handle and process. Moreover, because the data are created from a variety of devices within a short time span, the characteristics of these data are that they are stored in different formats and created quickly, which can, to a large extent, be regarded as a big data problem. To provide a more convenient service and environment of healthcare, this paper proposes a cyber-physical system for patient-centric healthcare applications and services, called Health-CPS, built on cloud and big data analytics technologies. This system consists of a data collection layer with a unified standard, a data management layer for distributed storage and parallel computing, and a data-oriented service layer. The results of this study show that the technologies of cloud and big data can be used to enhance the performance of the healthcare system so that humans can then enjoy various smart healthcare applications and services.",fullPaper,jv280
Computer Science,p1130,d3,a1352af5cdf57823a772efb81b93307db709c128,j280,IEEE Systems Journal,Health-CPS: Healthcare Cyber-Physical System Assisted by Cloud and Big Data,"The advances in information technology have witnessed great progress on healthcare technologies in various domains nowadays. However, these new technologies have also made healthcare data not only much bigger but also much more difficult to handle and process. Moreover, because the data are created from a variety of devices within a short time span, the characteristics of these data are that they are stored in different formats and created quickly, which can, to a large extent, be regarded as a big data problem. To provide a more convenient service and environment of healthcare, this paper proposes a cyber-physical system for patient-centric healthcare applications and services, called Health-CPS, built on cloud and big data analytics technologies. This system consists of a data collection layer with a unified standard, a data management layer for distributed storage and parallel computing, and a data-oriented service layer. The results of this study show that the technologies of cloud and big data can be used to enhance the performance of the healthcare system so that humans can then enjoy various smart healthcare applications and services.",fullPaper,jv280
Engineering,p1147,d0,bc0fd9a19026326a07295525599503a01b02f4a4,j286,Journal of strategic information systems,Debating big data: A literature review on realizing value from big data,Abstract,fullPaper,jv286
Computer Science,p1147,d3,bc0fd9a19026326a07295525599503a01b02f4a4,j286,Journal of strategic information systems,Debating big data: A literature review on realizing value from big data,Abstract,fullPaper,jv286
Engineering,p1151,d0,2d2fe4a73c98933ae9b8df73c8452b0d8be6475e,c3,Knowledge Discovery and Data Mining,An Intelligent Fault Diagnosis Method Using Unsupervised Feature Learning Towards Mechanical Big Data,"Intelligent fault diagnosis is a promising tool to deal with mechanical big data due to its ability in rapidly and efficiently processing collected signals and providing accurate diagnosis results. In traditional intelligent diagnosis methods, however, the features are manually extracted depending on prior knowledge and diagnostic expertise. Such processes take advantage of human ingenuity but are time-consuming and labor-intensive. Inspired by the idea of unsupervised feature learning that uses artificial intelligence techniques to learn features from raw data, a two-stage learning method is proposed for intelligent diagnosis of machines. In the first learning stage of the method, sparse filtering, an unsupervised two-layer neural network, is used to directly learn features from mechanical vibration signals. In the second stage, softmax regression is employed to classify the health conditions based on the learned features. The proposed method is validated by a motor bearing dataset and a locomotive bearing dataset, respectively. The results show that the proposed method obtains fairly high diagnosis accuracies and is superior to the existing methods for the motor bearing dataset. Because of learning features adaptively, the proposed method reduces the need of human labor and makes intelligent fault diagnosis handle big data more easily.",poster,cp3
Computer Science,p1151,d3,2d2fe4a73c98933ae9b8df73c8452b0d8be6475e,c3,Knowledge Discovery and Data Mining,An Intelligent Fault Diagnosis Method Using Unsupervised Feature Learning Towards Mechanical Big Data,"Intelligent fault diagnosis is a promising tool to deal with mechanical big data due to its ability in rapidly and efficiently processing collected signals and providing accurate diagnosis results. In traditional intelligent diagnosis methods, however, the features are manually extracted depending on prior knowledge and diagnostic expertise. Such processes take advantage of human ingenuity but are time-consuming and labor-intensive. Inspired by the idea of unsupervised feature learning that uses artificial intelligence techniques to learn features from raw data, a two-stage learning method is proposed for intelligent diagnosis of machines. In the first learning stage of the method, sparse filtering, an unsupervised two-layer neural network, is used to directly learn features from mechanical vibration signals. In the second stage, softmax regression is employed to classify the health conditions based on the learned features. The proposed method is validated by a motor bearing dataset and a locomotive bearing dataset, respectively. The results show that the proposed method obtains fairly high diagnosis accuracies and is superior to the existing methods for the motor bearing dataset. Because of learning features adaptively, the proposed method reduces the need of human labor and makes intelligent fault diagnosis handle big data more easily.",poster,cp3
Engineering,p1159,d0,0b842f5d17165280687784ccf6b844242f7bfb7c,j291,Computers and Electronics in Agriculture,A review on the practice of big data analysis in agriculture,Abstract,fullPaper,jv291
Computer Science,p1159,d3,0b842f5d17165280687784ccf6b844242f7bfb7c,j291,Computers and Electronics in Agriculture,A review on the practice of big data analysis in agriculture,Abstract,fullPaper,jv291
Engineering,p1161,d0,c7f686b1047d6519ea1f90c8e3bfd6e7e2d7aadf,j292,Information Manager (The),Toward the development of a big data analytics capability,Abstract,fullPaper,jv292
Computer Science,p1161,d3,c7f686b1047d6519ea1f90c8e3bfd6e7e2d7aadf,j292,Information Manager (The),Toward the development of a big data analytics capability,Abstract,fullPaper,jv292
Engineering,p1176,d0,5ebf42bc9c25a1baf09f18bfade506644a930fea,j190,IEEE Network,Big Data Driven Vehicular Networks,"VANETs enable information exchange among vehicles, other end devices and public networks, which plays a key role in road safety/infotainment, intelligent transportation systems, and self-driving systems. As vehicular connectivity soars, and new on-road mobile applications and technologies emerge, VANETs are generating an ever-increasing amount of data, requiring fast and reliable transmissions through VANETs. On the other hand, a variety of VANETs related data can be analyzed and utilized to improve the performance of VANETs. In this article, we first review VANETs technologies to efficiently and reliably transmit big data. Then, the methods employing big data for studying VANETs characteristics and improving VANETs performance are discussed. Furthermore, we present a case study where machine learning schemes are applied to analyze VANETs measurement data for efficiently",fullPaper,jv190
Computer Science,p1176,d3,5ebf42bc9c25a1baf09f18bfade506644a930fea,j190,IEEE Network,Big Data Driven Vehicular Networks,"VANETs enable information exchange among vehicles, other end devices and public networks, which plays a key role in road safety/infotainment, intelligent transportation systems, and self-driving systems. As vehicular connectivity soars, and new on-road mobile applications and technologies emerge, VANETs are generating an ever-increasing amount of data, requiring fast and reliable transmissions through VANETs. On the other hand, a variety of VANETs related data can be analyzed and utilized to improve the performance of VANETs. In this article, we first review VANETs technologies to efficiently and reliably transmit big data. Then, the methods employing big data for studying VANETs characteristics and improving VANETs performance are discussed. Furthermore, we present a case study where machine learning schemes are applied to analyze VANETs measurement data for efficiently",fullPaper,jv190
Engineering,p1190,d0,a7e7c77b07d88dfc4937b189a308a236ac120d0e,j65,International Journal of Information Management,The role of big data in smart city,Abstract,fullPaper,jv65
Computer Science,p1190,d3,a7e7c77b07d88dfc4937b189a308a236ac120d0e,j65,International Journal of Information Management,The role of big data in smart city,Abstract,fullPaper,jv65
Engineering,p1194,d0,7403e177957cbb51f17018210da02d2ceab88f8a,c9,Big Data,Diversity in Big Data: A Review,"Big data technology offers unprecedented opportunities to society as a whole and also to its individual members. At the same time, this technology poses significant risks to those it overlooks. In this article, we give an overview of recent technical work on diversity, particularly in selection tasks, discuss connections between diversity and fairness, and identify promising directions for future work that will position diversity as an important component of a data-responsible society. We argue that diversity should come to the forefront of our discourse, for reasons that are both ethical-to mitigate the risks of exclusion-and utilitarian, to enable more powerful, accurate, and engaging data analysis and use.",fullPaper,cp9
Medicine,p1194,d1,7403e177957cbb51f17018210da02d2ceab88f8a,c9,Big Data,Diversity in Big Data: A Review,"Big data technology offers unprecedented opportunities to society as a whole and also to its individual members. At the same time, this technology poses significant risks to those it overlooks. In this article, we give an overview of recent technical work on diversity, particularly in selection tasks, discuss connections between diversity and fairness, and identify promising directions for future work that will position diversity as an important component of a data-responsible society. We argue that diversity should come to the forefront of our discourse, for reasons that are both ethical-to mitigate the risks of exclusion-and utilitarian, to enable more powerful, accurate, and engaging data analysis and use.",fullPaper,cp9
Computer Science,p1194,d3,7403e177957cbb51f17018210da02d2ceab88f8a,c9,Big Data,Diversity in Big Data: A Review,"Big data technology offers unprecedented opportunities to society as a whole and also to its individual members. At the same time, this technology poses significant risks to those it overlooks. In this article, we give an overview of recent technical work on diversity, particularly in selection tasks, discuss connections between diversity and fairness, and identify promising directions for future work that will position diversity as an important component of a data-responsible society. We argue that diversity should come to the forefront of our discourse, for reasons that are both ethical-to mitigate the risks of exclusion-and utilitarian, to enable more powerful, accurate, and engaging data analysis and use.",fullPaper,cp9
Engineering,p1201,d0,6d97dca6deb258c30381956e9af93b12bd5c6d03,j219,International Journal of Production Research,Big Data Analytics for Physical Internet-based intelligent manufacturing shop floors,"Physical Internet (PI, π) has been widely used for transforming and upgrading the logistics and supply chain management worldwide. This study extends the PI concept into manufacturing shop floors where typical logistics resources are converted into smart manufacturing objects (SMOs) using Internet of Things (IoT) and wireless technologies to create a RFID-enabled intelligent shop floor environment. In such PI-based environment, enormous RFID data could be captured and collected. This study introduces a Big Data Analytics for RFID logistics data by defining different behaviours of SMOs. Several findings are significant. It is observed that task weight is primarily considered in the logistics decision-making in this case. Additionally, the highest residence time occurs in a buffer with the value of 12.17 (unit of time) which is 40.57% of the total delivery time. That implies the high work-in-progress inventory level in this buffer. Key findings and observations are generated into managerial implications, which are useful for various users to make logistics decisions under PI-enabled intelligent shop floors.",fullPaper,jv219
Computer Science,p1201,d3,6d97dca6deb258c30381956e9af93b12bd5c6d03,j219,International Journal of Production Research,Big Data Analytics for Physical Internet-based intelligent manufacturing shop floors,"Physical Internet (PI, π) has been widely used for transforming and upgrading the logistics and supply chain management worldwide. This study extends the PI concept into manufacturing shop floors where typical logistics resources are converted into smart manufacturing objects (SMOs) using Internet of Things (IoT) and wireless technologies to create a RFID-enabled intelligent shop floor environment. In such PI-based environment, enormous RFID data could be captured and collected. This study introduces a Big Data Analytics for RFID logistics data by defining different behaviours of SMOs. Several findings are significant. It is observed that task weight is primarily considered in the logistics decision-making in this case. Additionally, the highest residence time occurs in a buffer with the value of 12.17 (unit of time) which is 40.57% of the total delivery time. That implies the high work-in-progress inventory level in this buffer. Key findings and observations are generated into managerial implications, which are useful for various users to make logistics decisions under PI-enabled intelligent shop floors.",fullPaper,jv219
Engineering,p1204,d0,8566f36d7917d4bd5705e1f7dff39197e2d22817,j258,IEEE Transactions on Industrial Informatics,A Manufacturing Big Data Solution for Active Preventive Maintenance,"Industry 4.0 has become more popular due to recent developments in cyber-physical systems, big data, cloud computing, and industrial wireless networks. Intelligent manufacturing has produced a revolutionary change, and evolving applications, such as product lifecycle management, are becoming a reality. In this paper, we propose and implement a manufacturing big data solution for active preventive maintenance in manufacturing environments. First, we provide the system architecture that is used for active preventive maintenance. Then, we analyze the method used for collection of manufacturing big data according to the data characteristics. Subsequently, we perform data processing in the cloud, including the cloud layer architecture, the real-time active maintenance mechanism, and the offline prediction and analysis method. Finally, we analyze a prototype platform and implement experiments to compare the traditionally used method with the proposed active preventive maintenance method. The manufacturing big data method used for active preventive maintenance has the potential to accelerate implementation of Industry 4.0.",fullPaper,jv258
Computer Science,p1204,d3,8566f36d7917d4bd5705e1f7dff39197e2d22817,j258,IEEE Transactions on Industrial Informatics,A Manufacturing Big Data Solution for Active Preventive Maintenance,"Industry 4.0 has become more popular due to recent developments in cyber-physical systems, big data, cloud computing, and industrial wireless networks. Intelligent manufacturing has produced a revolutionary change, and evolving applications, such as product lifecycle management, are becoming a reality. In this paper, we propose and implement a manufacturing big data solution for active preventive maintenance in manufacturing environments. First, we provide the system architecture that is used for active preventive maintenance. Then, we analyze the method used for collection of manufacturing big data according to the data characteristics. Subsequently, we perform data processing in the cloud, including the cloud layer architecture, the real-time active maintenance mechanism, and the offline prediction and analysis method. Finally, we analyze a prototype platform and implement experiments to compare the traditionally used method with the proposed active preventive maintenance method. The manufacturing big data method used for active preventive maintenance has the potential to accelerate implementation of Industry 4.0.",fullPaper,jv258
Engineering,p1210,d0,3eb25255a46bb99c8fb2dd10fa547f1bc19810bb,j307,Electronic Markets,Big data analytics in E-commerce: a systematic review and agenda for future research,Abstract,fullPaper,jv307
Computer Science,p1210,d3,3eb25255a46bb99c8fb2dd10fa547f1bc19810bb,j307,Electronic Markets,Big data analytics in E-commerce: a systematic review and agenda for future research,Abstract,fullPaper,jv307
Engineering,p1213,d0,9e6aee8a492c009c91bbc52a7bc0e4d481dc2d4a,j60,Nature,Smart manufacturing must embrace big data,Abstract,fullPaper,jv60
Medicine,p1213,d1,9e6aee8a492c009c91bbc52a7bc0e4d481dc2d4a,j60,Nature,Smart manufacturing must embrace big data,Abstract,fullPaper,jv60
Engineering,p1215,d0,37d9283061bb8057adff53ff4033dd11ccdf2a0c,c84,EUROCON Conference,Blockchain solutions for big data challenges: A literature review,"The popularity of Blockchain technology and the huge extent of its application, results with much ongoing research in different practical and scientific areas. Although still new and in experimenting phase, the Blockchain is being seen as a revolutionary solution, addressing modern technology concerns like decentralization, trust, identity, data ownership and data-driven decisions. At the same time, the world is facing an expansion in quantity and diversity of digital data that are generated by both users and machines. While actively searching for the best way to store, organize and process Big Data, the Blockchain technology comes in providing significant input. Its proposed solutions about decentralized management of private data, digital property resolution, IoT communication and public institutions' reforms are having significant impact on how Big Data may evolve. This paper presents the novel solutions associated with some of the Big Data areas that can be empowered by the Blockchain technology.",fullPaper,cp84
Computer Science,p1215,d3,37d9283061bb8057adff53ff4033dd11ccdf2a0c,c84,EUROCON Conference,Blockchain solutions for big data challenges: A literature review,"The popularity of Blockchain technology and the huge extent of its application, results with much ongoing research in different practical and scientific areas. Although still new and in experimenting phase, the Blockchain is being seen as a revolutionary solution, addressing modern technology concerns like decentralization, trust, identity, data ownership and data-driven decisions. At the same time, the world is facing an expansion in quantity and diversity of digital data that are generated by both users and machines. While actively searching for the best way to store, organize and process Big Data, the Blockchain technology comes in providing significant input. Its proposed solutions about decentralized management of private data, digital property resolution, IoT communication and public institutions' reforms are having significant impact on how Big Data may evolve. This paper presents the novel solutions associated with some of the Big Data areas that can be empowered by the Blockchain technology.",fullPaper,cp84
Engineering,p1231,d0,db0175df5df3deb48753545625ca003fe3dd2640,j311,Economics of Innovation and New Technology,BIG data – BIG gains? Understanding the link between big data analytics and innovation,"ABSTRACT This paper analyzes the relationship between firms' use of big data analytics and their innovative performance in terms of product innovations. Since big data technologies provide new data information practices, they create novel decision-making possibilities, which are widely believed to support firms' innovation process. Applying German firm-level data within a knowledge production function framework we find suggestive evidence that big data analytics is a relevant determinant for the likelihood of a firm becoming a product innovator as well as for the market success of product innovations. These results hold for the manufacturing as well as for the service sector but are contingent on firms' investment in IT-specific skills. Overall, the results support the view that big data analytics have the potential to enable innovation.",fullPaper,jv311
Business,p1231,d9,db0175df5df3deb48753545625ca003fe3dd2640,j311,Economics of Innovation and New Technology,BIG data – BIG gains? Understanding the link between big data analytics and innovation,"ABSTRACT This paper analyzes the relationship between firms' use of big data analytics and their innovative performance in terms of product innovations. Since big data technologies provide new data information practices, they create novel decision-making possibilities, which are widely believed to support firms' innovation process. Applying German firm-level data within a knowledge production function framework we find suggestive evidence that big data analytics is a relevant determinant for the likelihood of a firm becoming a product innovator as well as for the market success of product innovations. These results hold for the manufacturing as well as for the service sector but are contingent on firms' investment in IT-specific skills. Overall, the results support the view that big data analytics have the potential to enable innovation.",fullPaper,jv311
Economics,p1231,d11,db0175df5df3deb48753545625ca003fe3dd2640,j311,Economics of Innovation and New Technology,BIG data – BIG gains? Understanding the link between big data analytics and innovation,"ABSTRACT This paper analyzes the relationship between firms' use of big data analytics and their innovative performance in terms of product innovations. Since big data technologies provide new data information practices, they create novel decision-making possibilities, which are widely believed to support firms' innovation process. Applying German firm-level data within a knowledge production function framework we find suggestive evidence that big data analytics is a relevant determinant for the likelihood of a firm becoming a product innovator as well as for the market success of product innovations. These results hold for the manufacturing as well as for the service sector but are contingent on firms' investment in IT-specific skills. Overall, the results support the view that big data analytics have the potential to enable innovation.",fullPaper,jv311
Engineering,p1236,d0,2e76fa107c50485985487b03a601eb0c11894193,c64,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication","Big Data: A Revolution That Will Transform How We Live, Work, and Think","Amazon Exclusive: Q&A with Kenneth Cukier and Viktor Mayer-Schonberger Q. What did it take to write Big Data? A. Kenn has written about technology and business from Europe, Asia, and the US for The Economist, and is well-connected to the data community. Viktor had researched the information economy as a professor at Harvard and now at Oxford, and his book Delete had been well received. So we thought we had a good basis to make a contribution in the area. As we wrote the book, we had to dig deep to find unheard stories about big data pioneers and interview them. We wanted Big Data to be about a big idea, but also to be full of examples and success stories -- and be engrossing to read. Q. Are you big datas cheerleaders? A. Absolutely not. We are the messengers of big data, not its evangelists. The big data age is happening, and in the book we take a look at the drivers, and big datas likely trajectory: how it will change how we work and live. We emphasize that the fundamental shift is not in the machines that calculate data, but in the data itself and how we use it. Q. In discovering big data applications, what was your biggest surprise? A. It is tempting to say that it was predicting exploding manholes, tracking inflation in real time, or how big data saves the lives of premature babies. But the biggest surprise for us perhaps was the very diversity of the uses of big data, and how it already is changing peoples everyday world. Many people see big data through the lens of the Internet economy, since Google and Facebook have so much data. But that misses the point: big data is everywhere. Q. Is Big Data then primarily a story about economic efficiency? A. Big data improves economic efficiency, but thats only a very small part of the story. We realized when talking to dozens and dozens of big data pioneers that it improves health care, advances better education, and helps predict societal changefrom urban sprawl to the spread of the flu. Big data is roaring through all sectors of the economy and all areas of life. Q. So big data offers only upside? A. Not at all. We are very concerned about what we call in our book the dark side of big data. However the real challenge is that the problem is not necessarily where we initially tend to think it is, such as surveillance and privacy. After looking into the potential misuses of big data, we became much more troubled by propensity -- that is, big data predictions being used to police and punish. And by the fetishization of data that may occur, whereby organizations may blindly defer to what the data says without understanding its limitations. Q. What can we do about this dark side? A. Knowing about it is the first step. We thought hard to suggest concrete steps that can be taken to minimize and mitigate big datas risk, and came up with a few ways to ensure transparency, guarantee human free will, and strike a better balance on privacy and the use of personal information. These are deeply serious issues. If we do not take action soon, it might be too late.",poster,cp64
Engineering,p1248,d0,beb7348536ac3fcc77a770ec22e58ba97113dda1,j315,IEEE Transactions on Biomedical Engineering,–Omic and Electronic Health Record Big Data Analytics for Precision Medicine,"<italic>Objective:</italic> Rapid advances of high-throughput technologies and wide adoption of electronic health records (EHRs) have led to fast accumulation of –omic and EHR data. These voluminous complex data contain abundant information for precision medicine, and big data analytics can extract such knowledge to improve the quality of healthcare. <italic>Methods:</italic> In this paper, we present –omic and EHR data characteristics, associated challenges, and data analytics including data preprocessing, mining, and modeling. <italic>Results:</italic> To demonstrate how big data analytics enables precision medicine, we provide two case studies, including identifying disease biomarkers from multi-omic data and incorporating –omic information into EHR. <italic>Conclusion: </italic> Big data analytics is able to address –omic and EHR data challenges for paradigm shift toward precision medicine. <italic>Significance:</italic> Big data analytics makes sense of –omic and EHR data to improve healthcare outcome. It has long lasting societal impact.",fullPaper,jv315
Medicine,p1248,d1,beb7348536ac3fcc77a770ec22e58ba97113dda1,j315,IEEE Transactions on Biomedical Engineering,–Omic and Electronic Health Record Big Data Analytics for Precision Medicine,"<italic>Objective:</italic> Rapid advances of high-throughput technologies and wide adoption of electronic health records (EHRs) have led to fast accumulation of –omic and EHR data. These voluminous complex data contain abundant information for precision medicine, and big data analytics can extract such knowledge to improve the quality of healthcare. <italic>Methods:</italic> In this paper, we present –omic and EHR data characteristics, associated challenges, and data analytics including data preprocessing, mining, and modeling. <italic>Results:</italic> To demonstrate how big data analytics enables precision medicine, we provide two case studies, including identifying disease biomarkers from multi-omic data and incorporating –omic information into EHR. <italic>Conclusion: </italic> Big data analytics is able to address –omic and EHR data challenges for paradigm shift toward precision medicine. <italic>Significance:</italic> Big data analytics makes sense of –omic and EHR data to improve healthcare outcome. It has long lasting societal impact.",fullPaper,jv315
Computer Science,p1248,d3,beb7348536ac3fcc77a770ec22e58ba97113dda1,j315,IEEE Transactions on Biomedical Engineering,–Omic and Electronic Health Record Big Data Analytics for Precision Medicine,"<italic>Objective:</italic> Rapid advances of high-throughput technologies and wide adoption of electronic health records (EHRs) have led to fast accumulation of –omic and EHR data. These voluminous complex data contain abundant information for precision medicine, and big data analytics can extract such knowledge to improve the quality of healthcare. <italic>Methods:</italic> In this paper, we present –omic and EHR data characteristics, associated challenges, and data analytics including data preprocessing, mining, and modeling. <italic>Results:</italic> To demonstrate how big data analytics enables precision medicine, we provide two case studies, including identifying disease biomarkers from multi-omic data and incorporating –omic information into EHR. <italic>Conclusion: </italic> Big data analytics is able to address –omic and EHR data challenges for paradigm shift toward precision medicine. <italic>Significance:</italic> Big data analytics makes sense of –omic and EHR data to improve healthcare outcome. It has long lasting societal impact.",fullPaper,jv315
Engineering,p1250,d0,acd2e65a122059f3dd0fed0aa1c4bf4dd8ce58d1,c75,International Conference on Predictive Models in Software Engineering,How AUDI AG Established Big Data Analytics in Its Digital Transformation,"Digital transformation, which often includes establishing big data analytics capabilities, poses considerable challenges for traditional manufacturing organizations, such as car companies. Successfully introducing big data analytics requires substantial organizational transformation and new organizational structures and business processes. Based on the three-stage evolution of big data analytics capabilities at AUDI, we provide recommendations for how traditional manufacturing organizations can successfully introduce big data analytics and master the related organizational transformations.",poster,cp75
Computer Science,p1250,d3,acd2e65a122059f3dd0fed0aa1c4bf4dd8ce58d1,c75,International Conference on Predictive Models in Software Engineering,How AUDI AG Established Big Data Analytics in Its Digital Transformation,"Digital transformation, which often includes establishing big data analytics capabilities, poses considerable challenges for traditional manufacturing organizations, such as car companies. Successfully introducing big data analytics requires substantial organizational transformation and new organizational structures and business processes. Based on the three-stage evolution of big data analytics capabilities at AUDI, we provide recommendations for how traditional manufacturing organizations can successfully introduce big data analytics and master the related organizational transformations.",poster,cp75
Engineering,p1263,d0,d17fe932f9ebd1b94488d9e94d7bd3743dd496db,j219,International Journal of Production Research,"Big data and supply chain decisions: the impact of volume, variety and velocity properties on the bullwhip effect","The bullwhip effect is causing inefficiencies in today’s supply chains. This study deals with the potential of big data on the improvement of the various supply chain processes. The aim of this paper is to elaborate which characteristic of big data (lever) has the greatest potential to mitigate the bullwhip effect. From previous research, starting points for big data applications are derived. By using an existing system dynamics model, the big data levers ‘velocity’, ‘volume’ and ‘variety’ are transferred into a simulation. Overall, positive impacts of all the big data levers are elaborated. Findings suggest that the data property ‘velocity’ relatively bears the greatest potential to enhance performance. The results of this research will help in justifying the application of big data in supply chain management. The paper contributes to the literature by operationalising big data in the control engineering analyses.",fullPaper,jv219
Computer Science,p1263,d3,d17fe932f9ebd1b94488d9e94d7bd3743dd496db,j219,International Journal of Production Research,"Big data and supply chain decisions: the impact of volume, variety and velocity properties on the bullwhip effect","The bullwhip effect is causing inefficiencies in today’s supply chains. This study deals with the potential of big data on the improvement of the various supply chain processes. The aim of this paper is to elaborate which characteristic of big data (lever) has the greatest potential to mitigate the bullwhip effect. From previous research, starting points for big data applications are derived. By using an existing system dynamics model, the big data levers ‘velocity’, ‘volume’ and ‘variety’ are transferred into a simulation. Overall, positive impacts of all the big data levers are elaborated. Findings suggest that the data property ‘velocity’ relatively bears the greatest potential to enhance performance. The results of this research will help in justifying the application of big data in supply chain management. The paper contributes to the literature by operationalising big data in the control engineering analyses.",fullPaper,jv219
Engineering,p1264,d0,056d86a6599429db18a31ec04d037589295e9028,c66,International Conference on Web and Social Media,The 'big data' revolution in healthcare: Accelerating value and innovation,"""An era of open information in healthcare is now under way. We have already experienced a decade of progress in digitizing medical records, as pharmaceutical companies and other organizations aggregate years of research and development data in electronic databases. The federal government and other public stakeholders have also accelerated the move toward transparency by making decades of stored data usable, searchable, and actionable by the healthcare sector as a whole. Together, these increases in data liquidity have brought the industry to the tipping point...""",poster,cp66
Engineering,p1267,d0,4272bb248e6221a4a7eee45bcdd9babb07c30f24,j319,Vikalpa The Journal for Decision Makers,Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy,"A former Wall Street quant sounds an alarm on the mathematical models that pervade modern life and threaten to rip apart our social fabricWe live in the age of the algorithm. Increasingly, the decisions that affect our liveswhere we go to school, whether we get a car loan, how much we pay for health insuranceare being made not by humans, but by mathematical models. In theory, this should lead to greater fairness: Everyone is judged according to the same rules, and bias is eliminated. But as Cathy ONeil reveals in this urgent and necessary book, the opposite is true. The models being used today are opaque, unregulated, and uncontestable, even when theyre wrong. Most troubling, they reinforce discrimination: If a poor student cant get a loan because a lending model deems him too risky (by virtue of his zip code), hes then cut off from the kind of education that could pull him out of poverty, and a vicious spiral ensues. Models are propping up the lucky and punishing the downtrodden, creating a toxic cocktail for democracy. Welcome to the dark side of Big Data. Tracing the arc of a persons life, ONeil exposes the black box models that shape our future, both as individuals and as a society. These weapons of math destruction score teachers and students, sort rsums, grant (or deny) loans, evaluate workers, target voters, set parole, and monitor our health. ONeil calls on modelers to take more responsibility for their algorithms and on policy makers to regulate their use. But in the end, its up to us to become more savvy about the models that govern our lives. This important book empowers us to ask the tough questions, uncover the truth, and demand change.",fullPaper,jv319
Mathematics,p1267,d6,4272bb248e6221a4a7eee45bcdd9babb07c30f24,j319,Vikalpa The Journal for Decision Makers,Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy,"A former Wall Street quant sounds an alarm on the mathematical models that pervade modern life and threaten to rip apart our social fabricWe live in the age of the algorithm. Increasingly, the decisions that affect our liveswhere we go to school, whether we get a car loan, how much we pay for health insuranceare being made not by humans, but by mathematical models. In theory, this should lead to greater fairness: Everyone is judged according to the same rules, and bias is eliminated. But as Cathy ONeil reveals in this urgent and necessary book, the opposite is true. The models being used today are opaque, unregulated, and uncontestable, even when theyre wrong. Most troubling, they reinforce discrimination: If a poor student cant get a loan because a lending model deems him too risky (by virtue of his zip code), hes then cut off from the kind of education that could pull him out of poverty, and a vicious spiral ensues. Models are propping up the lucky and punishing the downtrodden, creating a toxic cocktail for democracy. Welcome to the dark side of Big Data. Tracing the arc of a persons life, ONeil exposes the black box models that shape our future, both as individuals and as a society. These weapons of math destruction score teachers and students, sort rsums, grant (or deny) loans, evaluate workers, target voters, set parole, and monitor our health. ONeil calls on modelers to take more responsibility for their algorithms and on policy makers to regulate their use. But in the end, its up to us to become more savvy about the models that govern our lives. This important book empowers us to ask the tough questions, uncover the truth, and demand change.",fullPaper,jv319
Engineering,p1269,d0,71ce7c52edbad555e7d3559f76c6c86971f76298,c72,Workshop on Research on Enterprise Networking,Macroeconomic Nowcasting and Forecasting with Big Data,"Data, data, data ... Economists know their importance well, especially when it comes to monitoring macroeconomic conditions -- the basis for making informed economic and policy decisions. Handling large and complex data sets was a challenge that macroeconomists engaged in real-time analysis faced long before ""big data"" became pervasive in other disciplines. We review how methods for tracking economic conditions using big data have evolved over time and explain how econometric techniques have advanced to mimic and automate best practices of forecasters on trading desks, at central banks, and in other market-monitoring roles. We present in detail the methodology underlying the New York Fed Staff Nowcast, which employs these innovative techniques to produce early estimates of GDP growth, synthesizing a wide range of macroeconomic data as they become available.",poster,cp72
Computer Science,p1269,d3,71ce7c52edbad555e7d3559f76c6c86971f76298,c72,Workshop on Research on Enterprise Networking,Macroeconomic Nowcasting and Forecasting with Big Data,"Data, data, data ... Economists know their importance well, especially when it comes to monitoring macroeconomic conditions -- the basis for making informed economic and policy decisions. Handling large and complex data sets was a challenge that macroeconomists engaged in real-time analysis faced long before ""big data"" became pervasive in other disciplines. We review how methods for tracking economic conditions using big data have evolved over time and explain how econometric techniques have advanced to mimic and automate best practices of forecasters on trading desks, at central banks, and in other market-monitoring roles. We present in detail the methodology underlying the New York Fed Staff Nowcast, which employs these innovative techniques to produce early estimates of GDP growth, synthesizing a wide range of macroeconomic data as they become available.",poster,cp72
Engineering,p1272,d0,ac7252f1ba8fc446c95934a5287843905a80fcf9,c109,Computer Vision and Pattern Recognition,Big data in operations and supply chain management: current trends and future perspectives,"Abstract Operations and supply chain management encompasses a vast domain and hence provides a myriad of opportunities for huge voluminous data generated from various sources in real time. Such huge data having the requisite properties of big data can be utilised to gain critical and fundamental insights towards optimising the operations and supply chain and thus making effective and efficient decisions. In the recent years, research interest in big data has increased substantially and therefore researchers and practitioners have also tried to tap the capabilities of big data to optimise operations and supply chain management. In this paper, the literature relating to the integration of big data with operations and supply chain management is reviewed. In particular, reviewing past work is primarily focused on three key areas of the operations and supply chain management, namely manufacturing, procurement and logistics where big data has been applied. In addition to reviewing past literature, paper also proposes application of big data in operations and supply chain management.",poster,cp109
Engineering,p1277,d0,116b2bc7b7f9c78d0c5589e8c1f9f723253e6232,c19,International Conference on Conceptual Structures,Internet of Things and Big Data Technologies for Next Generation Healthcare,Abstract,poster,cp19
Engineering,p1278,d0,e41b32583e19885e7692c1168a3e241cef6e898a,c114,Chinese Conference on Biometric Recognition,Big Data Analytics for Smart Manufacturing: Case Studies in Semiconductor Manufacturing,"Smart manufacturing (SM) is a term generally applied to the improvement in manufacturing operations through integration of systems, linking of physical and cyber capabilities, and taking advantage of information including leveraging the big data evolution. SM adoption has been occurring unevenly across industries, thus there is an opportunity to look to other industries to determine solution and roadmap paths for industries such as biochemistry or biology. The big data evolution affords an opportunity for managing significantly larger amounts of information and acting on it with analytics for improved diagnostics and prognostics. The analytics approaches can be defined in terms of dimensions to understand their requirements and capabilities, and to determine technology gaps. The semiconductor manufacturing industry has been taking advantage of the big data and analytics evolution by improving existing capabilities such as fault detection, and supporting new capabilities such as predictive maintenance. For most of these capabilities: (1) data quality is the most important big data factor in delivering high quality solutions; and (2) incorporating subject matter expertise in analytics is often required for realizing effective on-line manufacturing solutions. In the future, an improved big data environment incorporating smart manufacturing concepts such as digital twin will further enable analytics; however, it is anticipated that the need for incorporating subject matter expertise in solution design will remain.",poster,cp114
Engineering,p1279,d0,99b389e6928d5d102386b3440dd7c407e7b7a131,j258,IEEE Transactions on Industrial Informatics,Applicability of Big Data Techniques to Smart Cities Deployments,"This paper presents the main foundations of big data applied to smart cities. A general Internet of Things based architecture is proposed to be applied to different smart cities applications. We describe two scenarios of big data analysis. One of them illustrates some services implemented in the smart campus of the University of Murcia. The second one is focused on a tram service scenario, where thousands of transit-card transactions should be processed. Results obtained from both scenarios show the potential of the applicability of this kind of techniques to provide profitable services of smart cities, such as the management of the energy consumption and comfort in smart buildings, and the detection of travel profiles in smart transport.",fullPaper,jv258
Computer Science,p1279,d3,99b389e6928d5d102386b3440dd7c407e7b7a131,j258,IEEE Transactions on Industrial Informatics,Applicability of Big Data Techniques to Smart Cities Deployments,"This paper presents the main foundations of big data applied to smart cities. A general Internet of Things based architecture is proposed to be applied to different smart cities applications. We describe two scenarios of big data analysis. One of them illustrates some services implemented in the smart campus of the University of Murcia. The second one is focused on a tram service scenario, where thousands of transit-card transactions should be processed. Results obtained from both scenarios show the potential of the applicability of this kind of techniques to provide profitable services of smart cities, such as the management of the energy consumption and comfort in smart buildings, and the detection of travel profiles in smart transport.",fullPaper,jv258
Engineering,p1286,d0,bdbec0c80b73919a656125bbd32f71f602980706,c120,SIGSAND-Europe Symposium,Big data driven smart energy management: From big data to big insights,Abstract,poster,cp120
Engineering,p1292,d0,5ddbe916e203ed85d99828677133dbc6e29dd678,j191,Advanced Engineering Informatics,"Big Data in the construction industry: A review of present status, opportunities, and future trends",Abstract,fullPaper,jv191
Computer Science,p1292,d3,5ddbe916e203ed85d99828677133dbc6e29dd678,j191,Advanced Engineering Informatics,"Big Data in the construction industry: A review of present status, opportunities, and future trends",Abstract,fullPaper,jv191
Engineering,p1296,d0,3b6c41af9410d9971325b82d521763cd607dbca7,c58,Extreme Science and Engineering Discovery Environment,Transport modelling in the age of big data,"ABSTRACT New Big Data sources such as mobile phone call data records, smart card data and geo-coded social media records allow to observe and understand mobility behaviour on an unprecedented level of detail. Despite the availability of such new Big Data sources, transport demand models used in planning practice still, almost exclusively, are based on conventional data such as travel diary surveys and population census. This literature review brings together recent advances in harnessing Big Data sources to understand travel behaviour and inform travel demand models that allow transport planners to compute what-if scenarios. From trip identification to activity inference, we review and analyse the existing data-mining methods that enable these opportunistically collected mobility traces inform transport demand models. We identify that future research should tap on the potential of probabilistic models and machine learning techniques as commonly used in data science. Those data-mining approaches are designed to handle the uncertainty of sparse and noisy data as it is the case for mobility traces derived from mobile phone data. In addition, they are suitable to integrate different related data sets in a data fusion scheme so as to enrich Big Data with information from travel diaries. In any case, we also acknowledge that sophisticated modelling knowledge has developed in the domain of transport planning and therefore we strongly advise that still, domain expert knowledge should build the fundament when applying data-driven approaches in transport planning. These new challenges call for a multidisciplinary collaboration between transport modellers and data scientists.",poster,cp58
Engineering,p1305,d0,b890447611d11dcd8b835183a35afef16c096eb9,j164,Annual Review of Chemical and Biomolecular Engineering,Big Data Analytics in Chemical Engineering.,"Big data analytics is the journey to turn data into insights for more informed business and operational decisions. As the chemical engineering community is collecting more data (volume) from different sources (variety), this journey becomes more challenging in terms of using the right data and the right tools (analytics) to make the right decisions in real time (velocity). This article highlights recent big data advancements in five industries, including chemicals, energy, semiconductors, pharmaceuticals, and food, and then discusses technical, platform, and culture challenges. To reach the next milestone in multiplying successes to the enterprise level, government, academia, and industry need to collaboratively focus on workforce development and innovation.",fullPaper,jv164
Medicine,p1305,d1,b890447611d11dcd8b835183a35afef16c096eb9,j164,Annual Review of Chemical and Biomolecular Engineering,Big Data Analytics in Chemical Engineering.,"Big data analytics is the journey to turn data into insights for more informed business and operational decisions. As the chemical engineering community is collecting more data (volume) from different sources (variety), this journey becomes more challenging in terms of using the right data and the right tools (analytics) to make the right decisions in real time (velocity). This article highlights recent big data advancements in five industries, including chemicals, energy, semiconductors, pharmaceuticals, and food, and then discusses technical, platform, and culture challenges. To reach the next milestone in multiplying successes to the enterprise level, government, academia, and industry need to collaboratively focus on workforce development and innovation.",fullPaper,jv164
Engineering,p1310,d0,4959bc93879786e7e3c5c80db6430bb1acbeca4f,c6,Annual Conference on Genetic and Evolutionary Computation,Big Data Knowledge System in Healthcare,Abstract,poster,cp6
Engineering,p1340,d0,6d5867465c55b2d8fda24591c3cb40af416de525,j185,Proceedings of the IEEE,Big Data for Remote Sensing: Challenges and Opportunities,"Every day a large number of Earth observation (EO) spaceborne and airborne sensors from many different countries provide a massive amount of remotely sensed data. Those data are used for different applications, such as natural hazard monitoring, global climate change, urban planning, etc. The applications are data driven and mostly interdisciplinary. Based on this it can truly be stated that we are now living in the age of big remote sensing data. Furthermore, these data are becoming an economic asset and a new important resource in many applications. In this paper, we specifically analyze the challenges and opportunities that big data bring in the context of remote sensing applications. Our focus is to analyze what exactly does big data mean in remote sensing applications and how can big data provide added value in this context. Furthermore, this paper describes the most challenging issues in managing, processing, and efficient exploitation of big data for remote sensing problems. In order to illustrate the aforementioned aspects, two case studies discussing the use of big data in remote sensing are demonstrated. In the first test case, big data are used to automatically detect marine oil spills using a large archive of remote sensing data. In the second test case, content-based information retrieval is performed using high-performance computing (HPC) to extract information from a large database of remote sensing images, collected after the terrorist attack to the World Trade Center in New York City. Both cases are used to illustrate the significant challenges and opportunities brought by the use of big data in remote sensing applications.",fullPaper,jv185
Computer Science,p1340,d3,6d5867465c55b2d8fda24591c3cb40af416de525,j185,Proceedings of the IEEE,Big Data for Remote Sensing: Challenges and Opportunities,"Every day a large number of Earth observation (EO) spaceborne and airborne sensors from many different countries provide a massive amount of remotely sensed data. Those data are used for different applications, such as natural hazard monitoring, global climate change, urban planning, etc. The applications are data driven and mostly interdisciplinary. Based on this it can truly be stated that we are now living in the age of big remote sensing data. Furthermore, these data are becoming an economic asset and a new important resource in many applications. In this paper, we specifically analyze the challenges and opportunities that big data bring in the context of remote sensing applications. Our focus is to analyze what exactly does big data mean in remote sensing applications and how can big data provide added value in this context. Furthermore, this paper describes the most challenging issues in managing, processing, and efficient exploitation of big data for remote sensing problems. In order to illustrate the aforementioned aspects, two case studies discussing the use of big data in remote sensing are demonstrated. In the first test case, big data are used to automatically detect marine oil spills using a large archive of remote sensing data. In the second test case, content-based information retrieval is performed using high-performance computing (HPC) to extract information from a large database of remote sensing images, collected after the terrorist attack to the World Trade Center in New York City. Both cases are used to illustrate the significant challenges and opportunities brought by the use of big data in remote sensing applications.",fullPaper,jv185
Engineering,p1344,d0,59dd22f39652c98a80ec1d9e5606754e9318bca3,j325,Journal of Industrial Information Integration,Data and knowledge mining with big data towards smart production,Abstract,fullPaper,jv325
Computer Science,p1344,d3,59dd22f39652c98a80ec1d9e5606754e9318bca3,j325,Journal of Industrial Information Integration,Data and knowledge mining with big data towards smart production,Abstract,fullPaper,jv325
Engineering,p1358,d0,93b00fd9a9a5f3424d4f2aad7c821f6b33d1d064,j185,Proceedings of the IEEE,Big Data for Modern Industry: Challenges and Trends [Point of View],"We are living in an era of data deluge and as a result, the term ‘‘big data’’ is appearing in many contexts, from meteorology, genomics, complex physics simulations, biological and environmental research, finance and business to healthcare. Explores ways in business and industry is working to manage data and reports on applications that support these initiatives.",fullPaper,jv185
Computer Science,p1358,d3,93b00fd9a9a5f3424d4f2aad7c821f6b33d1d064,j185,Proceedings of the IEEE,Big Data for Modern Industry: Challenges and Trends [Point of View],"We are living in an era of data deluge and as a result, the term ‘‘big data’’ is appearing in many contexts, from meteorology, genomics, complex physics simulations, biological and environmental research, finance and business to healthcare. Explores ways in business and industry is working to manage data and reports on applications that support these initiatives.",fullPaper,jv185
Engineering,p1364,d0,9921cd9e5abf49659f72a1b95cf61368b4243332,j280,IEEE Systems Journal,Big Data Meet Green Challenges: Big Data Toward Green Applications,"Big data are widely recognized as being one of the most powerful drivers to promote productivity, improve efficiency, and support innovation. It is highly expected to explore the power of big data and turn big data into big values. To answer the interesting question whether there are inherent correlations between the two tendencies of big data and green challenges, a recent study has investigated the issues on greening the whole life cycle of big data systems. This paper would like to discover the relations between the trend of big data era and that of the new generation green revolution through a comprehensive and panoramic literature survey in big data technologies toward various green objectives and a discussion on relevant challenges and future directions.",fullPaper,jv280
Computer Science,p1364,d3,9921cd9e5abf49659f72a1b95cf61368b4243332,j280,IEEE Systems Journal,Big Data Meet Green Challenges: Big Data Toward Green Applications,"Big data are widely recognized as being one of the most powerful drivers to promote productivity, improve efficiency, and support innovation. It is highly expected to explore the power of big data and turn big data into big values. To answer the interesting question whether there are inherent correlations between the two tendencies of big data and green challenges, a recent study has investigated the issues on greening the whole life cycle of big data systems. This paper would like to discover the relations between the trend of big data era and that of the new generation green revolution through a comprehensive and panoramic literature survey in big data technologies toward various green objectives and a discussion on relevant challenges and future directions.",fullPaper,jv280
Engineering,p1373,d0,3701d3a06cec47dc28b5d6a482a41ced2711bea6,c92,International Symposium on Computer Architecture,Analytics: The real-world use of big data in financial services studying with judge system events,Abstract,poster,cp92
Engineering,p1378,d0,62d3d79cf1845bac650167d300dc696b4eec4214,c119,International Conference on Business Process Management,The Age of Big Data,"The Workshop was hosted by The Law and Technology Centre of the Faculty of Law, The University of Hong Kong",poster,cp119
Engineering,p1379,d0,1c45bfad2900f553364b9f877ca31e1ab3a310a1,j244,The International Journal of Advanced Manufacturing Technology,Big Data in product lifecycle management,Abstract,fullPaper,jv244
Engineering,p1380,d0,c91a589e6c222051bbd37020c0bd4d7698f87cc8,c11,European Conference on Modelling and Simulation,Big Data in Building Energy Efficiency: Understanding of Big Data and Main Challenges,Abstract,poster,cp11
Engineering,p1381,d0,ce1b0553c09b725f6fd1fc0b39f2dc7c428d3088,j331,AIP Conference Proceedings,What is big data? A consensual definition and a review of key research topics,"Although Big Data is a trending buzzword in both academia and the industry, its meaning is still shrouded by much conceptual vagueness. The term is used to describe a wide range of concepts: from the technological ability to store, aggregate, and process data, to the cultural shift that is pervasively invading business and society, both drowning in information overload. The lack of a formal definition has led research to evolve into multiple and inconsistent paths. Furthermore, the existing ambiguity among researchers and practitioners undermines an efficient development of the subject. In this paper we have reviewed the existing literature on Big Data and analyzed its previous definitions in order to pursue two results: first, to provide a summary of the key research areas related to the phenomenon, identifying emerging trends and suggesting opportunities for future development; second, to provide a consensual definition for Big Data, by synthesizing common themes of existing works and patterns in previous definitions.",fullPaper,jv331
Engineering,p1394,d0,f4900599936427fd1c7714cae15ff00590396a7c,j280,IEEE Systems Journal,Big Data Meet Green Challenges: Greening Big Data,"Nowadays, there are two significant tendencies, how to process the enormous amount of data, big data, and how to deal with the green issues related to sustainability and environmental concerns. An interesting question is whether there are inherent correlations between the two tendencies in general. To answer this question, this paper firstly makes a comprehensive literature survey on how to green big data systems in terms of the whole life cycle of big data processing, and then this paper studies the relevance between big data and green metrics and proposes two new metrics, effective energy efficiency and effective resource efficiency in order to bring new views and potentials of green metrics for the future times of big data.",fullPaper,jv280
Computer Science,p1394,d3,f4900599936427fd1c7714cae15ff00590396a7c,j280,IEEE Systems Journal,Big Data Meet Green Challenges: Greening Big Data,"Nowadays, there are two significant tendencies, how to process the enormous amount of data, big data, and how to deal with the green issues related to sustainability and environmental concerns. An interesting question is whether there are inherent correlations between the two tendencies in general. To answer this question, this paper firstly makes a comprehensive literature survey on how to green big data systems in terms of the whole life cycle of big data processing, and then this paper studies the relevance between big data and green metrics and proposes two new metrics, effective energy efficiency and effective resource efficiency in order to bring new views and potentials of green metrics for the future times of big data.",fullPaper,jv280
Engineering,p1403,d0,4a32712674651fd200e1de8e41e9685f21a163b4,c95,Cyber ..,The Ethics of Big Data in Big Agriculture,"This paper examines the ethics of big data in agriculture, focusing on the power asymmetry between farmers and large agribusinesses like Monsanto. Following the recent purchase of Climate Corp., Monsanto is currently the most prominent biotech agribusiness to buy into big data. With wireless sensors on tractors monitoring or dictating every decision a farmer makes, Monsanto can now aggregate large quantities of previously proprietary farming data, enabling a privileged position with unique insights on a field-by-field basis into a third or more of the US farmland. This power asymmetry may be rebalanced through open-sourced data, and publicly-funded data analytic tools which rival Climate Corp. in complexity and innovation for use in the public domain.",poster,cp95
Engineering,p1410,d0,c1d4584096390eb23e7daadfa4819421c738bacd,j337,Information Systems Journal,"Big data, big risks","The ‘big data’ literature, academic as well as professional, has a very strong focus on opportunities. Far less attention has been paid to the threats that arise from repurposing data, consolidating data from multiple sources, applying analytical tools to the resulting collections, drawing inferences, and acting on them. On the basis of a review of quality factors in ‘big data’ and ‘big data analytics’, illustrated by means of scenario analysis, this paper draws attention to the moral and legal responsibility of computing researchers and professionals to temper their excitement, and apply reality checks to their promotional activities.",fullPaper,jv337
Computer Science,p1410,d3,c1d4584096390eb23e7daadfa4819421c738bacd,j337,Information Systems Journal,"Big data, big risks","The ‘big data’ literature, academic as well as professional, has a very strong focus on opportunities. Far less attention has been paid to the threats that arise from repurposing data, consolidating data from multiple sources, applying analytical tools to the resulting collections, drawing inferences, and acting on them. On the basis of a review of quality factors in ‘big data’ and ‘big data analytics’, illustrated by means of scenario analysis, this paper draws attention to the moral and legal responsibility of computing researchers and professionals to temper their excitement, and apply reality checks to their promotional activities.",fullPaper,jv337
Engineering,p1418,d0,dfbfa43fa090fdd21e4b8e694f61136ebeeaab93,c89,Conference on Uncertainty in Artificial Intelligence,"Smart Cities: Big Data, Civic Hackers, and the Quest for a New Utopia","We live in a world defined by urbanization and digital ubiquity, where mobile broadband connections outnumber fixed ones, machines dominate a new ""internet of things,"" and more people live in cities than in the countryside. In Smart Cities, urbanist and technology expert Anthony Townsend takes a broad historical look at the forces that have shaped the planning and design of cities and information technologies from the rise of the great industrial cities of the nineteenth century to the present. A century ago, the telegraph and the mechanical tabulator were used to tame cities of millions. Today, cellular networks and cloud computing tie together the complex choreography of mega-regions of tens of millions of people. In response, cities worldwide are deploying technology to address both the timeless challenges of government and the mounting problems posed by human settlements of previously unimaginable size and complexity. In Chicago, GPS sensors on snow plows feed a real-time ""plow tracker"" map that everyone can access. In Zaragoza, Spain, a ""citizen card"" can get you on the free city-wide Wi-Fi network, unlock a bike share, check a book out of the library, and pay for your bus ride home. In New York, a guerrilla group of citizen-scientists installed sensors in local sewers to alert you when stormwater runoff overwhelms the system, dumping waste into local waterways. As technology barons, entrepreneurs, mayors, and an emerging vanguard of civic hackers are trying to shape this new frontier, Smart Cities considers the motivations, aspirations, and shortcomings of them all while offering a new civics to guide our efforts as we build the future together, one click at a time.",poster,cp89
Engineering,p1419,d0,f3146bc0967971dacf13e95db383c4ca09d8073f,j338,Industrial management & data systems,How organisations leverage Big Data: a maturity model,"Purpose 
 
 
 
 
While it is commonly recognised that Big Data have an immense potential to generate value for business organisations, appropriating value from Big Data and, in particular, Big Data-enabled analytics is still an open issue for many organisations. The purpose of this paper is to develop a maturity model to support organisations in the realisation of the value created by Big Data. 
 
 
 
 
Design/methodology/approach 
 
 
 
 
The maturity model is developed following a qualitative approach based on literature analysis and semi-structured interviews with domain experts. The completeness and usefulness of the model is evaluated qualitatively by practitioners, whereas the applicability of the model is evaluated by Big Data maturity assessments in three real-world organisations. 
 
 
 
 
Findings 
 
 
 
 
The proposed maturity model is considered exhaustive by domain experts and has helped the three assessed organisations to develop a more critical understanding of the next steps to take. 
 
 
 
 
Originality/value 
 
 
 
 
The maturity model integrates existing industry-developed maturity models into one single coherent Big Data maturity model. The proposed model answers the call for research on Big Data to abstract from technical issues to focus on the business implications of Big Data initiatives.",fullPaper,jv338
Computer Science,p1419,d3,f3146bc0967971dacf13e95db383c4ca09d8073f,j338,Industrial management & data systems,How organisations leverage Big Data: a maturity model,"Purpose 
 
 
 
 
While it is commonly recognised that Big Data have an immense potential to generate value for business organisations, appropriating value from Big Data and, in particular, Big Data-enabled analytics is still an open issue for many organisations. The purpose of this paper is to develop a maturity model to support organisations in the realisation of the value created by Big Data. 
 
 
 
 
Design/methodology/approach 
 
 
 
 
The maturity model is developed following a qualitative approach based on literature analysis and semi-structured interviews with domain experts. The completeness and usefulness of the model is evaluated qualitatively by practitioners, whereas the applicability of the model is evaluated by Big Data maturity assessments in three real-world organisations. 
 
 
 
 
Findings 
 
 
 
 
The proposed maturity model is considered exhaustive by domain experts and has helped the three assessed organisations to develop a more critical understanding of the next steps to take. 
 
 
 
 
Originality/value 
 
 
 
 
The maturity model integrates existing industry-developed maturity models into one single coherent Big Data maturity model. The proposed model answers the call for research on Big Data to abstract from technical issues to focus on the business implications of Big Data initiatives.",fullPaper,jv338
Engineering,p1426,d0,fe74f9cc0d3b7b8c23e907b75a818e0346490f52,c87,International Conference on Big Data Research,Digital Humanitarians: How Big Data Is Changing the Face of Humanitarian Response,"The overflow of information generated during disasters can be as paralyzing to humanitarian response as the lack of information. Making sense of this information--Big Data--is proving an impossible challenge for traditional humanitarian organizations, which is precisely why they're turning to Digital Humanitarians. This new humanitarians mobilize online to make sense of vast volumes of data--social media and text messages; satellite and aerial imagery--in direct support of relief efforts worldwide. How? They craft ingenious crowdsourcing solutions with trail-blazing insights from artificial intelligence. This book charts the spectacular rise of Digital Humanitarians, highlighting how their humanity coupled with innovative Big Data solutions is changing humanitarian relief for forever. Praise for the book: ...examines how new uses of technology and vast quantities of digital data are transforming the way societies prepare for, respond to, cope with, and ultimately understand humanitarian disasters. --Dr. Enzo Bollettino, Executive Director, The Harvard Humanitarian Initiative, Harvard University ...explains the strengths and potential weaknesses of using big data and crowdsourced analytics in crisis situations. It is at once a deeply personal and intellectually satisfying book.--Professor Steven Livingston, Professor of Media & Public and International Affairs, Elliott School of International Affairs, George Washington University",poster,cp87
Engineering,p1431,d0,2a6a9c939c2abecf4c7acb7d9e439f0e53e5f399,c95,Cyber ..,Industrial Big Data as a Result of IoT Adoption in Manufacturing,Abstract,poster,cp95
Engineering,p1436,d0,8f9b0fac9e81c89b72a22171d1a06247cb8efa79,c19,International Conference on Conceptual Structures,Big Data for Social Transportation,"Big data for social transportation brings us unprecedented opportunities for resolving transportation problems for which traditional approaches are not competent and for building the next-generation intelligent transportation systems. Although social data have been applied for transportation analysis, there are still many challenges. First, social data evolve with time and contain abundant information, posing a crucial need for data collection and cleaning. Meanwhile, each type of data has specific advantages and limitations for social transportation, and one data type alone is not capable of describing the overall state of a transportation system. Systematic data fusing approaches or frameworks for combining social signal data with different features, structures, resolutions, and precision are needed. Second, data processing and mining techniques, such as natural language processing and analysis of streaming data, require further revolutions in effective utilization of real-time traffic information. Third, social data are connected to cyber and physical spaces. To address practical problems in social transportation, a suite of schemes are demanded for realizing big data in social transportation systems, such as crowdsourcing, visual analysis, and task-based services. In this paper, we overview data sources, analytical approaches, and application systems for social transportation, and we also suggest a few future research directions for this new social transportation field.",poster,cp19
Computer Science,p1436,d3,8f9b0fac9e81c89b72a22171d1a06247cb8efa79,c19,International Conference on Conceptual Structures,Big Data for Social Transportation,"Big data for social transportation brings us unprecedented opportunities for resolving transportation problems for which traditional approaches are not competent and for building the next-generation intelligent transportation systems. Although social data have been applied for transportation analysis, there are still many challenges. First, social data evolve with time and contain abundant information, posing a crucial need for data collection and cleaning. Meanwhile, each type of data has specific advantages and limitations for social transportation, and one data type alone is not capable of describing the overall state of a transportation system. Systematic data fusing approaches or frameworks for combining social signal data with different features, structures, resolutions, and precision are needed. Second, data processing and mining techniques, such as natural language processing and analysis of streaming data, require further revolutions in effective utilization of real-time traffic information. Third, social data are connected to cyber and physical spaces. To address practical problems in social transportation, a suite of schemes are demanded for realizing big data in social transportation systems, such as crowdsourcing, visual analysis, and task-based services. In this paper, we overview data sources, analytical approaches, and application systems for social transportation, and we also suggest a few future research directions for this new social transportation field.",poster,cp19
Engineering,p1438,d0,6aec26e2bc3423c71375956bcd2de6b064ebacc0,c93,ASE BigData & SocialInformatics,A Hierarchical Distributed Fog Computing Architecture for Big Data Analysis in Smart Cities,"The ubiquitous deployment of various kinds of sensors in smart cities requires a new computing paradigm to support Internet of Things (IoT) services and applications, and big data analysis. Fog Computing, which extends Cloud Computing to the edge of network, fits this need. In this paper, we present a hierarchical distributed Fog Computing architecture to support the integration of massive number of infrastructure components and services in future smart cities. To secure future communities, it is necessary to build large-scale, geospatial sensing networks, perform big data analysis, identify anomalous and hazardous events, and offer optimal responses in real-time. We analyze case studies using a smart pipeline monitoring system based on fiber optic sensors and sequential learning algorithms to detect events threatening pipeline safety. A working prototype was constructed to experimentally evaluate event detection performance of the recognition of 12 distinct events. These experimental results demonstrate the feasibility of the system's city-wide implementation in the future.",fullPaper,cp93
Computer Science,p1438,d3,6aec26e2bc3423c71375956bcd2de6b064ebacc0,c93,ASE BigData & SocialInformatics,A Hierarchical Distributed Fog Computing Architecture for Big Data Analysis in Smart Cities,"The ubiquitous deployment of various kinds of sensors in smart cities requires a new computing paradigm to support Internet of Things (IoT) services and applications, and big data analysis. Fog Computing, which extends Cloud Computing to the edge of network, fits this need. In this paper, we present a hierarchical distributed Fog Computing architecture to support the integration of massive number of infrastructure components and services in future smart cities. To secure future communities, it is necessary to build large-scale, geospatial sensing networks, perform big data analysis, identify anomalous and hazardous events, and offer optimal responses in real-time. We analyze case studies using a smart pipeline monitoring system based on fiber optic sensors and sequential learning algorithms to detect events threatening pipeline safety. A working prototype was constructed to experimentally evaluate event detection performance of the recognition of 12 distinct events. These experimental results demonstrate the feasibility of the system's city-wide implementation in the future.",fullPaper,cp93
Engineering,p1455,d0,d0d5c4fc59d9e82318f33c75346de6c4f828a7e0,j342,IEEE Transactions on Smart Grid,Energy Big Data Analytics and Security: Challenges and Opportunities,"The limited available fossil fuels and the call for sustainable environment have brought about new technologies for the high efficiency in the use of fossil fuels and introduction of renewable energy. Smart grid is an emerging technology that can fulfill such demands by incorporating advanced information and communications technology (ICT). The pervasive deployment of the advanced ICT, especially the smart metering, will generate big energy data in terms of volume, velocity, and variety. The generated big data can bring huge benefits to the better energy planning, efficient energy generation, and distribution. As such data involve end users' privacy and secure operation of the critical infrastructure, there will be new security issues. This paper is to survey and discuss new findings and developments in the existing big energy data analytics and security. Several taxonomies have been proposed to express the intriguing relationships of various variables in the field.",fullPaper,jv342
Computer Science,p1455,d3,d0d5c4fc59d9e82318f33c75346de6c4f828a7e0,j342,IEEE Transactions on Smart Grid,Energy Big Data Analytics and Security: Challenges and Opportunities,"The limited available fossil fuels and the call for sustainable environment have brought about new technologies for the high efficiency in the use of fossil fuels and introduction of renewable energy. Smart grid is an emerging technology that can fulfill such demands by incorporating advanced information and communications technology (ICT). The pervasive deployment of the advanced ICT, especially the smart metering, will generate big energy data in terms of volume, velocity, and variety. The generated big data can bring huge benefits to the better energy planning, efficient energy generation, and distribution. As such data involve end users' privacy and secure operation of the critical infrastructure, there will be new security issues. This paper is to survey and discuss new findings and developments in the existing big energy data analytics and security. Several taxonomies have been proposed to express the intriguing relationships of various variables in the field.",fullPaper,jv342
Engineering,p1462,d0,c73c0695ee7b2b6d20d443dca272966172cb6868,j344,Journal of International Humanitarian Action,Crisis analytics: big data-driven crisis response,Abstract,fullPaper,jv344
Computer Science,p1462,d3,c73c0695ee7b2b6d20d443dca272966172cb6868,j344,Journal of International Humanitarian Action,Crisis analytics: big data-driven crisis response,Abstract,fullPaper,jv344
Engineering,p1463,d0,9a411034c3631e878008599d41b8214c36d95dfe,j44,Social Science Research Network,Is Big Data challenging criminology?,"The advent of ‘Big Data’ and machine learning algorithms is predicted to transform how we work and think. Specifically, it is said that the capacity of Big Data analytics to move from sampling to census, its ability to deal with messy data and the demonstrated utility of moving from causality to correlation have fundamentally changed the practice of social sciences. Some have even predicted the end of theory—where the question why is replaced by what—and an enduring challenge to disciplinary expertise. This article critically reviews the available literature against such claims and draws on the example of predictive policing to discuss the likely impact of Big Data analytics on criminological research and policy.",fullPaper,jv44
Sociology,p1463,d4,9a411034c3631e878008599d41b8214c36d95dfe,j44,Social Science Research Network,Is Big Data challenging criminology?,"The advent of ‘Big Data’ and machine learning algorithms is predicted to transform how we work and think. Specifically, it is said that the capacity of Big Data analytics to move from sampling to census, its ability to deal with messy data and the demonstrated utility of moving from causality to correlation have fundamentally changed the practice of social sciences. Some have even predicted the end of theory—where the question why is replaced by what—and an enduring challenge to disciplinary expertise. This article critically reviews the available literature against such claims and draws on the example of predictive policing to discuss the likely impact of Big Data analytics on criminological research and policy.",fullPaper,jv44
Engineering,p1563,d0,072a0db716fb6f8332323f076b71554716a7271c,j355,IEEE Engineering in Medicine and Biology Magazine,The impact of the MIT-BIH Arrhythmia Database,"The MIT-BIH Arrhythmia Database was the first generally available set of standard test material for evaluation of arrhythmia detectors, and it has been used for that purpose as well as for basic research into cardiac dynamics at about 500 sites worldwide since 1980. It has lived a far longer life than any of its creators ever expected. Together with the American Heart Association Database, it played an interesting role in stimulating manufacturers of arrhythmia analyzers to compete on the basis of objectively measurable performance, and much of the current appreciation of the value of common databases, both for basic research and for medical device development and evaluation, can be attributed to this experience. In this article, we briefly review the history of the database, describe its contents, discuss what we have learned about database design and construction, and take a look at some of the later projects that have been stimulated by both the successes and the limitations of the MIT-BIH Arrhythmia Database.",fullPaper,jv355
Medicine,p1563,d1,072a0db716fb6f8332323f076b71554716a7271c,j355,IEEE Engineering in Medicine and Biology Magazine,The impact of the MIT-BIH Arrhythmia Database,"The MIT-BIH Arrhythmia Database was the first generally available set of standard test material for evaluation of arrhythmia detectors, and it has been used for that purpose as well as for basic research into cardiac dynamics at about 500 sites worldwide since 1980. It has lived a far longer life than any of its creators ever expected. Together with the American Heart Association Database, it played an interesting role in stimulating manufacturers of arrhythmia analyzers to compete on the basis of objectively measurable performance, and much of the current appreciation of the value of common databases, both for basic research and for medical device development and evaluation, can be attributed to this experience. In this article, we briefly review the history of the database, describe its contents, discuss what we have learned about database design and construction, and take a look at some of the later projects that have been stimulated by both the successes and the limitations of the MIT-BIH Arrhythmia Database.",fullPaper,jv355
Engineering,p1614,d0,1cf4a6954b419b5478c96119fc1e79aa90f87dea,c50,Conference on Emerging Network Experiment and Technology,The Cambridge Structural Database in retrospect and prospect.,"The Cambridge Crystallographic Data Centre (CCDC) was established in 1965 to record numerical, chemical and bibliographic data relating to published organic and metal-organic crystal structures. The Cambridge Structural Database (CSD) now stores data for nearly 700,000 structures and is a comprehensive and fully retrospective historical archive of small-molecule crystallography. Nearly 40,000 new structures are added each year. As X-ray crystallography celebrates its centenary as a subject, and the CCDC approaches its own 50th year, this article traces the origins of the CCDC as a publicly funded organization and its onward development into a self-financing charitable institution. Principally, however, we describe the growth of the CSD and its extensive associated software system, and summarize its impact and value as a basis for research in structural chemistry, materials science and the life sciences, including drug discovery and drug development. Finally, the article considers the CCDC's funding model in relation to open access and open data paradigms.",poster,cp50
Medicine,p1614,d1,1cf4a6954b419b5478c96119fc1e79aa90f87dea,c50,Conference on Emerging Network Experiment and Technology,The Cambridge Structural Database in retrospect and prospect.,"The Cambridge Crystallographic Data Centre (CCDC) was established in 1965 to record numerical, chemical and bibliographic data relating to published organic and metal-organic crystal structures. The Cambridge Structural Database (CSD) now stores data for nearly 700,000 structures and is a comprehensive and fully retrospective historical archive of small-molecule crystallography. Nearly 40,000 new structures are added each year. As X-ray crystallography celebrates its centenary as a subject, and the CCDC approaches its own 50th year, this article traces the origins of the CCDC as a publicly funded organization and its onward development into a self-financing charitable institution. Principally, however, we describe the growth of the CSD and its extensive associated software system, and summarize its impact and value as a basis for research in structural chemistry, materials science and the life sciences, including drug discovery and drug development. Finally, the article considers the CCDC's funding model in relation to open access and open data paradigms.",poster,cp50
Engineering,p1804,d0,015525f864ccaf28efbdaed46029598441121a9e,c0,International Conference on Machine Learning,UCID: an uncompressed color image database,"Standardised image databases or rather the lack of them are one of the main weaknesses in the field of content based image retrieval (CBIR). Authors often use their own images or do not specify the source of their datasets. Naturally this makes comparison of results somewhat difficult. While a first approach towards a common colour image set has been taken by the MPEG 7 committee their database does not cater for all strands of research in the CBIR community. In particular as the MPEG-7 images only exist in compressed form it does not allow for an objective evaluation of image retrieval algorithms that operate in the compressed domain or to judge the influence image compression has on the performance of CBIR algorithms. In this paper we introduce a new dataset, UCID (pronounced ""use it"") - an Uncompressed Colour Image Dataset which tries to bridge this gap. The UCID dataset currently consists of 1338 uncompressed images together with a ground truth of a series of query images with corresponding models that an ideal CBIR algorithm would retrieve. While its initial intention was to provide a dataset for the evaluation of compressed domain algorithms, the UCID database also represents a good benchmark set for the evaluation of any kind of CBIR method as well as an image set that can be used to evaluate image compression and colour quantisation algorithms.",poster,cp0
Computer Science,p1804,d3,015525f864ccaf28efbdaed46029598441121a9e,c0,International Conference on Machine Learning,UCID: an uncompressed color image database,"Standardised image databases or rather the lack of them are one of the main weaknesses in the field of content based image retrieval (CBIR). Authors often use their own images or do not specify the source of their datasets. Naturally this makes comparison of results somewhat difficult. While a first approach towards a common colour image set has been taken by the MPEG 7 committee their database does not cater for all strands of research in the CBIR community. In particular as the MPEG-7 images only exist in compressed form it does not allow for an objective evaluation of image retrieval algorithms that operate in the compressed domain or to judge the influence image compression has on the performance of CBIR algorithms. In this paper we introduce a new dataset, UCID (pronounced ""use it"") - an Uncompressed Colour Image Dataset which tries to bridge this gap. The UCID dataset currently consists of 1338 uncompressed images together with a ground truth of a series of query images with corresponding models that an ideal CBIR algorithm would retrieve. While its initial intention was to provide a dataset for the evaluation of compressed domain algorithms, the UCID database also represents a good benchmark set for the evaluation of any kind of CBIR method as well as an image set that can be used to evaluate image compression and colour quantisation algorithms.",poster,cp0
Engineering,p1888,d0,798e312dd67798024da74f9a8f92946af88c7cd4,c61,"International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems",Comparative study of retinal vessel segmentation methods on a new publicly available database,"In this work we compare the performance of a number of vessel segmentation algorithms on a newly constructed retinal vessel image database. Retinal vessel segmentation is important for the detection of numerous eye diseases and plays an important role in automatic retinal disease screening systems. A large number of methods for retinal vessel segmentation have been published, yet an evaluation of these methods on a common database of screening images has not been performed. To compare the performance of retinal vessel segmentation methods we have constructed a large database of retinal images. The database contains forty images in which the vessel trees have been manually segmented. For twenty of those forty images a second independent manual segmentation is available. This allows for a comparison between the performance of automatic methods and the performance of a human observer. The database is available to the research community. Interested researchers are encouraged to upload their segmentation results to our website (http://www.isi.uu.nl/Research/Databases). The performance of five different algorithms has been compared. Four of these methods have been implemented as described in the literature. The fifth pixel classification based method was developed specifically for the segmentation of retinal vessels and is the only supervised method in this test. We define the segmentation accuracy with respect to our gold standard as the performance measure. Results show that the pixel classification method performs best, but the second observer still performs significantly better.",poster,cp61
Computer Science,p1888,d3,798e312dd67798024da74f9a8f92946af88c7cd4,c61,"International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems",Comparative study of retinal vessel segmentation methods on a new publicly available database,"In this work we compare the performance of a number of vessel segmentation algorithms on a newly constructed retinal vessel image database. Retinal vessel segmentation is important for the detection of numerous eye diseases and plays an important role in automatic retinal disease screening systems. A large number of methods for retinal vessel segmentation have been published, yet an evaluation of these methods on a common database of screening images has not been performed. To compare the performance of retinal vessel segmentation methods we have constructed a large database of retinal images. The database contains forty images in which the vessel trees have been manually segmented. For twenty of those forty images a second independent manual segmentation is available. This allows for a comparison between the performance of automatic methods and the performance of a human observer. The database is available to the research community. Interested researchers are encouraged to upload their segmentation results to our website (http://www.isi.uu.nl/Research/Databases). The performance of five different algorithms has been compared. Four of these methods have been implemented as described in the literature. The fifth pixel classification based method was developed specifically for the segmentation of retinal vessels and is the only supervised method in this test. We define the segmentation accuracy with respect to our gold standard as the performance measure. Results show that the pixel classification method performs best, but the second observer still performs significantly better.",poster,cp61
Engineering,p1894,d0,97dcab33aa0f1b8c98eec95e52e13596f3fb890d,c16,International Conference on Data Science and Advanced Analytics,The ecoinvent Database: Overview and Methodological Framework (7 pp),Abstract,poster,cp16
Engineering,p1930,d0,353988f8d56224b9d46aa34059e499c638dcbc2e,j395,Journal of chemical information and computer sciences,The United Kingdom Chemical Database Service,"The Chemical Database Service (CDS) is a national service, funded by the Chemistry Programme of the United Kingdom Engineering and Physical Sciences Research Council (EPSRC). It provides access for UK academics to a range of chemistry databases in the areas of crystallography, synthetic organic chemistry, spectroscopy, and physical chemistry. Three post-doctoral chemists are available to assist users with problems, run training courses, and also give advice to the community on accessing other sources of chemical data and software.",fullPaper,jv395
Computer Science,p1930,d3,353988f8d56224b9d46aa34059e499c638dcbc2e,j395,Journal of chemical information and computer sciences,The United Kingdom Chemical Database Service,"The Chemical Database Service (CDS) is a national service, funded by the Chemistry Programme of the United Kingdom Engineering and Physical Sciences Research Council (EPSRC). It provides access for UK academics to a range of chemistry databases in the areas of crystallography, synthetic organic chemistry, spectroscopy, and physical chemistry. Three post-doctoral chemists are available to assist users with problems, run training courses, and also give advice to the community on accessing other sources of chemical data and software.",fullPaper,jv395
Engineering,p1946,d0,dea76435761e7fa1fea68aa5224c3ed0ca1e0e4e,c120,SIGSAND-Europe Symposium,Third millenium ideal gas and condensed phase thermochemical database for combustion (with update from active thermochemical tables).,"The thermochemical database of species involved in combustion processes is and has been available for free use for over 25 years. It was first published in print in 1984, approximately 8 years after it was first assembled, and contained 215 species at the time. This is the 7th printed edition and most likely will be the last one in print in the present format, which involves substantial manual labor. The database currently contains more than 1300 species, specifically organic molecules and radicals, but also inorganic species connected to combustion and air pollution. Since 1991 this database is freely available on the internet, at the Technion-IIT ftp server, and it is continuously expanded and corrected. The database is mirrored daily at an official mirror site, and at random at about a dozen unofficial mirror and 'finger' sites. The present edition contains numerous corrections and many recalculations of data of provisory type by the G3//B3LYP method, a high-accuracy composite ab initio calculation. About 300 species are newly calculated and are not yet published elsewhere. In anticipation of the full coupling, which is under development, the database started incorporating the available (as yet unpublished) values from Active Thermochemical Tables. The electronic version nowmore » also contains an XML file of the main database to allow transfer to other formats and ease finding specific information of interest. The database is used by scientists, educators, engineers and students at all levels, dealing primarily with combustion and air pollution, jet engines, rocket propulsion, fireworks, but also by researchers involved in upper atmosphere kinetics, astrophysics, abrasion metallurgy, etc. This introductory article contains explanations of the database and the means to use it, its sources, ways of calculation, and assessments of the accuracy of data.« less",poster,cp120
Engineering,p2003,d0,d2613acedcd1059a54ba3c55fca11e9638ee7013,c113,International Conference on Mobile Data Management,Business Model Generation A Handbook For Visionaries Game Changers And Challengers,"The book entitled “Business Model Generation: A Handbook for visionaries, game changers and challengers” though written by Osterwalder and Pigneur (2010) was also co-created by 470 practitioners from 45 countries. The book is thus a good example of how a global creative collaboration effort can contribute positively to the business and management literature and subsequently to the advancement of society (Alam and Hoque, 2010; Alam et al., 2010a, b). Consisting of five main chapters (Canvas, Patterns, Design, Strategy and Process) and two additional chapters Outlook and Afterword, “Business Model Generation” should be read by those motivated to “defy outmoded business models and design tomorrow’s enterprises” (front cover). The objectives of the five main chapters in the book are as follows:",poster,cp113
Engineering,p2005,d0,f117c6f12d067bd66dad40996b3931c069daa2da,c81,ACM Symposium on Applied Computing,Business Intelligence and Analytics: From Big Data to Big Impact,"Business intelligence and analytics (BI&A) has emerged as an important area of study for both practitioners and researchers, reflecting the magnitude and impact of data-related problems to be solved in contemporary business organizations. This introduction to the MIS Quarterly Special Issue on Business Intelligence Research first provides a framework that identifies the evolution, applications, and emerging research areas of BI&A. BI&A 1.0, BI&A 2.0, and BI&A 3.0 are defined and described in terms of their key characteristics and capabilities. Current research in BI&A is analyzed and challenges and opportunities associated with BI&A research and education are identified. We also report a bibliometric study of critical BI&A publications, researchers, and research topics based on more than a decade of related academic and industry publications. Finally, the six articles that comprise this special issue are introduced and characterized in terms of the proposed BI&A research framework.",poster,cp81
Computer Science,p2005,d3,f117c6f12d067bd66dad40996b3931c069daa2da,c81,ACM Symposium on Applied Computing,Business Intelligence and Analytics: From Big Data to Big Impact,"Business intelligence and analytics (BI&A) has emerged as an important area of study for both practitioners and researchers, reflecting the magnitude and impact of data-related problems to be solved in contemporary business organizations. This introduction to the MIS Quarterly Special Issue on Business Intelligence Research first provides a framework that identifies the evolution, applications, and emerging research areas of BI&A. BI&A 1.0, BI&A 2.0, and BI&A 3.0 are defined and described in terms of their key characteristics and capabilities. Current research in BI&A is analyzed and challenges and opportunities associated with BI&A research and education are identified. We also report a bibliometric study of critical BI&A publications, researchers, and research topics based on more than a decade of related academic and industry publications. Finally, the six articles that comprise this special issue are introduced and characterized in terms of the proposed BI&A research framework.",poster,cp81
Engineering,p2016,d0,9ae6e83cffc77c660900aee8a2982e045700126e,c98,Vision,Partnerships from cannibals with forks: The triple bottom line of 21st‐century business,"Editor's Note: John Elkington's new book, Cannibals with Forks: The Triple Bottom Line of 21st-Century Business, has been hailed as “practical, compassionate and deeply informed, a brilliant synthesis of his genius for cutting through the thicket of tough issues–in the world of business and sustainability–and producing elegant solutions that can be applied today” (Paul Hawken). We are pleased to have the opportunity to publish a selection from this award-winning book. In this discussion of partnerships, Elkington explores how effective, long-term partnerships will be crucial for companies making the transition to sustainability and offers approaches and examples of keen interest. Special thanks to Capstone Publishers, U.K., for their gracious cooperation.",poster,cp98
Political Science,p2016,d15,9ae6e83cffc77c660900aee8a2982e045700126e,c98,Vision,Partnerships from cannibals with forks: The triple bottom line of 21st‐century business,"Editor's Note: John Elkington's new book, Cannibals with Forks: The Triple Bottom Line of 21st-Century Business, has been hailed as “practical, compassionate and deeply informed, a brilliant synthesis of his genius for cutting through the thicket of tough issues–in the world of business and sustainability–and producing elegant solutions that can be applied today” (Paul Hawken). We are pleased to have the opportunity to publish a selection from this award-winning book. In this discussion of partnerships, Elkington explores how effective, long-term partnerships will be crucial for companies making the transition to sustainability and offers approaches and examples of keen interest. Special thanks to Capstone Publishers, U.K., for their gracious cooperation.",poster,cp98
Engineering,p2038,d0,f9af326fc7bb8b25b62ad5e7e6dfc92079f33edc,c43,European Conference on Machine Learning,"Business Model Generation: A handbook for visionaries, game changers and challengers","Business Model Generation is a handbook for visionaries, game changers, and challengers striving to defy outmoded business models and design tomorrow's enterprises. If your organization needs to adapt to harsh new realities, but you don't yet have a strategy that will get you out in front of your competitors, you need Business Model Generation.
Co-created by 470 ""Business Model Canvas"" practitioners from 45 countries, the book features a beautiful, highly visual, 4-color design that takes powerful strategic ideas and tools, and makes them easy to implement in your organization. It explains the most common Business Model patterns, based on concepts from leading business thinkers, and helps you reinterpret them for your own context. You will learn how to systematically understand, design, and implement a game-changing business model--or analyze and renovate an old one. Along the way, you'll understand at a much deeper level your customers, distribution channels, partners, revenue streams, costs, and your core value proposition.

Business Model Generation features practical innovation techniques used today by leading consultants and companies worldwide, including 3M, Ericsson, Capgemini, Deloitte, and others. Designed for doers, it is for those ready to abandon outmoded thinking and embrace new models of value creation: for executives, consultants, entrepreneurs, and leaders of all organizations. If you're ready to change the rules, you belong to ""the business model generation!""",poster,cp43
Business,p2038,d9,f9af326fc7bb8b25b62ad5e7e6dfc92079f33edc,c43,European Conference on Machine Learning,"Business Model Generation: A handbook for visionaries, game changers and challengers","Business Model Generation is a handbook for visionaries, game changers, and challengers striving to defy outmoded business models and design tomorrow's enterprises. If your organization needs to adapt to harsh new realities, but you don't yet have a strategy that will get you out in front of your competitors, you need Business Model Generation.
Co-created by 470 ""Business Model Canvas"" practitioners from 45 countries, the book features a beautiful, highly visual, 4-color design that takes powerful strategic ideas and tools, and makes them easy to implement in your organization. It explains the most common Business Model patterns, based on concepts from leading business thinkers, and helps you reinterpret them for your own context. You will learn how to systematically understand, design, and implement a game-changing business model--or analyze and renovate an old one. Along the way, you'll understand at a much deeper level your customers, distribution channels, partners, revenue streams, costs, and your core value proposition.

Business Model Generation features practical innovation techniques used today by leading consultants and companies worldwide, including 3M, Ericsson, Capgemini, Deloitte, and others. Designed for doers, it is for those ready to abandon outmoded thinking and embrace new models of value creation: for executives, consultants, entrepreneurs, and leaders of all organizations. If you're ready to change the rules, you belong to ""the business model generation!""",poster,cp43
Engineering,p2075,d0,7df18f98e3f55e06a68f1d658c66e93840acf7b2,c46,Ideal,"The Internet Galaxy: Reflections on the Internet, Business, and Society","From the Publisher: 
Manuel Castells is one of the world's leading thinkers on the new information age, hailed by The Economist as ""the first significant philosopher of cyberspace,"" and by Christian Science Monitor as ""a pioneer who has hacked out a logical, well-documented, and coherent picture of early 21st century civilization, even as it rockets forward largely in a blur."" Now, in The Internet Galaxy, this brilliantly insightful writer speculates on how the Internet will change our lives. 
Castells believes that we are ""entering, full speed, the Internet Galaxy, in the midst of informed bewilderment."" His aim in this exciting and profound work is to help us to understand how the Internet came into being, and how it is affecting every area of human life--from work, politics, planning and development, media, and privacy, to our social interaction and life in the home. We are at ground zero of the new network society. In this book, its major commentator reveals the Internet's huge capacity to liberate, but also its ability to marginalize and exclude those who do not have access to it. Castells provides no glib solutions, but asks us all to take responsibility for the future of this new information age. 
The Internet is becoming the essential communication and information medium in our society, and stands alongside electricity and the printing press as one of the greatest innovations of all time. The Internet Galaxy offers an illuminating look at how this new technology will influence business, the economy, and our daily lives.",poster,cp46
Engineering,p2079,d0,437e569724b454aab00a819653a68125cb5f30ea,c59,Australian Software Engineering Conference,Designing the Business Models for Circular Economy—Towards the Conceptual Framework,"Switching from the current linear model of economy to a circular one has recently attracted increased attention from major global companies e.g., Google, Unilever, Renault, and policymakers attending the World Economic Forum. The reasons for this are the huge financial, social and environmental benefits. However, the global shift from one model of economy to another also concerns smaller companies on a micro-level. Thus, comprehensive knowledge on designing circular business models is needed to stimulate and foster implementation of the circular economy. Existing business models for the circular economy have limited transferability and there is no comprehensive framework supporting every kind of company in designing a circular business model. This study employs a literature review to identify and classify the circular economy characteristics according to a business model structure. The investigation in the eight sub-domains of research on circular business models was used to redefine the components of the business model canvas in the context of the circular economy. Two new components—the take-back system and adoption factors—have been identified, thereby leading to the conceptualization of an extended framework for the circular business model canvas. Additionally, the triple fit challenge has been recognized as an enabler of the transition towards a circular business model. Some directions for further research have been outlined, as well.",poster,cp59
Engineering,p2085,d0,37875c9bb4159639d77bf0f7188ff23d593850d2,c113,International Conference on Mobile Data Management,The Long Tail: Why the Future of Business is Selling Less of More,"What happens when the bottlenecks that stand between supply and demand in our culture go away and everything becomes available to everyone? The Long Tail is a powerful new force i n our economy: the rise of the niche. As the cost of reaching consumers drops dramatically, our markets are shifting from a one-size-fits-all model of mass appeal to one of unlimited variety for unique tastes. From supermarket shelves to advertising agencies, the ability to offer vast choice is changing everything, and causing us to rethink where our markets lie and how to get to them. Unlimited selection is revealing truths about what consumers want and how they want to get it, from DVDs at Netflix to songs on iTunes to advertising on Google. However, this is not just a virtue of online marketplaces; it is an example of an entirely new economic model for business, one that is just beginning to show its power. After a century of obsessing over the few products at the head of the demand curve, the new economics of distribution allow us to turn our focus to the many more products in the tail, which collectively can create a new market as big as the one we already know. The Long Tail is really about the economics of abundance. New efficiencies in distribution, manufacturing, and marketing are essentially resetting the definition of what’s commercially viable across the board. If the 20th century was about hits, the 21st will be equally about niches.",poster,cp113
Engineering,p2117,d0,ff5e4b35b3aa4b3f4d71640ee7e9ab10ee8f5ca5,c118,International Conference on Image Analysis and Processing,The Six Core Elements of Business Process Management,Abstract,poster,cp118
Computer Science,p2117,d3,ff5e4b35b3aa4b3f4d71640ee7e9ab10ee8f5ca5,c118,International Conference on Image Analysis and Processing,The Six Core Elements of Business Process Management,Abstract,poster,cp118
Business,p2117,d9,ff5e4b35b3aa4b3f4d71640ee7e9ab10ee8f5ca5,c118,International Conference on Image Analysis and Processing,The Six Core Elements of Business Process Management,Abstract,poster,cp118
Engineering,p2127,d0,dd9a88b2c21325228d1f1e4feaec1b01b11a43b0,c97,International Conference on Computational Logic,Riding the Waves of Culture: Understanding Diversity in Global Business,"First published nearly 20 years ago, RIDING THE WAVES OF CULTURE has now become the standard guide to leading effectively in an international business context. Now, the third edition takes you beyond cross-cultural awareness and 'issues' to help you take strategic advantage of cultural differences in the business environment. Leveraging their expansive cultural database as well as brand-new research findings, the authors explain how to build the skills, sensitivity and cultural awareness necessary for managing effectively across cultural borders - and seize all the opportunities diversity brings to an organisation.",poster,cp97
Engineering,p2131,d0,b241d167a2b194bec93f22c5608013845f89cdb9,c118,International Conference on Image Analysis and Processing,Business dynamics: Systems thinking and modeling for a complex world,"Need an excellent e-book? business dynamics systems thinking and modeling for a complex world with cd rom by , the best one! Wan na get it? Discover this exceptional electronic book by below now. Download or read online is available. Why we are the most effective website for downloading this business dynamics systems thinking and modeling for a complex world with cd rom Naturally, you could pick the book in numerous data kinds and also media. Look for ppt, txt, pdf, word, rar, zip, and also kindle? Why not? Get them below, currently!",poster,cp118
Engineering,p2149,d0,33dff66a8774900c57ee8cd9188c3e694be06c4c,c85,IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering,The Visible Hand: The Managerial Revolution in American Business,Abstract,poster,cp85
Engineering,p2198,d0,436656cbd9d0bd6541e1055db5f5e94daa72ef52,j62,Business & Information Systems Engineering,Business Models,Abstract,fullPaper,jv62
Engineering,p2217,d0,2fac6c52c495c7f66265c9a3668b6f67c295b745,c55,Design Automation Conference,The Business Model Navigator: 55 Models That Will Revolutionise Your Business,"A strong business model is the bedrock to business success. But all too often we fail to adapt, clinging to outdated models that are no longer delivering the results we need. The brains behind The Business Model Navigator have discovered that just 55 business models are responsible for 90% of the world's most successful businesses. These 55 models - from the Add-On model used by Ryanair to the Subscription model used by Spotify - provide the blueprints you need to revolutionise your business and drive powerful change. As well as providing a practical framework for adapting and innovating your business model, this book also includes each of the 55 models in a quick-read format that covers: * What it is * Who invented it and who uses it now * When and how to apply it ""An excellent toolkit for developing your business model."" Dr Heinz Derenbach, CEO, Bosch Software Innovations",poster,cp55
Engineering,p2250,d0,77f99d7e47161acea44a825d633bfabe0f404c60,c23,International Conference on Open and Big Data,The experience economy : work is theatre & every business a stage,"Future economic growth lies in the value of experiences and transformations--good and services are no longer enough. We are on the threshold, say authors Pine and Gilmore, of the Experience Economy, a new economic era in which all businesses must orchestrate memorable events for their customers. The Experience Economy offers a creative, highly original, and yet eminently practical strategy for companies to script and stage the experiences that will transform the value of what they produce. From America Online to Walt Disney, the authors draw from a rich and varied mix of examples that showcase businesses in the midst of creating personal experiences for both consumers and businesses. The authors urge managers to look beyond traditional pricing factors like time and cost, and consider charging for the value of the transformation that an experience offers. Goods and services, say Pine and Gilmore, are no longer enough. Experiences and transformations are the basis for future economic growth, and The Experience Economy is the script from which managers can begin to direct their own transformations.",poster,cp23
Engineering,p2259,d0,c46cd035ba0a739d0d6ba5e793aac6ed60d8e911,j424,Business Process Management Journal,Ten principles of good business process management,"Purpose – The purpose of this paper is to foster a common understanding of business process management (BPM) by proposing a set of ten principles that characterize BPM as a research domain and guide its successful use in organizational practice. Design/methodology/approach – The identification and discussion of the principles reflects the viewpoint, which was informed by extant literature and focus groups, including 20 BPM experts from academia and practice. Findings – The authors identify ten principles which represent a set of capabilities essential for mastering contemporary and future challenges in BPM. Their antonyms signify potential roadblocks and bad practices in BPM. The authors also identify a set of open research questions that can guide future BPM research. Research limitations/implications – The findings suggest several areas of research regarding each of the identified principles of good BPM. Also, the principles themselves should be systematically and empirically examined in future studies....",fullPaper,jv424
Computer Science,p2259,d3,c46cd035ba0a739d0d6ba5e793aac6ed60d8e911,j424,Business Process Management Journal,Ten principles of good business process management,"Purpose – The purpose of this paper is to foster a common understanding of business process management (BPM) by proposing a set of ten principles that characterize BPM as a research domain and guide its successful use in organizational practice. Design/methodology/approach – The identification and discussion of the principles reflects the viewpoint, which was informed by extant literature and focus groups, including 20 BPM experts from academia and practice. Findings – The authors identify ten principles which represent a set of capabilities essential for mastering contemporary and future challenges in BPM. Their antonyms signify potential roadblocks and bad practices in BPM. The authors also identify a set of open research questions that can guide future BPM research. Research limitations/implications – The findings suggest several areas of research regarding each of the identified principles of good BPM. Also, the principles themselves should be systematically and empirically examined in future studies....",fullPaper,jv424
Sociology,p2259,d4,c46cd035ba0a739d0d6ba5e793aac6ed60d8e911,j424,Business Process Management Journal,Ten principles of good business process management,"Purpose – The purpose of this paper is to foster a common understanding of business process management (BPM) by proposing a set of ten principles that characterize BPM as a research domain and guide its successful use in organizational practice. Design/methodology/approach – The identification and discussion of the principles reflects the viewpoint, which was informed by extant literature and focus groups, including 20 BPM experts from academia and practice. Findings – The authors identify ten principles which represent a set of capabilities essential for mastering contemporary and future challenges in BPM. Their antonyms signify potential roadblocks and bad practices in BPM. The authors also identify a set of open research questions that can guide future BPM research. Research limitations/implications – The findings suggest several areas of research regarding each of the identified principles of good BPM. Also, the principles themselves should be systematically and empirically examined in future studies....",fullPaper,jv424
Engineering,p2306,d0,77ed59b7ad32bf92ce77c216b7833c5bc3cbca61,c88,International Conference on Big Data Computing and Communications,The Performance Prism: The Scorecard for Measuring and Managing Business Success,"This book takes a radically different look at performance measurement and sets out explicitly to identify how managers can use measurement data to improve business performance. The key features of the proposed book are: * Critical reviews of the major measurement frameworks available today - including the balanced scorecard, the business excellence model and shareholder value analysis. * Introduction of a new measurement framework - the performance prism - that addresses the shortcomings of the exisiting measurement frameworks. * Explanation of the tools, techniques and methodologies that can be used to measure stakeholder satisfaction, strategies, processes, capabilities and stakeholder contribution - the five facets of the performance prism. * Explanation of how managers can make the most of their measurement data. Specific discussions about how managers can analyse business performance data and identify appropriate improvement priorities. OUTLINE Chapter 1 - The Measurement Crisis This chapter will explore why measurement is on management's agenda and review recent developments in the field. A key theme in the chapter will be the changing nature of the measurement crisis. In the 1980s, when people first started worrying about the effectiveness of their measurement systems, the problem was that we were measuring the wrong things. Today the problem is that we are measuring too much. The chapter will explore this theme and encourage management to take a step back and think clearly about what they want from their measurement systems. Chapter 2 - The Measurement Frameworks This chapter will review the major measurement frameworks that are available today and identify their strengths and weaknesses. Frameworks to be covered will include the balanced scorecard, the business excellence model, shareholder value. A key theme that will run throughout the chapter will be the fact that each of these measurement frameworks offers a partial view of business performance. Chapter 3 - The Performance Prism This chapter will introduce a new, comprehensive measurement framework - the performance prism. The prism looks at measurement from a stakeholder perspective. When deciding what to measure managers have to first identify who their stakeholders are and what they want and need. Only then can they begin to decide what they should measure. Many of the existing works on measurement fail to recognise this, with numerous authors suggesting that measures should be derived from strategy. Measures should be consistent with strategy, but they should not be derived from strategy. Organisations have strategies because they want to deliver value to stakeholders. Hence the starting point for any discussion of measurement has to be stakeholders. The chapter on the performance prism will build upon this theme and explain how the performance prism provides a structure that encourages managers to answer five inter-related questions when designing their measurement systems: * Stakeholder satisfaction - who are the stakeholders and what do they want and need? * Strategies - what strategies do we need to deliver value to stakeholders? * Processes - what processes do we require to deliver these strategies? * Capabilities - what capabilities do we require to execute these processes? * Stakeholder contribution - what do we want and need from our stakeholders to enable all of the above to happen? Chapter 4 - Measuring Stakeholder Satisfaction This chapter will explore developments in the measurement of stakeholder satisfaction - investors, customers, employees, suppliers, regulators and local communities. Chapter 5 - Measuring Strategies This chapter will explore developments in the measurement of strategy - especially execution and effectiveness. Chapter 6 - Measuring Processes This chapter will explore developments in the measurement of processes - especially those associated with generating demand, fulfilling demand, developing new products and services and planning and managing the enterprise. Chapter 7 - Measuring Capabilities This chapter will explore developments in the measurement of capabilities - combinations of technology, people, practices and infrastructure. Chapter 8 - Measuring Stakeholder Contribution This chapter will explore developments in the measurement of stakeholder contribution - investment levels, customer and employee loyalty, supplier performance. Chapter 9 - Analysing Performance Data This chapter will build on the previous chapters by illustrating how performance data can be analysed once the data has been captured. Various tools, techniques and methodologies will be discussed and explained. There will be a strong emphasis throughout the chapter on the practicalities of making the most of measurement data. Chapter 10 - Improving Business Performance This chapter will concentrate on how to drive improvements in business performance following a systematic analysis of the data. Practical examples will be offered throughout that will illustrate how particular firms have used their measurement data. Appendix - The Measures Catalogue The authors have been working together over a period of months to construct a catalogue of best practice performance measures that identifies specific measures of performance that organisations can use to monitor their progress. The catalogue is structured around the performance prism framework and is currently available on the web (www.cranfield.ac.uk/som/cbp). While it would be impossible to include a paper copy of the catalogue as an appendix, it might be possible to offer it on a CD as further value added feature of the book.",poster,cp88
Engineering,p2312,d0,1fce3a7588aae2b18b976849556e2f36c8bf4b78,c1,International Conference on Human Factors in Computing Systems,Reframing Business: When the Map Changes the Landscape,"In 1983 Richard Normann published the world's first book presenting an integrated framework on the management of service producing companies. Now he provides a new approach to strategy: an original way to think about organisations and create a different future. In this demanding but rewarding book he shows that providing organisations are prepared to rethink the way they do business they can occupy the competitive high ground of the future. To do this they must transform concepts and frameworks into action. Provides new business models. Shows companies how to reframe their business and take advantage of the opportunities created in the space of ""unbundling and rebundling"". 
 
Nominated for the Igor Ansoff Strategic Management Award 2002",poster,cp1
Engineering,p2326,d0,eaf062a5424cb4dad5f35efb1c64e3e8b8ed5737,c110,Biometrics and Identity Management,Real-Time Measurement of Business Conditions,"We construct a framework for measuring economic activity at high frequency, potentially in real time. We use a variety of stock and flow data observed at mixed frequencies (including very high frequencies), and we use a dynamic factor model that permits exact filtering. We illustrate the framework in a prototype empirical example and a simulation study calibrated to the example.",poster,cp110
Computer Science,p2326,d3,eaf062a5424cb4dad5f35efb1c64e3e8b8ed5737,c110,Biometrics and Identity Management,Real-Time Measurement of Business Conditions,"We construct a framework for measuring economic activity at high frequency, potentially in real time. We use a variety of stock and flow data observed at mixed frequencies (including very high frequencies), and we use a dynamic factor model that permits exact filtering. We illustrate the framework in a prototype empirical example and a simulation study calibrated to the example.",poster,cp110
Economics,p2326,d11,eaf062a5424cb4dad5f35efb1c64e3e8b8ed5737,c110,Biometrics and Identity Management,Real-Time Measurement of Business Conditions,"We construct a framework for measuring economic activity at high frequency, potentially in real time. We use a variety of stock and flow data observed at mixed frequencies (including very high frequencies), and we use a dynamic factor model that permits exact filtering. We illustrate the framework in a prototype empirical example and a simulation study calibrated to the example.",poster,cp110
Engineering,p2328,d0,3cf4f6d6123c0c5bf3106b4278f5807ba759f938,c37,International Workshop on the Semantic Web,"Business Process Change: A Study of Methodologies, Techniques, and Tools","Growth in Business Process Reengineering (BPR) consulting services has led to a proliferation of methods for conducting BPR. Sifting through vendor promotional hype and identifying a set of techniques and tools that best meets a particular project's needs can be a daunting task. This article investigates BPR Methods, Techniques, and Tools (MTTs) and places them within an empirically derived reference framework. A comprehensive picture of BPR emerges that includes MTTs that help in reengineering strategy, people, management, structure, and technology dimensions of business processes. A BPR planning approach for customizing this framework based on unique project characteristics is then offered to assist in selecting those BPR project activities and techniques to be emphasized. This flexible framework and comprehensive survey of commonly used BPR techniques and tools forms a knowledge base to improve business process change practice and provides a basis for future BPR research.",poster,cp37
Computer Science,p2328,d3,3cf4f6d6123c0c5bf3106b4278f5807ba759f938,c37,International Workshop on the Semantic Web,"Business Process Change: A Study of Methodologies, Techniques, and Tools","Growth in Business Process Reengineering (BPR) consulting services has led to a proliferation of methods for conducting BPR. Sifting through vendor promotional hype and identifying a set of techniques and tools that best meets a particular project's needs can be a daunting task. This article investigates BPR Methods, Techniques, and Tools (MTTs) and places them within an empirically derived reference framework. A comprehensive picture of BPR emerges that includes MTTs that help in reengineering strategy, people, management, structure, and technology dimensions of business processes. A BPR planning approach for customizing this framework based on unique project characteristics is then offered to assist in selecting those BPR project activities and techniques to be emphasized. This flexible framework and comprehensive survey of commonly used BPR techniques and tools forms a knowledge base to improve business process change practice and provides a basis for future BPR research.",poster,cp37
Engineering,p2345,d0,278c2e6b21447e75bb13608dfa3a7215b958c788,c38,IEEE Global Engineering Education Conference,"Software Reuse: Architecture, Process And Organization For Business Success","Systematic software reuse is the most effective way to significantly improve software development. Many organizations adopt object technology expecting significant reuse. Without an explicit reuse process, they will not succeed. Companies succeeding with reuse find that architecture, process, organization, culture, management and other non-technical factors are usually more critical than technology. This paper describes HP's software reuse practice and adoption experience, and a systematic approach to component-based software engineering based on object-oriented business and system modeling.",poster,cp38
Computer Science,p2345,d3,278c2e6b21447e75bb13608dfa3a7215b958c788,c38,IEEE Global Engineering Education Conference,"Software Reuse: Architecture, Process And Organization For Business Success","Systematic software reuse is the most effective way to significantly improve software development. Many organizations adopt object technology expecting significant reuse. Without an explicit reuse process, they will not succeed. Companies succeeding with reuse find that architecture, process, organization, culture, management and other non-technical factors are usually more critical than technology. This paper describes HP's software reuse practice and adoption experience, and a systematic approach to component-based software engineering based on object-oriented business and system modeling.",poster,cp38
Engineering,p2353,d0,d254d9d0f650f211e10c42976ad39da5ca01b5d5,c83,"International Convention on Information and Communication Technology, Electronics and Microelectronics",Business Process Management: The Third Wave,"From the Publisher: 
The first limited distribution edition of this groundbreaking book was published in September 2002, and subsequently designed for ""fast track"" reads by either business or technology readers. The business impact is covered in the first 197 pages. Ten years ago, Computer Sciences Corporation's James Champy co-authored the New York Times best seller, Reengineering the Corporation, that set the world alight with over 2,000,000 copies in print. But that was last decade. Ten years on, Computer Sciences Corporation's Howard Smith, has co-authored the book that reinvents reengineering and sets the business agenda for the decade ahead.",poster,cp83
Engineering,p2375,d0,e5a2f90ec5a45539ea005f3ab305c4f0d4fc1a20,c35,"International Conference on Internet of Things, Big Data and Security",Case Study Methodology in Business Research,"The complete guide for how to design and conduct theory-testing and other case studies Case Study Methodology in Business Research sets out structures and guidelines that assist students and researchers from a wide range of disciplines to develop their case study research in a consistent and rigorous manner. It clarifies the differences between practice-oriented and theory-oriented research and, within the latter category, between theory-testing and theory-building. It describes in detail how to design and conduct different types of case study research, providing students and researchers with everything they need for their project.The main aims are to: present a broad spectrum of types of case study research (including practice-oriented case studies, theory-building case studies and theory-testing case studies) in one consistent methodological framework. emphasize and clearly illustrate that the case study is the preferred research strategy for testing deterministic propositions such as those expressing a necessary condition case by case and that the survey is the preferred research strategy for testing probabilistic propositions. stress the role of replication in all theory-testing research, irrespective of which research strategy is chosen for a specific test. give more weight to the importance of theory-testing relative to theory-building. Case Study Methodology in Business Research is a clear, concise and comprehensive text for case study methodology. 
 
Templates are supplied for case study protocol and how to report a case study. A modular textbook primarily aimed at serving research methodology courses for final year undergraduate students and graduate students in Business Administration and Management, which is also useful as a handbook for researchers. Written by Jan Dul, Professor of Technology and Human Factors, RSM Erasmus University, Rotterdam and Tony Hak, Associate professor of Research Methodology, RSM Erasmus University, Rotterdam, in collaboration with other authors from RSM Erasmus University.",poster,cp35
Engineering,p2410,d0,f7e7b3d40b45bf245cc6dc7b9c1d7c762f333647,c31,Information Security Solutions Europe,Essentials of Entrepreneurship and Small Business Management,Section I. The Challenge of Entrepreneurship Chapter 1. The Foundations of Entrepreneurship Chapter 2. Inside the Entrepreneurial Mind: From Ideas to Reality Section II. Building the Business Plan: Beginning Considerations Chapter 3. Designing a Competitive Business Model and Building a Solid Strategic Plan Chapter 4. Conducting a Feasibility Analysis and Crafting a Winning Business Plan Chapter 5. Forms of Business Ownership Chapter 6. Franchising and the Entrepreneur Chapter 7. Buying an Existing Business Section III. Building the Business Plan: Marketing and Financial Considerations Chapter 8. Building a Powerful Marketing Plan Chapter 9. E-Commerce and the Entrepreneur Chapter 10. Pricing Strategies Chapter 11. Creating a Successful Financial Plan Chapter 12. Managing Cash Flow Section IV. Putting the Business Plan to Work: Sources of Funds Chapter 13. Sources of Financing: Debt and Equity Chapter 14. Choosing the Right Location and Layout Chapter 15. Global Aspects of Entrepreneurship Chapter 16. Building a New Venture Team and Planning for the Next,poster,cp31
Engineering,p2412,d0,2f8b07d5059caef5f8876055d45d43707e8fcd0b,c9,Big Data,Seizing the White Space: Business Model Innovation for Growth and Renewal,Table of Contents Foreword Acknowledgments SECTION I: A NEW MODEL FOR GROWTH AND RENEWAL 1 The White Space and Business Model Innovation 2 The Four-Box Business Model Framework SECTION II: WHEN NEW BUSINESS MODELS ARE NEEDED 3 The White Space Within: Transforming Existing Markets 4 The White Space Beyond: Creating New Markets 5 The White Space Between: Dealing with Industry Discontinuity SECTION III: BUSINESS MODEL INNOVATION AS A REPEATABLE PROCESS 6 Designing a New Business Model 7 Implementation 8 Overcoming Incumbent Challenges Epilogue,poster,cp9
Engineering,p2421,d0,1219b1692e5c4afc400acbf23e5625c556d10fde,c107,Annual Haifa Experimental Systems Conference,"The business model: Theoretical roots, recent developments, and future research","The paper provides a broad and multifaceted review of the received literature on business models, in which we attempt to explore the origin of the construct and to examine the business model concept through multiple disciplinary and subject-matter lenses. The review reveals that scholars do not agree on what a business model is, and that the literature is developing largely in silos, according to the phenomena of interest to the respective researchers. However, we also found some emerging common ground among students of business models. Specifically, i) the business model is emerging as a new unit of analysis; ii) business models emphasize a system-level, holistic approach towards explaining how firms do business; iii) organizational activities play an important role in the various conceptualizations of business models that have been proposed, and iv) business models seek not only to explain the ways in which value is captured but also how it is created. These emerging themes could serve as important catalysts towards a more unified study of business models.",poster,cp107
Engineering,p2422,d0,4d5134c829ed07e15fedaa514473f43d5f4b3f11,c57,IEEE International Geoscience and Remote Sensing Symposium,e-Business: Roadmap for Success,"(All chapters conclude with a Memo to the CEO.) Foreword. Preface. Moving from e-Commerce to e-Business. What Makes This Book Different? Who Should Read This Book? How This Book Is Organized. Acknowledgments. 1. From e-Commerce to e-Business. Linking Today's Business with Tomorrow's Technology. e-Business = Structural Transformation. e-Business Requires Flexible Business Designs. Challenge Traditional Definitions of Value. Define Value in Terms of the Whole Customer Experience. e-Business Communities: Engineering the End-to-End Value Stream. Integrate, Integrate, Integrate: Create the New Techno- Enterprise. Needed: A New Generation of e-Business Leaders. 2. e-Business Trend Spotting. Increase Speed of Service: For the Customer, Time Is Money. Empower Your Customer: Self-Service. Provide Integrated Solutions, Not Piecemeal Products. Integrate Your Sales and Service: Customization and Integration. Ease of Use: Make Customer Service Consistent and Reliable. Provide Flexible Fulfillment and Convenient Service Delivery. Contract Manufacturing: Become Brand Intensive, Not Capital Intensive. Learn to Outsource: You Cannot Be Good at Everything. Increase Process Visibility: Destroy the Black Box. Learn the Trends in Employee Retention. Integrated Enterprise Applications: Connect the Corporation. Meld Voice, Data, and Video. Multichannel Integration: Look at the Big Picture. Wireless Applications Enter the Mainstream. Middleware: Supporting the Integration Mandate. What Is Common to All These Trends? 3. Think e-Business Design, Not Just Technology. Constructing an e-Business Design. The First Step of e-Business Design: Self-Diagnosis. The Second Step of e-Business Design: Reversing the Value Chain. The Third Step of e-Business: Choosing a Narrow Focus. Case Study: Service Excellence at American Express. Case Study: Operational Excellence at Dell Computer. Case Study: Continuous Innovation at Cisco Systems. Business Design Lessons Learned. 4. Constructing the e-Business Architecture. Why Is Application Integration Important? The New Era of Cross-Functional Integrated Apps. Integrating Application Clusters into an e-Business Architecture. Aligning the e-Business Design with Application Integration. 5. Customer Relationship Management: Integrating Processes to Build Relationships. Why Customer Relationship Management? Defining Customer Relationship Management. Organizing around the Customer: The New CRM Architecture. Supporting Requirements of the Next-Generation CRM Infrastructure. Organizational Challenges in Implementing CRM. Next-Generation CRM Trends. Building a CRM Infrastructure: A Manager's Roadmap. 6. Selling-Chain Management: Transforming Sales into Interactive Order Acquisition. Defining Selling-Chain Management. Business Forces Driving the Need for Selling-Chain Management. Technology Forces Driving the Need for Selling-Chain Management. Managing the Order Acquisition Process. Cisco and Selling-Chain Management. Elements of Selling-Chain Infrastructure. The Custom Foot: Transforming Shoe Sales with Technology. 7. Enterprise Resource Planning: The e-Business Backbone. Why Is Management Willingly Paying Millions for ERP Suites? ERP Decision = Enterprise Architecture Planning. The COTS ERP That Keeps on Ticking: The SAP Juggernaut. ERP Usage in the Real World. ERP Implementation: Catching the Bull by the Horns. The Future of ERP Applications. 8. Supply Chain Management: Interenterprise Fusion. Defining Supply Chain Management. Basics of Internet-Enabled SCM: e-Supply Chain 101. Basics of Internet-Enabled SCM: e-Supply Chain 201. e-Supply Chain Fusion: e-Supply Chain 301. e-Supply Chain Fusion Management Issues. The Future: e-Supply Chains in 200X. Supply Chain Management: A Manager's Roadmap. 9. e-Procurement: The Next Wave of Cost Reduction. Structural Transition: From Isolated Purchasing to Real-Time Process Integration. Why Is Procurement a Top-Management Issue? What Exactly Is Operating Resource Procurement? Operating Resource Procurement at Microsoft: MS Market. Procurement Business Problem: Lack of Process Integration. Next-Generation Integrated Procurement Applications. Elements of Buy-Side e-Procurement Solutions. Buy-Side Applications for the Procurement Professional. Elements of Sell-Side e-Procurement Solutions. The e-Procurement Manager's Roadmap. 10. Knowledge-Tone Applications: The Next Generation of Decision Support Systems. Knowledge Apps: Why They Are Important. Knowledge Tone Is an Application Framework. Emerging Classes of Knowledge-Tone Applications. Knowledge-Tone Usage in the Real World. Tech Trends Driving Knowledge-Tone Framework Investments. Elements of the Knowledge-Tone Architectural Framework. Core Technologies: Data Warehousing. Enabling Technologies: Online Analytical Processing. A Roadmap to Knowledge-Tone Framework. 11. Developing the e-Business Design. The Challenges of e-Business Strategy Creation. Roadmap to Moving Your Company into e-Business. Phase 1: Knowledge Building. Phase 2: Capability Evaluation. Phase 3: e-Business Design. e-Business Design in Action: The Case of E*TRADE. 12. Translating e-Business Strategy into Action. e-Business Blueprint Creation Is Serious Business. Basic Steps of e-Business Blueprint Planning. Doing the Right Projects: A Prioritization Blueprint. Putting It All Together: The e-Business Blueprint Case. Key Elements of a Business Case. Communicate, Communicate, Communicate. e-Business Project Planning Checklist. Doing the Projects Right: An Execution Blueprint. Why e-Business Initiatives Fail. Endnotes. Index.",poster,cp57
Engineering,p2435,d0,08700c0e8a591d8e689fa290b34954db926be65d,c96,Human Language Technology - The Baltic Perspectiv,Business Networks,Abstract,poster,cp96
Computer Science,p2435,d3,08700c0e8a591d8e689fa290b34954db926be65d,c96,Human Language Technology - The Baltic Perspectiv,Business Networks,Abstract,poster,cp96
Engineering,p2443,d0,adac59e8e0570c796aa476aed1f340d53269bba5,c119,International Conference on Business Process Management,Modeling Control Objectives for Business Process Compliance,Abstract,fullPaper,cp119
Computer Science,p2443,d3,adac59e8e0570c796aa476aed1f340d53269bba5,c119,International Conference on Business Process Management,Modeling Control Objectives for Business Process Compliance,Abstract,fullPaper,cp119
Engineering,p2447,d0,ea196ee71722ad9fe398991d7de7664b6b8ee7ff,c117,Very Large Data Bases Conference,"E-commerce: Business, Technology, Society","From the Publisher: 
B> E-Commerce: Concepts and Issues provides an overview of the current and next generations of e-commerce. The book emphasizes the three major driving forces behind e-commerce: technology change, business development, and social controversies. Each of these driving forces is represented in every chapter, and together they provide a coherent conceptual framework for understanding e-commerce. The result is sophisticated conceptual treatment of a very diverse subject that is aimed specifically at readers interested in business concepts, IS/IT developments, and computer science applications. It is written by an author team with extensive teaching, writing, and business experience. This book offers in-depth coverage of concepts in technology, the Internet, economics, marketing, IS/IT, accounting, privacy, intellectual property, equity, and governance. Its unifying conceptual framework built around the themes of business, technology, and society helps readers make sense out of the development of this field. The book presents numerous real-world examples in every chapter. An attractive full-color design is also featured. This book is aimed specifically at readers interested in business concepts, IS/IT developments, and computer science applications of e-commerce.",poster,cp117
Engineering,p2451,d0,2fa60a50edf4bd67188b268629e1d72e9c5fa2c9,c11,European Conference on Modelling and Simulation,e-Business 2.0: Roadmap for Success,"Revolutions begin with attacks on the language. Today's managers are bombarded with an alphabet soup of technology acronyms such as CRM, ERP, and SCM. Managers must manage in the fog of this technology revolution. Kalakota and Robinson's book provides a lens for managers who want to understand the terminology and take action--to grasp the opportunities or defend against the invading virtual competitors. When the history of e-business is written--perhaps sooner than we think--this book will go down as a management landmark that helped clear the fog of mumbo jumbo and provided a beacon through which managers could chart their course. --Michael Quinn, eStrategy Director, B2B Commerce Ltd.To survive and succeed in today's complex business world, all companies--from established industry leaders to feisty upstarts--must develop a strategy that allows them to take maximum advantage of the latest trends in technology. Successful companies have implemented focused e-business strategies to build cutting-edge enterprises that serve and retain customers, manage suppliers, and integrate selling chains better than ever before. Others, unfortunately, are lured into ill-fated ventures by the ever-changing roster of buzzwords, fads and analogies.A timely follow-up to the best-seller on this crucial topic, e-Business 2.0 reveals how managers are rewiring the enterprise to take maximum advantage of e-business. Ravi Kalakota and Marcia Robinson present an innovative application framework that guides the migration from a traditional business model to an e-business model. Drawing on their extensive personal experience working with leading businesses, they provide a clear picture of the benefits and challenges that e-business companies face and identify the fundamental design principles for building a successful e-business blueprint.This new edition incorporates the latest strategies and techniques gained from the experiences of the first generation of e-businesses. In addition to updated information on application framework design, the book now features more detail on strategy and e-business execution. e-Business 2.0 shows how e-commerce has evolved into e-business and identifies the 20 key e-business trends that are shaping today's economy. It then addresses core application frameworks--customer relationship management, selling chain management, enterprise resource planning, supply chain management, e-procurement, and business intelligence--and shows how each forms the foundation of an e-business strategy. 0201721651B06012001",poster,cp11
Engineering,p2458,d0,f3ed2c8153107bf529d1ba13a04ca8e8f5568152,c38,IEEE Global Engineering Education Conference,Business Process Modeling Notation,Abstract,poster,cp38
Computer Science,p2458,d3,f3ed2c8153107bf529d1ba13a04ca8e8f5568152,c38,IEEE Global Engineering Education Conference,Business Process Modeling Notation,Abstract,poster,cp38
Engineering,p2467,d0,f0367d2e60ac469bf1bc098bc332a5f1b24f0723,c37,International Workshop on the Semantic Web,Internet Business Models and Strategies,"Note: Second edition, August, 2002. Adopted within the first year of publication by 155 universities in 20 countries worldwide and translated into Chinese, Korean, Italian, Russian, and Polish; adopted by over 200 universities total. Third Edition, under development for release in 2013. Reference CSI-BOOK-2003-001 Record created on 2004-09-07, modified on 2017-05-12",poster,cp37
Medicine,p6,d1,fb29359d794265c0931d756858a70c9265b5693d,c82,Symposium on Networked Systems Design and Implementation,The R Language: An Engine for Bioinformatics and Data Science,"The R programming language is approaching its 30th birthday, and in the last three decades it has achieved a prominent role in statistics, bioinformatics, and data science in general. It currently ranks among the top 10 most popular languages worldwide, and its community has produced tens of thousands of extensions and packages, with scopes ranging from machine learning to transcriptome data analysis. In this review, we provide an historical chronicle of how R became what it is today, describing all its current features and capabilities. We also illustrate the major tools of R, such as the current R editors and integrated development environments (IDEs), the R Shiny web server, the R methods for machine learning, and its relationship with other programming languages. We also discuss the role of R in science in general as a driver for reproducibility. Overall, we hope to provide both a complete snapshot of R today and a practical compendium of the major features and applications of this programming language.",poster,cp82
Medicine,p8,d1,70fe060c12b3f100148d1a2be1e8f4254022543e,c115,International Conference on Information Integration and Web-based Applications & Services,The case for data science in experimental chemistry: examples and recommendations,Abstract,poster,cp115
Medicine,p11,d1,108acf9a358512a40191d857e2456aeaaac3303b,j3,Genome Research,Diversifying the genomic data science research community,"Over the past 20 years, the explosion of genomic data collection and the cloud computing revolution have made computational and data science research accessible to anyone with a web browser and an internet connection. However, students at institutions with limited resources have received relatively little exposure to curricula or professional development opportunities that lead to careers in genomic data science. To broaden participation in genomics research, the scientific community needs to support these programs in local education and research at underserved institutions (UIs). These include community colleges, historically Black colleges and universities, Hispanic-serving institutions, and tribal colleges and universities that support ethnically, racially, and socioeconomically underrepresented students in the United States. We have formed the Genomic Data Science Community Network to support students, faculty, and their networks to identify opportunities and broaden access to genomic data science. These opportunities include expanding access to infrastructure and data, providing UI faculty development opportunities, strengthening collaborations among faculty, recognizing UI teaching and research excellence, fostering student awareness, developing modular and open-source resources, expanding course-based undergraduate research experiences (CUREs), building curriculum, supporting student professional development and research, and removing financial barriers through funding programs and collaborator support.",fullPaper,jv3
Computer Science,p11,d3,108acf9a358512a40191d857e2456aeaaac3303b,j3,Genome Research,Diversifying the genomic data science research community,"Over the past 20 years, the explosion of genomic data collection and the cloud computing revolution have made computational and data science research accessible to anyone with a web browser and an internet connection. However, students at institutions with limited resources have received relatively little exposure to curricula or professional development opportunities that lead to careers in genomic data science. To broaden participation in genomics research, the scientific community needs to support these programs in local education and research at underserved institutions (UIs). These include community colleges, historically Black colleges and universities, Hispanic-serving institutions, and tribal colleges and universities that support ethnically, racially, and socioeconomically underrepresented students in the United States. We have formed the Genomic Data Science Community Network to support students, faculty, and their networks to identify opportunities and broaden access to genomic data science. These opportunities include expanding access to infrastructure and data, providing UI faculty development opportunities, strengthening collaborations among faculty, recognizing UI teaching and research excellence, fostering student awareness, developing modular and open-source resources, expanding course-based undergraduate research experiences (CUREs), building curriculum, supporting student professional development and research, and removing financial barriers through funding programs and collaborator support.",fullPaper,jv3
Medicine,p13,d1,7f29044de1a0e5a6d3ec1d33fb6ad482f3d10dd4,j5,SN Computer Science,"Data Science and Analytics: An Overview from Data-Driven Smart Computing, Decision-Making and Applications Perspective",Abstract,fullPaper,jv5
Computer Science,p13,d3,7f29044de1a0e5a6d3ec1d33fb6ad482f3d10dd4,j5,SN Computer Science,"Data Science and Analytics: An Overview from Data-Driven Smart Computing, Decision-Making and Applications Perspective",Abstract,fullPaper,jv5
Medicine,p14,d1,370d248f97b75c4040e5828a658bbe4c3b80bf1e,j6,Genome Biology,Eleven grand challenges in single-cell data science,Abstract,fullPaper,jv6
Biology,p14,d5,370d248f97b75c4040e5828a658bbe4c3b80bf1e,j6,Genome Biology,Eleven grand challenges in single-cell data science,Abstract,fullPaper,jv6
Medicine,p17,d1,b79ca6fd3df135a9bcf778844be625b764fbcfb3,j8,Nature Methods,Nucleus segmentation across imaging experiments: the 2018 Data Science Bowl,Abstract,fullPaper,jv8
Computer Science,p17,d3,b79ca6fd3df135a9bcf778844be625b764fbcfb3,j8,Nature Methods,Nucleus segmentation across imaging experiments: the 2018 Data Science Bowl,Abstract,fullPaper,jv8
Medicine,p18,d1,9bcf291c6245a3c2ee101babf4c1f0bbfa166f92,j7,Journal of Big Data,Data science approach to stock prices forecasting in Indonesia during Covid-19 using Long Short-Term Memory (LSTM),Abstract,fullPaper,jv7
Medicine,p20,d1,44321686d59d889af1760357940f04fbb6629597,j9,Irish Journal of Medical Science,"The role of data science in healthcare advancements: applications, benefits, and future prospects",Abstract,fullPaper,jv9
Medicine,p21,d1,8bba999de25bfb288b3f7f88e1d907aab02638b6,j10,Chemical Reviews,Big-Data Science in Porous Materials: Materials Genomics and Machine Learning,"By combining metal nodes with organic linkers we can potentially synthesize millions of possible metal–organic frameworks (MOFs). The fact that we have so many materials opens many exciting avenues but also create new challenges. We simply have too many materials to be processed using conventional, brute force, methods. In this review, we show that having so many materials allows us to use big-data methods as a powerful technique to study these materials and to discover complex correlations. The first part of the review gives an introduction to the principles of big-data science. We show how to select appropriate training sets, survey approaches that are used to represent these materials in feature space, and review different learning architectures, as well as evaluation and interpretation strategies. In the second part, we review how the different approaches of machine learning have been applied to porous materials. In particular, we discuss applications in the field of gas storage and separation, the stability of these materials, their electronic properties, and their synthesis. Given the increasing interest of the scientific community in machine learning, we expect this list to rapidly expand in the coming years.",fullPaper,jv10
Physics,p21,d2,8bba999de25bfb288b3f7f88e1d907aab02638b6,j10,Chemical Reviews,Big-Data Science in Porous Materials: Materials Genomics and Machine Learning,"By combining metal nodes with organic linkers we can potentially synthesize millions of possible metal–organic frameworks (MOFs). The fact that we have so many materials opens many exciting avenues but also create new challenges. We simply have too many materials to be processed using conventional, brute force, methods. In this review, we show that having so many materials allows us to use big-data methods as a powerful technique to study these materials and to discover complex correlations. The first part of the review gives an introduction to the principles of big-data science. We show how to select appropriate training sets, survey approaches that are used to represent these materials in feature space, and review different learning architectures, as well as evaluation and interpretation strategies. In the second part, we review how the different approaches of machine learning have been applied to porous materials. In particular, we discuss applications in the field of gas storage and separation, the stability of these materials, their electronic properties, and their synthesis. Given the increasing interest of the scientific community in machine learning, we expect this list to rapidly expand in the coming years.",fullPaper,jv10
Computer Science,p21,d3,8bba999de25bfb288b3f7f88e1d907aab02638b6,j10,Chemical Reviews,Big-Data Science in Porous Materials: Materials Genomics and Machine Learning,"By combining metal nodes with organic linkers we can potentially synthesize millions of possible metal–organic frameworks (MOFs). The fact that we have so many materials opens many exciting avenues but also create new challenges. We simply have too many materials to be processed using conventional, brute force, methods. In this review, we show that having so many materials allows us to use big-data methods as a powerful technique to study these materials and to discover complex correlations. The first part of the review gives an introduction to the principles of big-data science. We show how to select appropriate training sets, survey approaches that are used to represent these materials in feature space, and review different learning architectures, as well as evaluation and interpretation strategies. In the second part, we review how the different approaches of machine learning have been applied to porous materials. In particular, we discuss applications in the field of gas storage and separation, the stability of these materials, their electronic properties, and their synthesis. Given the increasing interest of the scientific community in machine learning, we expect this list to rapidly expand in the coming years.",fullPaper,jv10
Materials Science,p21,d7,8bba999de25bfb288b3f7f88e1d907aab02638b6,j10,Chemical Reviews,Big-Data Science in Porous Materials: Materials Genomics and Machine Learning,"By combining metal nodes with organic linkers we can potentially synthesize millions of possible metal–organic frameworks (MOFs). The fact that we have so many materials opens many exciting avenues but also create new challenges. We simply have too many materials to be processed using conventional, brute force, methods. In this review, we show that having so many materials allows us to use big-data methods as a powerful technique to study these materials and to discover complex correlations. The first part of the review gives an introduction to the principles of big-data science. We show how to select appropriate training sets, survey approaches that are used to represent these materials in feature space, and review different learning architectures, as well as evaluation and interpretation strategies. In the second part, we review how the different approaches of machine learning have been applied to porous materials. In particular, we discuss applications in the field of gas storage and separation, the stability of these materials, their electronic properties, and their synthesis. Given the increasing interest of the scientific community in machine learning, we expect this list to rapidly expand in the coming years.",fullPaper,jv10
Chemistry,p21,d8,8bba999de25bfb288b3f7f88e1d907aab02638b6,j10,Chemical Reviews,Big-Data Science in Porous Materials: Materials Genomics and Machine Learning,"By combining metal nodes with organic linkers we can potentially synthesize millions of possible metal–organic frameworks (MOFs). The fact that we have so many materials opens many exciting avenues but also create new challenges. We simply have too many materials to be processed using conventional, brute force, methods. In this review, we show that having so many materials allows us to use big-data methods as a powerful technique to study these materials and to discover complex correlations. The first part of the review gives an introduction to the principles of big-data science. We show how to select appropriate training sets, survey approaches that are used to represent these materials in feature space, and review different learning architectures, as well as evaluation and interpretation strategies. In the second part, we review how the different approaches of machine learning have been applied to porous materials. In particular, we discuss applications in the field of gas storage and separation, the stability of these materials, their electronic properties, and their synthesis. Given the increasing interest of the scientific community in machine learning, we expect this list to rapidly expand in the coming years.",fullPaper,jv10
Medicine,p24,d1,0405bfdc3f0ecb8e9d31ae68911731e61a65c01d,j11,Journal of Surgical Oncology,Surgical data science and artificial intelligence for surgical education,"Surgical data science (SDS) aims to improve the quality of interventional healthcare and its value through the capture, organization, analysis, and modeling of procedural data. As data capture has increased and artificial intelligence (AI) has advanced, SDS can help to unlock augmented and automated coaching, feedback, assessment, and decision support in surgery. We review major concepts in SDS and AI as applied to surgical education and surgical oncology.",fullPaper,jv11
Medicine,p28,d1,459c91a1593808c3d6edf50eba621b048d08cda2,c61,"International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems",Data science,Abstract,poster,cp61
Medicine,p29,d1,2d420c7f1675d41b01e694739a25b8b189f2c95f,c3,Knowledge Discovery and Data Mining,Data Science in the Food Industry.,"Food safety is one of the main challenges of the agri-food industry that is expected to be addressed in the current environment of tremendous technological progress, where consumers' lifestyles and preferences are in a constant state of flux. Food chain transparency and trust are drivers for food integrity control and for improvements in efficiency and economic growth. Similarly, the circular economy has great potential to reduce wastage and improve the efficiency of operations in multi-stakeholder ecosystems. Throughout the food chain cycle, all food commodities are exposed to multiple hazards, resulting in a high likelihood of contamination. Such biological or chemical hazards may be naturally present at any stage of food production, whether accidentally introduced or fraudulently imposed, risking consumers' health and their faith in the food industry. Nowadays, a massive amount of data is generated, not only from the next generation of food safety monitoring systems and along the entire food chain (primary production included) but also from the Internet of things, media, and other devices. These data should be used for the benefit of society, and the scientific field of data science should be a vital player in helping to make this possible.",poster,cp3
Business,p29,d9,2d420c7f1675d41b01e694739a25b8b189f2c95f,c3,Knowledge Discovery and Data Mining,Data Science in the Food Industry.,"Food safety is one of the main challenges of the agri-food industry that is expected to be addressed in the current environment of tremendous technological progress, where consumers' lifestyles and preferences are in a constant state of flux. Food chain transparency and trust are drivers for food integrity control and for improvements in efficiency and economic growth. Similarly, the circular economy has great potential to reduce wastage and improve the efficiency of operations in multi-stakeholder ecosystems. Throughout the food chain cycle, all food commodities are exposed to multiple hazards, resulting in a high likelihood of contamination. Such biological or chemical hazards may be naturally present at any stage of food production, whether accidentally introduced or fraudulently imposed, risking consumers' health and their faith in the food industry. Nowadays, a massive amount of data is generated, not only from the next generation of food safety monitoring systems and along the entire food chain (primary production included) but also from the Internet of things, media, and other devices. These data should be used for the benefit of society, and the scientific field of data science should be a vital player in helping to make this possible.",poster,cp3
Medicine,p31,d1,0751d2fa3a54cbbb4d594f2ee47c3aa7e4003a24,j14,IEEE Transactions on Artificial Intelligence,Leveraging Data Science to Combat COVID-19: A Comprehensive Review,"COVID-19, an infectious disease caused by the SARS-CoV-2 virus, was declared a pandemic by the World Health Organisation (WHO) in March 2020. By mid-August 2020, more than 21 million people have tested positive worldwide. Infections have been growing rapidly and tremendous efforts are being made to fight the disease. In this paper, we attempt to systematise the various COVID-19 research activities leveraging data science, where we define data science broadly to encompass the various methods and tools—including those from artificial intelligence (AI), machine learning (ML), statistics, modeling, simulation, and data visualization—that can be used to store, process, and extract insights from data. In addition to reviewing the rapidly growing body of recent research, we survey public datasets and repositories that can be used for further work to track COVID-19 spread and mitigation strategies. As part of this, we present a bibliometric analysis of the papers produced in this short span of time. Finally, building on these insights, we highlight common challenges and pitfalls observed across the surveyed works. We also created a live resource repository at https://github.com/Data-Science-and-COVID-19/Leveraging-Data-Science-To-Combat-COVID-19-A-Comprehensive-Review that we intend to keep updated with the latest resources including new papers and datasets.",fullPaper,jv14
Computer Science,p31,d3,0751d2fa3a54cbbb4d594f2ee47c3aa7e4003a24,j14,IEEE Transactions on Artificial Intelligence,Leveraging Data Science to Combat COVID-19: A Comprehensive Review,"COVID-19, an infectious disease caused by the SARS-CoV-2 virus, was declared a pandemic by the World Health Organisation (WHO) in March 2020. By mid-August 2020, more than 21 million people have tested positive worldwide. Infections have been growing rapidly and tremendous efforts are being made to fight the disease. In this paper, we attempt to systematise the various COVID-19 research activities leveraging data science, where we define data science broadly to encompass the various methods and tools—including those from artificial intelligence (AI), machine learning (ML), statistics, modeling, simulation, and data visualization—that can be used to store, process, and extract insights from data. In addition to reviewing the rapidly growing body of recent research, we survey public datasets and repositories that can be used for further work to track COVID-19 spread and mitigation strategies. As part of this, we present a bibliometric analysis of the papers produced in this short span of time. Finally, building on these insights, we highlight common challenges and pitfalls observed across the surveyed works. We also created a live resource repository at https://github.com/Data-Science-and-COVID-19/Leveraging-Data-Science-To-Combat-COVID-19-A-Comprehensive-Review that we intend to keep updated with the latest resources including new papers and datasets.",fullPaper,jv14
Medicine,p34,d1,f42c69dbd792155fee6f4d2c525971f8d43f138b,c37,International Workshop on the Semantic Web,Finding Related Tables in Data Lakes for Interactive Data Science,"Many modern data science applications build on data lakes, schema-agnostic repositories of data files and data products that offer limited organization and management capabilities. There is a need to build data lake search capabilities into data science environments, so scientists and analysts can find tables, schemas, workflows, and datasets useful to their task at hand. We develop search and management solutions for the Jupyter Notebook data science platform, to enable scientists to augment training data, find potential features to extract, clean data, and find joinable or linkable tables. Our core methods also generalize to other settings where computational tasks involve execution of programs or scripts.",poster,cp37
Computer Science,p34,d3,f42c69dbd792155fee6f4d2c525971f8d43f138b,c37,International Workshop on the Semantic Web,Finding Related Tables in Data Lakes for Interactive Data Science,"Many modern data science applications build on data lakes, schema-agnostic repositories of data files and data products that offer limited organization and management capabilities. There is a need to build data lake search capabilities into data science environments, so scientists and analysts can find tables, schemas, workflows, and datasets useful to their task at hand. We develop search and management solutions for the Jupyter Notebook data science platform, to enable scientists to augment training data, find potential features to extract, clean data, and find joinable or linkable tables. Our core methods also generalize to other settings where computational tasks involve execution of programs or scripts.",poster,cp37
Medicine,p41,d1,0669286d8d4ca8ec2fdf16b7813157c21eb690be,j19,Scientific Data,Heidelberg colorectal data set for surgical data science in the sensor operating room,Abstract,fullPaper,jv19
Computer Science,p41,d3,0669286d8d4ca8ec2fdf16b7813157c21eb690be,j19,Scientific Data,Heidelberg colorectal data set for surgical data science in the sensor operating room,Abstract,fullPaper,jv19
Medicine,p46,d1,317b30303d162ba11f450183bc9c19587bc5bd0e,c37,International Workshop on the Semantic Web,Ten simple rules for writing Dockerfiles for reproducible data science,"Computational science has been greatly improved by the use of containers for packaging software and data dependencies. In a scholarly context, the main drivers for using these containers are transparency and support of reproducibility; in turn, a workflow’s reproducibility can be greatly affected by the choices that are made with respect to building containers. In many cases, the build process for the container’s image is created from instructions provided in a Dockerfile format. In support of this approach, we present a set of rules to help researchers write understandable Dockerfiles for typical data science workflows. By following the rules in this article, researchers can create containers suitable for sharing with fellow scientists, for including in scholarly communication such as education or scientific papers, and for effective and sustainable personal workflows.",poster,cp37
Computer Science,p46,d3,317b30303d162ba11f450183bc9c19587bc5bd0e,c37,International Workshop on the Semantic Web,Ten simple rules for writing Dockerfiles for reproducible data science,"Computational science has been greatly improved by the use of containers for packaging software and data dependencies. In a scholarly context, the main drivers for using these containers are transparency and support of reproducibility; in turn, a workflow’s reproducibility can be greatly affected by the choices that are made with respect to building containers. In many cases, the build process for the container’s image is created from instructions provided in a Dockerfile format. In support of this approach, we present a set of rules to help researchers write understandable Dockerfiles for typical data science workflows. By following the rules in this article, researchers can create containers suitable for sharing with fellow scientists, for including in scholarly communication such as education or scientific papers, and for effective and sustainable personal workflows.",poster,cp37
Medicine,p53,d1,c016852106ac787678105fd9dd22e57ba620517c,j22,Patterns,Human Data Science,Abstract,fullPaper,jv22
Computer Science,p53,d3,c016852106ac787678105fd9dd22e57ba620517c,j22,Patterns,Human Data Science,Abstract,fullPaper,jv22
Psychology,p53,d10,c016852106ac787678105fd9dd22e57ba620517c,j22,Patterns,Human Data Science,Abstract,fullPaper,jv22
Medicine,p56,d1,27b9d1182e913decc7ef6a3509245fa6b6fd509d,j23,Proceedings of the National Academy of Sciences of the United States of America,Veridical data science,"Significance Predictability, computability, and stability (PCS) are three core principles of data science. They embed the scientific principles of prediction and replication in data-driven decision making while recognizing the central role of computation. Based on these principles, we propose the PCS framework, including workflow and documentation (in R Markdown or Jupyter Notebook). The PCS framework aims at responsible, reliable, reproducible, and transparent analysis across fields of science, social science, engineering, business, and government. It can be used as a recommendation system for scientific hypothesis generation and experimental design. In particular, we propose (basic) PCS inference for reliability measures on data results, extending statistical inference to a much broader scope as current data science practice entails. Building and expanding on principles of statistics, machine learning, and scientific inquiry, we propose the predictability, computability, and stability (PCS) framework for veridical data science. Our framework, composed of both a workflow and documentation, aims to provide responsible, reliable, reproducible, and transparent results across the data science life cycle. The PCS workflow uses predictability as a reality check and considers the importance of computation in data collection/storage and algorithm design. It augments predictability and computability with an overarching stability principle. Stability expands on statistical uncertainty considerations to assess how human judgment calls impact data results through data and model/algorithm perturbations. As part of the PCS workflow, we develop PCS inference procedures, namely PCS perturbation intervals and PCS hypothesis testing, to investigate the stability of data results relative to problem formulation, data cleaning, modeling decisions, and interpretations. We illustrate PCS inference through neuroscience and genomics projects of our own and others. Moreover, we demonstrate its favorable performance over existing methods in terms of receiver operating characteristic (ROC) curves in high-dimensional, sparse linear model simulations, including a wide range of misspecified models. Finally, we propose PCS documentation based on R Markdown or Jupyter Notebook, with publicly available, reproducible codes and narratives to back up human choices made throughout an analysis. The PCS workflow and documentation are demonstrated in a genomics case study available on Zenodo.",fullPaper,jv23
Computer Science,p56,d3,27b9d1182e913decc7ef6a3509245fa6b6fd509d,j23,Proceedings of the National Academy of Sciences of the United States of America,Veridical data science,"Significance Predictability, computability, and stability (PCS) are three core principles of data science. They embed the scientific principles of prediction and replication in data-driven decision making while recognizing the central role of computation. Based on these principles, we propose the PCS framework, including workflow and documentation (in R Markdown or Jupyter Notebook). The PCS framework aims at responsible, reliable, reproducible, and transparent analysis across fields of science, social science, engineering, business, and government. It can be used as a recommendation system for scientific hypothesis generation and experimental design. In particular, we propose (basic) PCS inference for reliability measures on data results, extending statistical inference to a much broader scope as current data science practice entails. Building and expanding on principles of statistics, machine learning, and scientific inquiry, we propose the predictability, computability, and stability (PCS) framework for veridical data science. Our framework, composed of both a workflow and documentation, aims to provide responsible, reliable, reproducible, and transparent results across the data science life cycle. The PCS workflow uses predictability as a reality check and considers the importance of computation in data collection/storage and algorithm design. It augments predictability and computability with an overarching stability principle. Stability expands on statistical uncertainty considerations to assess how human judgment calls impact data results through data and model/algorithm perturbations. As part of the PCS workflow, we develop PCS inference procedures, namely PCS perturbation intervals and PCS hypothesis testing, to investigate the stability of data results relative to problem formulation, data cleaning, modeling decisions, and interpretations. We illustrate PCS inference through neuroscience and genomics projects of our own and others. Moreover, we demonstrate its favorable performance over existing methods in terms of receiver operating characteristic (ROC) curves in high-dimensional, sparse linear model simulations, including a wide range of misspecified models. Finally, we propose PCS documentation based on R Markdown or Jupyter Notebook, with publicly available, reproducible codes and narratives to back up human choices made throughout an analysis. The PCS workflow and documentation are demonstrated in a genomics case study available on Zenodo.",fullPaper,jv23
Mathematics,p56,d6,27b9d1182e913decc7ef6a3509245fa6b6fd509d,j23,Proceedings of the National Academy of Sciences of the United States of America,Veridical data science,"Significance Predictability, computability, and stability (PCS) are three core principles of data science. They embed the scientific principles of prediction and replication in data-driven decision making while recognizing the central role of computation. Based on these principles, we propose the PCS framework, including workflow and documentation (in R Markdown or Jupyter Notebook). The PCS framework aims at responsible, reliable, reproducible, and transparent analysis across fields of science, social science, engineering, business, and government. It can be used as a recommendation system for scientific hypothesis generation and experimental design. In particular, we propose (basic) PCS inference for reliability measures on data results, extending statistical inference to a much broader scope as current data science practice entails. Building and expanding on principles of statistics, machine learning, and scientific inquiry, we propose the predictability, computability, and stability (PCS) framework for veridical data science. Our framework, composed of both a workflow and documentation, aims to provide responsible, reliable, reproducible, and transparent results across the data science life cycle. The PCS workflow uses predictability as a reality check and considers the importance of computation in data collection/storage and algorithm design. It augments predictability and computability with an overarching stability principle. Stability expands on statistical uncertainty considerations to assess how human judgment calls impact data results through data and model/algorithm perturbations. As part of the PCS workflow, we develop PCS inference procedures, namely PCS perturbation intervals and PCS hypothesis testing, to investigate the stability of data results relative to problem formulation, data cleaning, modeling decisions, and interpretations. We illustrate PCS inference through neuroscience and genomics projects of our own and others. Moreover, we demonstrate its favorable performance over existing methods in terms of receiver operating characteristic (ROC) curves in high-dimensional, sparse linear model simulations, including a wide range of misspecified models. Finally, we propose PCS documentation based on R Markdown or Jupyter Notebook, with publicly available, reproducible codes and narratives to back up human choices made throughout an analysis. The PCS workflow and documentation are demonstrated in a genomics case study available on Zenodo.",fullPaper,jv23
Medicine,p59,d1,4b5505a54799d796ae94115409b01ee33a7e2b20,j25,Journal of Epidemiology and Community Health,Glossary for public health surveillance in the age of data science,"Public health surveillance is the ongoing systematic collection, analysis and interpretation of data, closely integrated with the timely dissemination of the resulting information to those responsible for preventing and controlling disease and injury. With the rapid development of data science, encompassing big data and artificial intelligence, and with the exponential growth of accessible and highly heterogeneous health-related data, from healthcare providers to user-generated online content, the field of surveillance and health monitoring is changing rapidly. It is, therefore, the right time for a short glossary of key terms in public health surveillance, with an emphasis on new data-science developments in the field.",fullPaper,jv25
Medicine,p65,d1,89f41c87c8849ce37e609c1010087291a4679a37,j26,Philosophical Transactions of the Royal Society of London. Biological Sciences,Outbreak analytics: a developing data science for informing the response to emerging pathogens,"Despite continued efforts to improve health systems worldwide, emerging pathogen epidemics remain a major public health concern. Effective response to such outbreaks relies on timely intervention, ideally informed by all available sources of data. The collection, visualization and analysis of outbreak data are becoming increasingly complex, owing to the diversity in types of data, questions and available methods to address them. Recent advances have led to the rise of outbreak analytics, an emerging data science focused on the technological and methodological aspects of the outbreak data pipeline, from collection to analysis, modelling and reporting to inform outbreak response. In this article, we assess the current state of the field. After laying out the context of outbreak response, we critically review the most common analytics components, their inter-dependencies, data requirements and the type of information they can provide to inform operations in real time. We discuss some challenges and opportunities and conclude on the potential role of outbreak analytics for improving our understanding of, and response to outbreaks of emerging pathogens. This article is part of the theme issue ‘Modelling infectious disease outbreaks in humans, animals and plants: epidemic forecasting and control‘. This theme issue is linked with the earlier issue ‘Modelling infectious disease outbreaks in humans, animals and plants: approaches and important themes’.",fullPaper,jv26
History,p65,d12,89f41c87c8849ce37e609c1010087291a4679a37,j26,Philosophical Transactions of the Royal Society of London. Biological Sciences,Outbreak analytics: a developing data science for informing the response to emerging pathogens,"Despite continued efforts to improve health systems worldwide, emerging pathogen epidemics remain a major public health concern. Effective response to such outbreaks relies on timely intervention, ideally informed by all available sources of data. The collection, visualization and analysis of outbreak data are becoming increasingly complex, owing to the diversity in types of data, questions and available methods to address them. Recent advances have led to the rise of outbreak analytics, an emerging data science focused on the technological and methodological aspects of the outbreak data pipeline, from collection to analysis, modelling and reporting to inform outbreak response. In this article, we assess the current state of the field. After laying out the context of outbreak response, we critically review the most common analytics components, their inter-dependencies, data requirements and the type of information they can provide to inform operations in real time. We discuss some challenges and opportunities and conclude on the potential role of outbreak analytics for improving our understanding of, and response to outbreaks of emerging pathogens. This article is part of the theme issue ‘Modelling infectious disease outbreaks in humans, animals and plants: epidemic forecasting and control‘. This theme issue is linked with the earlier issue ‘Modelling infectious disease outbreaks in humans, animals and plants: approaches and important themes’.",fullPaper,jv26
Medicine,p74,d1,0c86e8d19d0fc62a5f829ea625ffd3e7fa9551b9,j31,Metabolomics,Toward collaborative open data science in metabolomics using Jupyter Notebooks and cloud computing,Abstract,fullPaper,jv31
Computer Science,p74,d3,0c86e8d19d0fc62a5f829ea625ffd3e7fa9551b9,j31,Metabolomics,Toward collaborative open data science in metabolomics using Jupyter Notebooks and cloud computing,Abstract,fullPaper,jv31
Medicine,p78,d1,b00f836c62d0ea7678d0f20aeec3397138633060,j34,Journal of Medical Internet Research,Health Care and Precision Medicine Research: Analysis of a Scalable Data Science Platform,"Background Health care data are increasing in volume and complexity. Storing and analyzing these data to implement precision medicine initiatives and data-driven research has exceeded the capabilities of traditional computer systems. Modern big data platforms must be adapted to the specific demands of health care and designed for scalability and growth. Objective The objectives of our study were to (1) demonstrate the implementation of a data science platform built on open source technology within a large, academic health care system and (2) describe 2 computational health care applications built on such a platform. Methods We deployed a data science platform based on several open source technologies to support real-time, big data workloads. We developed data-acquisition workflows for Apache Storm and NiFi in Java and Python to capture patient monitoring and laboratory data for downstream analytics. Results Emerging data management approaches, along with open source technologies such as Hadoop, can be used to create integrated data lakes to store large, real-time datasets. This infrastructure also provides a robust analytics platform where health care and biomedical research data can be analyzed in near real time for precision medicine and computational health care use cases. Conclusions The implementation and use of integrated data science platforms offer organizations the opportunity to combine traditional datasets, including data from the electronic health record, with emerging big data sources, such as continuous patient monitoring and real-time laboratory results. These platforms can enable cost-effective and scalable analytics for the information that will be key to the delivery of precision medicine initiatives. Organizations that can take advantage of the technical advances found in data science platforms will have the opportunity to provide comprehensive access to health care data for computational health care and precision medicine research.",fullPaper,jv34
Computer Science,p78,d3,b00f836c62d0ea7678d0f20aeec3397138633060,j34,Journal of Medical Internet Research,Health Care and Precision Medicine Research: Analysis of a Scalable Data Science Platform,"Background Health care data are increasing in volume and complexity. Storing and analyzing these data to implement precision medicine initiatives and data-driven research has exceeded the capabilities of traditional computer systems. Modern big data platforms must be adapted to the specific demands of health care and designed for scalability and growth. Objective The objectives of our study were to (1) demonstrate the implementation of a data science platform built on open source technology within a large, academic health care system and (2) describe 2 computational health care applications built on such a platform. Methods We deployed a data science platform based on several open source technologies to support real-time, big data workloads. We developed data-acquisition workflows for Apache Storm and NiFi in Java and Python to capture patient monitoring and laboratory data for downstream analytics. Results Emerging data management approaches, along with open source technologies such as Hadoop, can be used to create integrated data lakes to store large, real-time datasets. This infrastructure also provides a robust analytics platform where health care and biomedical research data can be analyzed in near real time for precision medicine and computational health care use cases. Conclusions The implementation and use of integrated data science platforms offer organizations the opportunity to combine traditional datasets, including data from the electronic health record, with emerging big data sources, such as continuous patient monitoring and real-time laboratory results. These platforms can enable cost-effective and scalable analytics for the information that will be key to the delivery of precision medicine initiatives. Organizations that can take advantage of the technical advances found in data science platforms will have the opportunity to provide comprehensive access to health care data for computational health care and precision medicine research.",fullPaper,jv34
Medicine,p81,d1,b134d892f4e76081f5fa36b0b7c2e7118be53907,j6,Genome Biology,Genomics and data science: an application within an umbrella,Abstract,fullPaper,jv6
Biology,p81,d5,b134d892f4e76081f5fa36b0b7c2e7118be53907,j6,Genome Biology,Genomics and data science: an application within an umbrella,Abstract,fullPaper,jv6
Medicine,p83,d1,863a35bdd1ae803491801e283c2ae79fe973cf68,j37,Journal of Biosciences,Microbiome data science,Abstract,fullPaper,jv37
Computer Science,p83,d3,863a35bdd1ae803491801e283c2ae79fe973cf68,j37,Journal of Biosciences,Microbiome data science,Abstract,fullPaper,jv37
Medicine,p89,d1,f9e0e85732f0736c0d5a6f0c63df5c7f1f245dcd,j40,BMC Medicine,From hype to reality: data science enabling personalized medicine,Abstract,fullPaper,jv40
Medicine,p92,d1,bb6adeeb3a21479cc45490a5c2ff6d8dd5e77603,c46,Ideal,Knowledge-based Biomedical Data Science 2019,"Knowledge-based biomedical data science involves the design and implementation of computer systems that act as if they knew about biomedicine. Such systems depend on formally represented knowledge in computer systems, often in the form of knowledge graphs. Here we survey recent progress in systems that use formally represented knowledge to address data science problems in both clinical and biological domains, as well as progress on approaches for creating knowledge graphs. Major themes include the relationships between knowledge graphs and machine learning, the use of natural language processing to construct knowledge graphs, and the expansion of novel knowledge-based approaches to clinical and biological domains.",poster,cp46
Computer Science,p92,d3,bb6adeeb3a21479cc45490a5c2ff6d8dd5e77603,c46,Ideal,Knowledge-based Biomedical Data Science 2019,"Knowledge-based biomedical data science involves the design and implementation of computer systems that act as if they knew about biomedicine. Such systems depend on formally represented knowledge in computer systems, often in the form of knowledge graphs. Here we survey recent progress in systems that use formally represented knowledge to address data science problems in both clinical and biological domains, as well as progress on approaches for creating knowledge graphs. Major themes include the relationships between knowledge graphs and machine learning, the use of natural language processing to construct knowledge graphs, and the expansion of novel knowledge-based approaches to clinical and biological domains.",poster,cp46
Medicine,p105,d1,6bec0106bebc93fc30ec47af9779d7e327639034,c100,IEEE International Conference on Computer Vision,Machine learning and data science in soft materials engineering,"In many branches of materials science it is now routine to generate data sets of such large size and dimensionality that conventional methods of analysis fail. Paradigms and tools from data science and machine learning can provide scalable approaches to identify and extract trends and patterns within voluminous data sets, perform guided traversals of high-dimensional phase spaces, and furnish data-driven strategies for inverse materials design. This topical review provides an accessible introduction to machine learning tools in the context of soft and biological materials by ‘de-jargonizing’ data science terminology, presenting a taxonomy of machine learning techniques, and surveying the mathematical underpinnings and software implementations of popular tools, including principal component analysis, independent component analysis, diffusion maps, support vector machines, and relative entropy. We present illustrative examples of machine learning applications in soft matter, including inverse design of self-assembling materials, nonlinear learning of protein folding landscapes, high-throughput antimicrobial peptide design, and data-driven materials design engines. We close with an outlook on the challenges and opportunities for the field.",poster,cp100
Physics,p105,d2,6bec0106bebc93fc30ec47af9779d7e327639034,c100,IEEE International Conference on Computer Vision,Machine learning and data science in soft materials engineering,"In many branches of materials science it is now routine to generate data sets of such large size and dimensionality that conventional methods of analysis fail. Paradigms and tools from data science and machine learning can provide scalable approaches to identify and extract trends and patterns within voluminous data sets, perform guided traversals of high-dimensional phase spaces, and furnish data-driven strategies for inverse materials design. This topical review provides an accessible introduction to machine learning tools in the context of soft and biological materials by ‘de-jargonizing’ data science terminology, presenting a taxonomy of machine learning techniques, and surveying the mathematical underpinnings and software implementations of popular tools, including principal component analysis, independent component analysis, diffusion maps, support vector machines, and relative entropy. We present illustrative examples of machine learning applications in soft matter, including inverse design of self-assembling materials, nonlinear learning of protein folding landscapes, high-throughput antimicrobial peptide design, and data-driven materials design engines. We close with an outlook on the challenges and opportunities for the field.",poster,cp100
Medicine,p111,d1,577564ac25a12b37972d77a35b589f6b2270a45f,j47,Chest,Big Data and Data Science in Critical Care.,Abstract,fullPaper,jv47
Medicine,p118,d1,e4c66275e46a66586365c851f0974a3c88baf3d7,c24,International Conference on Data Technologies and Applications,Network embedding in biomedical data science,"Owning to the rapid development of computer technologies, an increasing number of relational data have been emerging in modern biomedical research. Many network-based learning methods have been proposed to perform analysis on such data, which provide people a deep understanding of topology and knowledge behind the biomedical networks and benefit a lot of applications for human healthcare. However, most network-based methods suffer from high computational and space cost. There remain challenges on handling high dimensionality and sparsity of the biomedical networks. The latest advances in network embedding technologies provide new effective paradigms to solve the network analysis problem. It converts network into a low-dimensional space while maximally preserves structural properties. In this way, downstream tasks such as link prediction and node classification can be done by traditional machine learning methods. In this survey, we conduct a comprehensive review of the literature on applying network embedding to advance the biomedical domain. We first briefly introduce the widely used network embedding models. After that, we carefully discuss how the network embedding approaches were performed on biomedical networks as well as how they accelerated the downstream tasks in biomedical science. Finally, we discuss challenges the existing network embedding applications in biomedical domains are faced with and suggest several promising future directions for a better improvement in human healthcare.",poster,cp24
Computer Science,p118,d3,e4c66275e46a66586365c851f0974a3c88baf3d7,c24,International Conference on Data Technologies and Applications,Network embedding in biomedical data science,"Owning to the rapid development of computer technologies, an increasing number of relational data have been emerging in modern biomedical research. Many network-based learning methods have been proposed to perform analysis on such data, which provide people a deep understanding of topology and knowledge behind the biomedical networks and benefit a lot of applications for human healthcare. However, most network-based methods suffer from high computational and space cost. There remain challenges on handling high dimensionality and sparsity of the biomedical networks. The latest advances in network embedding technologies provide new effective paradigms to solve the network analysis problem. It converts network into a low-dimensional space while maximally preserves structural properties. In this way, downstream tasks such as link prediction and node classification can be done by traditional machine learning methods. In this survey, we conduct a comprehensive review of the literature on applying network embedding to advance the biomedical domain. We first briefly introduce the widely used network embedding models. After that, we carefully discuss how the network embedding approaches were performed on biomedical networks as well as how they accelerated the downstream tasks in biomedical science. Finally, we discuss challenges the existing network embedding applications in biomedical domains are faced with and suggest several promising future directions for a better improvement in human healthcare.",poster,cp24
Medicine,p119,d1,0ec2d4c804dd2b4446e1808dc85b4fe4a27b1766,j49,Nature Biomedical Engineering,Surgical data science for next-generation interventions,Abstract,fullPaper,jv49
Computer Science,p119,d3,0ec2d4c804dd2b4446e1808dc85b4fe4a27b1766,j49,Nature Biomedical Engineering,Surgical data science for next-generation interventions,Abstract,fullPaper,jv49
Psychology,p119,d10,0ec2d4c804dd2b4446e1808dc85b4fe4a27b1766,j49,Nature Biomedical Engineering,Surgical data science for next-generation interventions,Abstract,fullPaper,jv49
Medicine,p120,d1,c0225f99c9b1619c3be74b63241faffe02d275d7,j23,Proceedings of the National Academy of Sciences of the United States of America,Science and data science,"Data science has attracted a lot of attention, promising to turn vast amounts of data into useful predictions and insights. In this article, we ask why scientists should care about data science. To answer, we discuss data science from three perspectives: statistical, computational, and human. Although each of the three is a critical component of data science, we argue that the effective combination of all three components is the essence of what data science is about.",fullPaper,jv23
Computer Science,p120,d3,c0225f99c9b1619c3be74b63241faffe02d275d7,j23,Proceedings of the National Academy of Sciences of the United States of America,Science and data science,"Data science has attracted a lot of attention, promising to turn vast amounts of data into useful predictions and insights. In this article, we ask why scientists should care about data science. To answer, we discuss data science from three perspectives: statistical, computational, and human. Although each of the three is a critical component of data science, we argue that the effective combination of all three components is the essence of what data science is about.",fullPaper,jv23
Medicine,p122,d1,89535aa63bc5dac6f3beb60b813abb77aa4309d1,c9,Big Data,Critique and Contribute: A Practice-Based Framework for Improving Critical Data Studies and Data Science,"Abstract What would data science look like if its key critics were engaged to help improve it, and how might critiques of data science improve with an approach that considers the day-to-day practices of data science? This article argues for scholars to bridge the conversations that seek to critique data science and those that seek to advance data science practice to identify and create the social and organizational arrangements necessary for a more ethical data science. We summarize four critiques that are commonly made in critical data studies: data are inherently interpretive, data are inextricable from context, data are mediated through the sociomaterial arrangements that produce them, and data serve as a medium for the negotiation and communication of values. We present qualitative research with academic data scientists, “data for good” projects, and specialized cross-disciplinary engineering teams to show evidence of these critiques in the day-to-day experience of data scientists as they acknowledge and grapple with the complexities of their work. Using ethnographic vignettes from two large multiresearcher field sites, we develop a set of concepts for analyzing and advancing the practice of data science and improving critical data studies, including (1) communication is central to the data science endeavor; (2) making sense of data is a collective process; (3) data are starting, not end points, and (4) data are sets of stories. We conclude with two calls to action for researchers and practitioners in data science and critical data studies alike. First, creating opportunities for bringing social scientific and humanistic expertise into data science practice simultaneously will advance both data science and critical data studies. Second, practitioners should leverage the insights from critical data studies to build new kinds of organizational arrangements, which we argue will help advance a more ethical data science. Engaging the insights of critical data studies will improve data science. Careful attention to the practices of data science will improve scholarly critiques. Genuine collaborative conversations between these different communities will help push for more ethical, and better, ways of knowing in increasingly datum-saturated societies.",fullPaper,cp9
Computer Science,p122,d3,89535aa63bc5dac6f3beb60b813abb77aa4309d1,c9,Big Data,Critique and Contribute: A Practice-Based Framework for Improving Critical Data Studies and Data Science,"Abstract What would data science look like if its key critics were engaged to help improve it, and how might critiques of data science improve with an approach that considers the day-to-day practices of data science? This article argues for scholars to bridge the conversations that seek to critique data science and those that seek to advance data science practice to identify and create the social and organizational arrangements necessary for a more ethical data science. We summarize four critiques that are commonly made in critical data studies: data are inherently interpretive, data are inextricable from context, data are mediated through the sociomaterial arrangements that produce them, and data serve as a medium for the negotiation and communication of values. We present qualitative research with academic data scientists, “data for good” projects, and specialized cross-disciplinary engineering teams to show evidence of these critiques in the day-to-day experience of data scientists as they acknowledge and grapple with the complexities of their work. Using ethnographic vignettes from two large multiresearcher field sites, we develop a set of concepts for analyzing and advancing the practice of data science and improving critical data studies, including (1) communication is central to the data science endeavor; (2) making sense of data is a collective process; (3) data are starting, not end points, and (4) data are sets of stories. We conclude with two calls to action for researchers and practitioners in data science and critical data studies alike. First, creating opportunities for bringing social scientific and humanistic expertise into data science practice simultaneously will advance both data science and critical data studies. Second, practitioners should leverage the insights from critical data studies to build new kinds of organizational arrangements, which we argue will help advance a more ethical data science. Engaging the insights of critical data studies will improve data science. Careful attention to the practices of data science will improve scholarly critiques. Genuine collaborative conversations between these different communities will help push for more ethical, and better, ways of knowing in increasingly datum-saturated societies.",fullPaper,cp9
Medicine,p124,d1,5a44f70130875b212452ad777ab02a4eb5cd35d9,j51,International Journal of Population Data Science,A Position Statement on Population Data Science: The Science of Data about People,"Information is increasingly digital, creating opportunities to respond to pressing issues about human populations using linked datasets that are large, complex, and diverse. The potential social and individual benefits that can come from data-intensive science are large, but raise challenges of balancing individual privacy and the public good, building appropriate socio-technical systems to support data-intensive science, and determining whether defining a new field of inquiry might help move those collective interests and activities forward. A combination of expert engagement, literature review, and iterative conversations led to our conclusion that defining the field of Population Data Science (challenge 3) will help address the other two challenges as well. We define Population Data Science succinctly as the science of data about people and note that it is related to but distinct from the fields of data science and informatics. A broader definition names four characteristics of: data use for positive impact on citizens and society; bringing together and analyzing data from multiple sources; finding population-level insights; and developing safe, privacy-sensitive and ethical infrastructure to support research. One implication of these characteristics is that few people possess all of the requisite knowledge and skills of Population Data Science, so this is by nature a multi-disciplinary field. Other implications include the need to advance various aspects of science, such as data linkage technology, various forms of analytics, and methods of public engagement. These implications are the beginnings of a research agenda for Population Data Science, which if approached as a collective field, can catalyze significant advances in our understanding of trends in society, health, and human behavior.",fullPaper,jv51
Sociology,p124,d4,5a44f70130875b212452ad777ab02a4eb5cd35d9,j51,International Journal of Population Data Science,A Position Statement on Population Data Science: The Science of Data about People,"Information is increasingly digital, creating opportunities to respond to pressing issues about human populations using linked datasets that are large, complex, and diverse. The potential social and individual benefits that can come from data-intensive science are large, but raise challenges of balancing individual privacy and the public good, building appropriate socio-technical systems to support data-intensive science, and determining whether defining a new field of inquiry might help move those collective interests and activities forward. A combination of expert engagement, literature review, and iterative conversations led to our conclusion that defining the field of Population Data Science (challenge 3) will help address the other two challenges as well. We define Population Data Science succinctly as the science of data about people and note that it is related to but distinct from the fields of data science and informatics. A broader definition names four characteristics of: data use for positive impact on citizens and society; bringing together and analyzing data from multiple sources; finding population-level insights; and developing safe, privacy-sensitive and ethical infrastructure to support research. One implication of these characteristics is that few people possess all of the requisite knowledge and skills of Population Data Science, so this is by nature a multi-disciplinary field. Other implications include the need to advance various aspects of science, such as data linkage technology, various forms of analytics, and methods of public engagement. These implications are the beginnings of a research agenda for Population Data Science, which if approached as a collective field, can catalyze significant advances in our understanding of trends in society, health, and human behavior.",fullPaper,jv51
Medicine,p128,d1,afe79672aa99b7f606cbff234ec2454cf2295554,j52,Ethnicity & Disease,Big Data Science: Opportunities and Challenges to Address Minority Health and Health Disparities in the 21st Century.,"Addressing minority health and health disparities has been a missing piece of the puzzle in Big Data science. This article focuses on three priority opportunities that Big Data science may offer to the reduction of health and health care disparities. One opportunity is to incorporate standardized information on demographic and social determinants in electronic health records in order to target ways to improve quality of care for the most disadvantaged populations over time. A second opportunity is to enhance public health surveillance by linking geographical variables and social determinants of health for geographically defined populations to clinical data and health outcomes. Third and most importantly, Big Data science may lead to a better understanding of the etiology of health disparities and understanding of minority health in order to guide intervention development. However, the promise of Big Data needs to be considered in light of significant challenges that threaten to widen health disparities. Care must be taken to incorporate diverse populations to realize the potential benefits. Specific recommendations include investing in data collection on small sample populations, building a diverse workforce pipeline for data science, actively seeking to reduce digital divides, developing novel ways to assure digital data privacy for small populations, and promoting widespread data sharing to benefit under-resourced minority-serving institutions and minority researchers. With deliberate efforts, Big Data presents a dramatic opportunity for reducing health disparities but without active engagement, it risks further widening them.",fullPaper,jv52
Business,p128,d9,afe79672aa99b7f606cbff234ec2454cf2295554,j52,Ethnicity & Disease,Big Data Science: Opportunities and Challenges to Address Minority Health and Health Disparities in the 21st Century.,"Addressing minority health and health disparities has been a missing piece of the puzzle in Big Data science. This article focuses on three priority opportunities that Big Data science may offer to the reduction of health and health care disparities. One opportunity is to incorporate standardized information on demographic and social determinants in electronic health records in order to target ways to improve quality of care for the most disadvantaged populations over time. A second opportunity is to enhance public health surveillance by linking geographical variables and social determinants of health for geographically defined populations to clinical data and health outcomes. Third and most importantly, Big Data science may lead to a better understanding of the etiology of health disparities and understanding of minority health in order to guide intervention development. However, the promise of Big Data needs to be considered in light of significant challenges that threaten to widen health disparities. Care must be taken to incorporate diverse populations to realize the potential benefits. Specific recommendations include investing in data collection on small sample populations, building a diverse workforce pipeline for data science, actively seeking to reduce digital divides, developing novel ways to assure digital data privacy for small populations, and promoting widespread data sharing to benefit under-resourced minority-serving institutions and minority researchers. With deliberate efforts, Big Data presents a dramatic opportunity for reducing health disparities but without active engagement, it risks further widening them.",fullPaper,jv52
Medicine,p135,d1,88761dffd173cd0e75e88c02d68f866f8cc43c14,j54,Data Science,Knowledge-based biomedical Data Science,"Computational manipulation of knowledge is an important, and often under-appreciated, aspect of biomedical Data Science. The first Data Science initiative from the US National Institutes of Health was entitled “Big Data to Knowledge (BD2K).” The main emphasis of the more than $200M allocated to that program has been on “Big Data;” the “Knowledge” component has largely been the implicit assumption that the work will lead to new biomedical knowledge. However, there is long-standing and highly productive work in computational knowledge representation and reasoning, and computational processing of knowledge has a role in the world of Data Science. Knowledge-based biomedical Data Science involves the design and implementation of computer systems that act as if they knew about biomedicine. There are many ways in which a computational approach might act as if it knew something: for example, it might be able to answer a natural language question about a biomedical topic, or pass an exam; it might be able to use existing biomedical knowledge to rank or evaluate hypotheses; it might explain or interpret data in light of prior knowledge, either in a Bayesian or other sort of framework. These are all examples of automated reasoning that act on computational representations of knowledge. After a brief survey of existing approaches to knowledge-based data science, this position paper argues that such research is ripe for expansion, and expanded application.",fullPaper,jv54
Computer Science,p135,d3,88761dffd173cd0e75e88c02d68f866f8cc43c14,j54,Data Science,Knowledge-based biomedical Data Science,"Computational manipulation of knowledge is an important, and often under-appreciated, aspect of biomedical Data Science. The first Data Science initiative from the US National Institutes of Health was entitled “Big Data to Knowledge (BD2K).” The main emphasis of the more than $200M allocated to that program has been on “Big Data;” the “Knowledge” component has largely been the implicit assumption that the work will lead to new biomedical knowledge. However, there is long-standing and highly productive work in computational knowledge representation and reasoning, and computational processing of knowledge has a role in the world of Data Science. Knowledge-based biomedical Data Science involves the design and implementation of computer systems that act as if they knew about biomedicine. There are many ways in which a computational approach might act as if it knew something: for example, it might be able to answer a natural language question about a biomedical topic, or pass an exam; it might be able to use existing biomedical knowledge to rank or evaluate hypotheses; it might explain or interpret data in light of prior knowledge, either in a Bayesian or other sort of framework. These are all examples of automated reasoning that act on computational representations of knowledge. After a brief survey of existing approaches to knowledge-based data science, this position paper argues that such research is ripe for expansion, and expanded application.",fullPaper,jv54
Medicine,p140,d1,dcecbf916b9e2c61042f0dc992bdfd8ac1c99b8d,c95,Cyber ..,Materials Knowledge Systems in Python—a Data Science Framework for Accelerated Development of Hierarchical Materials,Abstract,poster,cp95
Computer Science,p140,d3,dcecbf916b9e2c61042f0dc992bdfd8ac1c99b8d,c95,Cyber ..,Materials Knowledge Systems in Python—a Data Science Framework for Accelerated Development of Hierarchical Materials,Abstract,poster,cp95
Materials Science,p140,d7,dcecbf916b9e2c61042f0dc992bdfd8ac1c99b8d,c95,Cyber ..,Materials Knowledge Systems in Python—a Data Science Framework for Accelerated Development of Hierarchical Materials,Abstract,poster,cp95
Medicine,p142,d1,b4332aaabec46e386fff31d066f278fc27cfa1cb,j55,Nature Human Behaviour,How data science can advance mental health research,Abstract,fullPaper,jv55
Psychology,p142,d10,b4332aaabec46e386fff31d066f278fc27cfa1cb,j55,Nature Human Behaviour,How data science can advance mental health research,Abstract,fullPaper,jv55
Medicine,p155,d1,972edbd8bd19486a37c0a9f34508634fc8733529,c85,IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering,Our path to better science in less time using open data science tools,Abstract,poster,cp85
Computer Science,p155,d3,972edbd8bd19486a37c0a9f34508634fc8733529,c85,IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering,Our path to better science in less time using open data science tools,Abstract,poster,cp85
Medicine,p160,d1,61b3ce156347a7f107df75924a45f81f12a0ef14,c34,International Conference on Data Warehousing and Knowledge Discovery,Surgical data science: the new knowledge domain,"Abstract Healthcare in general, and surgery/interventional care in particular, is evolving through rapid advances in technology and increasing complexity of care, with the goal of maximizing the quality and value of care. Whereas innovations in diagnostic and therapeutic technologies have driven past improvements in the quality of surgical care, future transformation in care will be enabled by data. Conventional methodologies, such as registry studies, are limited in their scope for discovery and research, extent and complexity of data, breadth of analytical techniques, and translation or integration of research findings into patient care. We foresee the emergence of surgical/interventional data science (SDS) as a key element to addressing these limitations and creating a sustainable path toward evidence-based improvement of interventional healthcare pathways. SDS will create tools to measure, model, and quantify the pathways or processes within the context of patient health states or outcomes and use information gained to inform healthcare decisions, guidelines, best practices, policy, and training, thereby improving the safety and quality of healthcare and its value. Data are pervasive throughout the surgical care pathway; thus, SDS can impact various aspects of care, including prevention, diagnosis, intervention, or postoperative recovery. The existing literature already provides preliminary results, suggesting how a data science approach to surgical decision-making could more accurately predict severe complications using complex data from preoperative, intraoperative, and postoperative contexts, how it could support intraoperative decision-making using both existing knowledge and continuous data streams throughout the surgical care pathway, and how it could enable effective collaboration between human care providers and intelligent technologies. In addition, SDS is poised to play a central role in surgical education, for example, through objective assessments, automated virtual coaching, and robot-assisted active learning of surgical skill. However, the potential for transforming surgical care and training through SDS may only be realized through a cultural shift that not only institutionalizes technology to seamlessly capture data but also assimilates individuals with expertise in data science into clinical research teams. Furthermore, collaboration with industry partners from the inception of the discovery process promotes optimal design of data products as well as their efficient translation and commercialization. As surgery continues to evolve through advances in technology that enhance delivery of care, SDS represents a new knowledge domain to engineer surgical care of the future.",poster,cp34
Computer Science,p160,d3,61b3ce156347a7f107df75924a45f81f12a0ef14,c34,International Conference on Data Warehousing and Knowledge Discovery,Surgical data science: the new knowledge domain,"Abstract Healthcare in general, and surgery/interventional care in particular, is evolving through rapid advances in technology and increasing complexity of care, with the goal of maximizing the quality and value of care. Whereas innovations in diagnostic and therapeutic technologies have driven past improvements in the quality of surgical care, future transformation in care will be enabled by data. Conventional methodologies, such as registry studies, are limited in their scope for discovery and research, extent and complexity of data, breadth of analytical techniques, and translation or integration of research findings into patient care. We foresee the emergence of surgical/interventional data science (SDS) as a key element to addressing these limitations and creating a sustainable path toward evidence-based improvement of interventional healthcare pathways. SDS will create tools to measure, model, and quantify the pathways or processes within the context of patient health states or outcomes and use information gained to inform healthcare decisions, guidelines, best practices, policy, and training, thereby improving the safety and quality of healthcare and its value. Data are pervasive throughout the surgical care pathway; thus, SDS can impact various aspects of care, including prevention, diagnosis, intervention, or postoperative recovery. The existing literature already provides preliminary results, suggesting how a data science approach to surgical decision-making could more accurately predict severe complications using complex data from preoperative, intraoperative, and postoperative contexts, how it could support intraoperative decision-making using both existing knowledge and continuous data streams throughout the surgical care pathway, and how it could enable effective collaboration between human care providers and intelligent technologies. In addition, SDS is poised to play a central role in surgical education, for example, through objective assessments, automated virtual coaching, and robot-assisted active learning of surgical skill. However, the potential for transforming surgical care and training through SDS may only be realized through a cultural shift that not only institutionalizes technology to seamlessly capture data but also assimilates individuals with expertise in data science into clinical research teams. Furthermore, collaboration with industry partners from the inception of the discovery process promotes optimal design of data products as well as their efficient translation and commercialization. As surgery continues to evolve through advances in technology that enhance delivery of care, SDS represents a new knowledge domain to engineer surgical care of the future.",poster,cp34
Medicine,p163,d1,843149b649b888fdb3649b8d4852263b62356799,c15,Pacific Symposium on Biocomputing,Democratizing data science through data science training,"The biomedical sciences have experienced an explosion of data which promises to overwhelm many current practitioners. Without easy access to data science training resources, biomedical researchers may find themselves unable to wrangle their own datasets. In 2014, to address the challenges posed such a data onslaught, the National Institutes of Health (NIH) launched the Big Data to Knowledge (BD2K) initiative. To this end, the BD2K Training Coordinating Center (TCC; bigdatau.org) was funded to facilitate both in-person and online learning, and open up the concepts of data science to the widest possible audience. Here, we describe the activities of the BD2K TCC and its focus on the construction of the Educational Resource Discovery Index (ERuDIte), which identifies, collects, describes, and organizes online data science materials from BD2K awardees, open online courses, and videos from scientific lectures and tutorials. ERuDIte now indexes over 9,500 resources. Given the richness of online training materials and the constant evolution of biomedical data science, computational methods applying information retrieval, natural language processing, and machine learning techniques are required - in effect, using data science to inform training in data science. In so doing, the TCC seeks to democratize novel insights and discoveries brought forth via large-scale data science training.",fullPaper,cp15
Computer Science,p163,d3,843149b649b888fdb3649b8d4852263b62356799,c15,Pacific Symposium on Biocomputing,Democratizing data science through data science training,"The biomedical sciences have experienced an explosion of data which promises to overwhelm many current practitioners. Without easy access to data science training resources, biomedical researchers may find themselves unable to wrangle their own datasets. In 2014, to address the challenges posed such a data onslaught, the National Institutes of Health (NIH) launched the Big Data to Knowledge (BD2K) initiative. To this end, the BD2K Training Coordinating Center (TCC; bigdatau.org) was funded to facilitate both in-person and online learning, and open up the concepts of data science to the widest possible audience. Here, we describe the activities of the BD2K TCC and its focus on the construction of the Educational Resource Discovery Index (ERuDIte), which identifies, collects, describes, and organizes online data science materials from BD2K awardees, open online courses, and videos from scientific lectures and tutorials. ERuDIte now indexes over 9,500 resources. Given the richness of online training materials and the constant evolution of biomedical data science, computational methods applying information retrieval, natural language processing, and machine learning techniques are required - in effect, using data science to inform training in data science. In so doing, the TCC seeks to democratize novel insights and discoveries brought forth via large-scale data science training.",fullPaper,cp15
Political Science,p163,d15,843149b649b888fdb3649b8d4852263b62356799,c15,Pacific Symposium on Biocomputing,Democratizing data science through data science training,"The biomedical sciences have experienced an explosion of data which promises to overwhelm many current practitioners. Without easy access to data science training resources, biomedical researchers may find themselves unable to wrangle their own datasets. In 2014, to address the challenges posed such a data onslaught, the National Institutes of Health (NIH) launched the Big Data to Knowledge (BD2K) initiative. To this end, the BD2K Training Coordinating Center (TCC; bigdatau.org) was funded to facilitate both in-person and online learning, and open up the concepts of data science to the widest possible audience. Here, we describe the activities of the BD2K TCC and its focus on the construction of the Educational Resource Discovery Index (ERuDIte), which identifies, collects, describes, and organizes online data science materials from BD2K awardees, open online courses, and videos from scientific lectures and tutorials. ERuDIte now indexes over 9,500 resources. Given the richness of online training materials and the constant evolution of biomedical data science, computational methods applying information retrieval, natural language processing, and machine learning techniques are required - in effect, using data science to inform training in data science. In so doing, the TCC seeks to democratize novel insights and discoveries brought forth via large-scale data science training.",fullPaper,cp15
Medicine,p165,d1,b2114228411d367cfa6ca091008291f250a2c490,j60,Nature,Deep learning and process understanding for data-driven Earth system science,Abstract,fullPaper,jv60
Computer Science,p165,d3,b2114228411d367cfa6ca091008291f250a2c490,j60,Nature,Deep learning and process understanding for data-driven Earth system science,Abstract,fullPaper,jv60
Medicine,p166,d1,589ebdd0d7b4a58f7fdfb07f116f62681bb9a915,j23,Proceedings of the National Academy of Sciences of the United States of America,Hack weeks as a model for data science education and collaboration,"Significance As scientific disciplines grapple with more datasets of rapidly increasing complexity and size, new approaches are urgently required to introduce new statistical and computational tools into research communities and improve the cross-disciplinary exchange of ideas. In this paper, we introduce a type of scientific workshop, called a hack week, which allows for fast dissemination of new methodologies into scientific communities and fosters exchange and collaboration within and between disciplines. We present implementations of this concept in astronomy, neuroscience, and geoscience and show that hack weeks produce positive learning outcomes, foster lasting collaborations, yield scientific results, and promote positive attitudes toward open science. Across many scientific disciplines, methods for recording, storing, and analyzing data are rapidly increasing in complexity. Skillfully using data science tools that manage this complexity requires training in new programming languages and frameworks as well as immersion in new modes of interaction that foster data sharing, collaborative software development, and exchange across disciplines. Learning these skills from traditional university curricula can be challenging because most courses are not designed to evolve on time scales that can keep pace with rapidly shifting data science methods. Here, we present the concept of a hack week as an effective model offering opportunities for networking and community building, education in state-of-the-art data science methods, and immersion in collaborative project work. We find that hack weeks are successful at cultivating collaboration and facilitating the exchange of knowledge. Participants self-report that these events help them in both their day-to-day research as well as their careers. Based on our results, we conclude that hack weeks present an effective, easy-to-implement, fairly low-cost tool to positively impact data analysis literacy in academic disciplines, foster collaboration, and cultivate best practices.",fullPaper,jv23
Physics,p166,d2,589ebdd0d7b4a58f7fdfb07f116f62681bb9a915,j23,Proceedings of the National Academy of Sciences of the United States of America,Hack weeks as a model for data science education and collaboration,"Significance As scientific disciplines grapple with more datasets of rapidly increasing complexity and size, new approaches are urgently required to introduce new statistical and computational tools into research communities and improve the cross-disciplinary exchange of ideas. In this paper, we introduce a type of scientific workshop, called a hack week, which allows for fast dissemination of new methodologies into scientific communities and fosters exchange and collaboration within and between disciplines. We present implementations of this concept in astronomy, neuroscience, and geoscience and show that hack weeks produce positive learning outcomes, foster lasting collaborations, yield scientific results, and promote positive attitudes toward open science. Across many scientific disciplines, methods for recording, storing, and analyzing data are rapidly increasing in complexity. Skillfully using data science tools that manage this complexity requires training in new programming languages and frameworks as well as immersion in new modes of interaction that foster data sharing, collaborative software development, and exchange across disciplines. Learning these skills from traditional university curricula can be challenging because most courses are not designed to evolve on time scales that can keep pace with rapidly shifting data science methods. Here, we present the concept of a hack week as an effective model offering opportunities for networking and community building, education in state-of-the-art data science methods, and immersion in collaborative project work. We find that hack weeks are successful at cultivating collaboration and facilitating the exchange of knowledge. Participants self-report that these events help them in both their day-to-day research as well as their careers. Based on our results, we conclude that hack weeks present an effective, easy-to-implement, fairly low-cost tool to positively impact data analysis literacy in academic disciplines, foster collaboration, and cultivate best practices.",fullPaper,jv23
Computer Science,p166,d3,589ebdd0d7b4a58f7fdfb07f116f62681bb9a915,j23,Proceedings of the National Academy of Sciences of the United States of America,Hack weeks as a model for data science education and collaboration,"Significance As scientific disciplines grapple with more datasets of rapidly increasing complexity and size, new approaches are urgently required to introduce new statistical and computational tools into research communities and improve the cross-disciplinary exchange of ideas. In this paper, we introduce a type of scientific workshop, called a hack week, which allows for fast dissemination of new methodologies into scientific communities and fosters exchange and collaboration within and between disciplines. We present implementations of this concept in astronomy, neuroscience, and geoscience and show that hack weeks produce positive learning outcomes, foster lasting collaborations, yield scientific results, and promote positive attitudes toward open science. Across many scientific disciplines, methods for recording, storing, and analyzing data are rapidly increasing in complexity. Skillfully using data science tools that manage this complexity requires training in new programming languages and frameworks as well as immersion in new modes of interaction that foster data sharing, collaborative software development, and exchange across disciplines. Learning these skills from traditional university curricula can be challenging because most courses are not designed to evolve on time scales that can keep pace with rapidly shifting data science methods. Here, we present the concept of a hack week as an effective model offering opportunities for networking and community building, education in state-of-the-art data science methods, and immersion in collaborative project work. We find that hack weeks are successful at cultivating collaboration and facilitating the exchange of knowledge. Participants self-report that these events help them in both their day-to-day research as well as their careers. Based on our results, we conclude that hack weeks present an effective, easy-to-implement, fairly low-cost tool to positively impact data analysis literacy in academic disciplines, foster collaboration, and cultivate best practices.",fullPaper,jv23
Medicine,p185,d1,3af056b2aed8724dcddea074eb68aff6dd11c926,j64,PLoS Biology,Building the biomedical data science workforce,"This article describes efforts at the National Institutes of Health (NIH) from 2013 to 2016 to train a national workforce in biomedical data science. We provide an analysis of the Big Data to Knowledge (BD2K) training program strengths and weaknesses with an eye toward future directions aimed at any funder and potential funding recipient worldwide. The focus is on extramurally funded programs that have a national or international impact rather than the training of NIH staff, which was addressed by the NIH’s internal Data Science Workforce Development Center. From its inception, the major goal of BD2K was to narrow the gap between needed and existing biomedical data science skills. As biomedical research increasingly relies on computational, mathematical, and statistical thinking, supporting the training and education of the workforce of tomorrow requires new emphases on analytical skills. From 2013 to 2016, BD2K jump-started training in this area for all levels, from graduate students to senior researchers.",fullPaper,jv64
Biology,p185,d5,3af056b2aed8724dcddea074eb68aff6dd11c926,j64,PLoS Biology,Building the biomedical data science workforce,"This article describes efforts at the National Institutes of Health (NIH) from 2013 to 2016 to train a national workforce in biomedical data science. We provide an analysis of the Big Data to Knowledge (BD2K) training program strengths and weaknesses with an eye toward future directions aimed at any funder and potential funding recipient worldwide. The focus is on extramurally funded programs that have a national or international impact rather than the training of NIH staff, which was addressed by the NIH’s internal Data Science Workforce Development Center. From its inception, the major goal of BD2K was to narrow the gap between needed and existing biomedical data science skills. As biomedical research increasingly relies on computational, mathematical, and statistical thinking, supporting the training and education of the workforce of tomorrow requires new emphases on analytical skills. From 2013 to 2016, BD2K jump-started training in this area for all levels, from graduate students to senior researchers.",fullPaper,jv64
Medicine,p195,d1,3b963487cbf944d51f33c2a0b41eb2aed7c68b89,c54,ACM SIGCOMM Symposium on Software Defined Networking Research,Locating ethics in data science: responsibility and accountability in global and distributed knowledge production systems,"The distributed and global nature of data science creates challenges for evaluating the quality, import and potential impact of the data and knowledge claims being produced. This has significant consequences for the management and oversight of responsibilities and accountabilities in data science. In particular, it makes it difficult to determine who is responsible for what output, and how such responsibilities relate to each other; what ‘participation’ means and which accountabilities it involves, with regard to data ownership, donation and sharing as well as data analysis, re-use and authorship; and whether the trust placed on automated tools for data mining and interpretation is warranted (especially as data processing strategies and tools are often developed separately from the situations of data use where ethical concerns typically emerge). To address these challenges, this paper advocates a participative, reflexive management of data practices. Regulatory structures should encourage data scientists to examine the historical lineages and ethical implications of their work at regular intervals. They should also foster awareness of the multitude of skills and perspectives involved in data science, highlighting how each perspective is partial and in need of confrontation with others. This approach has the potential to improve not only the ethical oversight for data science initiatives, but also the quality and reliability of research outputs. This article is part of the themed issue ‘The ethical impact of data science’.",poster,cp54
Political Science,p195,d15,3b963487cbf944d51f33c2a0b41eb2aed7c68b89,c54,ACM SIGCOMM Symposium on Software Defined Networking Research,Locating ethics in data science: responsibility and accountability in global and distributed knowledge production systems,"The distributed and global nature of data science creates challenges for evaluating the quality, import and potential impact of the data and knowledge claims being produced. This has significant consequences for the management and oversight of responsibilities and accountabilities in data science. In particular, it makes it difficult to determine who is responsible for what output, and how such responsibilities relate to each other; what ‘participation’ means and which accountabilities it involves, with regard to data ownership, donation and sharing as well as data analysis, re-use and authorship; and whether the trust placed on automated tools for data mining and interpretation is warranted (especially as data processing strategies and tools are often developed separately from the situations of data use where ethical concerns typically emerge). To address these challenges, this paper advocates a participative, reflexive management of data practices. Regulatory structures should encourage data scientists to examine the historical lineages and ethical implications of their work at regular intervals. They should also foster awareness of the multitude of skills and perspectives involved in data science, highlighting how each perspective is partial and in need of confrontation with others. This approach has the potential to improve not only the ethical oversight for data science initiatives, but also the quality and reliability of research outputs. This article is part of the themed issue ‘The ethical impact of data science’.",poster,cp54
Medicine,p201,d1,7c614fe86cc11c0430dd12b44e018e16e5dcf742,j41,American Statistician,A Guide to Teaching Data Science,"ABSTRACT Demand for data science education is surging and traditional courses offered by statistics departments are not meeting the needs of those seeking training. This has led to a number of opinion pieces advocating for an update to the Statistics curriculum. The unifying recommendation is that computing should play a more prominent role. We strongly agree with this recommendation, but advocate the main priority is to bring applications to the forefront as proposed by Nolan and Speed in 1999. We also argue that the individuals tasked with developing data science courses should not only have statistical training, but also have experience analyzing data with the main objective of solving real-world problems. Here, we share a set of general principles and offer a detailed guide derived from our successful experience developing and teaching a graduate-level, introductory data science course centered entirely on case studies. We argue for the importance of statistical thinking, as defined by Wild and Pfannkuch in 1999 and describe how our approach teaches students three key skills needed to succeed in data science, which we refer to as creating, connecting, and computing. This guide can also be used for statisticians wanting to gain more practical knowledge about data science before embarking on teaching an introductory course. Supplementary materials for this article are available online.",fullPaper,jv41
Computer Science,p201,d3,7c614fe86cc11c0430dd12b44e018e16e5dcf742,j41,American Statistician,A Guide to Teaching Data Science,"ABSTRACT Demand for data science education is surging and traditional courses offered by statistics departments are not meeting the needs of those seeking training. This has led to a number of opinion pieces advocating for an update to the Statistics curriculum. The unifying recommendation is that computing should play a more prominent role. We strongly agree with this recommendation, but advocate the main priority is to bring applications to the forefront as proposed by Nolan and Speed in 1999. We also argue that the individuals tasked with developing data science courses should not only have statistical training, but also have experience analyzing data with the main objective of solving real-world problems. Here, we share a set of general principles and offer a detailed guide derived from our successful experience developing and teaching a graduate-level, introductory data science course centered entirely on case studies. We argue for the importance of statistical thinking, as defined by Wild and Pfannkuch in 1999 and describe how our approach teaches students three key skills needed to succeed in data science, which we refer to as creating, connecting, and computing. This guide can also be used for statisticians wanting to gain more practical knowledge about data science before embarking on teaching an introductory course. Supplementary materials for this article are available online.",fullPaper,jv41
Sociology,p201,d4,7c614fe86cc11c0430dd12b44e018e16e5dcf742,j41,American Statistician,A Guide to Teaching Data Science,"ABSTRACT Demand for data science education is surging and traditional courses offered by statistics departments are not meeting the needs of those seeking training. This has led to a number of opinion pieces advocating for an update to the Statistics curriculum. The unifying recommendation is that computing should play a more prominent role. We strongly agree with this recommendation, but advocate the main priority is to bring applications to the forefront as proposed by Nolan and Speed in 1999. We also argue that the individuals tasked with developing data science courses should not only have statistical training, but also have experience analyzing data with the main objective of solving real-world problems. Here, we share a set of general principles and offer a detailed guide derived from our successful experience developing and teaching a graduate-level, introductory data science course centered entirely on case studies. We argue for the importance of statistical thinking, as defined by Wild and Pfannkuch in 1999 and describe how our approach teaches students three key skills needed to succeed in data science, which we refer to as creating, connecting, and computing. This guide can also be used for statisticians wanting to gain more practical knowledge about data science before embarking on teaching an introductory course. Supplementary materials for this article are available online.",fullPaper,jv41
Mathematics,p201,d6,7c614fe86cc11c0430dd12b44e018e16e5dcf742,j41,American Statistician,A Guide to Teaching Data Science,"ABSTRACT Demand for data science education is surging and traditional courses offered by statistics departments are not meeting the needs of those seeking training. This has led to a number of opinion pieces advocating for an update to the Statistics curriculum. The unifying recommendation is that computing should play a more prominent role. We strongly agree with this recommendation, but advocate the main priority is to bring applications to the forefront as proposed by Nolan and Speed in 1999. We also argue that the individuals tasked with developing data science courses should not only have statistical training, but also have experience analyzing data with the main objective of solving real-world problems. Here, we share a set of general principles and offer a detailed guide derived from our successful experience developing and teaching a graduate-level, introductory data science course centered entirely on case studies. We argue for the importance of statistical thinking, as defined by Wild and Pfannkuch in 1999 and describe how our approach teaches students three key skills needed to succeed in data science, which we refer to as creating, connecting, and computing. This guide can also be used for statisticians wanting to gain more practical knowledge about data science before embarking on teaching an introductory course. Supplementary materials for this article are available online.",fullPaper,jv41
Psychology,p201,d10,7c614fe86cc11c0430dd12b44e018e16e5dcf742,j41,American Statistician,A Guide to Teaching Data Science,"ABSTRACT Demand for data science education is surging and traditional courses offered by statistics departments are not meeting the needs of those seeking training. This has led to a number of opinion pieces advocating for an update to the Statistics curriculum. The unifying recommendation is that computing should play a more prominent role. We strongly agree with this recommendation, but advocate the main priority is to bring applications to the forefront as proposed by Nolan and Speed in 1999. We also argue that the individuals tasked with developing data science courses should not only have statistical training, but also have experience analyzing data with the main objective of solving real-world problems. Here, we share a set of general principles and offer a detailed guide derived from our successful experience developing and teaching a graduate-level, introductory data science course centered entirely on case studies. We argue for the importance of statistical thinking, as defined by Wild and Pfannkuch in 1999 and describe how our approach teaches students three key skills needed to succeed in data science, which we refer to as creating, connecting, and computing. This guide can also be used for statisticians wanting to gain more practical knowledge about data science before embarking on teaching an introductory course. Supplementary materials for this article are available online.",fullPaper,jv41
Medicine,p209,d1,bd1c1d5540f246090e740c0d5a0fa7f2c64059d1,c9,Big Data,Data Science and its Relationship to Big Data and Data-Driven Decision Making,"Companies have realized they need to hire data scientists, academic institutions are scrambling to put together data-science programs, and publications are touting data science as a hot-even ""sexy""-career choice. However, there is confusion about what exactly data science is, and this confusion could lead to disillusionment as the concept diffuses into meaningless buzz. In this article, we argue that there are good reasons why it has been hard to pin down exactly what is data science. One reason is that data science is intricately intertwined with other important concepts also of growing importance, such as big data and data-driven decision making. Another reason is the natural tendency to associate what a practitioner does with the definition of the practitioner's field; this can result in overlooking the fundamentals of the field. We believe that trying to define the boundaries of data science precisely is not of the utmost importance. We can debate the boundaries of the field in an academic setting, but in order for data science to serve business effectively, it is important (i) to understand its relationships to other important related concepts, and (ii) to begin to identify the fundamental principles underlying data science. Once we embrace (ii), we can much better understand and explain exactly what data science has to offer. Furthermore, only once we embrace (ii) should we be comfortable calling it data science. In this article, we present a perspective that addresses all these concepts. We close by offering, as examples, a partial list of fundamental principles underlying data science.",fullPaper,cp9
Computer Science,p209,d3,bd1c1d5540f246090e740c0d5a0fa7f2c64059d1,c9,Big Data,Data Science and its Relationship to Big Data and Data-Driven Decision Making,"Companies have realized they need to hire data scientists, academic institutions are scrambling to put together data-science programs, and publications are touting data science as a hot-even ""sexy""-career choice. However, there is confusion about what exactly data science is, and this confusion could lead to disillusionment as the concept diffuses into meaningless buzz. In this article, we argue that there are good reasons why it has been hard to pin down exactly what is data science. One reason is that data science is intricately intertwined with other important concepts also of growing importance, such as big data and data-driven decision making. Another reason is the natural tendency to associate what a practitioner does with the definition of the practitioner's field; this can result in overlooking the fundamentals of the field. We believe that trying to define the boundaries of data science precisely is not of the utmost importance. We can debate the boundaries of the field in an academic setting, but in order for data science to serve business effectively, it is important (i) to understand its relationships to other important related concepts, and (ii) to begin to identify the fundamental principles underlying data science. Once we embrace (ii), we can much better understand and explain exactly what data science has to offer. Furthermore, only once we embrace (ii) should we be comfortable calling it data science. In this article, we present a perspective that addresses all these concepts. We close by offering, as examples, a partial list of fundamental principles underlying data science.",fullPaper,cp9
Psychology,p209,d10,bd1c1d5540f246090e740c0d5a0fa7f2c64059d1,c9,Big Data,Data Science and its Relationship to Big Data and Data-Driven Decision Making,"Companies have realized they need to hire data scientists, academic institutions are scrambling to put together data-science programs, and publications are touting data science as a hot-even ""sexy""-career choice. However, there is confusion about what exactly data science is, and this confusion could lead to disillusionment as the concept diffuses into meaningless buzz. In this article, we argue that there are good reasons why it has been hard to pin down exactly what is data science. One reason is that data science is intricately intertwined with other important concepts also of growing importance, such as big data and data-driven decision making. Another reason is the natural tendency to associate what a practitioner does with the definition of the practitioner's field; this can result in overlooking the fundamentals of the field. We believe that trying to define the boundaries of data science precisely is not of the utmost importance. We can debate the boundaries of the field in an academic setting, but in order for data science to serve business effectively, it is important (i) to understand its relationships to other important related concepts, and (ii) to begin to identify the fundamental principles underlying data science. Once we embrace (ii), we can much better understand and explain exactly what data science has to offer. Furthermore, only once we embrace (ii) should we be comfortable calling it data science. In this article, we present a perspective that addresses all these concepts. We close by offering, as examples, a partial list of fundamental principles underlying data science.",fullPaper,cp9
Medicine,p227,d1,3eb5f2d152ead21ce528f9781c66197010eea3c8,c116,International Society for Music Information Retrieval Conference,A Case for Data Commons: Toward Data Science as a Service,"Data commons collocate data, storage, and computing infrastructure with core services and commonly used tools and applications for managing, analyzing, and sharing data to create an interoperable resource for the research community. An architecture for data commons is described, as well as some lessons learned from operating several large-scale data commons.",poster,cp116
Computer Science,p227,d3,3eb5f2d152ead21ce528f9781c66197010eea3c8,c116,International Society for Music Information Retrieval Conference,A Case for Data Commons: Toward Data Science as a Service,"Data commons collocate data, storage, and computing infrastructure with core services and commonly used tools and applications for managing, analyzing, and sharing data to create an interoperable resource for the research community. An architecture for data commons is described, as well as some lessons learned from operating several large-scale data commons.",poster,cp116
Medicine,p234,d1,b885916e9af51010ca7ebafbc9270f0e5e207b38,j71,Nursing Administration Quarterly,Nursing Knowledge: Big Data Science—Implications for Nurse Leaders,"The integration of Big Data from electronic health records and other information systems within and across health care enterprises provides an opportunity to develop actionable predictive models that can increase the confidence of nursing leaders' decisions to improve patient outcomes and safety and control costs. As health care shifts to the community, mobile health applications add to the Big Data available. There is an evolving national action plan that includes nursing data in Big Data science, spearheaded by the University of Minnesota School of Nursing. For the past 3 years, diverse stakeholders from practice, industry, education, research, and professional organizations have collaborated through the “Nursing Knowledge: Big Data Science” conferences to create and act on recommendations for inclusion of nursing data, integrated with patient-generated, interprofessional, and contextual data. It is critical for nursing leaders to understand the value of Big Data science and the ways to standardize data and workflow processes to take advantage of newer cutting edge analytics to support analytic methods to control costs and improve patient quality and safety.",fullPaper,jv71
Medicine,p240,d1,b70cfcc6bbb764728f8aa55aa173cc692eb77bdf,j76,Frontiers in Genetics,The Process of Analyzing Data is the Emergent Feature of Data Science,"In recent years the term “data science” gained considerable attention worldwide. In a A Very Short History Of Data Science by Press (2013), the first appearance of the term is ascribed to Peter Naur in 1974 (Concise Survey of Computer Methods). Regardless who used the term first and in what context it has been used, we think that data science is a good term to indicate that data are the focus of scientific research. This is in analogy to computer science, where the first department of computer science in the USA had been established in 1962 at Purdue University, at a time when the first electronic computers became available and it was still not clear enough what computers can do, one created therefore a new field where the computer was the focus of the study. In this paper, we want to address a couple of questions in order to demystify the meaning and the goals of data science in general.",fullPaper,jv76
Computer Science,p240,d3,b70cfcc6bbb764728f8aa55aa173cc692eb77bdf,j76,Frontiers in Genetics,The Process of Analyzing Data is the Emergent Feature of Data Science,"In recent years the term “data science” gained considerable attention worldwide. In a A Very Short History Of Data Science by Press (2013), the first appearance of the term is ascribed to Peter Naur in 1974 (Concise Survey of Computer Methods). Regardless who used the term first and in what context it has been used, we think that data science is a good term to indicate that data are the focus of scientific research. This is in analogy to computer science, where the first department of computer science in the USA had been established in 1962 at Purdue University, at a time when the first electronic computers became available and it was still not clear enough what computers can do, one created therefore a new field where the computer was the focus of the study. In this paper, we want to address a couple of questions in order to demystify the meaning and the goals of data science in general.",fullPaper,jv76
Medicine,p242,d1,23a57b1e2beb4235d2020ed57f484c947e3d0816,c9,Big Data,The Quantified Self: Fundamental Disruption in Big Data Science and Biological Discovery,"A key contemporary trend emerging in big data science is the quantified self (QS)-individuals engaged in the self-tracking of any kind of biological, physical, behavioral, or environmental information as n=1 individuals or in groups. There are opportunities for big data scientists to develop new models to support QS data collection, integration, and analysis, and also to lead in defining open-access database resources and privacy standards for how personal data is used. Next-generation QS applications could include tools for rendering QS data meaningful in behavior change, establishing baselines and variability in objective metrics, applying new kinds of pattern recognition techniques, and aggregating multiple self-tracking data streams from wearable electronics, biosensors, mobile phones, genomic data, and cloud-based services. The long-term vision of QS activity is that of a systemic monitoring approach where an individual's continuous personal information climate provides real-time performance optimization suggestions. There are some potential limitations related to QS activity-barriers to widespread adoption and a critique regarding scientific soundness-but these may be overcome. One interesting aspect of QS activity is that it is fundamentally a quantitative and qualitative phenomenon since it includes both the collection of objective metrics data and the subjective experience of the impact of these data. Some of this dynamic is being explored as the quantified self is becoming the qualified self in two new ways: by applying QS methods to the tracking of qualitative phenomena such as mood, and by understanding that QS data collection is just the first step in creating qualitative feedback loops for behavior change. In the long-term future, the quantified self may become additionally transformed into the extended exoself as data quantification and self-tracking enable the development of new sense capabilities that are not possible with ordinary senses. The individual body becomes a more knowable, calculable, and administrable object through QS activity, and individuals have an increasingly intimate relationship with data as it mediates the experience of reality.",fullPaper,cp9
Computer Science,p242,d3,23a57b1e2beb4235d2020ed57f484c947e3d0816,c9,Big Data,The Quantified Self: Fundamental Disruption in Big Data Science and Biological Discovery,"A key contemporary trend emerging in big data science is the quantified self (QS)-individuals engaged in the self-tracking of any kind of biological, physical, behavioral, or environmental information as n=1 individuals or in groups. There are opportunities for big data scientists to develop new models to support QS data collection, integration, and analysis, and also to lead in defining open-access database resources and privacy standards for how personal data is used. Next-generation QS applications could include tools for rendering QS data meaningful in behavior change, establishing baselines and variability in objective metrics, applying new kinds of pattern recognition techniques, and aggregating multiple self-tracking data streams from wearable electronics, biosensors, mobile phones, genomic data, and cloud-based services. The long-term vision of QS activity is that of a systemic monitoring approach where an individual's continuous personal information climate provides real-time performance optimization suggestions. There are some potential limitations related to QS activity-barriers to widespread adoption and a critique regarding scientific soundness-but these may be overcome. One interesting aspect of QS activity is that it is fundamentally a quantitative and qualitative phenomenon since it includes both the collection of objective metrics data and the subjective experience of the impact of these data. Some of this dynamic is being explored as the quantified self is becoming the qualified self in two new ways: by applying QS methods to the tracking of qualitative phenomena such as mood, and by understanding that QS data collection is just the first step in creating qualitative feedback loops for behavior change. In the long-term future, the quantified self may become additionally transformed into the extended exoself as data quantification and self-tracking enable the development of new sense capabilities that are not possible with ordinary senses. The individual body becomes a more knowable, calculable, and administrable object through QS activity, and individuals have an increasingly intimate relationship with data as it mediates the experience of reality.",fullPaper,cp9
Medicine,p245,d1,57039a11d9fb423595a4e16129f7cc7f3ff2cac7,c92,International Symposium on Computer Architecture,Data science ethics in government,"Data science can offer huge opportunities for government. With the ability to process larger and more complex datasets than ever before, it can provide better insights for policymakers and make services more tailored and efficient. As with all new technologies, there is a risk that we do not take up its opportunities and miss out on its enormous potential. We want people to feel confident to innovate with data. So, over the past 18 months, the Government Data Science Partnership has taken an open, evidence-based and user-centred approach to creating an ethical framework. It is a practical document that brings all the legal guidance together in one place, and is written in the context of new data science capabilities. As part of its development, we ran a public dialogue on data science ethics, including deliberative workshops, an experimental conjoint survey and an online engagement tool. The research supported the principles set out in the framework as well as provided useful insight into how we need to communicate about data science. It found that people had a low awareness of the term ‘data science’, but that showing data science examples can increase broad support for government exploring innovative uses of data. But people's support is highly context driven. People consider acceptability on a case-by-case basis, first thinking about the overall policy goals and likely intended outcome, and then weighing up privacy and unintended consequences. The ethical framework is a crucial start, but it does not solve all the challenges it highlights, particularly as technology is creating new challenges and opportunities every day. Continued research is needed into data minimization and anonymization, robust data models, algorithmic accountability, and transparency and data security. It also has revealed the need to set out a renewed deal between the citizen and state on data, to maintain and solidify trust in how we use people's data for social good. This article is part of the themed issue ‘The ethical impact of data science’.",poster,cp92
Business,p245,d9,57039a11d9fb423595a4e16129f7cc7f3ff2cac7,c92,International Symposium on Computer Architecture,Data science ethics in government,"Data science can offer huge opportunities for government. With the ability to process larger and more complex datasets than ever before, it can provide better insights for policymakers and make services more tailored and efficient. As with all new technologies, there is a risk that we do not take up its opportunities and miss out on its enormous potential. We want people to feel confident to innovate with data. So, over the past 18 months, the Government Data Science Partnership has taken an open, evidence-based and user-centred approach to creating an ethical framework. It is a practical document that brings all the legal guidance together in one place, and is written in the context of new data science capabilities. As part of its development, we ran a public dialogue on data science ethics, including deliberative workshops, an experimental conjoint survey and an online engagement tool. The research supported the principles set out in the framework as well as provided useful insight into how we need to communicate about data science. It found that people had a low awareness of the term ‘data science’, but that showing data science examples can increase broad support for government exploring innovative uses of data. But people's support is highly context driven. People consider acceptability on a case-by-case basis, first thinking about the overall policy goals and likely intended outcome, and then weighing up privacy and unintended consequences. The ethical framework is a crucial start, but it does not solve all the challenges it highlights, particularly as technology is creating new challenges and opportunities every day. Continued research is needed into data minimization and anonymization, robust data models, algorithmic accountability, and transparency and data security. It also has revealed the need to set out a renewed deal between the citizen and state on data, to maintain and solidify trust in how we use people's data for social good. This article is part of the themed issue ‘The ethical impact of data science’.",poster,cp92
Medicine,p251,d1,9ba08d45d60130c7e5880f63a980b185a86e177c,c9,Big Data,A Big Data Guide to Understanding Climate Change: The Case for Theory-Guided Data Science,"Global climate change and its impact on human life has become one of our era's greatest challenges. Despite the urgency, data science has had little impact on furthering our understanding of our planet in spite of the abundance of climate data. This is a stark contrast from other fields such as advertising or electronic commerce where big data has been a great success story. This discrepancy stems from the complex nature of climate data as well as the scientific questions climate science brings forth. This article introduces a data science audience to the challenges and opportunities to mine large climate datasets, with an emphasis on the nuanced difference between mining climate data and traditional big data approaches. We focus on data, methods, and application challenges that must be addressed in order for big data to fulfill their promise with regard to climate science applications. More importantly, we highlight research showing that solely relying on traditional big data techniques results in dubious findings, and we instead propose a theory-guided data science paradigm that uses scientific theory to constrain both the big data techniques as well as the results-interpretation process to extract accurate insight from large climate data.",fullPaper,cp9
Computer Science,p251,d3,9ba08d45d60130c7e5880f63a980b185a86e177c,c9,Big Data,A Big Data Guide to Understanding Climate Change: The Case for Theory-Guided Data Science,"Global climate change and its impact on human life has become one of our era's greatest challenges. Despite the urgency, data science has had little impact on furthering our understanding of our planet in spite of the abundance of climate data. This is a stark contrast from other fields such as advertising or electronic commerce where big data has been a great success story. This discrepancy stems from the complex nature of climate data as well as the scientific questions climate science brings forth. This article introduces a data science audience to the challenges and opportunities to mine large climate datasets, with an emphasis on the nuanced difference between mining climate data and traditional big data approaches. We focus on data, methods, and application challenges that must be addressed in order for big data to fulfill their promise with regard to climate science applications. More importantly, we highlight research showing that solely relying on traditional big data techniques results in dubious findings, and we instead propose a theory-guided data science paradigm that uses scientific theory to constrain both the big data techniques as well as the results-interpretation process to extract accurate insight from large climate data.",fullPaper,cp9
Geography,p251,d13,9ba08d45d60130c7e5880f63a980b185a86e177c,c9,Big Data,A Big Data Guide to Understanding Climate Change: The Case for Theory-Guided Data Science,"Global climate change and its impact on human life has become one of our era's greatest challenges. Despite the urgency, data science has had little impact on furthering our understanding of our planet in spite of the abundance of climate data. This is a stark contrast from other fields such as advertising or electronic commerce where big data has been a great success story. This discrepancy stems from the complex nature of climate data as well as the scientific questions climate science brings forth. This article introduces a data science audience to the challenges and opportunities to mine large climate datasets, with an emphasis on the nuanced difference between mining climate data and traditional big data approaches. We focus on data, methods, and application challenges that must be addressed in order for big data to fulfill their promise with regard to climate science applications. More importantly, we highlight research showing that solely relying on traditional big data techniques results in dubious findings, and we instead propose a theory-guided data science paradigm that uses scientific theory to constrain both the big data techniques as well as the results-interpretation process to extract accurate insight from large climate data.",fullPaper,cp9
Medicine,p264,d1,3d7fcd1399573fb5cb455de6f85f149e0ab53828,c9,Big Data,The Science of Data Science,Abstract,fullPaper,cp9
Computer Science,p264,d3,3d7fcd1399573fb5cb455de6f85f149e0ab53828,c9,Big Data,The Science of Data Science,Abstract,fullPaper,cp9
Medicine,p272,d1,8f63eed1c6aef4e96a08281563c2305ff55e7ab9,j82,BMC Bioinformatics,ImageJ2: ImageJ for the next generation of scientific image data,Abstract,fullPaper,jv82
Computer Science,p272,d3,8f63eed1c6aef4e96a08281563c2305ff55e7ab9,j82,BMC Bioinformatics,ImageJ2: ImageJ for the next generation of scientific image data,Abstract,fullPaper,jv82
Biology,p272,d5,8f63eed1c6aef4e96a08281563c2305ff55e7ab9,j82,BMC Bioinformatics,ImageJ2: ImageJ for the next generation of scientific image data,Abstract,fullPaper,jv82
Medicine,p275,d1,6a324214a73610d8819e004e7ebd7dd23107d1f8,j60,Nature,Computing: A vision for data science,Abstract,fullPaper,jv60
Computer Science,p275,d3,6a324214a73610d8819e004e7ebd7dd23107d1f8,j60,Nature,Computing: A vision for data science,Abstract,fullPaper,jv60
Medicine,p280,d1,6009556bdf3aa3a111a6ddc2c9200a59af1e13e2,j23,Proceedings of the National Academy of Sciences of the United States of America,"Active learning increases student performance in science, engineering, and mathematics","Significance The President’s Council of Advisors on Science and Technology has called for a 33% increase in the number of science, technology, engineering, and mathematics (STEM) bachelor’s degrees completed per year and recommended adoption of empirically validated teaching practices as critical to achieving that goal. The studies analyzed here document that active learning leads to increases in examination performance that would raise average grades by a half a letter, and that failure rates under traditional lecturing increase by 55% over the rates observed under active learning. The analysis supports theory claiming that calls to increase the number of students receiving STEM degrees could be answered, at least in part, by abandoning traditional lecturing in favor of active learning. To test the hypothesis that lecturing maximizes learning and course performance, we metaanalyzed 225 studies that reported data on examination scores or failure rates when comparing student performance in undergraduate science, technology, engineering, and mathematics (STEM) courses under traditional lecturing versus active learning. The effect sizes indicate that on average, student performance on examinations and concept inventories increased by 0.47 SDs under active learning (n = 158 studies), and that the odds ratio for failing was 1.95 under traditional lecturing (n = 67 studies). These results indicate that average examination scores improved by about 6% in active learning sections, and that students in classes with traditional lecturing were 1.5 times more likely to fail than were students in classes with active learning. Heterogeneity analyses indicated that both results hold across the STEM disciplines, that active learning increases scores on concept inventories more than on course examinations, and that active learning appears effective across all class sizes—although the greatest effects are in small (n ≤ 50) classes. Trim and fill analyses and fail-safe n calculations suggest that the results are not due to publication bias. The results also appear robust to variation in the methodological rigor of the included studies, based on the quality of controls over student quality and instructor identity. This is the largest and most comprehensive metaanalysis of undergraduate STEM education published to date. The results raise questions about the continued use of traditional lecturing as a control in research studies, and support active learning as the preferred, empirically validated teaching practice in regular classrooms.",fullPaper,jv23
Computer Science,p280,d3,6009556bdf3aa3a111a6ddc2c9200a59af1e13e2,j23,Proceedings of the National Academy of Sciences of the United States of America,"Active learning increases student performance in science, engineering, and mathematics","Significance The President’s Council of Advisors on Science and Technology has called for a 33% increase in the number of science, technology, engineering, and mathematics (STEM) bachelor’s degrees completed per year and recommended adoption of empirically validated teaching practices as critical to achieving that goal. The studies analyzed here document that active learning leads to increases in examination performance that would raise average grades by a half a letter, and that failure rates under traditional lecturing increase by 55% over the rates observed under active learning. The analysis supports theory claiming that calls to increase the number of students receiving STEM degrees could be answered, at least in part, by abandoning traditional lecturing in favor of active learning. To test the hypothesis that lecturing maximizes learning and course performance, we metaanalyzed 225 studies that reported data on examination scores or failure rates when comparing student performance in undergraduate science, technology, engineering, and mathematics (STEM) courses under traditional lecturing versus active learning. The effect sizes indicate that on average, student performance on examinations and concept inventories increased by 0.47 SDs under active learning (n = 158 studies), and that the odds ratio for failing was 1.95 under traditional lecturing (n = 67 studies). These results indicate that average examination scores improved by about 6% in active learning sections, and that students in classes with traditional lecturing were 1.5 times more likely to fail than were students in classes with active learning. Heterogeneity analyses indicated that both results hold across the STEM disciplines, that active learning increases scores on concept inventories more than on course examinations, and that active learning appears effective across all class sizes—although the greatest effects are in small (n ≤ 50) classes. Trim and fill analyses and fail-safe n calculations suggest that the results are not due to publication bias. The results also appear robust to variation in the methodological rigor of the included studies, based on the quality of controls over student quality and instructor identity. This is the largest and most comprehensive metaanalysis of undergraduate STEM education published to date. The results raise questions about the continued use of traditional lecturing as a control in research studies, and support active learning as the preferred, empirically validated teaching practice in regular classrooms.",fullPaper,jv23
Mathematics,p280,d6,6009556bdf3aa3a111a6ddc2c9200a59af1e13e2,j23,Proceedings of the National Academy of Sciences of the United States of America,"Active learning increases student performance in science, engineering, and mathematics","Significance The President’s Council of Advisors on Science and Technology has called for a 33% increase in the number of science, technology, engineering, and mathematics (STEM) bachelor’s degrees completed per year and recommended adoption of empirically validated teaching practices as critical to achieving that goal. The studies analyzed here document that active learning leads to increases in examination performance that would raise average grades by a half a letter, and that failure rates under traditional lecturing increase by 55% over the rates observed under active learning. The analysis supports theory claiming that calls to increase the number of students receiving STEM degrees could be answered, at least in part, by abandoning traditional lecturing in favor of active learning. To test the hypothesis that lecturing maximizes learning and course performance, we metaanalyzed 225 studies that reported data on examination scores or failure rates when comparing student performance in undergraduate science, technology, engineering, and mathematics (STEM) courses under traditional lecturing versus active learning. The effect sizes indicate that on average, student performance on examinations and concept inventories increased by 0.47 SDs under active learning (n = 158 studies), and that the odds ratio for failing was 1.95 under traditional lecturing (n = 67 studies). These results indicate that average examination scores improved by about 6% in active learning sections, and that students in classes with traditional lecturing were 1.5 times more likely to fail than were students in classes with active learning. Heterogeneity analyses indicated that both results hold across the STEM disciplines, that active learning increases scores on concept inventories more than on course examinations, and that active learning appears effective across all class sizes—although the greatest effects are in small (n ≤ 50) classes. Trim and fill analyses and fail-safe n calculations suggest that the results are not due to publication bias. The results also appear robust to variation in the methodological rigor of the included studies, based on the quality of controls over student quality and instructor identity. This is the largest and most comprehensive metaanalysis of undergraduate STEM education published to date. The results raise questions about the continued use of traditional lecturing as a control in research studies, and support active learning as the preferred, empirically validated teaching practice in regular classrooms.",fullPaper,jv23
Medicine,p283,d1,abc0a9eb3ae901ece2f532f504c336fbb6ba81ca,j86,Advancement of science,"Data‐Driven Materials Science: Status, Challenges, and Perspectives","Data‐driven science is heralded as a new paradigm in materials science. In this field, data is the new resource, and knowledge is extracted from materials datasets that are too big or complex for traditional human reasoning—typically with the intent to discover new or improved materials or materials phenomena. Multiple factors, including the open science movement, national funding, and progress in information technology, have fueled its development. Such related tools as materials databases, machine learning, and high‐throughput methods are now established as parts of the materials research toolset. However, there are a variety of challenges that impede progress in data‐driven materials science: data veracity, integration of experimental and computational data, data longevity, standardization, and the gap between industrial interests and academic efforts. In this perspective article, the historical development and current state of data‐driven materials science, building from the early evolution of open science to the rapid expansion of materials data infrastructures are discussed. Key successes and challenges so far are also reviewed, providing a perspective on the future development of the field.",fullPaper,jv86
Physics,p283,d2,abc0a9eb3ae901ece2f532f504c336fbb6ba81ca,j86,Advancement of science,"Data‐Driven Materials Science: Status, Challenges, and Perspectives","Data‐driven science is heralded as a new paradigm in materials science. In this field, data is the new resource, and knowledge is extracted from materials datasets that are too big or complex for traditional human reasoning—typically with the intent to discover new or improved materials or materials phenomena. Multiple factors, including the open science movement, national funding, and progress in information technology, have fueled its development. Such related tools as materials databases, machine learning, and high‐throughput methods are now established as parts of the materials research toolset. However, there are a variety of challenges that impede progress in data‐driven materials science: data veracity, integration of experimental and computational data, data longevity, standardization, and the gap between industrial interests and academic efforts. In this perspective article, the historical development and current state of data‐driven materials science, building from the early evolution of open science to the rapid expansion of materials data infrastructures are discussed. Key successes and challenges so far are also reviewed, providing a perspective on the future development of the field.",fullPaper,jv86
Medicine,p284,d1,e60d9464935582cda41becd7c1455c09392a2a93,j87,Psychological Science in the Public Interest,The Science of Visual Data Communication: What Works,"Effectively designed data visualizations allow viewers to use their powerful visual systems to understand patterns in data across science, education, health, and public policy. But ineffectively designed visualizations can cause confusion, misunderstanding, or even distrust—especially among viewers with low graphical literacy. We review research-backed guidelines for creating effective and intuitive visualizations oriented toward communicating data to students, coworkers, and the general public. We describe how the visual system can quickly extract broad statistics from a display, whereas poorly designed displays can lead to misperceptions and illusions. Extracting global statistics is fast, but comparing between subsets of values is slow. Effective graphics avoid taxing working memory, guide attention, and respect familiar conventions. Data visualizations can play a critical role in teaching and communication, provided that designers tailor those visualizations to their audience.",fullPaper,jv87
Medicine,p296,d1,ca5e7580993170b1fe621bc16383ad2dfa6803b5,j94,Comprehensive Reviews in Food Science and Food Safety,Utilization of text mining as a big data analysis tool for food science and nutrition.,"Big data analysis has found applications in many industries due to its ability to turn huge amounts of data into insights for informed business and operational decisions. Advanced data mining techniques have been applied in many sectors of supply chains in the food industry. However, the previous work has mainly focused on the analysis of instrument-generated data such as those from hyperspectral imaging, spectroscopy, and biometric receptors. The importance of digital text data in the food and nutrition has only recently gained attention due to advancements in big data analytics. The purpose of this review is to provide an overview of the data sources, computational methods, and applications of text data in the food industry. Text mining techniques such as word-level analysis (e.g., frequency analysis), word association analysis (e.g., network analysis), and advanced techniques (e.g., text classification, text clustering, topic modeling, information retrieval, and sentiment analysis) will be discussed. Applications of text data analysis will be illustrated with respect to food safety and food fraud surveillance, dietary pattern characterization, consumer-opinion mining, new-product development, food knowledge discovery, food supply-chain management, and online food services. The goal is to provide insights for intelligent decision-making to improve food production, food safety, and human nutrition.",fullPaper,jv94
Computer Science,p296,d3,ca5e7580993170b1fe621bc16383ad2dfa6803b5,j94,Comprehensive Reviews in Food Science and Food Safety,Utilization of text mining as a big data analysis tool for food science and nutrition.,"Big data analysis has found applications in many industries due to its ability to turn huge amounts of data into insights for informed business and operational decisions. Advanced data mining techniques have been applied in many sectors of supply chains in the food industry. However, the previous work has mainly focused on the analysis of instrument-generated data such as those from hyperspectral imaging, spectroscopy, and biometric receptors. The importance of digital text data in the food and nutrition has only recently gained attention due to advancements in big data analytics. The purpose of this review is to provide an overview of the data sources, computational methods, and applications of text data in the food industry. Text mining techniques such as word-level analysis (e.g., frequency analysis), word association analysis (e.g., network analysis), and advanced techniques (e.g., text classification, text clustering, topic modeling, information retrieval, and sentiment analysis) will be discussed. Applications of text data analysis will be illustrated with respect to food safety and food fraud surveillance, dietary pattern characterization, consumer-opinion mining, new-product development, food knowledge discovery, food supply-chain management, and online food services. The goal is to provide insights for intelligent decision-making to improve food production, food safety, and human nutrition.",fullPaper,jv94
Medicine,p298,d1,4449bd1f5bb8f86fec9ca7ded29ac8bf322c0114,j95,Ecology Letters,"AVONET: morphological, ecological and geographical data for all birds.","Functional traits offer a rich quantitative framework for developing and testing theories in evolutionary biology, ecology and ecosystem science. However, the potential of functional traits to drive theoretical advances and refine models of global change can only be fully realised when species-level information is complete. Here we present the AVONET dataset containing comprehensive functional trait data for all birds, including six ecological variables, 11 continuous morphological traits, and information on range size and location. Raw morphological measurements are presented from 90,020 individuals of 11,009 extant bird species sampled from 181 countries. These data are also summarised as species averages in three taxonomic formats, allowing integration with a global phylogeny, geographical range maps, IUCN Red List data and the eBird citizen science database. The AVONET dataset provides the most detailed picture of continuous trait variation for any major radiation of organisms, offering a global template for testing hypotheses and exploring the evolutionary origins, structure and functioning of biodiversity.",fullPaper,jv95
Medicine,p300,d1,3f809897b51d824846cf5a56f2a7b4292f7bc4a4,j60,Nature,Materials science: Share corrosion data,Abstract,fullPaper,jv60
Materials Science,p300,d7,3f809897b51d824846cf5a56f2a7b4292f7bc4a4,j60,Nature,Materials science: Share corrosion data,Abstract,fullPaper,jv60
Medicine,p301,d1,0e00f0dbfc381661826f8ddbafe73e33bcfe040f,j48,BioScience,Using Semistructured Surveys to Improve Citizen Science Data for Monitoring Biodiversity,"Abstract Biodiversity is being lost at an unprecedented rate, and monitoring is crucial for understanding the causal drivers and assessing solutions. Most biodiversity monitoring data are collected by volunteers through citizen science projects, and often crucial information is lacking to account for the inevitable biases that observers introduce during data collection. We contend that citizen science projects intended to support biodiversity monitoring must gather information about the observation process as well as species occurrence. We illustrate this using eBird, a global citizen science project that collects information on bird occurrences as well as vital contextual information on the observation process while maintaining broad participation. Our fundamental argument is that regardless of what species are being monitored, when citizen science projects collect a small set of basic information about how participants make their observations, the scientific value of the data collected will be dramatically improved.",fullPaper,jv48
Geography,p301,d13,0e00f0dbfc381661826f8ddbafe73e33bcfe040f,j48,BioScience,Using Semistructured Surveys to Improve Citizen Science Data for Monitoring Biodiversity,"Abstract Biodiversity is being lost at an unprecedented rate, and monitoring is crucial for understanding the causal drivers and assessing solutions. Most biodiversity monitoring data are collected by volunteers through citizen science projects, and often crucial information is lacking to account for the inevitable biases that observers introduce during data collection. We contend that citizen science projects intended to support biodiversity monitoring must gather information about the observation process as well as species occurrence. We illustrate this using eBird, a global citizen science project that collects information on bird occurrences as well as vital contextual information on the observation process while maintaining broad participation. Our fundamental argument is that regardless of what species are being monitored, when citizen science projects collect a small set of basic information about how participants make their observations, the scientific value of the data collected will be dramatically improved.",fullPaper,jv48
Medicine,p306,d1,22737046fbbe822deaaffddddb8f16be076d3f95,c8,Frontiers in Education Conference,"Open Science, Open Data, and Open Scholarship: European Policies to Make Science Fit for the Twenty-First Century","Open science will make science more efficient, reliable, and responsive to societal challenges. The European Commission has sought to advance open science policy from its inception in a holistic and integrated way, covering all aspects of the research cycle from scientific discovery and review to sharing knowledge, publishing, and outreach. We present the steps taken with a forward-looking perspective on the challenges laying ahead, in particular the necessary change of the rewards and incentives system for researchers (for which various actors are co-responsible and which goes beyond the mandate of the European Commission). Finally, we discuss the role of artificial intelligence (AI) within an open science perspective.",poster,cp8
Computer Science,p306,d3,22737046fbbe822deaaffddddb8f16be076d3f95,c8,Frontiers in Education Conference,"Open Science, Open Data, and Open Scholarship: European Policies to Make Science Fit for the Twenty-First Century","Open science will make science more efficient, reliable, and responsive to societal challenges. The European Commission has sought to advance open science policy from its inception in a holistic and integrated way, covering all aspects of the research cycle from scientific discovery and review to sharing knowledge, publishing, and outreach. We present the steps taken with a forward-looking perspective on the challenges laying ahead, in particular the necessary change of the rewards and incentives system for researchers (for which various actors are co-responsible and which goes beyond the mandate of the European Commission). Finally, we discuss the role of artificial intelligence (AI) within an open science perspective.",poster,cp8
Political Science,p306,d15,22737046fbbe822deaaffddddb8f16be076d3f95,c8,Frontiers in Education Conference,"Open Science, Open Data, and Open Scholarship: European Policies to Make Science Fit for the Twenty-First Century","Open science will make science more efficient, reliable, and responsive to societal challenges. The European Commission has sought to advance open science policy from its inception in a holistic and integrated way, covering all aspects of the research cycle from scientific discovery and review to sharing knowledge, publishing, and outreach. We present the steps taken with a forward-looking perspective on the challenges laying ahead, in particular the necessary change of the rewards and incentives system for researchers (for which various actors are co-responsible and which goes beyond the mandate of the European Commission). Finally, we discuss the role of artificial intelligence (AI) within an open science perspective.",poster,cp8
Medicine,p308,d1,6a697a4b3bdbbfb7681d9f9a518fc0be73744037,j101,Physical Review Letters,Big data of materials science: critical role of the descriptor.,"Statistical learning of materials properties or functions so far starts with a largely silent, nonchallenged step: the choice of the set of descriptive parameters (termed descriptor). However, when the scientific connection between the descriptor and the actuating mechanisms is unclear, the causality of the learned descriptor-property relation is uncertain. Thus, a trustful prediction of new promising materials, identification of anomalies, and scientific advancement are doubtful. We analyze this issue and define requirements for a suitable descriptor. For a classic example, the energy difference of zinc blende or wurtzite and rocksalt semiconductors, we demonstrate how a meaningful descriptor can be found systematically.",fullPaper,jv101
Physics,p308,d2,6a697a4b3bdbbfb7681d9f9a518fc0be73744037,j101,Physical Review Letters,Big data of materials science: critical role of the descriptor.,"Statistical learning of materials properties or functions so far starts with a largely silent, nonchallenged step: the choice of the set of descriptive parameters (termed descriptor). However, when the scientific connection between the descriptor and the actuating mechanisms is unclear, the causality of the learned descriptor-property relation is uncertain. Thus, a trustful prediction of new promising materials, identification of anomalies, and scientific advancement are doubtful. We analyze this issue and define requirements for a suitable descriptor. For a classic example, the energy difference of zinc blende or wurtzite and rocksalt semiconductors, we demonstrate how a meaningful descriptor can be found systematically.",fullPaper,jv101
Medicine,p309,d1,4e2f43dab69d690dc86422949e410ebf37f522d4,c76,Group,Bayesian data analysis.,"Bayesian methods have garnered huge interest in cognitive science as an approach to models of cognition and perception. On the other hand, Bayesian methods for data analysis have not yet made much headway in cognitive science against the institutionalized inertia of 20th century null hypothesis significance testing (NHST). Ironically, specific Bayesian models of cognition and perception may not long endure the ravages of empirical verification, but generic Bayesian methods for data analysis will eventually dominate. It is time that Bayesian data analysis became the norm for empirical methods in cognitive science. This article reviews a fatal flaw of NHST and introduces the reader to some benefits of Bayesian data analysis. The article presents illustrative examples of multiple comparisons in Bayesian analysis of variance and Bayesian approaches to statistical power. Copyright © 2010 John Wiley & Sons, Ltd. For further resources related to this article, please visit the WIREs website.",poster,cp76
Computer Science,p309,d3,4e2f43dab69d690dc86422949e410ebf37f522d4,c76,Group,Bayesian data analysis.,"Bayesian methods have garnered huge interest in cognitive science as an approach to models of cognition and perception. On the other hand, Bayesian methods for data analysis have not yet made much headway in cognitive science against the institutionalized inertia of 20th century null hypothesis significance testing (NHST). Ironically, specific Bayesian models of cognition and perception may not long endure the ravages of empirical verification, but generic Bayesian methods for data analysis will eventually dominate. It is time that Bayesian data analysis became the norm for empirical methods in cognitive science. This article reviews a fatal flaw of NHST and introduces the reader to some benefits of Bayesian data analysis. The article presents illustrative examples of multiple comparisons in Bayesian analysis of variance and Bayesian approaches to statistical power. Copyright © 2010 John Wiley & Sons, Ltd. For further resources related to this article, please visit the WIREs website.",poster,cp76
Medicine,p310,d1,f4c01d8780c86abdcfdd52c60843a2499fd5c1b6,j102,Perspectives on Psychological Science,Using Smartphones to Collect Behavioral Data in Psychological Science,"Smartphones now offer the promise of collecting behavioral data unobtrusively, in situ, as it unfolds in the course of daily life. Data can be collected from the onboard sensors and other phone logs embedded in today’s off-the-shelf smartphone devices. These data permit fine-grained, continuous collection of people’s social interactions (e.g., speaking rates in conversation, size of social groups, calls, and text messages), daily activities (e.g., physical activity and sleep), and mobility patterns (e.g., frequency and duration of time spent at various locations). In this article, we have drawn on the lessons from the first wave of smartphone-sensing research to highlight areas of opportunity for psychological research, present practical considerations for designing smartphone studies, and discuss the ongoing methodological and ethical challenges associated with research in this domain. It is our hope that these practical guidelines will facilitate the use of smartphones as a behavioral observation tool in psychological science.",fullPaper,jv102
Computer Science,p310,d3,f4c01d8780c86abdcfdd52c60843a2499fd5c1b6,j102,Perspectives on Psychological Science,Using Smartphones to Collect Behavioral Data in Psychological Science,"Smartphones now offer the promise of collecting behavioral data unobtrusively, in situ, as it unfolds in the course of daily life. Data can be collected from the onboard sensors and other phone logs embedded in today’s off-the-shelf smartphone devices. These data permit fine-grained, continuous collection of people’s social interactions (e.g., speaking rates in conversation, size of social groups, calls, and text messages), daily activities (e.g., physical activity and sleep), and mobility patterns (e.g., frequency and duration of time spent at various locations). In this article, we have drawn on the lessons from the first wave of smartphone-sensing research to highlight areas of opportunity for psychological research, present practical considerations for designing smartphone studies, and discuss the ongoing methodological and ethical challenges associated with research in this domain. It is our hope that these practical guidelines will facilitate the use of smartphones as a behavioral observation tool in psychological science.",fullPaper,jv102
Medicine,p312,d1,4436ca7e9f91b7ad9ad6a09dbe12f48d9f6c3e7f,j104,Science,Data-driven predictions in the science of science,"The desire to predict discoveries—to have some idea, in advance, of what will be discovered, by whom, when, and where—pervades nearly all aspects of modern science, from individual scientists to publishers, from funding agencies to hiring committees. In this Essay, we survey the emerging and interdisciplinary field of the “science of science” and what it teaches us about the predictability of scientific discovery. We then discuss future opportunities for improving predictions derived from the science of science and its potential impact, positive and negative, on the scientific community.",fullPaper,jv104
Sociology,p312,d4,4436ca7e9f91b7ad9ad6a09dbe12f48d9f6c3e7f,j104,Science,Data-driven predictions in the science of science,"The desire to predict discoveries—to have some idea, in advance, of what will be discovered, by whom, when, and where—pervades nearly all aspects of modern science, from individual scientists to publishers, from funding agencies to hiring committees. In this Essay, we survey the emerging and interdisciplinary field of the “science of science” and what it teaches us about the predictability of scientific discovery. We then discuss future opportunities for improving predictions derived from the science of science and its potential impact, positive and negative, on the scientific community.",fullPaper,jv104
Medicine,p318,d1,3e02906da7498b5fdbc6f0eea4b6bb9f2d86dd00,c93,ASE BigData & SocialInformatics,ON PATIENT FLOW IN HOSPITALS: A DATA-BASED QUEUEING-SCIENCE PERSPECTIVE,"Hospitals are complex systems with essential societal benefits and huge mounting costs. These costs are exacerbated by inefficiencies in hospital processes, which are often manifested by congestion and long delays in patient care. Thus, a queueing-network view of patient flow in hospitals is natural for studying and improving its performance. The goal of our research is to explore patient flow data through the lens of a queueing scientist. The means is exploratory data analysis (EDA) in a large Israeli hospital, which reveals important features that are not readily explainable by existing models. Questions raised by our EDA include: Can a simple (parsimonious) queueing model usefully capture the complex operational reality of the Emergency Department (ED)? What time scales and operational regimes are relevant for modeling patient length of stay in the Internal Wards (IWs)? How do protocols of patient transfer between the ED and the IWs influence patient delay, workload division and fairness? EDA also unde...",poster,cp93
Medicine,p323,d1,439ede62248e5f6202982afead02b33d3feffae7,j107,Nucleic Acids Research,TCGAbiolinks: an R/Bioconductor package for integrative analysis of TCGA data,"The Cancer Genome Atlas (TCGA) research network has made public a large collection of clinical and molecular phenotypes of more than 10 000 tumor patients across 33 different tumor types. Using this cohort, TCGA has published over 20 marker papers detailing the genomic and epigenomic alterations associated with these tumor types. Although many important discoveries have been made by TCGA's research network, opportunities still exist to implement novel methods, thereby elucidating new biological pathways and diagnostic markers. However, mining the TCGA data presents several bioinformatics challenges, such as data retrieval and integration with clinical data and other molecular data types (e.g. RNA and DNA methylation). We developed an R/Bioconductor package called TCGAbiolinks to address these challenges and offer bioinformatics solutions by using a guided workflow to allow users to query, download and perform integrative analyses of TCGA data. We combined methods from computer science and statistics into the pipeline and incorporated methodologies developed in previous TCGA marker studies and in our own group. Using four different TCGA tumor types (Kidney, Brain, Breast and Colon) as examples, we provide case studies to illustrate examples of reproducibility, integrative analysis and utilization of different Bioconductor packages to advance and accelerate novel discoveries.",fullPaper,jv107
Biology,p323,d5,439ede62248e5f6202982afead02b33d3feffae7,j107,Nucleic Acids Research,TCGAbiolinks: an R/Bioconductor package for integrative analysis of TCGA data,"The Cancer Genome Atlas (TCGA) research network has made public a large collection of clinical and molecular phenotypes of more than 10 000 tumor patients across 33 different tumor types. Using this cohort, TCGA has published over 20 marker papers detailing the genomic and epigenomic alterations associated with these tumor types. Although many important discoveries have been made by TCGA's research network, opportunities still exist to implement novel methods, thereby elucidating new biological pathways and diagnostic markers. However, mining the TCGA data presents several bioinformatics challenges, such as data retrieval and integration with clinical data and other molecular data types (e.g. RNA and DNA methylation). We developed an R/Bioconductor package called TCGAbiolinks to address these challenges and offer bioinformatics solutions by using a guided workflow to allow users to query, download and perform integrative analyses of TCGA data. We combined methods from computer science and statistics into the pipeline and incorporated methodologies developed in previous TCGA marker studies and in our own group. Using four different TCGA tumor types (Kidney, Brain, Breast and Colon) as examples, we provide case studies to illustrate examples of reproducibility, integrative analysis and utilization of different Bioconductor packages to advance and accelerate novel discoveries.",fullPaper,jv107
Medicine,p324,d1,b5fb74dfc71c92113c84a0e8f0502e0e76b4dbda,j108,Social Science Research,The role of administrative data in the big data revolution in social science research.,Abstract,fullPaper,jv108
Computer Science,p324,d3,b5fb74dfc71c92113c84a0e8f0502e0e76b4dbda,j108,Social Science Research,The role of administrative data in the big data revolution in social science research.,Abstract,fullPaper,jv108
Medicine,p327,d1,233e702fa7ccfd55061680e3af9bd2f7efe5e08f,j60,Nature,Science of Science,"The whys and wherefores of SciSci The science of science (SciSci) is based on a transdisciplinary approach that uses large data sets to study the mechanisms underlying the doing of science—from the choice of a research problem to career trajectories and progress within a field. In a Review, Fortunato et al. explain that the underlying rationale is that with a deeper understanding of the precursors of impactful science, it will be possible to develop systems and policies that improve each scientist's ability to succeed and enhance the prospects of science as a whole. Science, this issue p. eaao0185 BACKGROUND The increasing availability of digital data on scholarly inputs and outputs—from research funding, productivity, and collaboration to paper citations and scientist mobility—offers unprecedented opportunities to explore the structure and evolution of science. The science of science (SciSci) offers a quantitative understanding of the interactions among scientific agents across diverse geographic and temporal scales: It provides insights into the conditions underlying creativity and the genesis of scientific discovery, with the ultimate goal of developing tools and policies that have the potential to accelerate science. In the past decade, SciSci has benefited from an influx of natural, computational, and social scientists who together have developed big data–based capabilities for empirical analysis and generative modeling that capture the unfolding of science, its institutions, and its workforce. The value proposition of SciSci is that with a deeper understanding of the factors that drive successful science, we can more effectively address environmental, societal, and technological problems. ADVANCES Science can be described as a complex, self-organizing, and evolving network of scholars, projects, papers, and ideas. This representation has unveiled patterns characterizing the emergence of new scientific fields through the study of collaboration networks and the path of impactful discoveries through the study of citation networks. Microscopic models have traced the dynamics of citation accumulation, allowing us to predict the future impact of individual papers. SciSci has revealed choices and trade-offs that scientists face as they advance both their own careers and the scientific horizon. For example, measurements indicate that scholars are risk-averse, preferring to study topics related to their current expertise, which constrains the potential of future discoveries. Those willing to break this pattern engage in riskier careers but become more likely to make major breakthroughs. Overall, the highest-impact science is grounded in conventional combinations of prior work but features unusual combinations. Last, as the locus of research is shifting into teams, SciSci is increasingly focused on the impact of team research, finding that small teams tend to disrupt science and technology with new ideas drawing on older and less prevalent ones. In contrast, large teams tend to develop recent, popular ideas, obtaining high, but often short-lived, impact. OUTLOOK SciSci offers a deep quantitative understanding of the relational structure between scientists, institutions, and ideas because it facilitates the identification of fundamental mechanisms responsible for scientific discovery. These interdisciplinary data-driven efforts complement contributions from related fields such as scientometrics and the economics and sociology of science. Although SciSci seeks long-standing universal laws and mechanisms that apply across various fields of science, a fundamental challenge going forward is accounting for undeniable differences in culture, habits, and preferences between different fields and countries. This variation makes some cross-domain insights difficult to appreciate and associated science policies difficult to implement. The differences among the questions, data, and skills specific to each discipline suggest that further insights can be gained from domain-specific SciSci studies, which model and identify opportunities adapted to the needs of individual research fields. The complexity of science. Science can be seen as an expanding and evolving network of ideas, scholars, and papers. SciSci searches for universal and domain-specific laws underlying the structure and dynamics of science. ILLUSTRATION: NICOLE SAMAY Identifying fundamental drivers of science and developing predictive models to capture its evolution are instrumental for the design of policies that can improve the scientific enterprise—for example, through enhanced career paths for scientists, better performance evaluation for organizations hosting research, discovery of novel effective funding vehicles, and even identification of promising regions along the scientific frontier. The science of science uses large-scale data on the production of science to search for universal and domain-specific patterns. Here, we review recent developments in this transdisciplinary field.",fullPaper,jv60
Sociology,p327,d4,233e702fa7ccfd55061680e3af9bd2f7efe5e08f,j60,Nature,Science of Science,"The whys and wherefores of SciSci The science of science (SciSci) is based on a transdisciplinary approach that uses large data sets to study the mechanisms underlying the doing of science—from the choice of a research problem to career trajectories and progress within a field. In a Review, Fortunato et al. explain that the underlying rationale is that with a deeper understanding of the precursors of impactful science, it will be possible to develop systems and policies that improve each scientist's ability to succeed and enhance the prospects of science as a whole. Science, this issue p. eaao0185 BACKGROUND The increasing availability of digital data on scholarly inputs and outputs—from research funding, productivity, and collaboration to paper citations and scientist mobility—offers unprecedented opportunities to explore the structure and evolution of science. The science of science (SciSci) offers a quantitative understanding of the interactions among scientific agents across diverse geographic and temporal scales: It provides insights into the conditions underlying creativity and the genesis of scientific discovery, with the ultimate goal of developing tools and policies that have the potential to accelerate science. In the past decade, SciSci has benefited from an influx of natural, computational, and social scientists who together have developed big data–based capabilities for empirical analysis and generative modeling that capture the unfolding of science, its institutions, and its workforce. The value proposition of SciSci is that with a deeper understanding of the factors that drive successful science, we can more effectively address environmental, societal, and technological problems. ADVANCES Science can be described as a complex, self-organizing, and evolving network of scholars, projects, papers, and ideas. This representation has unveiled patterns characterizing the emergence of new scientific fields through the study of collaboration networks and the path of impactful discoveries through the study of citation networks. Microscopic models have traced the dynamics of citation accumulation, allowing us to predict the future impact of individual papers. SciSci has revealed choices and trade-offs that scientists face as they advance both their own careers and the scientific horizon. For example, measurements indicate that scholars are risk-averse, preferring to study topics related to their current expertise, which constrains the potential of future discoveries. Those willing to break this pattern engage in riskier careers but become more likely to make major breakthroughs. Overall, the highest-impact science is grounded in conventional combinations of prior work but features unusual combinations. Last, as the locus of research is shifting into teams, SciSci is increasingly focused on the impact of team research, finding that small teams tend to disrupt science and technology with new ideas drawing on older and less prevalent ones. In contrast, large teams tend to develop recent, popular ideas, obtaining high, but often short-lived, impact. OUTLOOK SciSci offers a deep quantitative understanding of the relational structure between scientists, institutions, and ideas because it facilitates the identification of fundamental mechanisms responsible for scientific discovery. These interdisciplinary data-driven efforts complement contributions from related fields such as scientometrics and the economics and sociology of science. Although SciSci seeks long-standing universal laws and mechanisms that apply across various fields of science, a fundamental challenge going forward is accounting for undeniable differences in culture, habits, and preferences between different fields and countries. This variation makes some cross-domain insights difficult to appreciate and associated science policies difficult to implement. The differences among the questions, data, and skills specific to each discipline suggest that further insights can be gained from domain-specific SciSci studies, which model and identify opportunities adapted to the needs of individual research fields. The complexity of science. Science can be seen as an expanding and evolving network of ideas, scholars, and papers. SciSci searches for universal and domain-specific laws underlying the structure and dynamics of science. ILLUSTRATION: NICOLE SAMAY Identifying fundamental drivers of science and developing predictive models to capture its evolution are instrumental for the design of policies that can improve the scientific enterprise—for example, through enhanced career paths for scientists, better performance evaluation for organizations hosting research, discovery of novel effective funding vehicles, and even identification of promising regions along the scientific frontier. The science of science uses large-scale data on the production of science to search for universal and domain-specific patterns. Here, we review recent developments in this transdisciplinary field.",fullPaper,jv60
Medicine,p338,d1,d2a595c5efb4b26245c4353d5d85cbe6c7ecac0f,j104,Science,Machine learning for data-driven discovery in solid Earth geoscience,"Automating geoscience analysis Solid Earth geoscience is a field that has very large set of observations, which are ideal for analysis with machine-learning methods. Bergen et al. review how these methods can be applied to solid Earth datasets. Adopting machine-learning techniques is important for extracting information and for understanding the increasing amount of complex data collected in the geosciences. Science, this issue p. eaau0323 BACKGROUND The solid Earth, oceans, and atmosphere together form a complex interacting geosystem. Processes relevant to understanding Earth’s geosystem behavior range in spatial scale from the atomic to the planetary, and in temporal scale from milliseconds to billions of years. Physical, chemical, and biological processes interact and have substantial influence on this complex geosystem, and humans interact with it in ways that are increasingly consequential to the future of both the natural world and civilization as the finiteness of Earth becomes increasingly apparent and limits on available energy, mineral resources, and fresh water increasingly affect the human condition. Earth is subject to a variety of geohazards that are poorly understood, yet increasingly impactful as our exposure grows through increasing urbanization, particularly in hazard-prone areas. We have a fundamental need to develop the best possible predictive understanding of how the geosystem works, and that understanding must be informed by both the present and the deep past. This understanding will come through the analysis of increasingly large geo-datasets and from computationally intensive simulations, often connected through inverse problems. Geoscientists are faced with the challenge of extracting as much useful information as possible and gaining new insights from these data, simulations, and the interplay between the two. Techniques from the rapidly evolving field of machine learning (ML) will play a key role in this effort. ADVANCES The confluence of ultrafast computers with large memory, rapid progress in ML algorithms, and the ready availability of large datasets place geoscience at the threshold of dramatic progress. We anticipate that this progress will come from the application of ML across three categories of research effort: (i) automation to perform a complex prediction task that cannot easily be described by a set of explicit commands; (ii) modeling and inverse problems to create a representation that approximates numerical simulations or captures relationships; and (iii) discovery to reveal new and often unanticipated patterns, structures, or relationships. Examples of automation include geologic mapping using remote-sensing data, characterizing the topology of fracture systems to model subsurface transport, and classifying volcanic ash particles to infer eruptive mechanism. Examples of modeling include approximating the viscoelastic response for complex rheology, determining wave speed models directly from tomographic data, and classifying diverse seismic events. Examples of discovery include predicting laboratory slip events using observations of acoustic emissions, detecting weak earthquake signals using similarity search, and determining the connectivity of subsurface reservoirs using groundwater tracer observations. OUTLOOK The use of ML in solid Earth geosciences is growing rapidly, but is still in its early stages and making uneven progress. Much remains to be done with existing datasets from long-standing data sources, which in many cases are largely unexplored. Newer, unconventional data sources such as light detection and ranging (LiDAR), fiber-optic sensing, and crowd-sourced measurements may demand new approaches through both the volume and the character of information that they present. Practical steps could accelerate and broaden the use of ML in the geosciences. Wider adoption of open-science principles such as open source code, open data, and open access will better position the solid Earth community to take advantage of rapid developments in ML and artificial intelligence. Benchmark datasets and challenge problems have played an important role in driving progress in artificial intelligence research by enabling rigorous performance comparison and could play a similar role in the geosciences. Testing on high-quality datasets produces better models, and benchmark datasets make these data widely available to the research community. They also help recruit expertise from allied disciplines. Close collaboration between geoscientists and ML researchers will aid in making quick progress in ML geoscience applications. Extracting maximum value from geoscientific data will require new approaches for combining data-driven methods, physical modeling, and algorithms capable of learning with limited, weak, or biased labels. Funding opportunities that target the intersection of these disciplines, as well as a greater component of data science and ML education in the geosciences, could help bring this effort to fruition. Digital geology. Digital representation of the geology of the conterminous United States. [Geology of the Conterminous United States at 1:2,500,000 scale; a digital representation of the 1974 P. B. King and H. M. Beikman map by P. G. Schruben, R. E. Arndt, W. J. Bawiec] The list of author affiliations is available in the full article online. Understanding the behavior of Earth through the diverse fields of the solid Earth geosciences is an increasingly important task. It is made challenging by the complex, interacting, and multiscale processes needed to understand Earth’s behavior and by the inaccessibility of nearly all of Earth’s subsurface to direct observation. Substantial increases in data availability and in the increasingly realistic character of computer simulations hold promise for accelerating progress, but developing a deeper understanding based on these capabilities is itself challenging. Machine learning will play a key role in this effort. We review the state of the field and make recommendations for how progress might be broadened and accelerated.",fullPaper,jv104
Medicine,p340,d1,16cecb0173adc68762b6e70daecb25089a5a6b6a,j0,Nature Biotechnology,ProteomeXchange provides globally co-ordinated proteomics data submission and dissemination,Abstract,fullPaper,jv0
Biology,p340,d5,16cecb0173adc68762b6e70daecb25089a5a6b6a,j0,Nature Biotechnology,ProteomeXchange provides globally co-ordinated proteomics data submission and dissemination,Abstract,fullPaper,jv0
Medicine,p341,d1,852c3c29c319ba1ae2c6efac3471a3f5c5b4a232,j111,Public Understanding of Science,Comparing science communication theory with practice: An assessment and critique using Australian data,"Scholars have variously described different models of science communication over the past 20 years. However, there has been little assessment of theorised models against science communication practice. This article compares 515 science engagement activities recorded in a 2012 Australian audit against the theorised characteristics of the three dominant models of deficit, dialogue and participation. Most engagement activities had objectives that reflected a mix of deficit and dialogue activities. Despite increases in scientific controversies like climate change, there appears to be a paucity of participatory activities in Australia. Those that do exist are mostly about people being involved with science through activities like citizen science. These participatory activities appear to coexist with and perhaps even depend on deficit activities. Science communication scholars could develop their models by examining the full range of objectives for engagement found in practice and by recognising that any engagement will likely include a mix of approaches.",fullPaper,jv111
Sociology,p341,d4,852c3c29c319ba1ae2c6efac3471a3f5c5b4a232,j111,Public Understanding of Science,Comparing science communication theory with practice: An assessment and critique using Australian data,"Scholars have variously described different models of science communication over the past 20 years. However, there has been little assessment of theorised models against science communication practice. This article compares 515 science engagement activities recorded in a 2012 Australian audit against the theorised characteristics of the three dominant models of deficit, dialogue and participation. Most engagement activities had objectives that reflected a mix of deficit and dialogue activities. Despite increases in scientific controversies like climate change, there appears to be a paucity of participatory activities in Australia. Those that do exist are mostly about people being involved with science through activities like citizen science. These participatory activities appear to coexist with and perhaps even depend on deficit activities. Science communication scholars could develop their models by examining the full range of objectives for engagement found in practice and by recognising that any engagement will likely include a mix of approaches.",fullPaper,jv111
Medicine,p342,d1,a461233e56079fc5af6e48d75f38be8c9ff87c1e,j112,Environmental Science and Technology,Machine Learning: New Ideas and Tools in Environmental Science and Engineering.,"The rapid increase in both the quantity and complexity of data that are being generated daily in the field of environmental science and engineering (ESE) demands accompanied advancement in data analytics. Advanced data analysis approaches, such as machine learning (ML), have become indispensable tools for revealing hidden patterns or deducing correlations for which conventional analytical methods face limitations or challenges. However, ML concepts and practices have not been widely utilized by researchers in ESE. This feature explores the potential of ML to revolutionize data analysis and modeling in the ESE field, and covers the essential knowledge needed for such applications. First, we use five examples to illustrate how ML addresses complex ESE problems. We then summarize four major types of applications of ML in ESE: making predictions; extracting feature importance; detecting anomalies; and discovering new materials or chemicals. Next, we introduce the essential knowledge required and current shortcomings in ML applications in ESE, with a focus on three important but often overlooked components when applying ML: correct model development, proper model interpretation, and sound applicability analysis. Finally, we discuss challenges and future opportunities in the application of ML tools in ESE to highlight the potential of ML in this field.",fullPaper,jv112
Medicine,p345,d1,2f35b305b6c56046b631b0cdb6d2f08e4ee577a7,j113,Behavior Research Methods,TurkPrime.com: A versatile crowdsourcing data acquisition platform for the behavioral sciences,Abstract,fullPaper,jv113
Computer Science,p345,d3,2f35b305b6c56046b631b0cdb6d2f08e4ee577a7,j113,Behavior Research Methods,TurkPrime.com: A versatile crowdsourcing data acquisition platform for the behavioral sciences,Abstract,fullPaper,jv113
Medicine,p350,d1,9b18fbe281496ad72bdd18e0a5883d235ebdfd87,j116,Clinical and Translational Science,"Biolink Model: A universal schema for knowledge graphs in clinical, biomedical, and translational science","Within clinical, biomedical, and translational science, an increasing number of projects are adopting graphs for knowledge representation. Graph‐based data models elucidate the interconnectedness among core biomedical concepts, enable data structures to be easily updated, and support intuitive queries, visualizations, and inference algorithms. However, knowledge discovery across these “knowledge graphs” (KGs) has remained difficult. Data set heterogeneity and complexity; the proliferation of ad hoc data formats; poor compliance with guidelines on findability, accessibility, interoperability, and reusability; and, in particular, the lack of a universally accepted, open‐access model for standardization across biomedical KGs has left the task of reconciling data sources to downstream consumers. Biolink Model is an open‐source data model that can be used to formalize the relationships between data structures in translational science. It incorporates object‐oriented classification and graph‐oriented features. The core of the model is a set of hierarchical, interconnected classes (or categories) and relationships between them (or predicates) representing biomedical entities such as gene, disease, chemical, anatomic structure, and phenotype. The model provides class and edge attributes and associations that guide how entities should relate to one another. Here, we highlight the need for a standardized data model for KGs, describe Biolink Model, and compare it with other models. We demonstrate the utility of Biolink Model in various initiatives, including the Biomedical Data Translator Consortium and the Monarch Initiative, and show how it has supported easier integration and interoperability of biomedical KGs, bringing together knowledge from multiple sources and helping to realize the goals of translational science.",fullPaper,jv116
Computer Science,p350,d3,9b18fbe281496ad72bdd18e0a5883d235ebdfd87,j116,Clinical and Translational Science,"Biolink Model: A universal schema for knowledge graphs in clinical, biomedical, and translational science","Within clinical, biomedical, and translational science, an increasing number of projects are adopting graphs for knowledge representation. Graph‐based data models elucidate the interconnectedness among core biomedical concepts, enable data structures to be easily updated, and support intuitive queries, visualizations, and inference algorithms. However, knowledge discovery across these “knowledge graphs” (KGs) has remained difficult. Data set heterogeneity and complexity; the proliferation of ad hoc data formats; poor compliance with guidelines on findability, accessibility, interoperability, and reusability; and, in particular, the lack of a universally accepted, open‐access model for standardization across biomedical KGs has left the task of reconciling data sources to downstream consumers. Biolink Model is an open‐source data model that can be used to formalize the relationships between data structures in translational science. It incorporates object‐oriented classification and graph‐oriented features. The core of the model is a set of hierarchical, interconnected classes (or categories) and relationships between them (or predicates) representing biomedical entities such as gene, disease, chemical, anatomic structure, and phenotype. The model provides class and edge attributes and associations that guide how entities should relate to one another. Here, we highlight the need for a standardized data model for KGs, describe Biolink Model, and compare it with other models. We demonstrate the utility of Biolink Model in various initiatives, including the Biomedical Data Translator Consortium and the Monarch Initiative, and show how it has supported easier integration and interoperability of biomedical KGs, bringing together knowledge from multiple sources and helping to realize the goals of translational science.",fullPaper,jv116
Medicine,p354,d1,efa5558bddd68abe4adc81adbbef6f739e648392,j64,PLoS Biology,Big Data: Astronomical or Genomical?,"Genomics is a Big Data science and is going to get much bigger, very soon, but it is not known whether the needs of genomics will exceed other Big Data domains. Projecting to the year 2025, we compared genomics with three other major generators of Big Data: astronomy, YouTube, and Twitter. Our estimates show that genomics is a “four-headed beast”—it is either on par with or the most demanding of the domains analyzed here in terms of data acquisition, storage, distribution, and analysis. We discuss aspects of new technologies that will need to be developed to rise up and meet the computational challenges that genomics poses for the near future. Now is the time for concerted, community-wide planning for the “genomical” challenges of the next decade.",fullPaper,jv64
Biology,p354,d5,efa5558bddd68abe4adc81adbbef6f739e648392,j64,PLoS Biology,Big Data: Astronomical or Genomical?,"Genomics is a Big Data science and is going to get much bigger, very soon, but it is not known whether the needs of genomics will exceed other Big Data domains. Projecting to the year 2025, we compared genomics with three other major generators of Big Data: astronomy, YouTube, and Twitter. Our estimates show that genomics is a “four-headed beast”—it is either on par with or the most demanding of the domains analyzed here in terms of data acquisition, storage, distribution, and analysis. We discuss aspects of new technologies that will need to be developed to rise up and meet the computational challenges that genomics poses for the near future. Now is the time for concerted, community-wide planning for the “genomical” challenges of the next decade.",fullPaper,jv64
Medicine,p360,d1,6802bbeea45ea9c44b8e9f69ee1d775f5af0717f,j119,International Journal of Exercise Science,Ethical Issues Relating to Scientific Discovery in Exercise Science.,"This work aims to present concepts related to ethical issues in conducting and reporting scientific research in a clear and straightforward manner. Considerations around research design including authorship, sound research practices, non-discrimination in subject recruitment, objectivity, respect for intellectual property, and financial interests are detailed. Further, concepts relating to the conducting of research including the competency of the researcher, conflicts of interest, accurately representing data, and ethical practices in human and animal research are presented. Attention pertaining to the dissemination of research including plagiarism, duplicate submission, redundant publication, and figure manipulation is offered. Other considerations including responsible mentoring, respect for colleagues, and social responsibility are set forth. The International Journal of Exercise Science will now require a statement in all subsequent published manuscripts that the authors have complied with each of the ethics statements contained in this work.",fullPaper,jv119
Sociology,p360,d4,6802bbeea45ea9c44b8e9f69ee1d775f5af0717f,j119,International Journal of Exercise Science,Ethical Issues Relating to Scientific Discovery in Exercise Science.,"This work aims to present concepts related to ethical issues in conducting and reporting scientific research in a clear and straightforward manner. Considerations around research design including authorship, sound research practices, non-discrimination in subject recruitment, objectivity, respect for intellectual property, and financial interests are detailed. Further, concepts relating to the conducting of research including the competency of the researcher, conflicts of interest, accurately representing data, and ethical practices in human and animal research are presented. Attention pertaining to the dissemination of research including plagiarism, duplicate submission, redundant publication, and figure manipulation is offered. Other considerations including responsible mentoring, respect for colleagues, and social responsibility are set forth. The International Journal of Exercise Science will now require a statement in all subsequent published manuscripts that the authors have complied with each of the ethics statements contained in this work.",fullPaper,jv119
Medicine,p364,d1,eaf5a5e0b32a055e288d5edcc5cd39f9f4d335ad,j121,Nature Communications,The misuse of colour in science communication,Abstract,fullPaper,jv121
Computer Science,p364,d3,eaf5a5e0b32a055e288d5edcc5cd39f9f4d335ad,j121,Nature Communications,The misuse of colour in science communication,Abstract,fullPaper,jv121
Medicine,p366,d1,a07a64ba110e0f9f7156f3bd1e376f0d2e1cddf1,j64,PLoS Biology,The Extent and Consequences of P-Hacking in Science,"A focus on novel, confirmatory, and statistically significant results leads to substantial bias in the scientific literature. One type of bias, known as “p-hacking,” occurs when researchers collect or select data or statistical analyses until nonsignificant results become significant. Here, we use text-mining to demonstrate that p-hacking is widespread throughout science. We then illustrate how one can test for p-hacking when performing a meta-analysis and show that, while p-hacking is probably common, its effect seems to be weak relative to the real effect sizes being measured. This result suggests that p-hacking probably does not drastically alter scientific consensuses drawn from meta-analyses.",fullPaper,jv64
Biology,p366,d5,a07a64ba110e0f9f7156f3bd1e376f0d2e1cddf1,j64,PLoS Biology,The Extent and Consequences of P-Hacking in Science,"A focus on novel, confirmatory, and statistically significant results leads to substantial bias in the scientific literature. One type of bias, known as “p-hacking,” occurs when researchers collect or select data or statistical analyses until nonsignificant results become significant. Here, we use text-mining to demonstrate that p-hacking is widespread throughout science. We then illustrate how one can test for p-hacking when performing a meta-analysis and show that, while p-hacking is probably common, its effect seems to be weak relative to the real effect sizes being measured. This result suggests that p-hacking probably does not drastically alter scientific consensuses drawn from meta-analyses.",fullPaper,jv64
Medicine,p371,d1,b598b8dd79654dc865b02c2af0a0bdb565d24049,j123,Ambio,Taking a ‘Big Data’ approach to data quality in a citizen science project,Abstract,fullPaper,jv123
Computer Science,p371,d3,b598b8dd79654dc865b02c2af0a0bdb565d24049,j123,Ambio,Taking a ‘Big Data’ approach to data quality in a citizen science project,Abstract,fullPaper,jv123
Medicine,p375,d1,cf9ecfbbd0095687c4cfbbbfa0546914e651b109,j124,Medicine & Science in Sports & Exercise,"Calibration of the Computer Science and Applications, Inc. accelerometer.","PURPOSE
We established accelerometer count ranges for the Computer Science and Applications, Inc. (CSA) activity monitor corresponding to commonly employed MET categories.


METHODS
Data were obtained from 50 adults (25 males, 25 females) during treadmill exercise at three different speeds (4.8, 6.4, and 9.7 km x h(-1)).


RESULTS
Activity counts and steady-state oxygen consumption were highly correlated (r = 0.88), and count ranges corresponding to light, moderate, hard, and very hard intensity levels were < or = 1951, 1952-5724, 5725-9498, > or = 9499 cnts x min(-1), respectively. A model to predict energy expenditure from activity counts and body mass was developed using data from a random sample of 35 subjects (r2 = 0.82, SEE = 1.40 kcal x min(-1)). Cross validation with data from the remaining 15 subjects revealed no significant differences between actual and predicted energy expenditure at any treadmill speed (SEE = 0.50-1.40 kcal x min(-1)).


CONCLUSIONS
These data provide a template on which patterns of activity can be classified into intensity levels using the CSA accelerometer.",fullPaper,jv124
Computer Science,p375,d3,cf9ecfbbd0095687c4cfbbbfa0546914e651b109,j124,Medicine & Science in Sports & Exercise,"Calibration of the Computer Science and Applications, Inc. accelerometer.","PURPOSE
We established accelerometer count ranges for the Computer Science and Applications, Inc. (CSA) activity monitor corresponding to commonly employed MET categories.


METHODS
Data were obtained from 50 adults (25 males, 25 females) during treadmill exercise at three different speeds (4.8, 6.4, and 9.7 km x h(-1)).


RESULTS
Activity counts and steady-state oxygen consumption were highly correlated (r = 0.88), and count ranges corresponding to light, moderate, hard, and very hard intensity levels were < or = 1951, 1952-5724, 5725-9498, > or = 9499 cnts x min(-1), respectively. A model to predict energy expenditure from activity counts and body mass was developed using data from a random sample of 35 subjects (r2 = 0.82, SEE = 1.40 kcal x min(-1)). Cross validation with data from the remaining 15 subjects revealed no significant differences between actual and predicted energy expenditure at any treadmill speed (SEE = 0.50-1.40 kcal x min(-1)).


CONCLUSIONS
These data provide a template on which patterns of activity can be classified into intensity levels using the CSA accelerometer.",fullPaper,jv124
Medicine,p377,d1,938a6209fe95dd4e5f801a14b6b650dc7b2f6108,j126,EMBO Reports,Could Big Data be the end of theory in science?,"Afew years ago, Chris Anderson, former editor in chief of Wired magazine, published a provocative and thought‐provoking article: “The end of theory: the data deluge makes the scientific method obsolete” (http://archive.wired.com/science/discoveries/magazine/16-07/pb_theory/). As the title indicates, Anderson asserted that in the era of petabyte information and supercomputing, the traditional, hypothesis‐driven scientific method would become obsolete. No more theories or hypotheses, no more discussions whether the experimental results refute or support the original hypotheses. In this new era, what counts are sophisticated algorithms and statistical tools to sift through a massive amount of data to find information that could be turned into knowledge.

> … [an] imagined future in which the long‐established way of doing scientific research is replaced by computers that divulge knowledge from data at the press of a button…

Anderson's essay started an intense discussion about the relative merits of data‐driven research versus hypothesis‐driven research that has much relevance for many areas of research, including bioinformatics, systems biology, epidemiology and ecology. Yet, his imagined future in which the long‐established way of doing scientific research is replaced by computers that divulge knowledge from data at the press of a button deserves some inquiry from an epistemological point of view. Is data‐driven research a genuine mode of knowledge production, or is it above all a tool to identify potentially useful information? Given the amount of scientific data available, is it now possible to dismiss the role of theoretical assumptions and hypotheses? Should this new mode of gathering information supersede the old way of doing research?

The scientific method encompasses an ongoing process of formulate a hypothesis‐test with an experiment–analyze the results‐reformulate the hypothesis. Such a way of proceeding has been in use for centuries and is basically accepted in our Western society as the most reliable way to produce robust knowledge.

However, Anderson is not the …",fullPaper,jv126
Sociology,p377,d4,938a6209fe95dd4e5f801a14b6b650dc7b2f6108,j126,EMBO Reports,Could Big Data be the end of theory in science?,"Afew years ago, Chris Anderson, former editor in chief of Wired magazine, published a provocative and thought‐provoking article: “The end of theory: the data deluge makes the scientific method obsolete” (http://archive.wired.com/science/discoveries/magazine/16-07/pb_theory/). As the title indicates, Anderson asserted that in the era of petabyte information and supercomputing, the traditional, hypothesis‐driven scientific method would become obsolete. No more theories or hypotheses, no more discussions whether the experimental results refute or support the original hypotheses. In this new era, what counts are sophisticated algorithms and statistical tools to sift through a massive amount of data to find information that could be turned into knowledge.

> … [an] imagined future in which the long‐established way of doing scientific research is replaced by computers that divulge knowledge from data at the press of a button…

Anderson's essay started an intense discussion about the relative merits of data‐driven research versus hypothesis‐driven research that has much relevance for many areas of research, including bioinformatics, systems biology, epidemiology and ecology. Yet, his imagined future in which the long‐established way of doing scientific research is replaced by computers that divulge knowledge from data at the press of a button deserves some inquiry from an epistemological point of view. Is data‐driven research a genuine mode of knowledge production, or is it above all a tool to identify potentially useful information? Given the amount of scientific data available, is it now possible to dismiss the role of theoretical assumptions and hypotheses? Should this new mode of gathering information supersede the old way of doing research?

The scientific method encompasses an ongoing process of formulate a hypothesis‐test with an experiment–analyze the results‐reformulate the hypothesis. Such a way of proceeding has been in use for centuries and is basically accepted in our Western society as the most reliable way to produce robust knowledge.

However, Anderson is not the …",fullPaper,jv126
Medicine,p378,d1,993c9eb9bba80e2d8993e8c99acca1825cd0302f,j104,Science,Next Steps for Citizen Science,"Strategic investments and coordination are needed for citizen science to reach its full potential. Around the globe, thousands of research projects are engaging millions of individuals—many of whom are not trained as scientists—in collecting, categorizing, transcribing, or analyzing scientific data. These projects, known as citizen science, cover a breadth of topics from microbiomes to native bees to water quality to galaxies. Most projects obtain or manage scientific information at scales or resolutions unattainable by individual researchers or research teams, whether enrolling thousands of individuals collecting data across several continents, enlisting small armies of participants in categorizing vast quantities of online data, or organizing small groups of volunteers to tackle local problems.",fullPaper,jv104
Geography,p378,d13,993c9eb9bba80e2d8993e8c99acca1825cd0302f,j104,Science,Next Steps for Citizen Science,"Strategic investments and coordination are needed for citizen science to reach its full potential. Around the globe, thousands of research projects are engaging millions of individuals—many of whom are not trained as scientists—in collecting, categorizing, transcribing, or analyzing scientific data. These projects, known as citizen science, cover a breadth of topics from microbiomes to native bees to water quality to galaxies. Most projects obtain or manage scientific information at scales or resolutions unattainable by individual researchers or research teams, whether enrolling thousands of individuals collecting data across several continents, enlisting small armies of participants in categorizing vast quantities of online data, or organizing small groups of volunteers to tackle local problems.",fullPaper,jv104
Medicine,p379,d1,48fc9c42522184c652742255fdf31f7b9ed7ebae,j127,Journal of Evidence-Based Medicine,Brief introduction of medical database and data mining technology in big data era,"Data mining technology can search for potentially valuable knowledge from a large amount of data, mainly divided into data preparation and data mining, and expression and analysis of results. It is a mature information processing technology and applies database technology. Database technology is a software science that researches manages, and applies databases. The data in the database are processed and analyzed by studying the underlying theory and implementation methods of the structure, storage, design, management, and application of the database. We have introduced several databases and data mining techniques to help a wide range of clinical researchers better understand and apply database technology.",fullPaper,jv127
Medicine,p381,d1,bb930cdb4ec5e1f10c4f9b4ece231c3fb20bd2d8,j129,Space Science Reviews,The Space Physics Environment Data Analysis System (SPEDAS),Abstract,fullPaper,jv129
Medicine,p382,d1,8e600778160ff986b5460bc2584066148e55e5d4,j104,Science,Protein structure determination using metagenome sequence data,"Filling in the protein fold picture Fewer than a third of the 14,849 known protein families have at least one member with an experimentally determined structure. This leaves more than 5000 protein families with no structural information. Protein modeling using residue-residue contacts inferred from evolutionary data has been successful in modeling unknown structures, but it requires large numbers of aligned sequences. Ovchinnikov et al. augmented such sequence alignments with metagenome sequence data (see the Perspective by Söding). They determined the number of sequences required to allow modeling, developed criteria for model quality, and, where possible, improved modeling by matching predicted contacts to known structures. Their method predicted quality structural models for 614 protein families, of which about 140 represent newly discovered protein folds. Science, this issue p. 294; see also p. 248 Combining metagenome data with protein structure prediction generates models for 614 families with unknown structures. Despite decades of work by structural biologists, there are still ~5200 protein families with unknown structure outside the range of comparative modeling. We show that Rosetta structure prediction guided by residue-residue contacts inferred from evolutionary information can accurately model proteins that belong to large families and that metagenome sequence data more than triple the number of protein families with sufficient sequences for accurate modeling. We then integrate metagenome data, contact-based structure matching, and Rosetta structure calculations to generate models for 614 protein families with currently unknown structures; 206 are membrane proteins and 137 have folds not represented in the Protein Data Bank. This approach provides the representative models for large protein families originally envisioned as the goal of the Protein Structure Initiative at a fraction of the cost.",fullPaper,jv104
Biology,p382,d5,8e600778160ff986b5460bc2584066148e55e5d4,j104,Science,Protein structure determination using metagenome sequence data,"Filling in the protein fold picture Fewer than a third of the 14,849 known protein families have at least one member with an experimentally determined structure. This leaves more than 5000 protein families with no structural information. Protein modeling using residue-residue contacts inferred from evolutionary data has been successful in modeling unknown structures, but it requires large numbers of aligned sequences. Ovchinnikov et al. augmented such sequence alignments with metagenome sequence data (see the Perspective by Söding). They determined the number of sequences required to allow modeling, developed criteria for model quality, and, where possible, improved modeling by matching predicted contacts to known structures. Their method predicted quality structural models for 614 protein families, of which about 140 represent newly discovered protein folds. Science, this issue p. 294; see also p. 248 Combining metagenome data with protein structure prediction generates models for 614 families with unknown structures. Despite decades of work by structural biologists, there are still ~5200 protein families with unknown structure outside the range of comparative modeling. We show that Rosetta structure prediction guided by residue-residue contacts inferred from evolutionary information can accurately model proteins that belong to large families and that metagenome sequence data more than triple the number of protein families with sufficient sequences for accurate modeling. We then integrate metagenome data, contact-based structure matching, and Rosetta structure calculations to generate models for 614 protein families with currently unknown structures; 206 are membrane proteins and 137 have folds not represented in the Protein Data Bank. This approach provides the representative models for large protein families originally envisioned as the goal of the Protein Structure Initiative at a fraction of the cost.",fullPaper,jv104
Medicine,p386,d1,b41fd82432999628e34d07e64ccda783273c15c0,j23,Proceedings of the National Academy of Sciences of the United States of America,Data integration enables global biodiversity synthesis,"Significance As anthropogenic impacts to Earth systems accelerate, biodiversity knowledge integration is urgently required to support responses to underpin a sustainable future. Consolidating information from disparate sources (e.g., community science programs, museums) and data types (e.g., environmental, biological) can connect the biological sciences across taxonomic, disciplinary, geographical, and socioeconomic boundaries. In an analysis of the research uses of the world’s largest cross-taxon biodiversity data network, we report the emerging roles of open-access data aggregation in the development of increasingly diverse, global research. These results indicate a new biodiversity science landscape centered on big data integration, informing ongoing initiatives and the strategic prioritization of biodiversity data aggregation across diverse knowledge domains, including environmental sciences and policy, evolutionary biology, conservation, and human health. The accessibility of global biodiversity information has surged in the past two decades, notably through widespread funding initiatives for museum specimen digitization and emergence of large-scale public participation in community science. Effective use of these data requires the integration of disconnected datasets, but the scientific impacts of consolidated biodiversity data networks have not yet been quantified. To determine whether data integration enables novel research, we carried out a quantitative text analysis and bibliographic synthesis of >4,000 studies published from 2003 to 2019 that use data mediated by the world’s largest biodiversity data network, the Global Biodiversity Information Facility (GBIF). Data available through GBIF increased 12-fold since 2007, a trend matched by global data use with roughly two publications using GBIF-mediated data per day in 2019. Data-use patterns were diverse by authorship, geographic extent, taxonomic group, and dataset type. Despite facilitating global authorship, legacies of colonial science remain. Studies involving species distribution modeling were most prevalent (31% of literature surveyed) but recently shifted in focus from theory to application. Topic prevalence was stable across the 17-y period for some research areas (e.g., macroecology), yet other topics proportionately declined (e.g., taxonomy) or increased (e.g., species interactions, disease). Although centered on biological subfields, GBIF-enabled research extends surprisingly across all major scientific disciplines. Biodiversity data mobilization through global data aggregation has enabled basic and applied research use at temporal, spatial, and taxonomic scales otherwise not possible, launching biodiversity sciences into a new era.",fullPaper,jv23
Medicine,p390,d1,88a55ee54aae117f06441459d1ad2330ce18d7e0,j131,Conservation Biology,Emerging problems of data quality in citizen science,Abstract,fullPaper,jv131
Sociology,p390,d4,88a55ee54aae117f06441459d1ad2330ce18d7e0,j131,Conservation Biology,Emerging problems of data quality in citizen science,Abstract,fullPaper,jv131
Medicine,p396,d1,c3665722a7cc81caca8c90ac3c5b0572f7bba055,j111,Public Understanding of Science,Can citizen science enhance public understanding of science?,"Over the past 20 years, thousands of citizen science projects engaging millions of participants in collecting and/or processing data have sprung up around the world. Here we review documented outcomes from four categories of citizen science projects which are defined by the nature of the activities in which their participants engage – Data Collection, Data Processing, Curriculum-based, and Community Science. We find strong evidence that scientific outcomes of citizen science are well documented, particularly for Data Collection and Data Processing projects. We find limited but growing evidence that citizen science projects achieve participant gains in knowledge about science knowledge and process, increase public awareness of the diversity of scientific research, and provide deeper meaning to participants’ hobbies. We also find some evidence that citizen science can contribute positively to social well-being by influencing the questions that are being addressed and by giving people a voice in local environmental decision making. While not all citizen science projects are intended to achieve a greater degree of public understanding of science, social change, or improved science -society relationships, those projects that do require effort and resources in four main categories: (1) project design, (2) outcomes measurement, (3) engagement of new audiences, and (4) new directions for research.",fullPaper,jv111
Sociology,p396,d4,c3665722a7cc81caca8c90ac3c5b0572f7bba055,j111,Public Understanding of Science,Can citizen science enhance public understanding of science?,"Over the past 20 years, thousands of citizen science projects engaging millions of participants in collecting and/or processing data have sprung up around the world. Here we review documented outcomes from four categories of citizen science projects which are defined by the nature of the activities in which their participants engage – Data Collection, Data Processing, Curriculum-based, and Community Science. We find strong evidence that scientific outcomes of citizen science are well documented, particularly for Data Collection and Data Processing projects. We find limited but growing evidence that citizen science projects achieve participant gains in knowledge about science knowledge and process, increase public awareness of the diversity of scientific research, and provide deeper meaning to participants’ hobbies. We also find some evidence that citizen science can contribute positively to social well-being by influencing the questions that are being addressed and by giving people a voice in local environmental decision making. While not all citizen science projects are intended to achieve a greater degree of public understanding of science, social change, or improved science -society relationships, those projects that do require effort and resources in four main categories: (1) project design, (2) outcomes measurement, (3) engagement of new audiences, and (4) new directions for research.",fullPaper,jv111
Medicine,p398,d1,ff1068a7e2acaa41fae2a8e1b180264434f06ce8,j104,Science,Liberating field science samples and data,"Promote reproducibility by moving beyond “available upon request” Transparency and reproducibility enhance the integrity of research results for scientific and public uses and empower novel research applications. Access to data, samples, methods, and reagents used to conduct research and analysis, as well as to the code used to analyze and process data and samples, is a fundamental requirement for transparency and reproducibility. The field sciences (e.g., geology, ecology, and archaeology), where each study is temporally (and often spatially) unique, provide exemplars for the importance of preserving data and samples for further analysis. Yet field sciences, if they even address such access, commonly do so by simply noting “data and samples available upon request.” They lag behind some laboratory sciences in making data and samples available to the broader research community. It is time for this to change. We discuss cultural, financial, and technical barriers to change and ways in which funders, publishers, scientific societies, and others are responding.",fullPaper,jv104
Medicine,p399,d1,ecb81c5d18e38b29316da77f69c8a36d5b98f196,j8,Nature Methods,scmap: projection of single-cell RNA-seq data across data sets,Abstract,fullPaper,jv8
Biology,p399,d5,ecb81c5d18e38b29316da77f69c8a36d5b98f196,j8,Nature Methods,scmap: projection of single-cell RNA-seq data across data sets,Abstract,fullPaper,jv8
Medicine,p400,d1,ecb81c5d18e38b29316da77f69c8a36d5b98f196,j8,Nature Methods,scmap: projection of single-cell RNA-seq data across data sets,Abstract,fullPaper,jv8
Biology,p400,d5,ecb81c5d18e38b29316da77f69c8a36d5b98f196,j8,Nature Methods,scmap: projection of single-cell RNA-seq data across data sets,Abstract,fullPaper,jv8
Medicine,p401,d1,952241d28abed7d221fc059845043a6463a522bc,c56,International Conference on Automated Software Engineering,Qualitative Descriptive Methods in Health Science Research,"Objective: The purpose of this methodology paper is to describe an approach to qualitative design known as qualitative descriptive that is well suited to junior health sciences researchers because it can be used with a variety of theoretical approaches, sampling techniques, and data collection strategies. Background: It is often difficult for junior qualitative researchers to pull together the tools and resources they need to embark on a high-quality qualitative research study and to manage the volumes of data they collect during qualitative studies. This paper seeks to pull together much needed resources and provide an overview of methods. Methods: A step-by-step guide to planning a qualitative descriptive study and analyzing the data is provided, utilizing exemplars from the authors’ research. Results: This paper presents steps to conducting a qualitative descriptive study under the following headings: describing the qualitative descriptive approach, designing a qualitative descriptive study, steps to data analysis, and ensuring rigor of findings. Conclusions: The qualitative descriptive approach results in a summary in everyday, factual language that facilitates understanding of a selected phenomenon across disciplines of health science researchers.",poster,cp56
Medicine,p404,d1,835a5484292f32a3c02f507cbd8fb1f5d9f4aacf,j133,Royal Society Open Science,The natural selection of bad science,"Poor research design and data analysis encourage false-positive findings. Such poor methods persist despite perennial calls for improvement, suggesting that they result from something more than just misunderstanding. The persistence of poor methods results partly from incentives that favour them, leading to the natural selection of bad science. This dynamic requires no conscious strategizing—no deliberate cheating nor loafing—by scientists, only that publication is a principal factor for career advancement. Some normative methods of analysis have almost certainly been selected to further publication instead of discovery. In order to improve the culture of science, a shift must be made away from correcting misunderstandings and towards rewarding understanding. We support this argument with empirical evidence and computational modelling. We first present a 60-year meta-analysis of statistical power in the behavioural sciences and show that power has not improved despite repeated demonstrations of the necessity of increasing power. To demonstrate the logical consequences of structural incentives, we then present a dynamic model of scientific communities in which competing laboratories investigate novel or previously published hypotheses using culturally transmitted research methods. As in the real world, successful labs produce more ‘progeny,’ such that their methods are more often copied and their students are more likely to start labs of their own. Selection for high output leads to poorer methods and increasingly high false discovery rates. We additionally show that replication slows but does not stop the process of methodological deterioration. Improving the quality of research requires change at the institutional level.",fullPaper,jv133
Physics,p404,d2,835a5484292f32a3c02f507cbd8fb1f5d9f4aacf,j133,Royal Society Open Science,The natural selection of bad science,"Poor research design and data analysis encourage false-positive findings. Such poor methods persist despite perennial calls for improvement, suggesting that they result from something more than just misunderstanding. The persistence of poor methods results partly from incentives that favour them, leading to the natural selection of bad science. This dynamic requires no conscious strategizing—no deliberate cheating nor loafing—by scientists, only that publication is a principal factor for career advancement. Some normative methods of analysis have almost certainly been selected to further publication instead of discovery. In order to improve the culture of science, a shift must be made away from correcting misunderstandings and towards rewarding understanding. We support this argument with empirical evidence and computational modelling. We first present a 60-year meta-analysis of statistical power in the behavioural sciences and show that power has not improved despite repeated demonstrations of the necessity of increasing power. To demonstrate the logical consequences of structural incentives, we then present a dynamic model of scientific communities in which competing laboratories investigate novel or previously published hypotheses using culturally transmitted research methods. As in the real world, successful labs produce more ‘progeny,’ such that their methods are more often copied and their students are more likely to start labs of their own. Selection for high output leads to poorer methods and increasingly high false discovery rates. We additionally show that replication slows but does not stop the process of methodological deterioration. Improving the quality of research requires change at the institutional level.",fullPaper,jv133
Computer Science,p404,d3,835a5484292f32a3c02f507cbd8fb1f5d9f4aacf,j133,Royal Society Open Science,The natural selection of bad science,"Poor research design and data analysis encourage false-positive findings. Such poor methods persist despite perennial calls for improvement, suggesting that they result from something more than just misunderstanding. The persistence of poor methods results partly from incentives that favour them, leading to the natural selection of bad science. This dynamic requires no conscious strategizing—no deliberate cheating nor loafing—by scientists, only that publication is a principal factor for career advancement. Some normative methods of analysis have almost certainly been selected to further publication instead of discovery. In order to improve the culture of science, a shift must be made away from correcting misunderstandings and towards rewarding understanding. We support this argument with empirical evidence and computational modelling. We first present a 60-year meta-analysis of statistical power in the behavioural sciences and show that power has not improved despite repeated demonstrations of the necessity of increasing power. To demonstrate the logical consequences of structural incentives, we then present a dynamic model of scientific communities in which competing laboratories investigate novel or previously published hypotheses using culturally transmitted research methods. As in the real world, successful labs produce more ‘progeny,’ such that their methods are more often copied and their students are more likely to start labs of their own. Selection for high output leads to poorer methods and increasingly high false discovery rates. We additionally show that replication slows but does not stop the process of methodological deterioration. Improving the quality of research requires change at the institutional level.",fullPaper,jv133
Mathematics,p404,d6,835a5484292f32a3c02f507cbd8fb1f5d9f4aacf,j133,Royal Society Open Science,The natural selection of bad science,"Poor research design and data analysis encourage false-positive findings. Such poor methods persist despite perennial calls for improvement, suggesting that they result from something more than just misunderstanding. The persistence of poor methods results partly from incentives that favour them, leading to the natural selection of bad science. This dynamic requires no conscious strategizing—no deliberate cheating nor loafing—by scientists, only that publication is a principal factor for career advancement. Some normative methods of analysis have almost certainly been selected to further publication instead of discovery. In order to improve the culture of science, a shift must be made away from correcting misunderstandings and towards rewarding understanding. We support this argument with empirical evidence and computational modelling. We first present a 60-year meta-analysis of statistical power in the behavioural sciences and show that power has not improved despite repeated demonstrations of the necessity of increasing power. To demonstrate the logical consequences of structural incentives, we then present a dynamic model of scientific communities in which competing laboratories investigate novel or previously published hypotheses using culturally transmitted research methods. As in the real world, successful labs produce more ‘progeny,’ such that their methods are more often copied and their students are more likely to start labs of their own. Selection for high output leads to poorer methods and increasingly high false discovery rates. We additionally show that replication slows but does not stop the process of methodological deterioration. Improving the quality of research requires change at the institutional level.",fullPaper,jv133
Psychology,p404,d10,835a5484292f32a3c02f507cbd8fb1f5d9f4aacf,j133,Royal Society Open Science,The natural selection of bad science,"Poor research design and data analysis encourage false-positive findings. Such poor methods persist despite perennial calls for improvement, suggesting that they result from something more than just misunderstanding. The persistence of poor methods results partly from incentives that favour them, leading to the natural selection of bad science. This dynamic requires no conscious strategizing—no deliberate cheating nor loafing—by scientists, only that publication is a principal factor for career advancement. Some normative methods of analysis have almost certainly been selected to further publication instead of discovery. In order to improve the culture of science, a shift must be made away from correcting misunderstandings and towards rewarding understanding. We support this argument with empirical evidence and computational modelling. We first present a 60-year meta-analysis of statistical power in the behavioural sciences and show that power has not improved despite repeated demonstrations of the necessity of increasing power. To demonstrate the logical consequences of structural incentives, we then present a dynamic model of scientific communities in which competing laboratories investigate novel or previously published hypotheses using culturally transmitted research methods. As in the real world, successful labs produce more ‘progeny,’ such that their methods are more often copied and their students are more likely to start labs of their own. Selection for high output leads to poorer methods and increasingly high false discovery rates. We additionally show that replication slows but does not stop the process of methodological deterioration. Improving the quality of research requires change at the institutional level.",fullPaper,jv133
Medicine,p408,d1,2e096b5fe420e09e3a7ea3b1e8f1501495d8b07e,j19,Scientific Data,Operationalizing the CARE and FAIR Principles for Indigenous data futures,Abstract,fullPaper,jv19
Medicine,p409,d1,0efa865dd45bcee8194bfe709b0f81789f6d5341,j19,Scientific Data,Data sharing practices and data availability upon request differ across scientific disciplines,Abstract,fullPaper,jv19
Medicine,p415,d1,4d12b00963aa6e0d9b9b84a62f0543de608fccb5,j135,PLoS ONE,"If We Share Data, Will Anyone Use Them? Data Sharing and Reuse in the Long Tail of Science and Technology","Research on practices to share and reuse data will inform the design of infrastructure to support data collection, management, and discovery in the long tail of science and technology. These are research domains in which data tend to be local in character, minimally structured, and minimally documented. We report on a ten-year study of the Center for Embedded Network Sensing (CENS), a National Science Foundation Science and Technology Center. We found that CENS researchers are willing to share their data, but few are asked to do so, and in only a few domain areas do their funders or journals require them to deposit data. Few repositories exist to accept data in CENS research areas.. Data sharing tends to occur only through interpersonal exchanges. CENS researchers obtain data from repositories, and occasionally from registries and individuals, to provide context, calibration, or other forms of background for their studies. Neither CENS researchers nor those who request access to CENS data appear to use external data for primary research questions or for replication of studies. CENS researchers are willing to share data if they receive credit and retain first rights to publish their results. Practices of releasing, sharing, and reusing of data in CENS reaffirm the gift culture of scholarship, in which goods are bartered between trusted colleagues rather than treated as commodities.",fullPaper,jv135
Business,p415,d9,4d12b00963aa6e0d9b9b84a62f0543de608fccb5,j135,PLoS ONE,"If We Share Data, Will Anyone Use Them? Data Sharing and Reuse in the Long Tail of Science and Technology","Research on practices to share and reuse data will inform the design of infrastructure to support data collection, management, and discovery in the long tail of science and technology. These are research domains in which data tend to be local in character, minimally structured, and minimally documented. We report on a ten-year study of the Center for Embedded Network Sensing (CENS), a National Science Foundation Science and Technology Center. We found that CENS researchers are willing to share their data, but few are asked to do so, and in only a few domain areas do their funders or journals require them to deposit data. Few repositories exist to accept data in CENS research areas.. Data sharing tends to occur only through interpersonal exchanges. CENS researchers obtain data from repositories, and occasionally from registries and individuals, to provide context, calibration, or other forms of background for their studies. Neither CENS researchers nor those who request access to CENS data appear to use external data for primary research questions or for replication of studies. CENS researchers are willing to share data if they receive credit and retain first rights to publish their results. Practices of releasing, sharing, and reusing of data in CENS reaffirm the gift culture of scholarship, in which goods are bartered between trusted colleagues rather than treated as commodities.",fullPaper,jv135
Medicine,p417,d1,ecef432e7f6c9f431d5b34706a8de1fdebec46f9,j137,Frontiers in Medicine,From Big Data to Precision Medicine,"For over a decade the term “Big data” has been used to describe the rapid increase in volume, variety and velocity of information available, not just in medical research but in almost every aspect of our lives. As scientists, we now have the capacity to rapidly generate, store and analyse data that, only a few years ago, would have taken many years to compile. However, “Big data” no longer means what it once did. The term has expanded and now refers not to just large data volume, but to our increasing ability to analyse and interpret those data. Tautologies such as “data analytics” and “data science” have emerged to describe approaches to the volume of available information as it grows ever larger. New methods dedicated to improving data collection, storage, cleaning, processing and interpretation continue to be developed, although not always by, or for, medical researchers. Exploiting new tools to extract meaning from large volume information has the potential to drive real change in clinical practice, from personalized therapy and intelligent drug design to population screening and electronic health record mining. As ever, where new technology promises “Big Advances,” significant challenges remain. Here we discuss both the opportunities and challenges posed to biomedical research by our increasing ability to tackle large datasets. Important challenges include the need for standardization of data content, format, and clinical definitions, a heightened need for collaborative networks with sharing of both data and expertise and, perhaps most importantly, a need to reconsider how and when analytic methodology is taught to medical researchers. We also set “Big data” analytics in context: recent advances may appear to promise a revolution, sweeping away conventional approaches to medical science. However, their real promise lies in their synergy with, not replacement of, classical hypothesis-driven methods. The generation of novel, data-driven hypotheses based on interpretable models will always require stringent validation and experimental testing. Thus, hypothesis-generating research founded on large datasets adds to, rather than replaces, traditional hypothesis driven science. Each can benefit from the other and it is through using both that we can improve clinical practice.",fullPaper,jv137
Computer Science,p417,d3,ecef432e7f6c9f431d5b34706a8de1fdebec46f9,j137,Frontiers in Medicine,From Big Data to Precision Medicine,"For over a decade the term “Big data” has been used to describe the rapid increase in volume, variety and velocity of information available, not just in medical research but in almost every aspect of our lives. As scientists, we now have the capacity to rapidly generate, store and analyse data that, only a few years ago, would have taken many years to compile. However, “Big data” no longer means what it once did. The term has expanded and now refers not to just large data volume, but to our increasing ability to analyse and interpret those data. Tautologies such as “data analytics” and “data science” have emerged to describe approaches to the volume of available information as it grows ever larger. New methods dedicated to improving data collection, storage, cleaning, processing and interpretation continue to be developed, although not always by, or for, medical researchers. Exploiting new tools to extract meaning from large volume information has the potential to drive real change in clinical practice, from personalized therapy and intelligent drug design to population screening and electronic health record mining. As ever, where new technology promises “Big Advances,” significant challenges remain. Here we discuss both the opportunities and challenges posed to biomedical research by our increasing ability to tackle large datasets. Important challenges include the need for standardization of data content, format, and clinical definitions, a heightened need for collaborative networks with sharing of both data and expertise and, perhaps most importantly, a need to reconsider how and when analytic methodology is taught to medical researchers. We also set “Big data” analytics in context: recent advances may appear to promise a revolution, sweeping away conventional approaches to medical science. However, their real promise lies in their synergy with, not replacement of, classical hypothesis-driven methods. The generation of novel, data-driven hypotheses based on interpretable models will always require stringent validation and experimental testing. Thus, hypothesis-generating research founded on large datasets adds to, rather than replaces, traditional hypothesis driven science. Each can benefit from the other and it is through using both that we can improve clinical practice.",fullPaper,jv137
Medicine,p419,d1,c1e49d830e67269d4d2053a5f124ea773c79b740,j104,Science,Computational social science: Obstacles and opportunities,"Data sharing, research ethics, and incentives must improve The field of computational social science (CSS) has exploded in prominence over the past decade, with thousands of papers published using observational data, experimental designs, and large-scale simulations that were once unfeasible or unavailable to researchers. These studies have greatly improved our understanding of important phenomena, ranging from social inequality to the spread of infectious diseases. The institutions supporting CSS in the academy have also grown substantially, as evidenced by the proliferation of conferences, workshops, and summer schools across the globe, across disciplines, and across sources of data. But the field has also fallen short in important ways. Many institutional structures around the field—including research ethics, pedagogy, and data infrastructure—are still nascent. We suggest opportunities to address these issues, especially in improving the alignment between the organization of the 20th-century university and the intellectual requirements of the field.",fullPaper,jv104
Computer Science,p419,d3,c1e49d830e67269d4d2053a5f124ea773c79b740,j104,Science,Computational social science: Obstacles and opportunities,"Data sharing, research ethics, and incentives must improve The field of computational social science (CSS) has exploded in prominence over the past decade, with thousands of papers published using observational data, experimental designs, and large-scale simulations that were once unfeasible or unavailable to researchers. These studies have greatly improved our understanding of important phenomena, ranging from social inequality to the spread of infectious diseases. The institutions supporting CSS in the academy have also grown substantially, as evidenced by the proliferation of conferences, workshops, and summer schools across the globe, across disciplines, and across sources of data. But the field has also fallen short in important ways. Many institutional structures around the field—including research ethics, pedagogy, and data infrastructure—are still nascent. We suggest opportunities to address these issues, especially in improving the alignment between the organization of the 20th-century university and the intellectual requirements of the field.",fullPaper,jv104
Medicine,p423,d1,04e5f980428e1ec35429356b3e43ea611fc0e975,j139,Sociological Methods & Research,Using Twitter for Demographic and Social Science Research: Tools for Data Collection and Processing,"Despite recent and growing interest in using Twitter to examine human behavior and attitudes, there is still significant room for growth regarding the ability to leverage Twitter data for social science research. In particular, gleaning demographic information about Twitter users—a key component of much social science research—remains a challenge. This article develops an accurate and reliable data processing approach for social science researchers interested in using Twitter data to examine behaviors and attitudes, as well as the demographic characteristics of the populations expressing or engaging in them. Using information gathered from Twitter users who state an intention to not vote in the 2012 presidential election, we describe and evaluate a method for processing data to retrieve demographic information reported by users that is not encoded as text (e.g., details of images) and evaluate the reliability of these techniques. We end by assessing the challenges of this data collection strategy and discussing how large-scale social media data may benefit demographic researchers.",fullPaper,jv139
Computer Science,p423,d3,04e5f980428e1ec35429356b3e43ea611fc0e975,j139,Sociological Methods & Research,Using Twitter for Demographic and Social Science Research: Tools for Data Collection and Processing,"Despite recent and growing interest in using Twitter to examine human behavior and attitudes, there is still significant room for growth regarding the ability to leverage Twitter data for social science research. In particular, gleaning demographic information about Twitter users—a key component of much social science research—remains a challenge. This article develops an accurate and reliable data processing approach for social science researchers interested in using Twitter data to examine behaviors and attitudes, as well as the demographic characteristics of the populations expressing or engaging in them. Using information gathered from Twitter users who state an intention to not vote in the 2012 presidential election, we describe and evaluate a method for processing data to retrieve demographic information reported by users that is not encoded as text (e.g., details of images) and evaluate the reliability of these techniques. We end by assessing the challenges of this data collection strategy and discussing how large-scale social media data may benefit demographic researchers.",fullPaper,jv139
Psychology,p423,d10,04e5f980428e1ec35429356b3e43ea611fc0e975,j139,Sociological Methods & Research,Using Twitter for Demographic and Social Science Research: Tools for Data Collection and Processing,"Despite recent and growing interest in using Twitter to examine human behavior and attitudes, there is still significant room for growth regarding the ability to leverage Twitter data for social science research. In particular, gleaning demographic information about Twitter users—a key component of much social science research—remains a challenge. This article develops an accurate and reliable data processing approach for social science researchers interested in using Twitter data to examine behaviors and attitudes, as well as the demographic characteristics of the populations expressing or engaging in them. Using information gathered from Twitter users who state an intention to not vote in the 2012 presidential election, we describe and evaluate a method for processing data to retrieve demographic information reported by users that is not encoded as text (e.g., details of images) and evaluate the reliability of these techniques. We end by assessing the challenges of this data collection strategy and discussing how large-scale social media data may benefit demographic researchers.",fullPaper,jv139
Medicine,p429,d1,8cd71d704f9d3eeb5eb697e412ba54b680f00636,j140,JMIR Medical Informatics,Big Data and Clinicians: A Review on the State of the Science,"Background In the past few decades, medically related data collection saw a huge increase, referred to as big data. These huge datasets bring challenges in storage, processing, and analysis. In clinical medicine, big data is expected to play an important role in identifying causality of patient symptoms, in predicting hazards of disease incidence or reoccurrence, and in improving primary-care quality. Objective The objective of this review was to provide an overview of the features of clinical big data, describe a few commonly employed computational algorithms, statistical methods, and software toolkits for data manipulation and analysis, and discuss the challenges and limitations in this realm. Methods We conducted a literature review to identify studies on big data in medicine, especially clinical medicine. We used different combinations of keywords to search PubMed, Science Direct, Web of Knowledge, and Google Scholar for literature of interest from the past 10 years. Results This paper reviewed studies that analyzed clinical big data and discussed issues related to storage and analysis of this type of data. Conclusions Big data is becoming a common feature of biological and clinical studies. Researchers who use clinical big data face multiple challenges, and the data itself has limitations. It is imperative that methodologies for data analysis keep pace with our ability to collect and store data.",fullPaper,jv140
Medicine,p433,d1,8807a8327e27298fd601fc65e6a9ccfae1cca195,j135,PLoS ONE,What Is Citizen Science? – A Scientometric Meta-Analysis,"Context The concept of citizen science (CS) is currently referred to by many actors inside and outside science and research. Several descriptions of this purportedly new approach of science are often heard in connection with large datasets and the possibilities of mobilizing crowds outside science to assists with observations and classifications. However, other accounts refer to CS as a way of democratizing science, aiding concerned communities in creating data to influence policy and as a way of promoting political decision processes involving environment and health. Objective In this study we analyse two datasets (N = 1935, N = 633) retrieved from the Web of Science (WoS) with the aim of giving a scientometric description of what the concept of CS entails. We account for its development over time, and what strands of research that has adopted CS and give an assessment of what scientific output has been achieved in CS-related projects. To attain this, scientometric methods have been combined with qualitative approaches to render more precise search terms. Results Results indicate that there are three main focal points of CS. The largest is composed of research on biology, conservation and ecology, and utilizes CS mainly as a methodology of collecting and classifying data. A second strand of research has emerged through geographic information research, where citizens participate in the collection of geographic data. Thirdly, there is a line of research relating to the social sciences and epidemiology, which studies and facilitates public participation in relation to environmental issues and health. In terms of scientific output, the largest body of articles are to be found in biology and conservation research. In absolute numbers, the amount of publications generated by CS is low (N = 1935), but over the past decade a new and very productive line of CS based on digital platforms has emerged for the collection and classification of data.",fullPaper,jv135
Medicine,p440,d1,8958efba7a02e3653f27c0e759882b2f3352e896,j19,Scientific Data,"Materials Cloud, a platform for open computational science",Abstract,fullPaper,jv19
Physics,p440,d2,8958efba7a02e3653f27c0e759882b2f3352e896,j19,Scientific Data,"Materials Cloud, a platform for open computational science",Abstract,fullPaper,jv19
Computer Science,p440,d3,8958efba7a02e3653f27c0e759882b2f3352e896,j19,Scientific Data,"Materials Cloud, a platform for open computational science",Abstract,fullPaper,jv19
Materials Science,p440,d7,8958efba7a02e3653f27c0e759882b2f3352e896,j19,Scientific Data,"Materials Cloud, a platform for open computational science",Abstract,fullPaper,jv19
Medicine,p445,d1,53834f0ee8df731cf0e629cd594dce0afaaa3d97,c41,IEEE International Conference on Data Engineering,The inevitable application of big data to health care.,"THE AMOUNT OF DATA BEING DIGITALLY COLLECTED AND stored is vast and expanding rapidly. As a result, the science of data management and analysis is also advancing to enable organizations to convert this vast resource into information and knowledge that helps them achieve their objectives. Computer scientists have invented the term big data to describe this evolving technology. Big data has been successfully used in astronomy (eg, the Sloan Digital Sky Survey of telescopic information), retail sales (eg, Walmart’s expansive number of transactions), search engines (eg, Google’s customization of individual searches based on previous web data), and politics (eg, a campaign’s focus of political advertisements on people most likely to support their candidate based on web searches). In this Viewpoint, we discuss the application of big data to health care, using an economic framework to highlight the opportunities it will offer and the roadblocks to implementation. We suggest that leveraging the collection of patient and practitioner data could be an important way to improve quality and efficiency of health care delivery. Widespread uptake of electronic health records (EHRs) has generated massive data sets. A survey by the American Hospital Association showed that adoption of EHRs has doubled from 2009 to 2011, partly a result of funding provided by the Health Information Technology for Economic and Clinical Health Act of 2009. Most EHRs now contain quantitative data (eg, laboratory values), qualitative data (eg, text-based documents and demographics), and transactional data (eg, a record of medication delivery). However, much of this rich data set is currently perceived as a byproduct of health care delivery, rather than a central asset to improve its efficiency. The transition of data from refuse to riches has been key in the big data revolution of other industries. Advances in analytic techniques in the computer sciences, especially in machine learning, have been a major catalyst for dealing with these large information sets. These analytic techniques are in contrast to traditional statistical methods (derived from the social and physical sciences), which are largely not useful for analysis of unstructured data such as text-based documents that do not fit into relational tables. One estimate suggests that 80% of business-related data exist in an unstructured format. The same could probably be said for health care data, a large proportion of which is text-based. In contrast to most consumer service industries, medicine adopted a practice of generating evidence from experimental (randomized trials) and quasi-experimental studies to inform patients and clinicians. The evidence-based movement is founded on the belief that scientific inquiry is superior to expert opinion and testimonials. In this way, medicine was ahead of many other industries in terms of recognizing the value of data and information guiding rational decision making. However, health care has lagged in uptake of newer techniques to leverage the rich information contained in EHRs. There are 4 ways big data may advance the economic mission of health care delivery by improving quality and efficiency. First, big data may greatly expand the capacity to generate new knowledge. The cost of answering many clinical questions prospectively, and even retrospectively, by collecting structured data is prohibitive. Analyzing the unstructured data contained within EHRs using computational techniques (eg, natural language processing to extract medical concepts from free-text documents) permits finer data acquisition in an automated fashion. For instance, automated identification within EHRs using natural language processing was superior in detecting postoperative complications compared with patient safety indicators based on discharge coding. Big data offers the potential to create an observational evidence base for clinical questions that would otherwise not be possible and may be especially helpful with issues of generalizability. The latter issue limits the application of conclusions derived from randomized trials performed on a narrow spectrum of participants to patients who exhibit very different characteristics. Second, big data may help with knowledge dissemination. Most physicians struggle to stay current with the latest evidence guiding clinical practice. The digitization of medical literature has greatly improved access; however, the sheer",poster,cp41
Medicine,p446,d1,d43e2d9b90c0f509c9f569b9d4bd431ebd711f4f,j145,Psychology Science,Sharing Data and Materials in Psychological Science,Abstract,fullPaper,jv145
Psychology,p446,d10,d43e2d9b90c0f509c9f569b9d4bd431ebd711f4f,j145,Psychology Science,Sharing Data and Materials in Psychological Science,Abstract,fullPaper,jv145
Medicine,p447,d1,83b2bc2583862fa662cdfeb6cc7950bb2972347d,j146,Immunologic research,ImmPort: disseminating data to the public for the future of immunology,Abstract,fullPaper,jv146
Medicine,p451,d1,1a46465ab69ec13d3c84d66166e979989afa596d,j104,Science,Comment on “Estimating the reproducibility of psychological science”,"A paper from the Open Science Collaboration (Research Articles, 28 August 2015, aac4716) attempting to replicate 100 published studies suggests that the reproducibility of psychological science is surprisingly low. We show that this article contains three statistical errors and provides no support for such a conclusion. Indeed, the data are consistent with the opposite conclusion, namely, that the reproducibility of psychological science is quite high.",fullPaper,jv104
Psychology,p451,d10,1a46465ab69ec13d3c84d66166e979989afa596d,j104,Science,Comment on “Estimating the reproducibility of psychological science”,"A paper from the Open Science Collaboration (Research Articles, 28 August 2015, aac4716) attempting to replicate 100 published studies suggests that the reproducibility of psychological science is surprisingly low. We show that this article contains three statistical errors and provides no support for such a conclusion. Indeed, the data are consistent with the opposite conclusion, namely, that the reproducibility of psychological science is quite high.",fullPaper,jv104
Medicine,p453,d1,e1ababf08c9ec103db854a2c1b4db611142cfdb7,c74,International Conference on Computational Linguistics,Linear Mixed Models for Longitudinal Data,Abstract,poster,cp74
Medicine,p467,d1,90478017154dd6e4dbcb71895c64c9ddddebfb8c,j148,Scientific Reports,Taxonomic bias in biodiversity data and societal preferences,Abstract,fullPaper,jv148
Biology,p467,d5,90478017154dd6e4dbcb71895c64c9ddddebfb8c,j148,Scientific Reports,Taxonomic bias in biodiversity data and societal preferences,Abstract,fullPaper,jv148
Medicine,p470,d1,85cd1c3c6346d8fe3b245cc41e2757631301bc27,j111,Public Understanding of Science,The lure of rationality: Why does the deficit model persist in science communication?,"Science communication has been historically predicated on the knowledge deficit model. Yet, empirical research has shown that public communication of science is more complex than what the knowledge deficit model suggests. In this essay, we pose four lines of reasoning and present empirical data for why we believe the deficit model still persists in public communication of science. First, we posit that scientists’ training results in the belief that public audiences can and do process information in a rational manner. Second, the persistence of this model may be a product of current institutional structures. Many graduate education programs in science, technology, engineering, and math (STEM) fields generally lack formal training in public communication. We offer empirical evidence that demonstrates that scientists who have less positive attitudes toward the social sciences are more likely to adhere to the knowledge deficit model of science communication. Third, we present empirical evidence of how scientists conceptualize “the public” and link this to attitudes toward the deficit model. We find that perceiving a knowledge deficit in the public is closely tied to scientists’ perceptions of the individuals who comprise the public. Finally, we argue that the knowledge deficit model is perpetuated because it can easily influence public policy for science issues. We propose some ways to uproot the deficit model and move toward more effective science communication efforts, which include training scientists in communication methods grounded in social science research and using approaches that engage community members around scientific issues.",fullPaper,jv111
Sociology,p470,d4,85cd1c3c6346d8fe3b245cc41e2757631301bc27,j111,Public Understanding of Science,The lure of rationality: Why does the deficit model persist in science communication?,"Science communication has been historically predicated on the knowledge deficit model. Yet, empirical research has shown that public communication of science is more complex than what the knowledge deficit model suggests. In this essay, we pose four lines of reasoning and present empirical data for why we believe the deficit model still persists in public communication of science. First, we posit that scientists’ training results in the belief that public audiences can and do process information in a rational manner. Second, the persistence of this model may be a product of current institutional structures. Many graduate education programs in science, technology, engineering, and math (STEM) fields generally lack formal training in public communication. We offer empirical evidence that demonstrates that scientists who have less positive attitudes toward the social sciences are more likely to adhere to the knowledge deficit model of science communication. Third, we present empirical evidence of how scientists conceptualize “the public” and link this to attitudes toward the deficit model. We find that perceiving a knowledge deficit in the public is closely tied to scientists’ perceptions of the individuals who comprise the public. Finally, we argue that the knowledge deficit model is perpetuated because it can easily influence public policy for science issues. We propose some ways to uproot the deficit model and move toward more effective science communication efforts, which include training scientists in communication methods grounded in social science research and using approaches that engage community members around scientific issues.",fullPaper,jv111
Medicine,p471,d1,41692ed07f393c1c3e335db99c7e3c5a0d265a78,j104,Science,Citation indexes for science; a new dimension in documentation through association of ideas.,"‘The uncritical citation of disputed data by a writer, whether it be deliberate or not, is a serious matter. Of course, knowingly propagandizing unsubstantiated claims is particularly abhorrent, but just as many naive students may be swayed by unfounded assertions presented by a writer who is unaware of the criticisms. Buried in scholarly journals, critical notes are increasingly likely to be overlooked with the passage of time, while the studies to which they pertain, having been reported more widely, are apt to be rediscovered.’ 1",fullPaper,jv104
Medicine,p476,d1,7a1b9cc42e6fc611970b451fbef795e72cbea46d,j149,Trends in Ecology & Evolution,Ecoinformatics: supporting ecology as a data-intensive science.,Abstract,fullPaper,jv149
Computer Science,p476,d3,7a1b9cc42e6fc611970b451fbef795e72cbea46d,j149,Trends in Ecology & Evolution,Ecoinformatics: supporting ecology as a data-intensive science.,Abstract,fullPaper,jv149
Medicine,p479,d1,954f2a7b1c6f28c4a845ccda5761eb09da032a64,j104,Science,Data sharing,"The Science family of journals is committed to sharing data relevant to public health emergencies, and therefore we are signatories to, and wholeheartedly endorse, the following statement by funders and journals.*",fullPaper,jv104
Medicine,p480,d1,06d2a3fde80c5644f14f743b29a57f6b02e850d9,j64,PLoS Biology,The iPlant Collaborative: Cyberinfrastructure for Enabling Data to Discovery for the Life Sciences,"The iPlant Collaborative provides life science research communities access to comprehensive, scalable, and cohesive computational infrastructure for data management; identity management; collaboration tools; and cloud, high-performance, high-throughput computing. iPlant provides training, learning material, and best practice resources to help all researchers make the best use of their data, expand their computational skill set, and effectively manage their data and computation when working as distributed teams. iPlant’s platform permits researchers to easily deposit and share their data and deploy new computational tools and analysis workflows, allowing the broader community to easily use and reuse those data and computational analyses.",fullPaper,jv64
Biology,p480,d5,06d2a3fde80c5644f14f743b29a57f6b02e850d9,j64,PLoS Biology,The iPlant Collaborative: Cyberinfrastructure for Enabling Data to Discovery for the Life Sciences,"The iPlant Collaborative provides life science research communities access to comprehensive, scalable, and cohesive computational infrastructure for data management; identity management; collaboration tools; and cloud, high-performance, high-throughput computing. iPlant provides training, learning material, and best practice resources to help all researchers make the best use of their data, expand their computational skill set, and effectively manage their data and computation when working as distributed teams. iPlant’s platform permits researchers to easily deposit and share their data and deploy new computational tools and analysis workflows, allowing the broader community to easily use and reuse those data and computational analyses.",fullPaper,jv64
Medicine,p481,d1,69732dcf45024f28e5c43de68d1208f6e737eada,c101,Interspeech,The BIG Data Center: from deposition to integration to translation,"Biological data are generated at unprecedentedly exponential rates, posing considerable challenges in big data deposition, integration and translation. The BIG Data Center, established at Beijing Institute of Genomics (BIG), Chinese Academy of Sciences, provides a suite of database resources, including (i) Genome Sequence Archive, a data repository specialized for archiving raw sequence reads, (ii) Gene Expression Nebulas, a data portal of gene expression profiles based entirely on RNA-Seq data, (iii) Genome Variation Map, a comprehensive collection of genome variations for featured species, (iv) Genome Warehouse, a centralized resource housing genome-scale data with particular focus on economically important animals and plants, (v) Methylation Bank, an integrated database of whole-genome single-base resolution methylomes and (vi) Science Wikis, a central access point for biological wikis developed for community annotations. The BIG Data Center is dedicated to constructing and maintaining biological databases through big data integration and value-added curation, conducting basic research to translate big data into big knowledge and providing freely open access to a variety of data resources in support of worldwide research activities in both academia and industry. All of these resources are publicly available and can be found at http://bigd.big.ac.cn.",poster,cp101
Computer Science,p481,d3,69732dcf45024f28e5c43de68d1208f6e737eada,c101,Interspeech,The BIG Data Center: from deposition to integration to translation,"Biological data are generated at unprecedentedly exponential rates, posing considerable challenges in big data deposition, integration and translation. The BIG Data Center, established at Beijing Institute of Genomics (BIG), Chinese Academy of Sciences, provides a suite of database resources, including (i) Genome Sequence Archive, a data repository specialized for archiving raw sequence reads, (ii) Gene Expression Nebulas, a data portal of gene expression profiles based entirely on RNA-Seq data, (iii) Genome Variation Map, a comprehensive collection of genome variations for featured species, (iv) Genome Warehouse, a centralized resource housing genome-scale data with particular focus on economically important animals and plants, (v) Methylation Bank, an integrated database of whole-genome single-base resolution methylomes and (vi) Science Wikis, a central access point for biological wikis developed for community annotations. The BIG Data Center is dedicated to constructing and maintaining biological databases through big data integration and value-added curation, conducting basic research to translate big data into big knowledge and providing freely open access to a variety of data resources in support of worldwide research activities in both academia and industry. All of these resources are publicly available and can be found at http://bigd.big.ac.cn.",poster,cp101
Medicine,p483,d1,edf27bb5272ea6fe244deb3bbc8da0429bfe3ac5,j104,Science,The reusable holdout: Preserving validity in adaptive data analysis,"Testing hypotheses privately Large data sets offer a vast scope for testing already-formulated ideas and exploring new ones. Unfortunately, researchers who attempt to do both on the same data set run the risk of making false discoveries, even when testing and exploration are carried out on distinct subsets of data. Based on ideas drawn from differential privacy, Dwork et al. now provide a theoretical solution. Ideas are tested against aggregate information, whereas individual data set components remain confidential. Preserving that privacy also preserves statistical inference validity. Science, this issue p. 636 A statistical approach allows large data sets to be reanalyzed to test new hypotheses. Misapplication of statistical data analysis is a common cause of spurious discoveries in scientific research. Existing approaches to ensuring the validity of inferences drawn from data assume a fixed procedure to be performed, selected before the data are examined. In common practice, however, data analysis is an intrinsically adaptive process, with new analyses generated on the basis of data exploration, as well as the results of previous analyses on the same data. We demonstrate a new approach for addressing the challenges of adaptivity based on insights from privacy-preserving data analysis. As an application, we show how to safely reuse a holdout data set many times to validate the results of adaptively chosen analyses.",fullPaper,jv104
Computer Science,p483,d3,edf27bb5272ea6fe244deb3bbc8da0429bfe3ac5,j104,Science,The reusable holdout: Preserving validity in adaptive data analysis,"Testing hypotheses privately Large data sets offer a vast scope for testing already-formulated ideas and exploring new ones. Unfortunately, researchers who attempt to do both on the same data set run the risk of making false discoveries, even when testing and exploration are carried out on distinct subsets of data. Based on ideas drawn from differential privacy, Dwork et al. now provide a theoretical solution. Ideas are tested against aggregate information, whereas individual data set components remain confidential. Preserving that privacy also preserves statistical inference validity. Science, this issue p. 636 A statistical approach allows large data sets to be reanalyzed to test new hypotheses. Misapplication of statistical data analysis is a common cause of spurious discoveries in scientific research. Existing approaches to ensuring the validity of inferences drawn from data assume a fixed procedure to be performed, selected before the data are examined. In common practice, however, data analysis is an intrinsically adaptive process, with new analyses generated on the basis of data exploration, as well as the results of previous analyses on the same data. We demonstrate a new approach for addressing the challenges of adaptivity based on insights from privacy-preserving data analysis. As an application, we show how to safely reuse a holdout data set many times to validate the results of adaptively chosen analyses.",fullPaper,jv104
Medicine,p486,d1,29196eb8c80a6fd6a159373f14ff323f081a8b7a,j104,Science,Physical and Virtual Laboratories in Science and Engineering Education,"The world needs young people who are skillful in and enthusiastic about science and who view science as their future career field. Ensuring that we will have such young people requires initiatives that engage students in interesting and motivating science experiences. Today, students can investigate scientific phenomena using the tools, data collection techniques, models, and theories of science in physical laboratories that support interactions with the material world or in virtual laboratories that take advantage of simulations. Here, we review a selection of the literature to contrast the value of physical and virtual investigations and to offer recommendations for combining the two to strengthen science learning.",fullPaper,jv104
Medicine,p490,d1,1715fdc4df6774d95ed63f3feb58fa93a84dbed7,j149,Trends in Ecology & Evolution,Data-intensive science applied to broad-scale citizen science.,Abstract,fullPaper,jv149
Computer Science,p490,d3,1715fdc4df6774d95ed63f3feb58fa93a84dbed7,j149,Trends in Ecology & Evolution,Data-intensive science applied to broad-scale citizen science.,Abstract,fullPaper,jv149
Medicine,p494,d1,04638c67b715b9d85ae5a44afd3730b83330fb66,j104,Science,Economics in the age of big data,"Background Economic science has evolved over several decades toward greater emphasis on empirical work. The data revolution of the past decade is likely to have a further and profound effect on economic research. Increasingly, economists make use of newly available large-scale administrative data or private sector data that often are obtained through collaborations with private firms, giving rise to new opportunities and challenges. The rising use of non–publicly available data in economic research. Here we show the percentage of papers published in the American Economic Review (AER) that obtained an exemption from the AER’s data availability policy, as a share of all papers published by the AER that relied on any form of data (excluding simulations and laboratory experiments). Notes and comments, as well as AER Papers and Proceedings issues, are not included in the analysis. We obtained a record of exemptions directly from the AER administrative staff and coded each exemption manually to reflect public sector versus private data. Our check of nonexempt papers suggests that the AER records may possibly understate the percentage of papers that actually obtained exemptions. The asterisk indicates that data run from when the AER started collecting these data (December 2005 issue) to the September 2014 issue. To make full use of the data, we define year 2006 to cover October 2005 through September 2006, year 2007 to cover October 2006 through September 2007, and so on. Advances These new data are affecting economic research along several dimensions. Many fields have shifted from a reliance on relatively small-sample government surveys to administrative data with universal or near-universal population coverage. This shift is transformative, as it allows researchers to rigorously examine variation in wages, health, productivity, education, and other measures across different subpopulations; construct consistent long-run statistical indices; generate new quasi-experimental research designs; and track diverse outcomes from natural and controlled experiments. Perhaps even more notable is the expansion of private sector data on economic activity. These data, sometimes available from public sources but other times obtained through data-sharing agreements with private firms, can help to create more granular and real-time measurement of aggregate economic statistics. The data also offer researchers a look inside the “black box” of firms and markets by providing meaningful statistics on economic behavior such as search and information gathering, communication, decision-making, and microlevel transactions. Collaborations with data-oriented firms also create new opportunities to conduct and evaluate randomized experiments. Economic theory plays an important role in the analysis of large data sets with complex structure. It can be difficult to organize and study this type of data (or even to decide which variables to construct) without a simplifying conceptual framework, which is where economic models become useful. Better data also allow for sharper tests of existing models and tests of theories that had previously been difficult to assess. Outlook The advent of big data is already allowing for better measurement of economic effects and outcomes and is enabling novel research designs across a range of topics. Over time, these data are likely to affect the types of questions economists pose, by allowing for more focus on population variation and the analysis of a broader range of economic activities and interactions. We also expect economists to increasingly adopt the large-data statistical methods that have been developed in neighboring fields and that often may complement traditional econometric techniques. These data opportunities also raise some important challenges. Perhaps the primary one is developing methods for researchers to access and explore data in ways that respect privacy and confidentiality concerns. This is a major issue in working with both government administrative data and private sector firms. Other challenges include developing the appropriate data management and programming capabilities, as well as designing creative and scalable approaches to summarize, describe, and analyze large-scale and relatively unstructured data sets. These challenges notwithstanding, the next few decades are likely to be a very exciting time for economic research. The quality and quantity of data on economic activity are expanding rapidly. Empirical research increasingly relies on newly available large-scale administrative data or private sector data that often is obtained through collaboration with private firms. Here we highlight some challenges in accessing and using these new data. We also discuss how new data sets may change the statistical methods used by economists and the types of questions posed in empirical research.",fullPaper,jv104
Business,p494,d9,04638c67b715b9d85ae5a44afd3730b83330fb66,j104,Science,Economics in the age of big data,"Background Economic science has evolved over several decades toward greater emphasis on empirical work. The data revolution of the past decade is likely to have a further and profound effect on economic research. Increasingly, economists make use of newly available large-scale administrative data or private sector data that often are obtained through collaborations with private firms, giving rise to new opportunities and challenges. The rising use of non–publicly available data in economic research. Here we show the percentage of papers published in the American Economic Review (AER) that obtained an exemption from the AER’s data availability policy, as a share of all papers published by the AER that relied on any form of data (excluding simulations and laboratory experiments). Notes and comments, as well as AER Papers and Proceedings issues, are not included in the analysis. We obtained a record of exemptions directly from the AER administrative staff and coded each exemption manually to reflect public sector versus private data. Our check of nonexempt papers suggests that the AER records may possibly understate the percentage of papers that actually obtained exemptions. The asterisk indicates that data run from when the AER started collecting these data (December 2005 issue) to the September 2014 issue. To make full use of the data, we define year 2006 to cover October 2005 through September 2006, year 2007 to cover October 2006 through September 2007, and so on. Advances These new data are affecting economic research along several dimensions. Many fields have shifted from a reliance on relatively small-sample government surveys to administrative data with universal or near-universal population coverage. This shift is transformative, as it allows researchers to rigorously examine variation in wages, health, productivity, education, and other measures across different subpopulations; construct consistent long-run statistical indices; generate new quasi-experimental research designs; and track diverse outcomes from natural and controlled experiments. Perhaps even more notable is the expansion of private sector data on economic activity. These data, sometimes available from public sources but other times obtained through data-sharing agreements with private firms, can help to create more granular and real-time measurement of aggregate economic statistics. The data also offer researchers a look inside the “black box” of firms and markets by providing meaningful statistics on economic behavior such as search and information gathering, communication, decision-making, and microlevel transactions. Collaborations with data-oriented firms also create new opportunities to conduct and evaluate randomized experiments. Economic theory plays an important role in the analysis of large data sets with complex structure. It can be difficult to organize and study this type of data (or even to decide which variables to construct) without a simplifying conceptual framework, which is where economic models become useful. Better data also allow for sharper tests of existing models and tests of theories that had previously been difficult to assess. Outlook The advent of big data is already allowing for better measurement of economic effects and outcomes and is enabling novel research designs across a range of topics. Over time, these data are likely to affect the types of questions economists pose, by allowing for more focus on population variation and the analysis of a broader range of economic activities and interactions. We also expect economists to increasingly adopt the large-data statistical methods that have been developed in neighboring fields and that often may complement traditional econometric techniques. These data opportunities also raise some important challenges. Perhaps the primary one is developing methods for researchers to access and explore data in ways that respect privacy and confidentiality concerns. This is a major issue in working with both government administrative data and private sector firms. Other challenges include developing the appropriate data management and programming capabilities, as well as designing creative and scalable approaches to summarize, describe, and analyze large-scale and relatively unstructured data sets. These challenges notwithstanding, the next few decades are likely to be a very exciting time for economic research. The quality and quantity of data on economic activity are expanding rapidly. Empirical research increasingly relies on newly available large-scale administrative data or private sector data that often is obtained through collaboration with private firms. Here we highlight some challenges in accessing and using these new data. We also discuss how new data sets may change the statistical methods used by economists and the types of questions posed in empirical research.",fullPaper,jv104
Medicine,p495,d1,687e00a5fec7d747d18866f60b7a21973e80b04f,c92,International Symposium on Computer Architecture,The ethics of smart cities and urban science,"Software-enabled technologies and urban big data have become essential to the functioning of cities. Consequently, urban operational governance and city services are becoming highly responsive to a form of data-driven urbanism that is the key mode of production for smart cities. At the heart of data-driven urbanism is a computational understanding of city systems that reduces urban life to logic and calculative rules and procedures, which is underpinned by an instrumental rationality and realist epistemology. This rationality and epistemology are informed by and sustains urban science and urban informatics, which seek to make cities more knowable and controllable. This paper examines the forms, practices and ethics of smart cities and urban science, paying particular attention to: instrumental rationality and realist epistemology; privacy, datafication, dataveillance and geosurveillance; and data uses, such as social sorting and anticipatory governance. It argues that smart city initiatives and urban science need to be re-cast in three ways: a re-orientation in how cities are conceived; a reconfiguring of the underlying epistemology to openly recognize the contingent and relational nature of urban systems, processes and science; and the adoption of ethical principles designed to realize benefits of smart cities and urban science while reducing pernicious effects. This article is part of the themed issue ‘The ethical impact of data science’.",poster,cp92
Sociology,p495,d4,687e00a5fec7d747d18866f60b7a21973e80b04f,c92,International Symposium on Computer Architecture,The ethics of smart cities and urban science,"Software-enabled technologies and urban big data have become essential to the functioning of cities. Consequently, urban operational governance and city services are becoming highly responsive to a form of data-driven urbanism that is the key mode of production for smart cities. At the heart of data-driven urbanism is a computational understanding of city systems that reduces urban life to logic and calculative rules and procedures, which is underpinned by an instrumental rationality and realist epistemology. This rationality and epistemology are informed by and sustains urban science and urban informatics, which seek to make cities more knowable and controllable. This paper examines the forms, practices and ethics of smart cities and urban science, paying particular attention to: instrumental rationality and realist epistemology; privacy, datafication, dataveillance and geosurveillance; and data uses, such as social sorting and anticipatory governance. It argues that smart city initiatives and urban science need to be re-cast in three ways: a re-orientation in how cities are conceived; a reconfiguring of the underlying epistemology to openly recognize the contingent and relational nature of urban systems, processes and science; and the adoption of ethical principles designed to realize benefits of smart cities and urban science while reducing pernicious effects. This article is part of the themed issue ‘The ethical impact of data science’.",poster,cp92
Medicine,p496,d1,c32b03c3b5bbc97b0ec30663da1ff555f30acd95,j151,Prevention Science,Principled Missing Data Treatments,Abstract,fullPaper,jv151
Computer Science,p496,d3,c32b03c3b5bbc97b0ec30663da1ff555f30acd95,j151,Prevention Science,Principled Missing Data Treatments,Abstract,fullPaper,jv151
Medicine,p499,d1,951eab2b27c673e0ff1a20800f576d4792f60d5f,j104,Science,Crisis informatics—New data for extraordinary times,"Focus on behaviors, not on fetishizing social media tools Crisis informatics is a multidisciplinary field combining computing and social science knowledge of disasters; its central tenet is that people use personal information and communication technology to respond to disaster in creative ways to cope with uncertainty. We study and develop computational support for collection and sociobehavioral analysis of online participation (i.e., tweets and Facebook posts) to address challenges in disaster warning, response, and recovery. Because such data are rarely tidy, we offer lessons—learned the hard way, as we have made every mistake described below—with respect to the opportunities and limitations of social media research on crisis events.",fullPaper,jv104
Sociology,p499,d4,951eab2b27c673e0ff1a20800f576d4792f60d5f,j104,Science,Crisis informatics—New data for extraordinary times,"Focus on behaviors, not on fetishizing social media tools Crisis informatics is a multidisciplinary field combining computing and social science knowledge of disasters; its central tenet is that people use personal information and communication technology to respond to disaster in creative ways to cope with uncertainty. We study and develop computational support for collection and sociobehavioral analysis of online participation (i.e., tweets and Facebook posts) to address challenges in disaster warning, response, and recovery. Because such data are rarely tidy, we offer lessons—learned the hard way, as we have made every mistake described below—with respect to the opportunities and limitations of social media research on crisis events.",fullPaper,jv104
Medicine,p526,d1,8f63eed1c6aef4e96a08281563c2305ff55e7ab9,j82,BMC Bioinformatics,ImageJ2: ImageJ for the next generation of scientific image data,Abstract,fullPaper,jv82
Computer Science,p526,d3,8f63eed1c6aef4e96a08281563c2305ff55e7ab9,j82,BMC Bioinformatics,ImageJ2: ImageJ for the next generation of scientific image data,Abstract,fullPaper,jv82
Biology,p526,d5,8f63eed1c6aef4e96a08281563c2305ff55e7ab9,j82,BMC Bioinformatics,ImageJ2: ImageJ for the next generation of scientific image data,Abstract,fullPaper,jv82
Medicine,p530,d1,9cfe870e09f627e2814572aa4e1e7bff8b657fc5,j8,Nature Methods,Low-N protein engineering with data-efficient deep learning,Abstract,fullPaper,jv8
Computer Science,p530,d3,9cfe870e09f627e2814572aa4e1e7bff8b657fc5,j8,Nature Methods,Low-N protein engineering with data-efficient deep learning,Abstract,fullPaper,jv8
Biology,p530,d5,9cfe870e09f627e2814572aa4e1e7bff8b657fc5,j8,Nature Methods,Low-N protein engineering with data-efficient deep learning,Abstract,fullPaper,jv8
Medicine,p531,d1,6009556bdf3aa3a111a6ddc2c9200a59af1e13e2,j23,Proceedings of the National Academy of Sciences of the United States of America,"Active learning increases student performance in science, engineering, and mathematics","Significance The President’s Council of Advisors on Science and Technology has called for a 33% increase in the number of science, technology, engineering, and mathematics (STEM) bachelor’s degrees completed per year and recommended adoption of empirically validated teaching practices as critical to achieving that goal. The studies analyzed here document that active learning leads to increases in examination performance that would raise average grades by a half a letter, and that failure rates under traditional lecturing increase by 55% over the rates observed under active learning. The analysis supports theory claiming that calls to increase the number of students receiving STEM degrees could be answered, at least in part, by abandoning traditional lecturing in favor of active learning. To test the hypothesis that lecturing maximizes learning and course performance, we metaanalyzed 225 studies that reported data on examination scores or failure rates when comparing student performance in undergraduate science, technology, engineering, and mathematics (STEM) courses under traditional lecturing versus active learning. The effect sizes indicate that on average, student performance on examinations and concept inventories increased by 0.47 SDs under active learning (n = 158 studies), and that the odds ratio for failing was 1.95 under traditional lecturing (n = 67 studies). These results indicate that average examination scores improved by about 6% in active learning sections, and that students in classes with traditional lecturing were 1.5 times more likely to fail than were students in classes with active learning. Heterogeneity analyses indicated that both results hold across the STEM disciplines, that active learning increases scores on concept inventories more than on course examinations, and that active learning appears effective across all class sizes—although the greatest effects are in small (n ≤ 50) classes. Trim and fill analyses and fail-safe n calculations suggest that the results are not due to publication bias. The results also appear robust to variation in the methodological rigor of the included studies, based on the quality of controls over student quality and instructor identity. This is the largest and most comprehensive metaanalysis of undergraduate STEM education published to date. The results raise questions about the continued use of traditional lecturing as a control in research studies, and support active learning as the preferred, empirically validated teaching practice in regular classrooms.",fullPaper,jv23
Computer Science,p531,d3,6009556bdf3aa3a111a6ddc2c9200a59af1e13e2,j23,Proceedings of the National Academy of Sciences of the United States of America,"Active learning increases student performance in science, engineering, and mathematics","Significance The President’s Council of Advisors on Science and Technology has called for a 33% increase in the number of science, technology, engineering, and mathematics (STEM) bachelor’s degrees completed per year and recommended adoption of empirically validated teaching practices as critical to achieving that goal. The studies analyzed here document that active learning leads to increases in examination performance that would raise average grades by a half a letter, and that failure rates under traditional lecturing increase by 55% over the rates observed under active learning. The analysis supports theory claiming that calls to increase the number of students receiving STEM degrees could be answered, at least in part, by abandoning traditional lecturing in favor of active learning. To test the hypothesis that lecturing maximizes learning and course performance, we metaanalyzed 225 studies that reported data on examination scores or failure rates when comparing student performance in undergraduate science, technology, engineering, and mathematics (STEM) courses under traditional lecturing versus active learning. The effect sizes indicate that on average, student performance on examinations and concept inventories increased by 0.47 SDs under active learning (n = 158 studies), and that the odds ratio for failing was 1.95 under traditional lecturing (n = 67 studies). These results indicate that average examination scores improved by about 6% in active learning sections, and that students in classes with traditional lecturing were 1.5 times more likely to fail than were students in classes with active learning. Heterogeneity analyses indicated that both results hold across the STEM disciplines, that active learning increases scores on concept inventories more than on course examinations, and that active learning appears effective across all class sizes—although the greatest effects are in small (n ≤ 50) classes. Trim and fill analyses and fail-safe n calculations suggest that the results are not due to publication bias. The results also appear robust to variation in the methodological rigor of the included studies, based on the quality of controls over student quality and instructor identity. This is the largest and most comprehensive metaanalysis of undergraduate STEM education published to date. The results raise questions about the continued use of traditional lecturing as a control in research studies, and support active learning as the preferred, empirically validated teaching practice in regular classrooms.",fullPaper,jv23
Mathematics,p531,d6,6009556bdf3aa3a111a6ddc2c9200a59af1e13e2,j23,Proceedings of the National Academy of Sciences of the United States of America,"Active learning increases student performance in science, engineering, and mathematics","Significance The President’s Council of Advisors on Science and Technology has called for a 33% increase in the number of science, technology, engineering, and mathematics (STEM) bachelor’s degrees completed per year and recommended adoption of empirically validated teaching practices as critical to achieving that goal. The studies analyzed here document that active learning leads to increases in examination performance that would raise average grades by a half a letter, and that failure rates under traditional lecturing increase by 55% over the rates observed under active learning. The analysis supports theory claiming that calls to increase the number of students receiving STEM degrees could be answered, at least in part, by abandoning traditional lecturing in favor of active learning. To test the hypothesis that lecturing maximizes learning and course performance, we metaanalyzed 225 studies that reported data on examination scores or failure rates when comparing student performance in undergraduate science, technology, engineering, and mathematics (STEM) courses under traditional lecturing versus active learning. The effect sizes indicate that on average, student performance on examinations and concept inventories increased by 0.47 SDs under active learning (n = 158 studies), and that the odds ratio for failing was 1.95 under traditional lecturing (n = 67 studies). These results indicate that average examination scores improved by about 6% in active learning sections, and that students in classes with traditional lecturing were 1.5 times more likely to fail than were students in classes with active learning. Heterogeneity analyses indicated that both results hold across the STEM disciplines, that active learning increases scores on concept inventories more than on course examinations, and that active learning appears effective across all class sizes—although the greatest effects are in small (n ≤ 50) classes. Trim and fill analyses and fail-safe n calculations suggest that the results are not due to publication bias. The results also appear robust to variation in the methodological rigor of the included studies, based on the quality of controls over student quality and instructor identity. This is the largest and most comprehensive metaanalysis of undergraduate STEM education published to date. The results raise questions about the continued use of traditional lecturing as a control in research studies, and support active learning as the preferred, empirically validated teaching practice in regular classrooms.",fullPaper,jv23
Medicine,p550,d1,ef2afdce9b71657522d743178ab39fb03a394647,j161,Journal of Medical Engineering & Technology,"Big data analytics in medical engineering and healthcare: methods, advances and challenges","Abstract Big data analytics are gaining popularity in medical engineering and healthcare use cases. Stakeholders are finding big data analytics reduce medical costs and personalise medical services for each individual patient. Big data analytics can be used in large-scale genetics studies, public health, personalised and precision medicine, new drug development, etc. The introduction of the types, sources, and features of big data in healthcare as well as the applications and benefits of big data and big data analytics in healthcare is key to understanding healthcare big data and will be discussed in this article. Major methods, platforms and tools of big data analytics in medical engineering and healthcare are also presented. Advances and technology progress of big data analytics in healthcare are introduced, which includes artificial intelligence (AI) with big data, infrastructure and cloud computing, advanced computation and data processing, privacy and cybersecurity, health economic outcomes and technology management, and smart healthcare with sensing, wearable devices and Internet of things (IoT). Current challenges of dealing with big data and big data analytics in medical engineering and healthcare as well as future work are also presented.",fullPaper,jv161
Business,p550,d9,ef2afdce9b71657522d743178ab39fb03a394647,j161,Journal of Medical Engineering & Technology,"Big data analytics in medical engineering and healthcare: methods, advances and challenges","Abstract Big data analytics are gaining popularity in medical engineering and healthcare use cases. Stakeholders are finding big data analytics reduce medical costs and personalise medical services for each individual patient. Big data analytics can be used in large-scale genetics studies, public health, personalised and precision medicine, new drug development, etc. The introduction of the types, sources, and features of big data in healthcare as well as the applications and benefits of big data and big data analytics in healthcare is key to understanding healthcare big data and will be discussed in this article. Major methods, platforms and tools of big data analytics in medical engineering and healthcare are also presented. Advances and technology progress of big data analytics in healthcare are introduced, which includes artificial intelligence (AI) with big data, infrastructure and cloud computing, advanced computation and data processing, privacy and cybersecurity, health economic outcomes and technology management, and smart healthcare with sensing, wearable devices and Internet of things (IoT). Current challenges of dealing with big data and big data analytics in medical engineering and healthcare as well as future work are also presented.",fullPaper,jv161
Medicine,p556,d1,a461233e56079fc5af6e48d75f38be8c9ff87c1e,j112,Environmental Science and Technology,Machine Learning: New Ideas and Tools in Environmental Science and Engineering.,"The rapid increase in both the quantity and complexity of data that are being generated daily in the field of environmental science and engineering (ESE) demands accompanied advancement in data analytics. Advanced data analysis approaches, such as machine learning (ML), have become indispensable tools for revealing hidden patterns or deducing correlations for which conventional analytical methods face limitations or challenges. However, ML concepts and practices have not been widely utilized by researchers in ESE. This feature explores the potential of ML to revolutionize data analysis and modeling in the ESE field, and covers the essential knowledge needed for such applications. First, we use five examples to illustrate how ML addresses complex ESE problems. We then summarize four major types of applications of ML in ESE: making predictions; extracting feature importance; detecting anomalies; and discovering new materials or chemicals. Next, we introduce the essential knowledge required and current shortcomings in ML applications in ESE, with a focus on three important but often overlooked components when applying ML: correct model development, proper model interpretation, and sound applicability analysis. Finally, we discuss challenges and future opportunities in the application of ML tools in ESE to highlight the potential of ML in this field.",fullPaper,jv112
Medicine,p557,d1,6bec0106bebc93fc30ec47af9779d7e327639034,c56,International Conference on Automated Software Engineering,Machine learning and data science in soft materials engineering,"In many branches of materials science it is now routine to generate data sets of such large size and dimensionality that conventional methods of analysis fail. Paradigms and tools from data science and machine learning can provide scalable approaches to identify and extract trends and patterns within voluminous data sets, perform guided traversals of high-dimensional phase spaces, and furnish data-driven strategies for inverse materials design. This topical review provides an accessible introduction to machine learning tools in the context of soft and biological materials by ‘de-jargonizing’ data science terminology, presenting a taxonomy of machine learning techniques, and surveying the mathematical underpinnings and software implementations of popular tools, including principal component analysis, independent component analysis, diffusion maps, support vector machines, and relative entropy. We present illustrative examples of machine learning applications in soft matter, including inverse design of self-assembling materials, nonlinear learning of protein folding landscapes, high-throughput antimicrobial peptide design, and data-driven materials design engines. We close with an outlook on the challenges and opportunities for the field.",poster,cp56
Physics,p557,d2,6bec0106bebc93fc30ec47af9779d7e327639034,c56,International Conference on Automated Software Engineering,Machine learning and data science in soft materials engineering,"In many branches of materials science it is now routine to generate data sets of such large size and dimensionality that conventional methods of analysis fail. Paradigms and tools from data science and machine learning can provide scalable approaches to identify and extract trends and patterns within voluminous data sets, perform guided traversals of high-dimensional phase spaces, and furnish data-driven strategies for inverse materials design. This topical review provides an accessible introduction to machine learning tools in the context of soft and biological materials by ‘de-jargonizing’ data science terminology, presenting a taxonomy of machine learning techniques, and surveying the mathematical underpinnings and software implementations of popular tools, including principal component analysis, independent component analysis, diffusion maps, support vector machines, and relative entropy. We present illustrative examples of machine learning applications in soft matter, including inverse design of self-assembling materials, nonlinear learning of protein folding landscapes, high-throughput antimicrobial peptide design, and data-driven materials design engines. We close with an outlook on the challenges and opportunities for the field.",poster,cp56
Medicine,p561,d1,06eb3c3ccae16fced2222f8a45877906f54f2164,j8,Nature Methods,Unified rational protein engineering with sequence-based deep representation learning,Abstract,fullPaper,jv8
Computer Science,p561,d3,06eb3c3ccae16fced2222f8a45877906f54f2164,j8,Nature Methods,Unified rational protein engineering with sequence-based deep representation learning,Abstract,fullPaper,jv8
Medicine,p578,d1,a0022ee86e99981f0cde48fb4258b8ae21b96c6e,j168,Nature Reviews. Cancer,Harnessing multimodal data integration to advance precision oncology,Abstract,fullPaper,jv168
Medicine,p587,d1,730fba26faa7f91dc6742a0c3521eb439670a825,j8,Nature Methods,Machine-learning-guided directed evolution for protein engineering,Abstract,fullPaper,jv8
Medicine,p592,d1,ade6d1a34ce18c333f531bf430118c5963a2f260,c19,International Conference on Conceptual Structures,Off-target Effects in CRISPR/Cas9-mediated Genome Engineering,Abstract,poster,cp19
Biology,p592,d5,ade6d1a34ce18c333f531bf430118c5963a2f260,c19,International Conference on Conceptual Structures,Off-target Effects in CRISPR/Cas9-mediated Genome Engineering,Abstract,poster,cp19
Medicine,p595,d1,718577588f38727ff28cf5321a5772a6fcdc1865,j173,IEEE Transactions on Pattern Analysis and Machine Intelligence,Gaussian Processes for Data-Efficient Learning in Robotics and Control,"Autonomous learning has been a promising direction in control and robotics for more than a decade since data-driven learning allows to reduce the amount of engineering knowledge, which is otherwise required. However, autonomous reinforcement learning (RL) approaches typically require many interactions with the system to learn controllers, which is a practical limitation in real systems, such as robots, where many interactions can be impractical and time consuming. To address this problem, current learning approaches typically require task-specific knowledge in form of expert demonstrations, realistic simulators, pre-shaped policies, or specific knowledge about the underlying dynamics. In this paper, we follow a different approach and speed up learning by extracting more information from data. In particular, we learn a probabilistic, non-parametric Gaussian process transition model of the system. By explicitly incorporating model uncertainty into long-term planning and controller learning our approach reduces the effects of model errors, a key problem in model-based learning. Compared to state-of-the art RL our model-based policy search method achieves an unprecedented speed of learning. We demonstrate its applicability to autonomous learning in real robot and control tasks.",fullPaper,jv173
Computer Science,p595,d3,718577588f38727ff28cf5321a5772a6fcdc1865,j173,IEEE Transactions on Pattern Analysis and Machine Intelligence,Gaussian Processes for Data-Efficient Learning in Robotics and Control,"Autonomous learning has been a promising direction in control and robotics for more than a decade since data-driven learning allows to reduce the amount of engineering knowledge, which is otherwise required. However, autonomous reinforcement learning (RL) approaches typically require many interactions with the system to learn controllers, which is a practical limitation in real systems, such as robots, where many interactions can be impractical and time consuming. To address this problem, current learning approaches typically require task-specific knowledge in form of expert demonstrations, realistic simulators, pre-shaped policies, or specific knowledge about the underlying dynamics. In this paper, we follow a different approach and speed up learning by extracting more information from data. In particular, we learn a probabilistic, non-parametric Gaussian process transition model of the system. By explicitly incorporating model uncertainty into long-term planning and controller learning our approach reduces the effects of model errors, a key problem in model-based learning. Compared to state-of-the art RL our model-based policy search method achieves an unprecedented speed of learning. We demonstrate its applicability to autonomous learning in real robot and control tasks.",fullPaper,jv173
Mathematics,p595,d6,718577588f38727ff28cf5321a5772a6fcdc1865,j173,IEEE Transactions on Pattern Analysis and Machine Intelligence,Gaussian Processes for Data-Efficient Learning in Robotics and Control,"Autonomous learning has been a promising direction in control and robotics for more than a decade since data-driven learning allows to reduce the amount of engineering knowledge, which is otherwise required. However, autonomous reinforcement learning (RL) approaches typically require many interactions with the system to learn controllers, which is a practical limitation in real systems, such as robots, where many interactions can be impractical and time consuming. To address this problem, current learning approaches typically require task-specific knowledge in form of expert demonstrations, realistic simulators, pre-shaped policies, or specific knowledge about the underlying dynamics. In this paper, we follow a different approach and speed up learning by extracting more information from data. In particular, we learn a probabilistic, non-parametric Gaussian process transition model of the system. By explicitly incorporating model uncertainty into long-term planning and controller learning our approach reduces the effects of model errors, a key problem in model-based learning. Compared to state-of-the art RL our model-based policy search method achieves an unprecedented speed of learning. We demonstrate its applicability to autonomous learning in real robot and control tasks.",fullPaper,jv173
Medicine,p607,d1,8aafd1f3bb8fd308f9989b6cb7df541c40dc5779,c44,Italian National Conference on Sensors,A Review of Distributed Optical Fiber Sensors for Civil Engineering Applications,"The application of structural health monitoring (SHM) systems to civil engineering structures has been a developing studied and practiced topic, that has allowed for a better understanding of structures’ conditions and increasingly lead to a more cost-effective management of those infrastructures. In this field, the use of fiber optic sensors has been studied, discussed and practiced with encouraging results. The possibility of understanding and monitor the distributed behavior of extensive stretches of critical structures it’s an enormous advantage that distributed fiber optic sensing provides to SHM systems. In the past decade, several R & D studies have been performed with the goal of improving the knowledge and developing new techniques associated with the application of distributed optical fiber sensors (DOFS) in order to widen the range of applications of these sensors and also to obtain more correct and reliable data. This paper presents, after a brief introduction to the theoretical background of DOFS, the latest developments related with the improvement of these products by presenting a wide range of laboratory experiments as well as an extended review of their diverse applications in civil engineering structures.",fullPaper,cp44
Computer Science,p607,d3,8aafd1f3bb8fd308f9989b6cb7df541c40dc5779,c44,Italian National Conference on Sensors,A Review of Distributed Optical Fiber Sensors for Civil Engineering Applications,"The application of structural health monitoring (SHM) systems to civil engineering structures has been a developing studied and practiced topic, that has allowed for a better understanding of structures’ conditions and increasingly lead to a more cost-effective management of those infrastructures. In this field, the use of fiber optic sensors has been studied, discussed and practiced with encouraging results. The possibility of understanding and monitor the distributed behavior of extensive stretches of critical structures it’s an enormous advantage that distributed fiber optic sensing provides to SHM systems. In the past decade, several R & D studies have been performed with the goal of improving the knowledge and developing new techniques associated with the application of distributed optical fiber sensors (DOFS) in order to widen the range of applications of these sensors and also to obtain more correct and reliable data. This paper presents, after a brief introduction to the theoretical background of DOFS, the latest developments related with the improvement of these products by presenting a wide range of laboratory experiments as well as an extended review of their diverse applications in civil engineering structures.",fullPaper,cp44
Medicine,p632,d1,98637661d9c5d9a6b749ed596b96e2d1fb1f9be3,c28,International Conference on Contemporary Computing,A Benchmark for Data Imputation Methods,"With the increasing importance and complexity of data pipelines, data quality became one of the key challenges in modern software applications. The importance of data quality has been recognized beyond the field of data engineering and database management systems (DBMSs). Also, for machine learning (ML) applications, high data quality standards are crucial to ensure robust predictive performance and responsible usage of automated decision making. One of the most frequent data quality problems is missing values. Incomplete datasets can break data pipelines and can have a devastating impact on downstream ML applications when not detected. While statisticians and, more recently, ML researchers have introduced a variety of approaches to impute missing values, comprehensive benchmarks comparing classical and modern imputation approaches under fair and realistic conditions are underrepresented. Here, we aim to fill this gap. We conduct a comprehensive suite of experiments on a large number of datasets with heterogeneous data and realistic missingness conditions, comparing both novel deep learning approaches and classical ML imputation methods when either only test or train and test data are affected by missing data. Each imputation method is evaluated regarding the imputation quality and the impact imputation has on a downstream ML task. Our results provide valuable insights into the performance of a variety of imputation methods under realistic conditions. We hope that our results help researchers and engineers to guide their data preprocessing method selection for automated data quality improvement.",poster,cp28
Computer Science,p632,d3,98637661d9c5d9a6b749ed596b96e2d1fb1f9be3,c28,International Conference on Contemporary Computing,A Benchmark for Data Imputation Methods,"With the increasing importance and complexity of data pipelines, data quality became one of the key challenges in modern software applications. The importance of data quality has been recognized beyond the field of data engineering and database management systems (DBMSs). Also, for machine learning (ML) applications, high data quality standards are crucial to ensure robust predictive performance and responsible usage of automated decision making. One of the most frequent data quality problems is missing values. Incomplete datasets can break data pipelines and can have a devastating impact on downstream ML applications when not detected. While statisticians and, more recently, ML researchers have introduced a variety of approaches to impute missing values, comprehensive benchmarks comparing classical and modern imputation approaches under fair and realistic conditions are underrepresented. Here, we aim to fill this gap. We conduct a comprehensive suite of experiments on a large number of datasets with heterogeneous data and realistic missingness conditions, comparing both novel deep learning approaches and classical ML imputation methods when either only test or train and test data are affected by missing data. Each imputation method is evaluated regarding the imputation quality and the impact imputation has on a downstream ML task. Our results provide valuable insights into the performance of a variety of imputation methods under realistic conditions. We hope that our results help researchers and engineers to guide their data preprocessing method selection for automated data quality improvement.",poster,cp28
Medicine,p655,d1,026a79ea386a75113d06acc0e02016f13c673fc1,j121,Nature Communications,Combining mechanistic and machine learning models for predictive engineering and optimization of tryptophan metabolism,Abstract,fullPaper,jv121
Computer Science,p655,d3,026a79ea386a75113d06acc0e02016f13c673fc1,j121,Nature Communications,Combining mechanistic and machine learning models for predictive engineering and optimization of tryptophan metabolism,Abstract,fullPaper,jv121
Medicine,p672,d1,f0fedbd445ad489ed20deb14fced8f11a5b8ba4b,j76,Frontiers in Genetics,Digital Twins in Health Care: Ethical Implications of an Emerging Engineering Paradigm,"Personalized medicine uses fine grained information on individual persons, to pinpoint deviations from the normal. ‘Digital Twins’ in engineering provide a conceptual framework to analyze these emerging data-driven health care practices, as well as their conceptual and ethical implications for therapy, preventative care and human enhancement. Digital Twins stand for a specific engineering paradigm, where individual physical artifacts are paired with digital models that dynamically reflects the status of those artifacts. When applied to persons, Digital Twins are an emerging technology that builds on in silico representations of an individual that dynamically reflect molecular status, physiological status and life style over time. We use Digital Twins as the hypothesis that one would be in the possession of very detailed bio-physical and lifestyle information of a person over time. This perspective redefines the concept of ‘normality’ or ‘health,’ as a set of patterns that are regular for a particular individual, against the backdrop of patterns observed in the population. This perspective also will impact what is considered therapy and what is enhancement, as can be illustrated with the cases of the ‘asymptomatic ill’ and life extension via anti-aging medicine. These changes are the consequence of how meaning is derived, in case measurement data is available. Moral distinctions namely may be based on patterns found in these data and the meanings that are grafted on these patterns. Ethical and societal implications of Digital Twins are explored. Digital Twins imply a data-driven approach to health care. This approach has the potential to deliver significant societal benefits, and can function as a social equalizer, by allowing for effective equalizing enhancement interventions. It can as well though be a driver for inequality, given the fact that a Digital Twin might not be an accessible technology for everyone, and given the fact that patterns identified across a population of Digital Twins can lead to segmentation and discrimination. This duality calls for governance as this emerging technology matures, including measures that ensure transparency of data usage and derived benefits, and data privacy.",fullPaper,jv76
Psychology,p672,d10,f0fedbd445ad489ed20deb14fced8f11a5b8ba4b,j76,Frontiers in Genetics,Digital Twins in Health Care: Ethical Implications of an Emerging Engineering Paradigm,"Personalized medicine uses fine grained information on individual persons, to pinpoint deviations from the normal. ‘Digital Twins’ in engineering provide a conceptual framework to analyze these emerging data-driven health care practices, as well as their conceptual and ethical implications for therapy, preventative care and human enhancement. Digital Twins stand for a specific engineering paradigm, where individual physical artifacts are paired with digital models that dynamically reflects the status of those artifacts. When applied to persons, Digital Twins are an emerging technology that builds on in silico representations of an individual that dynamically reflect molecular status, physiological status and life style over time. We use Digital Twins as the hypothesis that one would be in the possession of very detailed bio-physical and lifestyle information of a person over time. This perspective redefines the concept of ‘normality’ or ‘health,’ as a set of patterns that are regular for a particular individual, against the backdrop of patterns observed in the population. This perspective also will impact what is considered therapy and what is enhancement, as can be illustrated with the cases of the ‘asymptomatic ill’ and life extension via anti-aging medicine. These changes are the consequence of how meaning is derived, in case measurement data is available. Moral distinctions namely may be based on patterns found in these data and the meanings that are grafted on these patterns. Ethical and societal implications of Digital Twins are explored. Digital Twins imply a data-driven approach to health care. This approach has the potential to deliver significant societal benefits, and can function as a social equalizer, by allowing for effective equalizing enhancement interventions. It can as well though be a driver for inequality, given the fact that a Digital Twin might not be an accessible technology for everyone, and given the fact that patterns identified across a population of Digital Twins can lead to segmentation and discrimination. This duality calls for governance as this emerging technology matures, including measures that ensure transparency of data usage and derived benefits, and data privacy.",fullPaper,jv76
Medicine,p679,d1,4d2758c7bc95e5f296a68a9ac6908771d6a81168,j135,PLoS ONE,Inferring Regulatory Networks from Expression Data Using Tree-Based Methods,"One of the pressing open problems of computational systems biology is the elucidation of the topology of genetic regulatory networks (GRNs) using high throughput genomic data, in particular microarray gene expression data. The Dialogue for Reverse Engineering Assessments and Methods (DREAM) challenge aims to evaluate the success of GRN inference algorithms on benchmarks of simulated data. In this article, we present GENIE3, a new algorithm for the inference of GRNs that was best performer in the DREAM4 In Silico Multifactorial challenge. GENIE3 decomposes the prediction of a regulatory network between p genes into p different regression problems. In each of the regression problems, the expression pattern of one of the genes (target gene) is predicted from the expression patterns of all the other genes (input genes), using tree-based ensemble methods Random Forests or Extra-Trees. The importance of an input gene in the prediction of the target gene expression pattern is taken as an indication of a putative regulatory link. Putative regulatory links are then aggregated over all genes to provide a ranking of interactions from which the whole network is reconstructed. In addition to performing well on the DREAM4 In Silico Multifactorial challenge simulated data, we show that GENIE3 compares favorably with existing algorithms to decipher the genetic regulatory network of Escherichia coli. It doesn't make any assumption about the nature of gene regulation, can deal with combinatorial and non-linear interactions, produces directed GRNs, and is fast and scalable. In conclusion, we propose a new algorithm for GRN inference that performs well on both synthetic and real gene expression data. The algorithm, based on feature selection with tree-based ensemble methods, is simple and generic, making it adaptable to other types of genomic data and interactions.",fullPaper,jv135
Biology,p679,d5,4d2758c7bc95e5f296a68a9ac6908771d6a81168,j135,PLoS ONE,Inferring Regulatory Networks from Expression Data Using Tree-Based Methods,"One of the pressing open problems of computational systems biology is the elucidation of the topology of genetic regulatory networks (GRNs) using high throughput genomic data, in particular microarray gene expression data. The Dialogue for Reverse Engineering Assessments and Methods (DREAM) challenge aims to evaluate the success of GRN inference algorithms on benchmarks of simulated data. In this article, we present GENIE3, a new algorithm for the inference of GRNs that was best performer in the DREAM4 In Silico Multifactorial challenge. GENIE3 decomposes the prediction of a regulatory network between p genes into p different regression problems. In each of the regression problems, the expression pattern of one of the genes (target gene) is predicted from the expression patterns of all the other genes (input genes), using tree-based ensemble methods Random Forests or Extra-Trees. The importance of an input gene in the prediction of the target gene expression pattern is taken as an indication of a putative regulatory link. Putative regulatory links are then aggregated over all genes to provide a ranking of interactions from which the whole network is reconstructed. In addition to performing well on the DREAM4 In Silico Multifactorial challenge simulated data, we show that GENIE3 compares favorably with existing algorithms to decipher the genetic regulatory network of Escherichia coli. It doesn't make any assumption about the nature of gene regulation, can deal with combinatorial and non-linear interactions, produces directed GRNs, and is fast and scalable. In conclusion, we propose a new algorithm for GRN inference that performs well on both synthetic and real gene expression data. The algorithm, based on feature selection with tree-based ensemble methods, is simple and generic, making it adaptable to other types of genomic data and interactions.",fullPaper,jv135
Medicine,p699,d1,d77934caf9d8195099ba2188059ecba7a9c324a7,j60,Nature,Microbial engineering for the production of advanced biofuels,Abstract,fullPaper,jv60
Business,p699,d9,d77934caf9d8195099ba2188059ecba7a9c324a7,j60,Nature,Microbial engineering for the production of advanced biofuels,Abstract,fullPaper,jv60
Medicine,p714,d1,1077bce87735472e03ac3685cf5a8edfea1710c4,j23,Proceedings of the National Academy of Sciences of the United States of America,"Female peers in small work groups enhance women's motivation, verbal participation, and career aspirations in engineering","Significance Advances in science, technology, engineering, and mathematics are critical to the American economy and require a robust workforce. The scarcity of women in this workforce is a well-recognized problem, but data-driven solutions to this problem are less common. We provide experimental evidence showing that gender composition of small groups in engineering has a substantial impact on undergraduate women’s persistence. Women participate more actively in engineering groups when members are mostly female vs. mostly male or in equal gender proportions. Women feel less anxious in female-majority groups vs. minority groups, especially as first-year students. Gender-parity groups are less effective than female-majority groups in promoting verbal participation. Female peers protect women’s confidence and engineering career aspirations despite masculine stereotypes about engineering. For years, public discourse in science education, technology, and policy-making has focused on the “leaky pipeline” problem: the observation that fewer women than men enter science, technology, engineering, and mathematics fields and more women than men leave. Less attention has focused on experimentally testing solutions to this problem. We report an experiment investigating one solution: we created “microenvironments” (small groups) in engineering with varying proportions of women to identify which environment increases motivation and participation, and whether outcomes depend on students’ academic stage. Female engineering students were randomly assigned to one of three engineering groups of varying sex composition: 75% women, 50% women, or 25% women. For first-years, group composition had a large effect: women in female-majority and sex-parity groups felt less anxious than women in female-minority groups. However, among advanced students, sex composition had no effect on anxiety. Importantly, group composition significantly affected verbal participation, regardless of women’s academic seniority: women participated more in female-majority groups than sex-parity or female-minority groups. Additionally, when assigned to female-minority groups, women who harbored implicit masculine stereotypes about engineering reported less confidence and engineering career aspirations. However, in sex-parity and female-majority groups, confidence and career aspirations remained high regardless of implicit stereotypes. These data suggest that creating small groups with high proportions of women in otherwise male-dominated fields is one way to keep women engaged and aspiring toward engineering careers. Although sex parity works sometimes, it is insufficient to boost women’s verbal participation in group work, which often affects learning and mastery.",fullPaper,jv23
Psychology,p714,d10,1077bce87735472e03ac3685cf5a8edfea1710c4,j23,Proceedings of the National Academy of Sciences of the United States of America,"Female peers in small work groups enhance women's motivation, verbal participation, and career aspirations in engineering","Significance Advances in science, technology, engineering, and mathematics are critical to the American economy and require a robust workforce. The scarcity of women in this workforce is a well-recognized problem, but data-driven solutions to this problem are less common. We provide experimental evidence showing that gender composition of small groups in engineering has a substantial impact on undergraduate women’s persistence. Women participate more actively in engineering groups when members are mostly female vs. mostly male or in equal gender proportions. Women feel less anxious in female-majority groups vs. minority groups, especially as first-year students. Gender-parity groups are less effective than female-majority groups in promoting verbal participation. Female peers protect women’s confidence and engineering career aspirations despite masculine stereotypes about engineering. For years, public discourse in science education, technology, and policy-making has focused on the “leaky pipeline” problem: the observation that fewer women than men enter science, technology, engineering, and mathematics fields and more women than men leave. Less attention has focused on experimentally testing solutions to this problem. We report an experiment investigating one solution: we created “microenvironments” (small groups) in engineering with varying proportions of women to identify which environment increases motivation and participation, and whether outcomes depend on students’ academic stage. Female engineering students were randomly assigned to one of three engineering groups of varying sex composition: 75% women, 50% women, or 25% women. For first-years, group composition had a large effect: women in female-majority and sex-parity groups felt less anxious than women in female-minority groups. However, among advanced students, sex composition had no effect on anxiety. Importantly, group composition significantly affected verbal participation, regardless of women’s academic seniority: women participated more in female-majority groups than sex-parity or female-minority groups. Additionally, when assigned to female-minority groups, women who harbored implicit masculine stereotypes about engineering reported less confidence and engineering career aspirations. However, in sex-parity and female-majority groups, confidence and career aspirations remained high regardless of implicit stereotypes. These data suggest that creating small groups with high proportions of women in otherwise male-dominated fields is one way to keep women engaged and aspiring toward engineering careers. Although sex parity works sometimes, it is insufficient to boost women’s verbal participation in group work, which often affects learning and mastery.",fullPaper,jv23
Medicine,p716,d1,ac6d8eaffe0481e51955eff242acc5b3faea925e,j192,Journal of Synchrotron Radiation,Data Analysis WorkbeNch (DAWN),DAWN is a generic data analysis software platform that has been developed for use at synchrotron beamlines for data visualization and analysis. Its generic design makes it suitable for use in a range of scientific and engineering applications.,fullPaper,jv192
Computer Science,p716,d3,ac6d8eaffe0481e51955eff242acc5b3faea925e,j192,Journal of Synchrotron Radiation,Data Analysis WorkbeNch (DAWN),DAWN is a generic data analysis software platform that has been developed for use at synchrotron beamlines for data visualization and analysis. Its generic design makes it suitable for use in a range of scientific and engineering applications.,fullPaper,jv192
Medicine,p717,d1,29196eb8c80a6fd6a159373f14ff323f081a8b7a,j104,Science,Physical and Virtual Laboratories in Science and Engineering Education,"The world needs young people who are skillful in and enthusiastic about science and who view science as their future career field. Ensuring that we will have such young people requires initiatives that engage students in interesting and motivating science experiences. Today, students can investigate scientific phenomena using the tools, data collection techniques, models, and theories of science in physical laboratories that support interactions with the material world or in virtual laboratories that take advantage of simulations. Here, we review a selection of the literature to contrast the value of physical and virtual investigations and to offer recommendations for combining the two to strengthen science learning.",fullPaper,jv104
Medicine,p720,d1,0dde092a85ce6451fc28685e7816a0df9dbaa21b,j193,Nature Genetics,Reverse engineering of regulatory networks in human B cells,Abstract,fullPaper,jv193
Biology,p720,d5,0dde092a85ce6451fc28685e7816a0df9dbaa21b,j193,Nature Genetics,Reverse engineering of regulatory networks in human B cells,Abstract,fullPaper,jv193
Medicine,p736,d1,f7d3f3a23c1b284a17adc93a922c56be38d221df,c9,Big Data,"On the Safety of Machine Learning: Cyber-Physical Systems, Decision Sciences, and Data Products","Machine learning algorithms increasingly influence our decisions and interact with us in all parts of our daily lives. Therefore, just as we consider the safety of power plants, highways, and a variety of other engineered socio-technical systems, we must also take into account the safety of systems involving machine learning. Heretofore, the definition of safety has not been formalized in a machine learning context. In this article, we do so by defining machine learning safety in terms of risk, epistemic uncertainty, and the harm incurred by unwanted outcomes. We then use this definition to examine safety in all sorts of applications in cyber-physical systems, decision sciences, and data products. We find that the foundational principle of modern statistical machine learning, empirical risk minimization, is not always a sufficient objective. We discuss how four different categories of strategies for achieving safety in engineering, including inherently safe design, safety reserves, safe fail, and procedural safeguards can be mapped to a machine learning context. We then discuss example techniques that can be adopted in each category, such as considering interpretability and causality of predictive models, objective functions beyond expected prediction accuracy, human involvement for labeling difficult or rare examples, and user experience design of software and open data.",fullPaper,cp9
Computer Science,p736,d3,f7d3f3a23c1b284a17adc93a922c56be38d221df,c9,Big Data,"On the Safety of Machine Learning: Cyber-Physical Systems, Decision Sciences, and Data Products","Machine learning algorithms increasingly influence our decisions and interact with us in all parts of our daily lives. Therefore, just as we consider the safety of power plants, highways, and a variety of other engineered socio-technical systems, we must also take into account the safety of systems involving machine learning. Heretofore, the definition of safety has not been formalized in a machine learning context. In this article, we do so by defining machine learning safety in terms of risk, epistemic uncertainty, and the harm incurred by unwanted outcomes. We then use this definition to examine safety in all sorts of applications in cyber-physical systems, decision sciences, and data products. We find that the foundational principle of modern statistical machine learning, empirical risk minimization, is not always a sufficient objective. We discuss how four different categories of strategies for achieving safety in engineering, including inherently safe design, safety reserves, safe fail, and procedural safeguards can be mapped to a machine learning context. We then discuss example techniques that can be adopted in each category, such as considering interpretability and causality of predictive models, objective functions beyond expected prediction accuracy, human involvement for labeling difficult or rare examples, and user experience design of software and open data.",fullPaper,cp9
Mathematics,p736,d6,f7d3f3a23c1b284a17adc93a922c56be38d221df,c9,Big Data,"On the Safety of Machine Learning: Cyber-Physical Systems, Decision Sciences, and Data Products","Machine learning algorithms increasingly influence our decisions and interact with us in all parts of our daily lives. Therefore, just as we consider the safety of power plants, highways, and a variety of other engineered socio-technical systems, we must also take into account the safety of systems involving machine learning. Heretofore, the definition of safety has not been formalized in a machine learning context. In this article, we do so by defining machine learning safety in terms of risk, epistemic uncertainty, and the harm incurred by unwanted outcomes. We then use this definition to examine safety in all sorts of applications in cyber-physical systems, decision sciences, and data products. We find that the foundational principle of modern statistical machine learning, empirical risk minimization, is not always a sufficient objective. We discuss how four different categories of strategies for achieving safety in engineering, including inherently safe design, safety reserves, safe fail, and procedural safeguards can be mapped to a machine learning context. We then discuss example techniques that can be adopted in each category, such as considering interpretability and causality of predictive models, objective functions beyond expected prediction accuracy, human involvement for labeling difficult or rare examples, and user experience design of software and open data.",fullPaper,cp9
Medicine,p751,d1,89535aa63bc5dac6f3beb60b813abb77aa4309d1,c9,Big Data,Critique and Contribute: A Practice-Based Framework for Improving Critical Data Studies and Data Science,"Abstract What would data science look like if its key critics were engaged to help improve it, and how might critiques of data science improve with an approach that considers the day-to-day practices of data science? This article argues for scholars to bridge the conversations that seek to critique data science and those that seek to advance data science practice to identify and create the social and organizational arrangements necessary for a more ethical data science. We summarize four critiques that are commonly made in critical data studies: data are inherently interpretive, data are inextricable from context, data are mediated through the sociomaterial arrangements that produce them, and data serve as a medium for the negotiation and communication of values. We present qualitative research with academic data scientists, “data for good” projects, and specialized cross-disciplinary engineering teams to show evidence of these critiques in the day-to-day experience of data scientists as they acknowledge and grapple with the complexities of their work. Using ethnographic vignettes from two large multiresearcher field sites, we develop a set of concepts for analyzing and advancing the practice of data science and improving critical data studies, including (1) communication is central to the data science endeavor; (2) making sense of data is a collective process; (3) data are starting, not end points, and (4) data are sets of stories. We conclude with two calls to action for researchers and practitioners in data science and critical data studies alike. First, creating opportunities for bringing social scientific and humanistic expertise into data science practice simultaneously will advance both data science and critical data studies. Second, practitioners should leverage the insights from critical data studies to build new kinds of organizational arrangements, which we argue will help advance a more ethical data science. Engaging the insights of critical data studies will improve data science. Careful attention to the practices of data science will improve scholarly critiques. Genuine collaborative conversations between these different communities will help push for more ethical, and better, ways of knowing in increasingly datum-saturated societies.",fullPaper,cp9
Computer Science,p751,d3,89535aa63bc5dac6f3beb60b813abb77aa4309d1,c9,Big Data,Critique and Contribute: A Practice-Based Framework for Improving Critical Data Studies and Data Science,"Abstract What would data science look like if its key critics were engaged to help improve it, and how might critiques of data science improve with an approach that considers the day-to-day practices of data science? This article argues for scholars to bridge the conversations that seek to critique data science and those that seek to advance data science practice to identify and create the social and organizational arrangements necessary for a more ethical data science. We summarize four critiques that are commonly made in critical data studies: data are inherently interpretive, data are inextricable from context, data are mediated through the sociomaterial arrangements that produce them, and data serve as a medium for the negotiation and communication of values. We present qualitative research with academic data scientists, “data for good” projects, and specialized cross-disciplinary engineering teams to show evidence of these critiques in the day-to-day experience of data scientists as they acknowledge and grapple with the complexities of their work. Using ethnographic vignettes from two large multiresearcher field sites, we develop a set of concepts for analyzing and advancing the practice of data science and improving critical data studies, including (1) communication is central to the data science endeavor; (2) making sense of data is a collective process; (3) data are starting, not end points, and (4) data are sets of stories. We conclude with two calls to action for researchers and practitioners in data science and critical data studies alike. First, creating opportunities for bringing social scientific and humanistic expertise into data science practice simultaneously will advance both data science and critical data studies. Second, practitioners should leverage the insights from critical data studies to build new kinds of organizational arrangements, which we argue will help advance a more ethical data science. Engaging the insights of critical data studies will improve data science. Careful attention to the practices of data science will improve scholarly critiques. Genuine collaborative conversations between these different communities will help push for more ethical, and better, ways of knowing in increasingly datum-saturated societies.",fullPaper,cp9
Medicine,p807,d1,024a2c03be8e468e7c4fdf9bda36cdc0eaae85fb,j60,Nature,Array programming with NumPy,Abstract,fullPaper,jv60
Computer Science,p807,d3,024a2c03be8e468e7c4fdf9bda36cdc0eaae85fb,j60,Nature,Array programming with NumPy,Abstract,fullPaper,jv60
Mathematics,p807,d6,024a2c03be8e468e7c4fdf9bda36cdc0eaae85fb,j60,Nature,Array programming with NumPy,Abstract,fullPaper,jv60
Medicine,p811,d1,c27e16cf9b626e983ad6372a25e98e060053afcb,j8,Nature Methods,Wisdom of crowds for robust gene network inference,Abstract,fullPaper,jv8
Biology,p811,d5,c27e16cf9b626e983ad6372a25e98e060053afcb,j8,Nature Methods,Wisdom of crowds for robust gene network inference,Abstract,fullPaper,jv8
Psychology,p811,d10,c27e16cf9b626e983ad6372a25e98e060053afcb,j8,Nature Methods,Wisdom of crowds for robust gene network inference,Abstract,fullPaper,jv8
Medicine,p814,d1,eceff752c3f87c9120ecab4fe63a960bfe330b9a,c12,The Compass,"Reveal, a general reverse engineering algorithm for inference of genetic network architectures.","Given the immanent gene expression mapping covering whole genomes during development, health and disease, we seek computational methods to maximize functional inference from such large data sets. Is it possible, in principle, to completely infer a complex regulatory network architecture from input/output patterns of its variables? We investigated this possibility using binary models of genetic networks. Trajectories, or state transition tables of Boolean nets, resemble time series of gene expression. By systematically analyzing the mutual information between input states and output states, one is able to infer the sets of input elements controlling each element or gene in the network. This process is unequivocal and exact for complete state transition tables. We implemented this REVerse Engineering ALgorithm (REVEAL) in a C program, and found the problem to be tractable within the conditions tested so far. For n = 50 (elements) and k = 3 (inputs per element), the analysis of incomplete state transition tables (100 state transition pairs out of a possible 10(15)) reliably produced the original rule and wiring sets. While this study is limited to synchronous Boolean networks, the algorithm is generalizable to include multi-state models, essentially allowing direct application to realistic biological data sets. The ability to adequately solve the inverse problem may enable in-depth analysis of complex dynamic systems in biology and other fields.",poster,cp12
Computer Science,p814,d3,eceff752c3f87c9120ecab4fe63a960bfe330b9a,c12,The Compass,"Reveal, a general reverse engineering algorithm for inference of genetic network architectures.","Given the immanent gene expression mapping covering whole genomes during development, health and disease, we seek computational methods to maximize functional inference from such large data sets. Is it possible, in principle, to completely infer a complex regulatory network architecture from input/output patterns of its variables? We investigated this possibility using binary models of genetic networks. Trajectories, or state transition tables of Boolean nets, resemble time series of gene expression. By systematically analyzing the mutual information between input states and output states, one is able to infer the sets of input elements controlling each element or gene in the network. This process is unequivocal and exact for complete state transition tables. We implemented this REVerse Engineering ALgorithm (REVEAL) in a C program, and found the problem to be tractable within the conditions tested so far. For n = 50 (elements) and k = 3 (inputs per element), the analysis of incomplete state transition tables (100 state transition pairs out of a possible 10(15)) reliably produced the original rule and wiring sets. While this study is limited to synchronous Boolean networks, the algorithm is generalizable to include multi-state models, essentially allowing direct application to realistic biological data sets. The ability to adequately solve the inverse problem may enable in-depth analysis of complex dynamic systems in biology and other fields.",poster,cp12
Medicine,p823,d1,3efd851140aa28e95221b55fcc5659eea97b172d,j202,IEEE Transactions on Neural Networks,The Graph Neural Network Model,"Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function tau(G,n) isin IRm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.",fullPaper,jv202
Computer Science,p823,d3,3efd851140aa28e95221b55fcc5659eea97b172d,j202,IEEE Transactions on Neural Networks,The Graph Neural Network Model,"Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function tau(G,n) isin IRm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.",fullPaper,jv202
Medicine,p835,d1,b9e5fa707e804d6008e5011b058244437c656a93,j0,Nature Biotechnology,Optimized sgRNA design to maximize activity and minimize off-target effects of CRISPR-Cas9,Abstract,fullPaper,jv0
Medicine,p843,d1,0d70f756c410370d56f9e61f25f68ab606533bf2,j89,bioRxiv,BEAST 2.5: An advanced software platform for Bayesian evolutionary analysis,"Elaboration of Bayesian phylogenetic inference methods has continued at pace in recent years with major new advances in nearly all aspects of the joint modelling of evolutionary data. It is increasingly appreciated that some evolutionary questions can only be adequately answered by combining evidence from multiple independent sources of data, including genome sequences, sampling dates, phenotypic data, radiocarbon dates, fossil occurrences, and biogeographic range information among others. Including all relevant data into a single joint model is very challenging both conceptually and computationally. Advanced computational software packages that allow robust development of compatible (sub-)models which can be composed into a full model hierarchy have played a key role in these developments. Developing such software frameworks is increasingly a major scientific activity in its own right, and comes with specific challenges, from practical software design, development and engineering challenges to statistical and conceptual modelling challenges. BEAST 2 is one such computational software platform, and was first announced over 4 years ago. Here we describe a series of major new developments in the BEAST 2 core platform and model hierarchy that have occurred since the first release of the software, culminating in the recent 2.5 release. Author summary Bayesian phylogenetic inference methods have undergone considerable development in recent years, and joint modelling of rich evolutionary data, including genomes, phenotypes and fossil occurrences is increasingly common. Advanced computational software packages that allow robust development of compatible (sub-)models which can be composed into a full model hierarchy have played a key role in these developments. Developing scientific software is increasingly crucial to advancement in many fields of biology. The challenges range from practical software development and engineering, distributed team coordination, conceptual development and statistical modelling, to validation and testing. BEAST 2 is one such computational software platform for phylogenetics, population genetics and phylodynamics, and was first announced over 4 years ago. Here we describe the full range of new tools and models available on the BEAST 2.5 platform, which expand joint evolutionary inference in many new directions, especially for joint inference over multiple data types, non-tree models and complex phylodynamics.",fullPaper,jv89
Computer Science,p843,d3,0d70f756c410370d56f9e61f25f68ab606533bf2,j89,bioRxiv,BEAST 2.5: An advanced software platform for Bayesian evolutionary analysis,"Elaboration of Bayesian phylogenetic inference methods has continued at pace in recent years with major new advances in nearly all aspects of the joint modelling of evolutionary data. It is increasingly appreciated that some evolutionary questions can only be adequately answered by combining evidence from multiple independent sources of data, including genome sequences, sampling dates, phenotypic data, radiocarbon dates, fossil occurrences, and biogeographic range information among others. Including all relevant data into a single joint model is very challenging both conceptually and computationally. Advanced computational software packages that allow robust development of compatible (sub-)models which can be composed into a full model hierarchy have played a key role in these developments. Developing such software frameworks is increasingly a major scientific activity in its own right, and comes with specific challenges, from practical software design, development and engineering challenges to statistical and conceptual modelling challenges. BEAST 2 is one such computational software platform, and was first announced over 4 years ago. Here we describe a series of major new developments in the BEAST 2 core platform and model hierarchy that have occurred since the first release of the software, culminating in the recent 2.5 release. Author summary Bayesian phylogenetic inference methods have undergone considerable development in recent years, and joint modelling of rich evolutionary data, including genomes, phenotypes and fossil occurrences is increasingly common. Advanced computational software packages that allow robust development of compatible (sub-)models which can be composed into a full model hierarchy have played a key role in these developments. Developing scientific software is increasingly crucial to advancement in many fields of biology. The challenges range from practical software development and engineering, distributed team coordination, conceptual development and statistical modelling, to validation and testing. BEAST 2 is one such computational software platform for phylogenetics, population genetics and phylodynamics, and was first announced over 4 years ago. Here we describe the full range of new tools and models available on the BEAST 2.5 platform, which expand joint evolutionary inference in many new directions, especially for joint inference over multiple data types, non-tree models and complex phylodynamics.",fullPaper,jv89
Biology,p843,d5,0d70f756c410370d56f9e61f25f68ab606533bf2,j89,bioRxiv,BEAST 2.5: An advanced software platform for Bayesian evolutionary analysis,"Elaboration of Bayesian phylogenetic inference methods has continued at pace in recent years with major new advances in nearly all aspects of the joint modelling of evolutionary data. It is increasingly appreciated that some evolutionary questions can only be adequately answered by combining evidence from multiple independent sources of data, including genome sequences, sampling dates, phenotypic data, radiocarbon dates, fossil occurrences, and biogeographic range information among others. Including all relevant data into a single joint model is very challenging both conceptually and computationally. Advanced computational software packages that allow robust development of compatible (sub-)models which can be composed into a full model hierarchy have played a key role in these developments. Developing such software frameworks is increasingly a major scientific activity in its own right, and comes with specific challenges, from practical software design, development and engineering challenges to statistical and conceptual modelling challenges. BEAST 2 is one such computational software platform, and was first announced over 4 years ago. Here we describe a series of major new developments in the BEAST 2 core platform and model hierarchy that have occurred since the first release of the software, culminating in the recent 2.5 release. Author summary Bayesian phylogenetic inference methods have undergone considerable development in recent years, and joint modelling of rich evolutionary data, including genomes, phenotypes and fossil occurrences is increasingly common. Advanced computational software packages that allow robust development of compatible (sub-)models which can be composed into a full model hierarchy have played a key role in these developments. Developing scientific software is increasingly crucial to advancement in many fields of biology. The challenges range from practical software development and engineering, distributed team coordination, conceptual development and statistical modelling, to validation and testing. BEAST 2 is one such computational software platform for phylogenetics, population genetics and phylodynamics, and was first announced over 4 years ago. Here we describe the full range of new tools and models available on the BEAST 2.5 platform, which expand joint evolutionary inference in many new directions, especially for joint inference over multiple data types, non-tree models and complex phylodynamics.",fullPaper,jv89
Medicine,p849,d1,78a47c832b16c78fadf29d0354e690f01ab90244,j205,Microbiology and Molecular Biology Reviews,Bacillus thuringiensis and Its Pesticidal Crystal Proteins,"SUMMARY During the past decade the pesticidal bacterium Bacillus thuringiensis has been the subject of intensive research. These efforts have yielded considerable data about the complex relationships between the structure, mechanism of action, and genetics of the organism’s pesticidal crystal proteins, and a coherent picture of these relationships is beginning to emerge. Other studies have focused on the ecological role of the B. thuringiensis crystal proteins, their performance in agricultural and other natural settings, and the evolution of resistance mechanisms in target pests. Armed with this knowledge base and with the tools of modern biotechnology, researchers are now reporting promising results in engineering more-useful toxins and formulations, in creating transgenic plants that express pesticidal activity, and in constructing integrated management strategies to insure that these products are utilized with maximum efficiency and benefit.",fullPaper,jv205
Biology,p849,d5,78a47c832b16c78fadf29d0354e690f01ab90244,j205,Microbiology and Molecular Biology Reviews,Bacillus thuringiensis and Its Pesticidal Crystal Proteins,"SUMMARY During the past decade the pesticidal bacterium Bacillus thuringiensis has been the subject of intensive research. These efforts have yielded considerable data about the complex relationships between the structure, mechanism of action, and genetics of the organism’s pesticidal crystal proteins, and a coherent picture of these relationships is beginning to emerge. Other studies have focused on the ecological role of the B. thuringiensis crystal proteins, their performance in agricultural and other natural settings, and the evolution of resistance mechanisms in target pests. Armed with this knowledge base and with the tools of modern biotechnology, researchers are now reporting promising results in engineering more-useful toxins and formulations, in creating transgenic plants that express pesticidal activity, and in constructing integrated management strategies to insure that these products are utilized with maximum efficiency and benefit.",fullPaper,jv205
Medicine,p855,d1,fded6f9cefb06c1da7acddd5b9b89b1eabb08b7d,c93,ASE BigData & SocialInformatics,The Ensembl genome database project,"The Ensembl (http://www.ensembl.org/) database project provides a bioinformatics framework to organise biology around the sequences of large genomes. It is a comprehensive source of stable automatic annotation of the human genome sequence, with confirmed gene predictions that have been integrated with external data sources, and is available as either an interactive web site or as flat files. It is also an open source software engineering project to develop a portable system able to handle very large genomes and associated requirements from sequence analysis to data storage and visualisation. The Ensembl site is one of the leading sources of human genome sequence annotation and provided much of the analysis for publication by the international human genome project of the draft genome. The Ensembl system is being installed around the world in both companies and academic sites on machines ranging from supercomputers to laptops.",poster,cp93
Computer Science,p855,d3,fded6f9cefb06c1da7acddd5b9b89b1eabb08b7d,c93,ASE BigData & SocialInformatics,The Ensembl genome database project,"The Ensembl (http://www.ensembl.org/) database project provides a bioinformatics framework to organise biology around the sequences of large genomes. It is a comprehensive source of stable automatic annotation of the human genome sequence, with confirmed gene predictions that have been integrated with external data sources, and is available as either an interactive web site or as flat files. It is also an open source software engineering project to develop a portable system able to handle very large genomes and associated requirements from sequence analysis to data storage and visualisation. The Ensembl site is one of the leading sources of human genome sequence annotation and provided much of the analysis for publication by the international human genome project of the draft genome. The Ensembl system is being installed around the world in both companies and academic sites on machines ranging from supercomputers to laptops.",poster,cp93
Biology,p855,d5,fded6f9cefb06c1da7acddd5b9b89b1eabb08b7d,c93,ASE BigData & SocialInformatics,The Ensembl genome database project,"The Ensembl (http://www.ensembl.org/) database project provides a bioinformatics framework to organise biology around the sequences of large genomes. It is a comprehensive source of stable automatic annotation of the human genome sequence, with confirmed gene predictions that have been integrated with external data sources, and is available as either an interactive web site or as flat files. It is also an open source software engineering project to develop a portable system able to handle very large genomes and associated requirements from sequence analysis to data storage and visualisation. The Ensembl site is one of the leading sources of human genome sequence annotation and provided much of the analysis for publication by the international human genome project of the draft genome. The Ensembl system is being installed around the world in both companies and academic sites on machines ranging from supercomputers to laptops.",poster,cp93
Medicine,p857,d1,2a3842f6070b4554ff21fe62b2a486657d9a304a,j104,Science,"How to Grow a Mind: Statistics, Structure, and Abstraction","In coming to understand the world—in learning concepts, acquiring language, and grasping causal relations—our minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?",fullPaper,jv104
Computer Science,p857,d3,2a3842f6070b4554ff21fe62b2a486657d9a304a,j104,Science,"How to Grow a Mind: Statistics, Structure, and Abstraction","In coming to understand the world—in learning concepts, acquiring language, and grasping causal relations—our minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?",fullPaper,jv104
Medicine,p867,d1,7a4ba4f2dd23c16e02e8e30335a770c0cd3321ed,j104,Science,Spintronics: A Spin-Based Electronics Vision for the Future,"This review describes a new paradigm of electronics based on the spin degree of freedom of the electron. Either adding the spin degree of freedom to conventional charge-based electronic devices or using the spin alone has the potential advantages of nonvolatility, increased data processing speed, decreased electric power consumption, and increased integration densities compared with conventional semiconductor devices. To successfully incorporate spins into existing semiconductor technology, one has to resolve technical issues such as efficient injection, transport, control and manipulation, and detection of spin polarization as well as spin-polarized currents. Recent advances in new materials engineering hold the promise of realizing spintronic devices in the near future. We review the current state of the spin-based devices, efforts in new materials fabrication, issues in spin transport, and optical spin manipulation.",fullPaper,jv104
Physics,p867,d2,7a4ba4f2dd23c16e02e8e30335a770c0cd3321ed,j104,Science,Spintronics: A Spin-Based Electronics Vision for the Future,"This review describes a new paradigm of electronics based on the spin degree of freedom of the electron. Either adding the spin degree of freedom to conventional charge-based electronic devices or using the spin alone has the potential advantages of nonvolatility, increased data processing speed, decreased electric power consumption, and increased integration densities compared with conventional semiconductor devices. To successfully incorporate spins into existing semiconductor technology, one has to resolve technical issues such as efficient injection, transport, control and manipulation, and detection of spin polarization as well as spin-polarized currents. Recent advances in new materials engineering hold the promise of realizing spintronic devices in the near future. We review the current state of the spin-based devices, efforts in new materials fabrication, issues in spin transport, and optical spin manipulation.",fullPaper,jv104
Medicine,p876,d1,8df72c48a7ce4418c683c4dd9bb300558ac71d47,c25,IEEE International Parallel and Distributed Processing Symposium,"Deep learning for healthcare: review, opportunities and challenges","Gaining knowledge and actionable insights from complex, high-dimensional and heterogeneous biomedical data remains a key challenge in transforming health care. Various types of data have been emerging in modern biomedical research, including electronic health records, imaging, -omics, sensor data and text, which are complex, heterogeneous, poorly annotated and generally unstructured. Traditional data mining and statistical learning approaches typically need to first perform feature engineering to obtain effective and more robust features from those data, and then build prediction or clustering models on top of them. There are lots of challenges on both steps in a scenario of complicated data and lacking of sufficient domain knowledge. The latest advances in deep learning technologies provide new effective paradigms to obtain end-to-end learning models from complex data. In this article, we review the recent literature on applying deep learning technologies to advance the health care domain. Based on the analyzed work, we suggest that deep learning approaches could be the vehicle for translating big biomedical data into improved human health. However, we also note limitations and needs for improved methods development and applications, especially in terms of ease-of-understanding for domain experts and citizen scientists. We discuss such challenges and suggest developing holistic and meaningful interpretable architectures to bridge deep learning models and human interpretability.",poster,cp25
Computer Science,p876,d3,8df72c48a7ce4418c683c4dd9bb300558ac71d47,c25,IEEE International Parallel and Distributed Processing Symposium,"Deep learning for healthcare: review, opportunities and challenges","Gaining knowledge and actionable insights from complex, high-dimensional and heterogeneous biomedical data remains a key challenge in transforming health care. Various types of data have been emerging in modern biomedical research, including electronic health records, imaging, -omics, sensor data and text, which are complex, heterogeneous, poorly annotated and generally unstructured. Traditional data mining and statistical learning approaches typically need to first perform feature engineering to obtain effective and more robust features from those data, and then build prediction or clustering models on top of them. There are lots of challenges on both steps in a scenario of complicated data and lacking of sufficient domain knowledge. The latest advances in deep learning technologies provide new effective paradigms to obtain end-to-end learning models from complex data. In this article, we review the recent literature on applying deep learning technologies to advance the health care domain. Based on the analyzed work, we suggest that deep learning approaches could be the vehicle for translating big biomedical data into improved human health. However, we also note limitations and needs for improved methods development and applications, especially in terms of ease-of-understanding for domain experts and citizen scientists. We discuss such challenges and suggest developing holistic and meaningful interpretable architectures to bridge deep learning models and human interpretability.",poster,cp25
Medicine,p884,d1,0198dac70004da88cddc17de6a233504cb0d1cfe,j209,ACS Synthetic Biology,"A Highly Characterized Yeast Toolkit for Modular, Multipart Assembly.","Saccharomyces cerevisiae is an increasingly attractive host for synthetic biology because of its long history in industrial fermentations. However, until recently, most synthetic biology systems have focused on bacteria. While there is a wealth of resources and literature about the biology of yeast, it can be daunting to navigate and extract the tools needed for engineering applications. Here we present a versatile engineering platform for yeast, which contains both a rapid, modular assembly method and a basic set of characterized parts. This platform provides a framework in which to create new designs, as well as data on promoters, terminators, degradation tags, and copy number to inform those designs. Additionally, we describe genome-editing tools for making modifications directly to the yeast chromosomes, which we find preferable to plasmids due to reduced variability in expression. With this toolkit, we strive to simplify the process of engineering yeast by standardizing the physical manipulations and suggesting best practices that together will enable more straightforward translation of materials and data from one group to another. Additionally, by relieving researchers of the burden of technical details, they can focus on higher-level aspects of experimental design.",fullPaper,jv209
Biology,p884,d5,0198dac70004da88cddc17de6a233504cb0d1cfe,j209,ACS Synthetic Biology,"A Highly Characterized Yeast Toolkit for Modular, Multipart Assembly.","Saccharomyces cerevisiae is an increasingly attractive host for synthetic biology because of its long history in industrial fermentations. However, until recently, most synthetic biology systems have focused on bacteria. While there is a wealth of resources and literature about the biology of yeast, it can be daunting to navigate and extract the tools needed for engineering applications. Here we present a versatile engineering platform for yeast, which contains both a rapid, modular assembly method and a basic set of characterized parts. This platform provides a framework in which to create new designs, as well as data on promoters, terminators, degradation tags, and copy number to inform those designs. Additionally, we describe genome-editing tools for making modifications directly to the yeast chromosomes, which we find preferable to plasmids due to reduced variability in expression. With this toolkit, we strive to simplify the process of engineering yeast by standardizing the physical manipulations and suggesting best practices that together will enable more straightforward translation of materials and data from one group to another. Additionally, by relieving researchers of the burden of technical details, they can focus on higher-level aspects of experimental design.",fullPaper,jv209
Medicine,p896,d1,7b0aa266562518af198631641cbd6db50ca8362f,j214,Nature reviews genetics,Constraint-based models predict metabolic and associated cellular functions,Abstract,fullPaper,jv214
Biology,p896,d5,7b0aa266562518af198631641cbd6db50ca8362f,j214,Nature reviews genetics,Constraint-based models predict metabolic and associated cellular functions,Abstract,fullPaper,jv214
Medicine,p902,d1,1082f31a589acf1d9e179ebbac008b666a298cf5,j218,Annals medicus,A review of rapid prototyping techniques for tissue engineering purposes,"Rapid prototyping (RP) is a common name for several techniques, which read in data from computer-aided design (CAD) drawings and manufacture automatically three-dimensional objects layer-by-layer according to the virtual design. The utilization of RP in tissue engineering enables the production of three-dimensional scaffolds with complex geometries and very fine structures. Adding micro- and nanometer details into the scaffolds improves the mechanical properties of the scaffold and ensures better cell adhesion to the scaffold surface. Thus, tissue engineering constructs can be customized according to the data acquired from the medical scans to match the each patient's individual needs. In addition RP enables the control of the scaffold porosity making it possible to fabricate applications with desired structural integrity. Unfortunately, every RP process has its own unique disadvantages in building tissue engineering scaffolds. Hence, the future research should be focused on the development of RP machines designed specifically for fabrication of tissue engineering scaffolds, although RP methods already can serve as a link between tissue and engineering.",fullPaper,jv218
Computer Science,p902,d3,1082f31a589acf1d9e179ebbac008b666a298cf5,j218,Annals medicus,A review of rapid prototyping techniques for tissue engineering purposes,"Rapid prototyping (RP) is a common name for several techniques, which read in data from computer-aided design (CAD) drawings and manufacture automatically three-dimensional objects layer-by-layer according to the virtual design. The utilization of RP in tissue engineering enables the production of three-dimensional scaffolds with complex geometries and very fine structures. Adding micro- and nanometer details into the scaffolds improves the mechanical properties of the scaffold and ensures better cell adhesion to the scaffold surface. Thus, tissue engineering constructs can be customized according to the data acquired from the medical scans to match the each patient's individual needs. In addition RP enables the control of the scaffold porosity making it possible to fabricate applications with desired structural integrity. Unfortunately, every RP process has its own unique disadvantages in building tissue engineering scaffolds. Hence, the future research should be focused on the development of RP machines designed specifically for fabrication of tissue engineering scaffolds, although RP methods already can serve as a link between tissue and engineering.",fullPaper,jv218
Materials Science,p902,d7,1082f31a589acf1d9e179ebbac008b666a298cf5,j218,Annals medicus,A review of rapid prototyping techniques for tissue engineering purposes,"Rapid prototyping (RP) is a common name for several techniques, which read in data from computer-aided design (CAD) drawings and manufacture automatically three-dimensional objects layer-by-layer according to the virtual design. The utilization of RP in tissue engineering enables the production of three-dimensional scaffolds with complex geometries and very fine structures. Adding micro- and nanometer details into the scaffolds improves the mechanical properties of the scaffold and ensures better cell adhesion to the scaffold surface. Thus, tissue engineering constructs can be customized according to the data acquired from the medical scans to match the each patient's individual needs. In addition RP enables the control of the scaffold porosity making it possible to fabricate applications with desired structural integrity. Unfortunately, every RP process has its own unique disadvantages in building tissue engineering scaffolds. Hence, the future research should be focused on the development of RP machines designed specifically for fabrication of tissue engineering scaffolds, although RP methods already can serve as a link between tissue and engineering.",fullPaper,jv218
Medicine,p903,d1,b9b36a61bcac98777a0ac06e4eb36b2959bef511,j23,Proceedings of the National Academy of Sciences of the United States of America,Automated reverse engineering of nonlinear dynamical systems,"Complex nonlinear dynamics arise in many fields of science and engineering, but uncovering the underlying differential equations directly from observations poses a challenging task. The ability to symbolically model complex networked systems is key to understanding them, an open problem in many disciplines. Here we introduce for the first time a method that can automatically generate symbolic equations for a nonlinear coupled dynamical system directly from time series data. This method is applicable to any system that can be described using sets of ordinary nonlinear differential equations, and assumes that the (possibly noisy) time series of all variables are observable. Previous automated symbolic modeling approaches of coupled physical systems produced linear models or required a nonlinear model to be provided manually. The advance presented here is made possible by allowing the method to model each (possibly coupled) variable separately, intelligently perturbing and destabilizing the system to extract its less observable characteristics, and automatically simplifying the equations during modeling. We demonstrate this method on four simulated and two real systems spanning mechanics, ecology, and systems biology. Unlike numerical models, symbolic models have explanatory value, suggesting that automated “reverse engineering” approaches for model-free symbolic nonlinear system identification may play an increasing role in our ability to understand progressively more complex systems in the future.",fullPaper,jv23
Computer Science,p903,d3,b9b36a61bcac98777a0ac06e4eb36b2959bef511,j23,Proceedings of the National Academy of Sciences of the United States of America,Automated reverse engineering of nonlinear dynamical systems,"Complex nonlinear dynamics arise in many fields of science and engineering, but uncovering the underlying differential equations directly from observations poses a challenging task. The ability to symbolically model complex networked systems is key to understanding them, an open problem in many disciplines. Here we introduce for the first time a method that can automatically generate symbolic equations for a nonlinear coupled dynamical system directly from time series data. This method is applicable to any system that can be described using sets of ordinary nonlinear differential equations, and assumes that the (possibly noisy) time series of all variables are observable. Previous automated symbolic modeling approaches of coupled physical systems produced linear models or required a nonlinear model to be provided manually. The advance presented here is made possible by allowing the method to model each (possibly coupled) variable separately, intelligently perturbing and destabilizing the system to extract its less observable characteristics, and automatically simplifying the equations during modeling. We demonstrate this method on four simulated and two real systems spanning mechanics, ecology, and systems biology. Unlike numerical models, symbolic models have explanatory value, suggesting that automated “reverse engineering” approaches for model-free symbolic nonlinear system identification may play an increasing role in our ability to understand progressively more complex systems in the future.",fullPaper,jv23
Medicine,p929,d1,ab7d2563a86a1cb51a60b4b70df62ffcbc9d7351,j221,Nature Reviews Microbiology,Constraining the metabolic genotype–phenotype relationship using a phylogeny of in silico methods,Abstract,fullPaper,jv221
Biology,p929,d5,ab7d2563a86a1cb51a60b4b70df62ffcbc9d7351,j221,Nature Reviews Microbiology,Constraining the metabolic genotype–phenotype relationship using a phylogeny of in silico methods,Abstract,fullPaper,jv221
Medicine,p933,d1,a9304a952cdcf732a2b771466a7b0efad8e499c8,j173,IEEE Transactions on Pattern Analysis and Machine Intelligence,Data Fusion by Matrix Factorization,"For most problems in science and engineering we can obtain data sets that describe the observed system from various perspectives and record the behavior of its individual components. Heterogeneous data sets can be collectively mined by data fusion. Fusion can focus on a specific target relation and exploit directly associated data together with contextual data and data about system's constraints. In the paper we describe a data fusion approach with penalized matrix tri-factorization (DFMF) that simultaneously factorizes data matrices to reveal hidden associations. The approach can directly consider any data that can be expressed in a matrix, including those from feature-based representations, ontologies, associations and networks. We demonstrate the utility of DFMF for gene function prediction task with eleven different data sources and for prediction of pharmacologic actions by fusing six data sources. Our data fusion algorithm compares favorably to alternative data integration approaches and achieves higher accuracy than can be obtained from any single data source alone.",fullPaper,jv173
Computer Science,p933,d3,a9304a952cdcf732a2b771466a7b0efad8e499c8,j173,IEEE Transactions on Pattern Analysis and Machine Intelligence,Data Fusion by Matrix Factorization,"For most problems in science and engineering we can obtain data sets that describe the observed system from various perspectives and record the behavior of its individual components. Heterogeneous data sets can be collectively mined by data fusion. Fusion can focus on a specific target relation and exploit directly associated data together with contextual data and data about system's constraints. In the paper we describe a data fusion approach with penalized matrix tri-factorization (DFMF) that simultaneously factorizes data matrices to reveal hidden associations. The approach can directly consider any data that can be expressed in a matrix, including those from feature-based representations, ontologies, associations and networks. We demonstrate the utility of DFMF for gene function prediction task with eleven different data sources and for prediction of pharmacologic actions by fusing six data sources. Our data fusion algorithm compares favorably to alternative data integration approaches and achieves higher accuracy than can be obtained from any single data source alone.",fullPaper,jv173
Mathematics,p933,d6,a9304a952cdcf732a2b771466a7b0efad8e499c8,j173,IEEE Transactions on Pattern Analysis and Machine Intelligence,Data Fusion by Matrix Factorization,"For most problems in science and engineering we can obtain data sets that describe the observed system from various perspectives and record the behavior of its individual components. Heterogeneous data sets can be collectively mined by data fusion. Fusion can focus on a specific target relation and exploit directly associated data together with contextual data and data about system's constraints. In the paper we describe a data fusion approach with penalized matrix tri-factorization (DFMF) that simultaneously factorizes data matrices to reveal hidden associations. The approach can directly consider any data that can be expressed in a matrix, including those from feature-based representations, ontologies, associations and networks. We demonstrate the utility of DFMF for gene function prediction task with eleven different data sources and for prediction of pharmacologic actions by fusing six data sources. Our data fusion algorithm compares favorably to alternative data integration approaches and achieves higher accuracy than can be obtained from any single data source alone.",fullPaper,jv173
Medicine,p943,d1,f6d5bdaa9ea5d90bf94608f68b1c8dd36d1ac843,j225,Molecular Systems Biology,How to infer gene networks from expression profiles,Abstract,fullPaper,jv225
Biology,p943,d5,f6d5bdaa9ea5d90bf94608f68b1c8dd36d1ac843,j225,Molecular Systems Biology,How to infer gene networks from expression profiles,Abstract,fullPaper,jv225
Medicine,p951,d1,7c0d41c9a15842f53629c54f6cbef18dcb96f480,c21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,Evolutionary optimization with data collocation for reverse engineering of biological networks,"MOTIVATION
Modern experimental biology is moving away from analyses of single elements to whole-organism measurements. Such measured time-course data contain a wealth of information about the structure and dynamic of the pathway or network. The dynamic modeling of the whole systems is formulated as a reverse problem that requires a well-suited mathematical model and a very efficient computational method to identify the model structure and parameters. Numerical integration for differential equations and finding global parameter values are still two major challenges in this field of the parameter estimation of nonlinear dynamic biological systems.


RESULTS
We compare three techniques of parameter estimation for nonlinear dynamic biological systems. In the proposed scheme, the modified collocation method is applied to convert the differential equations to the system of algebraic equations. The observed time-course data are then substituted into the algebraic system equations to decouple system interactions in order to obtain the approximate model profiles. Hybrid differential evolution (HDE) with population size of five is able to find a global solution. The method is not only suited for parameter estimation but also can be applied for structure identification. The solution obtained by HDE is then used as the starting point for a local search method to yield the refined estimates.",poster,cp21
Computer Science,p951,d3,7c0d41c9a15842f53629c54f6cbef18dcb96f480,c21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,Evolutionary optimization with data collocation for reverse engineering of biological networks,"MOTIVATION
Modern experimental biology is moving away from analyses of single elements to whole-organism measurements. Such measured time-course data contain a wealth of information about the structure and dynamic of the pathway or network. The dynamic modeling of the whole systems is formulated as a reverse problem that requires a well-suited mathematical model and a very efficient computational method to identify the model structure and parameters. Numerical integration for differential equations and finding global parameter values are still two major challenges in this field of the parameter estimation of nonlinear dynamic biological systems.


RESULTS
We compare three techniques of parameter estimation for nonlinear dynamic biological systems. In the proposed scheme, the modified collocation method is applied to convert the differential equations to the system of algebraic equations. The observed time-course data are then substituted into the algebraic system equations to decouple system interactions in order to obtain the approximate model profiles. Hybrid differential evolution (HDE) with population size of five is able to find a global solution. The method is not only suited for parameter estimation but also can be applied for structure identification. The solution obtained by HDE is then used as the starting point for a local search method to yield the refined estimates.",poster,cp21
Medicine,p964,d1,12358b92076419f2b147fdabda5c3d8c1cfa02a4,j232,Biotechnology Journal,Better library design: data‐driven protein engineering,"Data‐driven protein engineering is increasingly used as an alternative to rational design and combinatorial engineering because it uses available knowledge to limit library size, while still allowing for the identification of unpredictable substitutions that lead to large effects. Recent advances in computational modeling and bioinformatics, as well as an increasing databank of experiments on functional variants, have led to new strategies to choose particular amino acid residues to vary in order to increase the chances of obtaining a variant protein with the desired property. Strategies for limiting diversity at each position, design of small sub‐libraries, and the performance of scouting experiments, have also been developed or even automated, further reducing the library size.",fullPaper,jv232
Biology,p964,d5,12358b92076419f2b147fdabda5c3d8c1cfa02a4,j232,Biotechnology Journal,Better library design: data‐driven protein engineering,"Data‐driven protein engineering is increasingly used as an alternative to rational design and combinatorial engineering because it uses available knowledge to limit library size, while still allowing for the identification of unpredictable substitutions that lead to large effects. Recent advances in computational modeling and bioinformatics, as well as an increasing databank of experiments on functional variants, have led to new strategies to choose particular amino acid residues to vary in order to increase the chances of obtaining a variant protein with the desired property. Strategies for limiting diversity at each position, design of small sub‐libraries, and the performance of scouting experiments, have also been developed or even automated, further reducing the library size.",fullPaper,jv232
Medicine,p966,d1,0166d107c091e2ea0c0d2ea172f48ab010677e4f,j104,Science,Anatomy of STEM teaching in North American universities,"Lecture is prominent, but practices vary A large body of evidence demonstrates that strategies that promote student interactions and cognitively engage students with content (1) lead to gains in learning and attitudinal outcomes for students in science, technology, engineering, and mathematics (STEM) courses (1, 2). Many educational and governmental bodies have called for and supported adoption of these student-centered strategies throughout the undergraduate STEM curriculum. But to the extent that we have pictures of the STEM undergraduate instructional landscape, it has mostly been provided through self-report surveys of faculty members, within a particular STEM discipline [e.g., (3–6)]. Such surveys are prone to reliability threats and can underestimate the complexity of classroom environments, and few are implemented nationally to provide valid and reliable data (7). Reflecting the limited state of these data, a report from the U.S. National Academies of Sciences, Engineering, and Medicine called for improved data collection to understand the use of evidence-based instructional practices (8). We report here a major step toward a characterization of STEM teaching practices in North American universities based on classroom observations from over 2000 classes taught by more than 500 STEM faculty members across 25 institutions.",fullPaper,jv104
Medicine,p969,d1,0a2b49f51fd6b65e26c458fdd3592f88b63a3b08,j225,Molecular Systems Biology,Applications of genome-scale metabolic reconstructions,Abstract,fullPaper,jv225
Biology,p969,d5,0a2b49f51fd6b65e26c458fdd3592f88b63a3b08,j225,Molecular Systems Biology,Applications of genome-scale metabolic reconstructions,Abstract,fullPaper,jv225
Medicine,p986,d1,5abdcb9c58ae3c25b5dfb320b56bdc196c757d67,j237,Critical Care Medicine,A look into the nature and causes of human errors in the intensive care unit*,"Objectives: The purpose of this study was to investigate the nature and causes of human errors in the intensive care unit (ICU), adopting approaches proposed by human factors engineering. The basic assumption was that errors occur and follow a pattern that can be uncovered. Design: Concurrent incident study. Setting: Medical-surgical ICU of a university hospital. Measurements and main results: Two types of data were collected: errors reported by physicians and nurses immediately after an error discovery; and activity profiles based on 24-h records taken by observers with human engineering experience on a sample of patients. During the 4 months of data collection, a total of 554 human errors were reported by the medical staff. Errors were rated for severity and classified according to the body system and type of medical activity involved. There was an average of 178 activities per patient per day and an estimated number of 1.7 errors per patient per day. For the ICU as a whole, a severe or potentially detrimental error occurred on average twice a day. Physicians and nurses were about equal contributors to the number of errors, although nurses had many more activities per day. Conclusions: A significant number of dangerous human errors occur in the ICU. Many of these errors could be attributed to problems of communication between the physicians and nurses. Applying human factor engineering concepts to the study of the weak points of a specific ICU may help to reduce the number of errors. Errors should not be considered as an incurable disease, but rather as preventable phenomena.",fullPaper,jv237
Medicine,p989,d1,c990749be102e1bb5cd393d1936d80d4550563f1,c15,Pacific Symposium on Biocomputing,Reverse Engineering of Metabolic Pathways from Observed Data Using Genetic Programming,"Recent work has demonstrated that genetic programming is capable of automatically creating complex networks (such as analog electrical circuits and controllers) whose behavior is modeled by linear and non-linear continuous-time differential equations and whose behavior matches prespecified output values. The concentrations of substances participating in networks of chemical reactions are also modeled by non-linear continuous-time differential equations. This paper demonstrates that it is possible to automatically create (reverse engineer) a network of chemical reactions from observed time-domain data. Genetic programming starts with observed time-domain concentrations of input substances and automatically creates both the topology of the network of chemical reactions and the rates of each reaction within the network such that the concentration of the final product of the automatically created network matches the observed time-domain data. Specifically, genetic programming automatically created metabolic pathways involved in the phospholipid cycle and the synthesis and degradation of ketone bodies.",fullPaper,cp15
Computer Science,p989,d3,c990749be102e1bb5cd393d1936d80d4550563f1,c15,Pacific Symposium on Biocomputing,Reverse Engineering of Metabolic Pathways from Observed Data Using Genetic Programming,"Recent work has demonstrated that genetic programming is capable of automatically creating complex networks (such as analog electrical circuits and controllers) whose behavior is modeled by linear and non-linear continuous-time differential equations and whose behavior matches prespecified output values. The concentrations of substances participating in networks of chemical reactions are also modeled by non-linear continuous-time differential equations. This paper demonstrates that it is possible to automatically create (reverse engineer) a network of chemical reactions from observed time-domain data. Genetic programming starts with observed time-domain concentrations of input substances and automatically creates both the topology of the network of chemical reactions and the rates of each reaction within the network such that the concentration of the final product of the automatically created network matches the observed time-domain data. Specifically, genetic programming automatically created metabolic pathways involved in the phospholipid cycle and the synthesis and degradation of ketone bodies.",fullPaper,cp15
Medicine,p1000,d1,ccca203382e5dd198c089a0f1d7af7bef0f694e9,j239,Molecular Plant,TBtools - an integrative toolkit developed for interactive analyses of big biological data.,Abstract,fullPaper,jv239
Biology,p1000,d5,ccca203382e5dd198c089a0f1d7af7bef0f694e9,j239,Molecular Plant,TBtools - an integrative toolkit developed for interactive analyses of big biological data.,Abstract,fullPaper,jv239
Medicine,p1004,d1,bf5a42b53d156c0811e88e60d2a49f9fd9367cae,j241,Harvard Business Review,Big data: the management revolution.,"Big data, the authors write, is far more powerful than the analytics of the past. Executives can measure and therefore manage more precisely than ever before. They can make better predictions and smarter decisions. They can target more-effective interventions in areas that so far have been dominated by gut and intuition rather than by data and rigor. The differences between big data and analytics are a matter of volume, velocity, and variety: More data now cross the internet every second than were stored in the entire internet 20 years ago. Nearly real-time information makes it possible for a company to be much more agile than its competitors. And that information can come from social networks, images, sensors, the web, or other unstructured sources. The managerial challenges, however, are very real. Senior decision makers have to learn to ask the right questions and embrace evidence-based decision making. Organizations must hire scientists who can find patterns in very large data sets and translate them into useful business information. IT departments have to work hard to integrate all the relevant internal and external sources of data. The authors offer two success stories to illustrate how companies are using big data: PASSUR Aerospace enables airlines to match their actual and estimated arrival times. Sears Holdings directly analyzes its incoming store data to make promotions much more precise and faster.",fullPaper,jv241
Computer Science,p1004,d3,bf5a42b53d156c0811e88e60d2a49f9fd9367cae,j241,Harvard Business Review,Big data: the management revolution.,"Big data, the authors write, is far more powerful than the analytics of the past. Executives can measure and therefore manage more precisely than ever before. They can make better predictions and smarter decisions. They can target more-effective interventions in areas that so far have been dominated by gut and intuition rather than by data and rigor. The differences between big data and analytics are a matter of volume, velocity, and variety: More data now cross the internet every second than were stored in the entire internet 20 years ago. Nearly real-time information makes it possible for a company to be much more agile than its competitors. And that information can come from social networks, images, sensors, the web, or other unstructured sources. The managerial challenges, however, are very real. Senior decision makers have to learn to ask the right questions and embrace evidence-based decision making. Organizations must hire scientists who can find patterns in very large data sets and translate them into useful business information. IT departments have to work hard to integrate all the relevant internal and external sources of data. The authors offer two success stories to illustrate how companies are using big data: PASSUR Aerospace enables airlines to match their actual and estimated arrival times. Sears Holdings directly analyzes its incoming store data to make promotions much more precise and faster.",fullPaper,jv241
Medicine,p1009,d1,41d4e093d5f7ed5aae1aaa9eb6c037742e4cf9b1,j7,Journal of Big Data,The use of Big Data Analytics in healthcare,Abstract,fullPaper,jv7
Computer Science,p1009,d3,41d4e093d5f7ed5aae1aaa9eb6c037742e4cf9b1,j7,Journal of Big Data,The use of Big Data Analytics in healthcare,Abstract,fullPaper,jv7
Medicine,p1017,d1,fe44200fed05f9a7c656f2245deded8fd5f5e1e6,j7,Journal of Big Data,CatBoost for big data: an interdisciplinary review,Abstract,fullPaper,jv7
Computer Science,p1017,d3,fe44200fed05f9a7c656f2245deded8fd5f5e1e6,j7,Journal of Big Data,CatBoost for big data: an interdisciplinary review,Abstract,fullPaper,jv7
Medicine,p1018,d1,a0d18dddaa995b126ad373e33767b9b881d16b2f,c68,Symposium on Advances in Databases and Information Systems,An Introductory Review of Deep Learning for Prediction Models With Big Data,"Deep learning models stand for a new learning paradigm in artificial intelligence (AI) and machine learning. Recent breakthrough results in image analysis and speech recognition have generated a massive interest in this field because also applications in many other domains providing big data seem possible. On a downside, the mathematical and computational methodology underlying deep learning models is very challenging, especially for interdisciplinary scientists. For this reason, we present in this paper an introductory review of deep learning approaches including Deep Feedforward Neural Networks (D-FFNN), Convolutional Neural Networks (CNNs), Deep Belief Networks (DBNs), Autoencoders (AEs), and Long Short-Term Memory (LSTM) networks. These models form the major core architectures of deep learning models currently used and should belong in any data scientist's toolbox. Importantly, those core architectural building blocks can be composed flexibly—in an almost Lego-like manner—to build new application-specific network architectures. Hence, a basic understanding of these network architectures is important to be prepared for future developments in AI.",poster,cp68
Computer Science,p1018,d3,a0d18dddaa995b126ad373e33767b9b881d16b2f,c68,Symposium on Advances in Databases and Information Systems,An Introductory Review of Deep Learning for Prediction Models With Big Data,"Deep learning models stand for a new learning paradigm in artificial intelligence (AI) and machine learning. Recent breakthrough results in image analysis and speech recognition have generated a massive interest in this field because also applications in many other domains providing big data seem possible. On a downside, the mathematical and computational methodology underlying deep learning models is very challenging, especially for interdisciplinary scientists. For this reason, we present in this paper an introductory review of deep learning approaches including Deep Feedforward Neural Networks (D-FFNN), Convolutional Neural Networks (CNNs), Deep Belief Networks (DBNs), Autoencoders (AEs), and Long Short-Term Memory (LSTM) networks. These models form the major core architectures of deep learning models currently used and should belong in any data scientist's toolbox. Importantly, those core architectural building blocks can be composed flexibly—in an almost Lego-like manner—to build new application-specific network architectures. Hence, a basic understanding of these network architectures is important to be prepared for future developments in AI.",poster,cp68
Medicine,p1024,d1,8894d431a768a35dc7ca4d762ebdba4f407b978c,c36,International Conference on Information Technology Based Higher Education and Training,The ProteomeXchange consortium in 2020: enabling ‘big data’ approaches in proteomics,"Abstract The ProteomeXchange (PX) consortium of proteomics resources (http://www.proteomexchange.org) has standardized data submission and dissemination of mass spectrometry proteomics data worldwide since 2012. In this paper, we describe the main developments since the previous update manuscript was published in Nucleic Acids Research in 2017. Since then, in addition to the four PX existing members at the time (PRIDE, PeptideAtlas including the PASSEL resource, MassIVE and jPOST), two new resources have joined PX: iProX (China) and Panorama Public (USA). We first describe the updated submission guidelines, now expanded to include six members. Next, with current data submission statistics, we demonstrate that the proteomics field is now actively embracing public open data policies. At the end of June 2019, more than 14 100 datasets had been submitted to PX resources since 2012, and from those, more than 9 500 in just the last three years. In parallel, an unprecedented increase of data re-use activities in the field, including ‘big data’ approaches, is enabling novel research and new data resources. At last, we also outline some of our future plans for the coming years.",poster,cp36
Computer Science,p1024,d3,8894d431a768a35dc7ca4d762ebdba4f407b978c,c36,International Conference on Information Technology Based Higher Education and Training,The ProteomeXchange consortium in 2020: enabling ‘big data’ approaches in proteomics,"Abstract The ProteomeXchange (PX) consortium of proteomics resources (http://www.proteomexchange.org) has standardized data submission and dissemination of mass spectrometry proteomics data worldwide since 2012. In this paper, we describe the main developments since the previous update manuscript was published in Nucleic Acids Research in 2017. Since then, in addition to the four PX existing members at the time (PRIDE, PeptideAtlas including the PASSEL resource, MassIVE and jPOST), two new resources have joined PX: iProX (China) and Panorama Public (USA). We first describe the updated submission guidelines, now expanded to include six members. Next, with current data submission statistics, we demonstrate that the proteomics field is now actively embracing public open data policies. At the end of June 2019, more than 14 100 datasets had been submitted to PX resources since 2012, and from those, more than 9 500 in just the last three years. In parallel, an unprecedented increase of data re-use activities in the field, including ‘big data’ approaches, is enabling novel research and new data resources. At last, we also outline some of our future plans for the coming years.",poster,cp36
Biology,p1024,d5,8894d431a768a35dc7ca4d762ebdba4f407b978c,c36,International Conference on Information Technology Based Higher Education and Training,The ProteomeXchange consortium in 2020: enabling ‘big data’ approaches in proteomics,"Abstract The ProteomeXchange (PX) consortium of proteomics resources (http://www.proteomexchange.org) has standardized data submission and dissemination of mass spectrometry proteomics data worldwide since 2012. In this paper, we describe the main developments since the previous update manuscript was published in Nucleic Acids Research in 2017. Since then, in addition to the four PX existing members at the time (PRIDE, PeptideAtlas including the PASSEL resource, MassIVE and jPOST), two new resources have joined PX: iProX (China) and Panorama Public (USA). We first describe the updated submission guidelines, now expanded to include six members. Next, with current data submission statistics, we demonstrate that the proteomics field is now actively embracing public open data policies. At the end of June 2019, more than 14 100 datasets had been submitted to PX resources since 2012, and from those, more than 9 500 in just the last three years. In parallel, an unprecedented increase of data re-use activities in the field, including ‘big data’ approaches, is enabling novel research and new data resources. At last, we also outline some of our future plans for the coming years.",poster,cp36
Medicine,p1030,d1,48fc9c42522184c652742255fdf31f7b9ed7ebae,j127,Journal of Evidence-Based Medicine,Brief introduction of medical database and data mining technology in big data era,"Data mining technology can search for potentially valuable knowledge from a large amount of data, mainly divided into data preparation and data mining, and expression and analysis of results. It is a mature information processing technology and applies database technology. Database technology is a software science that researches manages, and applies databases. The data in the database are processed and analyzed by studying the underlying theory and implementation methods of the structure, storage, design, management, and application of the database. We have introduced several databases and data mining techniques to help a wide range of clinical researchers better understand and apply database technology.",fullPaper,jv127
Medicine,p1036,d1,9e3816be8cf4821d74e258de10ee471382936a30,j250,Nature Network Boston,Privacy in the age of medical big data,Abstract,fullPaper,jv250
Business,p1036,d9,9e3816be8cf4821d74e258de10ee471382936a30,j250,Nature Network Boston,Privacy in the age of medical big data,Abstract,fullPaper,jv250
Medicine,p1043,d1,dac7344737cb824634f757aede2dd46a6eed204b,c81,ACM Symposium on Applied Computing,Big data analytics in healthcare: promise and potential,Abstract,poster,cp81
Computer Science,p1043,d3,dac7344737cb824634f757aede2dd46a6eed204b,c81,ACM Symposium on Applied Computing,Big data analytics in healthcare: promise and potential,Abstract,poster,cp81
Medicine,p1044,d1,9810bcaf5ac1792e6a2738a86f85ce270d448040,j121,Nature Communications,Flexible and durable wood-based triboelectric nanogenerators for self-powered sensing in athletic big data analytics,Abstract,fullPaper,jv121
Computer Science,p1044,d3,9810bcaf5ac1792e6a2738a86f85ce270d448040,j121,Nature Communications,Flexible and durable wood-based triboelectric nanogenerators for self-powered sensing in athletic big data analytics,Abstract,fullPaper,jv121
Medicine,p1047,d1,82870bc488b57cdf5ea62877109a7278af2926b3,j252,Annual Review of Pharmacology and Toxicology,Big Data and Artificial Intelligence Modeling for Drug Discovery.,"Due to the massive data sets available for drug candidates, modern drug discovery has advanced to the big data era. Central to this shift is the development of artificial intelligence approaches to implementing innovative modeling based on the dynamic, heterogeneous, and large nature of drug data sets. As a result, recently developed artificial intelligence approaches such as deep learning and relevant modeling studies provide new solutions to efficacy and safety evaluations of drug candidates based on big data modeling and analysis. The resulting models provided deep insights into the continuum from chemical structure to in vitro, in vivo, and clinical outcomes. The relevant novel data mining, curation, and management techniques provided critical support to recent modeling studies. In summary, the new advancement of artificial intelligence in the big data era has paved the road to future rational drug development and optimization, which will have a significant impact on drug discovery procedures and, eventually, public health. Expected final online publication date for the Annual Review of Psychology, Volume 71 is January 4, 2020. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.",fullPaper,jv252
Computer Science,p1047,d3,82870bc488b57cdf5ea62877109a7278af2926b3,j252,Annual Review of Pharmacology and Toxicology,Big Data and Artificial Intelligence Modeling for Drug Discovery.,"Due to the massive data sets available for drug candidates, modern drug discovery has advanced to the big data era. Central to this shift is the development of artificial intelligence approaches to implementing innovative modeling based on the dynamic, heterogeneous, and large nature of drug data sets. As a result, recently developed artificial intelligence approaches such as deep learning and relevant modeling studies provide new solutions to efficacy and safety evaluations of drug candidates based on big data modeling and analysis. The resulting models provided deep insights into the continuum from chemical structure to in vitro, in vivo, and clinical outcomes. The relevant novel data mining, curation, and management techniques provided critical support to recent modeling studies. In summary, the new advancement of artificial intelligence in the big data era has paved the road to future rational drug development and optimization, which will have a significant impact on drug discovery procedures and, eventually, public health. Expected final online publication date for the Annual Review of Psychology, Volume 71 is January 4, 2020. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.",fullPaper,jv252
Medicine,p1050,d1,718b5a40dba91bfa0bdfb9ac9ca4381425d2ff95,j250,Nature Network Boston,Axes of a revolution: challenges and promises of big data in healthcare,Abstract,fullPaper,jv250
Business,p1050,d9,718b5a40dba91bfa0bdfb9ac9ca4381425d2ff95,j250,Nature Network Boston,Axes of a revolution: challenges and promises of big data in healthcare,Abstract,fullPaper,jv250
Medicine,p1052,d1,91fc647899f801c9d351349ce73779918f90a713,j255,The Lancet Oncology,Big data and machine learning algorithms for health-care delivery.,Abstract,fullPaper,jv255
Medicine,p1055,d1,ecef432e7f6c9f431d5b34706a8de1fdebec46f9,j137,Frontiers in Medicine,From Big Data to Precision Medicine,"For over a decade the term “Big data” has been used to describe the rapid increase in volume, variety and velocity of information available, not just in medical research but in almost every aspect of our lives. As scientists, we now have the capacity to rapidly generate, store and analyse data that, only a few years ago, would have taken many years to compile. However, “Big data” no longer means what it once did. The term has expanded and now refers not to just large data volume, but to our increasing ability to analyse and interpret those data. Tautologies such as “data analytics” and “data science” have emerged to describe approaches to the volume of available information as it grows ever larger. New methods dedicated to improving data collection, storage, cleaning, processing and interpretation continue to be developed, although not always by, or for, medical researchers. Exploiting new tools to extract meaning from large volume information has the potential to drive real change in clinical practice, from personalized therapy and intelligent drug design to population screening and electronic health record mining. As ever, where new technology promises “Big Advances,” significant challenges remain. Here we discuss both the opportunities and challenges posed to biomedical research by our increasing ability to tackle large datasets. Important challenges include the need for standardization of data content, format, and clinical definitions, a heightened need for collaborative networks with sharing of both data and expertise and, perhaps most importantly, a need to reconsider how and when analytic methodology is taught to medical researchers. We also set “Big data” analytics in context: recent advances may appear to promise a revolution, sweeping away conventional approaches to medical science. However, their real promise lies in their synergy with, not replacement of, classical hypothesis-driven methods. The generation of novel, data-driven hypotheses based on interpretable models will always require stringent validation and experimental testing. Thus, hypothesis-generating research founded on large datasets adds to, rather than replaces, traditional hypothesis driven science. Each can benefit from the other and it is through using both that we can improve clinical practice.",fullPaper,jv137
Computer Science,p1055,d3,ecef432e7f6c9f431d5b34706a8de1fdebec46f9,j137,Frontiers in Medicine,From Big Data to Precision Medicine,"For over a decade the term “Big data” has been used to describe the rapid increase in volume, variety and velocity of information available, not just in medical research but in almost every aspect of our lives. As scientists, we now have the capacity to rapidly generate, store and analyse data that, only a few years ago, would have taken many years to compile. However, “Big data” no longer means what it once did. The term has expanded and now refers not to just large data volume, but to our increasing ability to analyse and interpret those data. Tautologies such as “data analytics” and “data science” have emerged to describe approaches to the volume of available information as it grows ever larger. New methods dedicated to improving data collection, storage, cleaning, processing and interpretation continue to be developed, although not always by, or for, medical researchers. Exploiting new tools to extract meaning from large volume information has the potential to drive real change in clinical practice, from personalized therapy and intelligent drug design to population screening and electronic health record mining. As ever, where new technology promises “Big Advances,” significant challenges remain. Here we discuss both the opportunities and challenges posed to biomedical research by our increasing ability to tackle large datasets. Important challenges include the need for standardization of data content, format, and clinical definitions, a heightened need for collaborative networks with sharing of both data and expertise and, perhaps most importantly, a need to reconsider how and when analytic methodology is taught to medical researchers. We also set “Big data” analytics in context: recent advances may appear to promise a revolution, sweeping away conventional approaches to medical science. However, their real promise lies in their synergy with, not replacement of, classical hypothesis-driven methods. The generation of novel, data-driven hypotheses based on interpretable models will always require stringent validation and experimental testing. Thus, hypothesis-generating research founded on large datasets adds to, rather than replaces, traditional hypothesis driven science. Each can benefit from the other and it is through using both that we can improve clinical practice.",fullPaper,jv137
Medicine,p1059,d1,75c364909914f17791837ec88090262aa6656d3e,j259,Gut,Big data in IBD: big progress for clinical practice,"IBD is a complex multifactorial inflammatory disease of the gut driven by extrinsic and intrinsic factors, including host genetics, the immune system, environmental factors and the gut microbiome. Technological advancements such as next-generation sequencing, high-throughput omics data generation and molecular networks have catalysed IBD research. The advent of artificial intelligence, in particular, machine learning, and systems biology has opened the avenue for the efficient integration and interpretation of big datasets for discovering clinically translatable knowledge. In this narrative review, we discuss how big data integration and machine learning have been applied to translational IBD research. Approaches such as machine learning may enable patient stratification, prediction of disease progression and therapy responses for fine-tuning treatment options with positive impacts on cost, health and safety. We also outline the challenges and opportunities presented by machine learning and big data in clinical IBD research.",fullPaper,jv259
Computer Science,p1059,d3,75c364909914f17791837ec88090262aa6656d3e,j259,Gut,Big data in IBD: big progress for clinical practice,"IBD is a complex multifactorial inflammatory disease of the gut driven by extrinsic and intrinsic factors, including host genetics, the immune system, environmental factors and the gut microbiome. Technological advancements such as next-generation sequencing, high-throughput omics data generation and molecular networks have catalysed IBD research. The advent of artificial intelligence, in particular, machine learning, and systems biology has opened the avenue for the efficient integration and interpretation of big datasets for discovering clinically translatable knowledge. In this narrative review, we discuss how big data integration and machine learning have been applied to translational IBD research. Approaches such as machine learning may enable patient stratification, prediction of disease progression and therapy responses for fine-tuning treatment options with positive impacts on cost, health and safety. We also outline the challenges and opportunities presented by machine learning and big data in clinical IBD research.",fullPaper,jv259
Medicine,p1067,d1,de6cf3534f39748a223b9bb2b59d2e7ffcb6ae03,j261,American Journal of Epidemiology,Using Big Data to Emulate a Target Trial When a Randomized Trial Is Not Available.,"Ideally, questions about comparative effectiveness or safety would be answered using an appropriately designed and conducted randomized experiment. When we cannot conduct a randomized experiment, we analyze observational data. Causal inference from large observational databases (big data) can be viewed as an attempt to emulate a randomized experiment-the target experiment or target trial-that would answer the question of interest. When the goal is to guide decisions among several strategies, causal analyses of observational data need to be evaluated with respect to how well they emulate a particular target trial. We outline a framework for comparative effectiveness research using big data that makes the target trial explicit. This framework channels counterfactual theory for comparing the effects of sustained treatment strategies, organizes analytic approaches, provides a structured process for the criticism of observational studies, and helps avoid common methodologic pitfalls.",fullPaper,jv261
Medicine,p1077,d1,75fb60dee6641656fc4c1b663bb95d1aa91c6798,j266,International Journal of Obesity,A Delphi study to build consensus on the definition and use of big data in obesity research,Abstract,fullPaper,jv266
Psychology,p1077,d10,75fb60dee6641656fc4c1b663bb95d1aa91c6798,j266,International Journal of Obesity,A Delphi study to build consensus on the definition and use of big data in obesity research,Abstract,fullPaper,jv266
Medicine,p1078,d1,00d681cfff7f0224ace607e515f4ed510e792df9,j104,Science,The Parable of Google Flu: Traps in Big Data Analysis,"Large errors in flu prediction were largely avoidable, which offers lessons for the use of big data. In February 2013, Google Flu Trends (GFT) made headlines but not for a reason that Google executives or the creators of the flu tracking system would have hoped. Nature reported that GFT was predicting more than double the proportion of doctor visits for influenza-like illness (ILI) than the Centers for Disease Control and Prevention (CDC), which bases its estimates on surveillance reports from laboratories across the United States (1, 2). This happened despite the fact that GFT was built to predict CDC reports. Given that GFT is often held up as an exemplary use of big data (3, 4), what lessons can we draw from this error?",fullPaper,jv104
Medicine,p1080,d1,efca2a32ce9c7a808c2c3efcc2c3dac032dfc8ea,c112,British Machine Vision Conference,Big Data and Machine Learning in Health Care.,"Nearly all aspects of modern life are in some way being changed by big data and machine learning. Netflix knows what movies people like to watch and Google knows what people want to know based on their search histories. Indeed, Google has recently begun to replace much of its existing non–machine learning technology with machine learning algorithms, and there is great optimism that these techniques can provide similar improvements across many sectors. It isnosurprisethenthatmedicineisawashwithclaims of revolution from the application of machine learning to big health care data. Recent examples have demonstrated that big data and machine learning can create algorithms that perform on par with human physicians.1 Though machine learning and big data may seem mysterious at first, they are in fact deeply related to traditional statistical models that are recognizable to most clinicians. It is our hope that elucidating these connections will demystify these techniques and provide a set of reasonable expectations for the role of machine learning and big data in health care. Machine learning was originally described as a program that learns to perform a task or make a decision automatically from data, rather than having the behavior explicitlyprogrammed.However,thisdefinitionisverybroad and could cover nearly any form of data-driven approach. For instance, consider the Framingham cardiovascular risk score,whichassignspointstovariousfactorsandproduces a number that predicts 10-year cardiovascular risk. Should this be considered an example of machine learning? The answer might obviously seem to be no. Closer inspection oftheFraminghamriskscorerevealsthattheanswermight not be as obvious as it first seems. The score was originally created2 by fitting a proportional hazards model to data frommorethan5300patients,andsothe“rule”wasinfact learnedentirelyfromdata.Designatingariskscoreasamachine learning algorithm might seem a strange notion, but this example reveals the uncertain nature of the original definition of machine learning. It is perhaps more useful to imagine an algorithm as existing along a continuum between fully human-guided vs fully machine-guided data analysis. To understand the degree to which a predictive or diagnostic algorithm can said to be an instance of machine learning requires understanding how much of its structure or parameters were predetermined by humans. The trade-off between human specificationofapredictivealgorithm’spropertiesvslearning those properties from data is what is known as the machine learning spectrum. Returning to the Framingham study, to create the original risk score statisticians and clinical experts worked together to make many important decisions, such as which variables to include in the model, therelationshipbetweenthedependentandindependent variables, and variable transformations and interactions. Since considerable human effort was used to define these properties, it would place low on the machine learning spectrum (#19 in the Figure and Supplement). Many evidence-based clinical practices are based on a statistical model of this sort, and so many clinical decisions in fact exist on the machine learning spectrum (middle left of Figure). On the extreme low end of the machine learning spectrum would be heuristics and rules of thumb that do not directly involve the use of any rules or models explicitly derived from data (bottom left of Figure). Suppose a new cardiovascular risk score is created that includes possible extensions to the original model. For example, it could be that risk factors should not be added but instead should be multiplied or divided, or perhaps a particularly important risk factor should square the entire score if it is present. Moreover, if it is not known in advance which variables will be important, but thousands of individual measurements have been collected, how should a good model be identified from among the infinite possibilities? This is precisely what a machine learning algorithm attempts to do. As humans impose fewer assumptions on the algorithm, it moves further up the machine learning spectrum. However, there is never a specific threshold wherein a model suddenly becomes “machine learning”; rather, all of these approaches exist along a continuum, determined by how many human assumptions are placed onto the algorithm. An example of an approach high on the machine learning spectrum has recently emerged in the form of so-called deep learning models. Deep learning models are stunningly complex networks of artificial neurons that were designed expressly to create accurate models directly from raw data. Researchers recently demonstrated a deep learning algorithm capable of detecting diabetic retinopathy (#4 in the Figure, top center) from retinal photographs at a sensitivity equal to or greater than that of ophthalmologists.1 This model learned the diagnosis procedure directly from the raw pixels of the images with no human intervention outside of a team of ophthalmologists who annotated each image with the correct diagnosis. Because they are able to learn the task with little human instruction or prior assumptions, these deep learning algorithms rank very high on the machine learning spectrum (Figure, light blue circles). Though they require less human guidance, deep learning algorithms for image recognition require enormous amounts of data to capture the full complexity, variety, and nuance inherent to real-world images. Consequently, these algorithms often require hundreds of thousands of examples to extract the salient image features that are correlated with the outcome of interest. Higher placement on the machine learning spectrum does not imply superiority, because different tasks require different levels of human involvement. While algorithms high on the spectrum are often very flexible and can learn many tasks, they are often uninterpretable VIEWPOINT",poster,cp112
Medicine,p1085,d1,4d1fdd81f033cd58f3723bfc61e7d12079647a7a,j270,New England Journal of Medicine,"Predicting the Future - Big Data, Machine Learning, and Clinical Medicine.","The algorithms of machine learning, which can sift through vast numbers of variables looking for combinations that reliably predict outcomes, will improve prognosis, displace much of the work of radiologists and anatomical pathologists, and improve diagnostic accuracy.",fullPaper,jv270
Medicine,p1091,d1,521e5c337be51b8f8fdb858580bb46a0545ab1f9,c91,International Symposium on High-Performance Computer Architecture,When Gaussian Process Meets Big Data: A Review of Scalable GPs,"The vast quantity of information brought by big data as well as the evolving computer hardware encourages success stories in the machine learning community. In the meanwhile, it poses challenges for the Gaussian process regression (GPR), a well-known nonparametric, and interpretable Bayesian model, which suffers from cubic complexity to data size. To improve the scalability while retaining desirable prediction quality, a variety of scalable GPs have been presented. However, they have not yet been comprehensively reviewed and analyzed to be well understood by both academia and industry. The review of scalable GPs in the GP community is timely and important due to the explosion of data size. To this end, this article is devoted to reviewing state-of-the-art scalable GPs involving two main categories: global approximations that distillate the entire data and local approximations that divide the data for subspace learning. Particularly, for global approximations, we mainly focus on sparse approximations comprising prior approximations that modify the prior but perform exact inference, posterior approximations that retain exact prior but perform approximate inference, and structured sparse approximations that exploit specific structures in kernel matrix; for local approximations, we highlight the mixture/product of experts that conducts model averaging from multiple local experts to boost predictions. To present a complete review, recent advances for improving the scalability and capability of scalable GPs are reviewed. Finally, the extensions and open issues of scalable GPs in various scenarios are reviewed and discussed to inspire novel ideas for future research avenues.",poster,cp91
Computer Science,p1091,d3,521e5c337be51b8f8fdb858580bb46a0545ab1f9,c91,International Symposium on High-Performance Computer Architecture,When Gaussian Process Meets Big Data: A Review of Scalable GPs,"The vast quantity of information brought by big data as well as the evolving computer hardware encourages success stories in the machine learning community. In the meanwhile, it poses challenges for the Gaussian process regression (GPR), a well-known nonparametric, and interpretable Bayesian model, which suffers from cubic complexity to data size. To improve the scalability while retaining desirable prediction quality, a variety of scalable GPs have been presented. However, they have not yet been comprehensively reviewed and analyzed to be well understood by both academia and industry. The review of scalable GPs in the GP community is timely and important due to the explosion of data size. To this end, this article is devoted to reviewing state-of-the-art scalable GPs involving two main categories: global approximations that distillate the entire data and local approximations that divide the data for subspace learning. Particularly, for global approximations, we mainly focus on sparse approximations comprising prior approximations that modify the prior but perform exact inference, posterior approximations that retain exact prior but perform approximate inference, and structured sparse approximations that exploit specific structures in kernel matrix; for local approximations, we highlight the mixture/product of experts that conducts model averaging from multiple local experts to boost predictions. To present a complete review, recent advances for improving the scalability and capability of scalable GPs are reviewed. Finally, the extensions and open issues of scalable GPs in various scenarios are reviewed and discussed to inspire novel ideas for future research avenues.",poster,cp91
Mathematics,p1091,d6,521e5c337be51b8f8fdb858580bb46a0545ab1f9,c91,International Symposium on High-Performance Computer Architecture,When Gaussian Process Meets Big Data: A Review of Scalable GPs,"The vast quantity of information brought by big data as well as the evolving computer hardware encourages success stories in the machine learning community. In the meanwhile, it poses challenges for the Gaussian process regression (GPR), a well-known nonparametric, and interpretable Bayesian model, which suffers from cubic complexity to data size. To improve the scalability while retaining desirable prediction quality, a variety of scalable GPs have been presented. However, they have not yet been comprehensively reviewed and analyzed to be well understood by both academia and industry. The review of scalable GPs in the GP community is timely and important due to the explosion of data size. To this end, this article is devoted to reviewing state-of-the-art scalable GPs involving two main categories: global approximations that distillate the entire data and local approximations that divide the data for subspace learning. Particularly, for global approximations, we mainly focus on sparse approximations comprising prior approximations that modify the prior but perform exact inference, posterior approximations that retain exact prior but perform approximate inference, and structured sparse approximations that exploit specific structures in kernel matrix; for local approximations, we highlight the mixture/product of experts that conducts model averaging from multiple local experts to boost predictions. To present a complete review, recent advances for improving the scalability and capability of scalable GPs are reviewed. Finally, the extensions and open issues of scalable GPs in various scenarios are reviewed and discussed to inspire novel ideas for future research avenues.",poster,cp91
Medicine,p1117,d1,32a616a105870c594476a9bba35dc5c602aae1d7,c82,Symposium on Networked Systems Design and Implementation,Big data in IBD: a look into the future,Abstract,poster,cp82
Medicine,p1118,d1,59755487e71adc7d78c8917f8c401c2d083c21f3,j250,Nature Network Boston,A Longitudinal Big Data Approach for Precision Health,Abstract,fullPaper,jv250
Medicine,p1119,d1,c7e9d4837edecc50c94a854e80807a27865dfd91,c27,International Conference Geographic Information Science,Putting the data before the algorithm in big data addressing personalized healthcare,Abstract,poster,cp27
Computer Science,p1119,d3,c7e9d4837edecc50c94a854e80807a27865dfd91,c27,International Conference Geographic Information Science,Putting the data before the algorithm in big data addressing personalized healthcare,Abstract,poster,cp27
Medicine,p1129,d1,e59d5ee879e408cccc7c055e44650dcb6f88c86f,j279,American Sociological Review,Big Data Surveillance: The Case of Policing,"This article examines the intersection of two structural developments: the growth of surveillance and the rise of “big data.” Drawing on observations and interviews conducted within the Los Angeles Police Department, I offer an empirical account of how the adoption of big data analytics does—and does not—transform police surveillance practices. I argue that the adoption of big data analytics facilitates amplifications of prior surveillance practices and fundamental transformations in surveillance activities. First, discretionary assessments of risk are supplemented and quantified using risk scores. Second, data are used for predictive, rather than reactive or explanatory, purposes. Third, the proliferation of automatic alert systems makes it possible to systematically surveil an unprecedentedly large number of people. Fourth, the threshold for inclusion in law enforcement databases is lower, now including individuals who have not had direct police contact. Fifth, previously separate data systems are merged, facilitating the spread of surveillance into a wide range of institutions. Based on these findings, I develop a theoretical model of big data surveillance that can be applied to institutional domains beyond the criminal justice system. Finally, I highlight the social consequences of big data surveillance for law and social inequality.",fullPaper,jv279
Political Science,p1129,d15,e59d5ee879e408cccc7c055e44650dcb6f88c86f,j279,American Sociological Review,Big Data Surveillance: The Case of Policing,"This article examines the intersection of two structural developments: the growth of surveillance and the rise of “big data.” Drawing on observations and interviews conducted within the Los Angeles Police Department, I offer an empirical account of how the adoption of big data analytics does—and does not—transform police surveillance practices. I argue that the adoption of big data analytics facilitates amplifications of prior surveillance practices and fundamental transformations in surveillance activities. First, discretionary assessments of risk are supplemented and quantified using risk scores. Second, data are used for predictive, rather than reactive or explanatory, purposes. Third, the proliferation of automatic alert systems makes it possible to systematically surveil an unprecedentedly large number of people. Fourth, the threshold for inclusion in law enforcement databases is lower, now including individuals who have not had direct police contact. Fifth, previously separate data systems are merged, facilitating the spread of surveillance into a wide range of institutions. Based on these findings, I develop a theoretical model of big data surveillance that can be applied to institutional domains beyond the criminal justice system. Finally, I highlight the social consequences of big data surveillance for law and social inequality.",fullPaper,jv279
Medicine,p1131,d1,00b0ae9246fb444b4862c4e4a40126d776974f7e,j60,Nature,Renewing Felsenstein’s Phylogenetic Bootstrap in the Era of Big Data,Abstract,fullPaper,jv60
Computer Science,p1131,d3,00b0ae9246fb444b4862c4e4a40126d776974f7e,j60,Nature,Renewing Felsenstein’s Phylogenetic Bootstrap in the Era of Big Data,Abstract,fullPaper,jv60
Medicine,p1135,d1,fd0e83749943cc43ab4dd6f4317cfd4299c44cfd,j282,Science and Engineering Ethics,"Big Data, Big Waste? A Reflection on the Environmental Sustainability of Big Data Initiatives",Abstract,fullPaper,jv282
Computer Science,p1135,d3,fd0e83749943cc43ab4dd6f4317cfd4299c44cfd,j282,Science and Engineering Ethics,"Big Data, Big Waste? A Reflection on the Environmental Sustainability of Big Data Initiatives",Abstract,fullPaper,jv282
Political Science,p1135,d15,fd0e83749943cc43ab4dd6f4317cfd4299c44cfd,j282,Science and Engineering Ethics,"Big Data, Big Waste? A Reflection on the Environmental Sustainability of Big Data Initiatives",Abstract,fullPaper,jv282
Medicine,p1139,d1,ac89bccb5806eeafc6122a7e27affea4fe8cb809,c18,International Conference on Exploring Services Science,Big data analytics for preventive medicine,Abstract,poster,cp18
Computer Science,p1139,d3,ac89bccb5806eeafc6122a7e27affea4fe8cb809,c18,International Conference on Exploring Services Science,Big data analytics for preventive medicine,Abstract,poster,cp18
Medicine,p1149,d1,04163a7e819b37f7ed1c9031ea128132e1465975,c19,International Conference on Conceptual Structures,VFDB 2016: hierarchical and refined dataset for big data analysis—10 years on,"The virulence factor database (VFDB, http://www.mgc.ac.cn/VFs/) is dedicated to providing up-to-date knowledge of virulence factors (VFs) of various bacterial pathogens. Since its inception the VFDB has served as a comprehensive repository of bacterial VFs for over a decade. The exponential growth in the amount of biological data is challenging to the current database in regard to big data analysis. We recently improved two aspects of the infrastructural dataset of VFDB: (i) removed the redundancy introduced by previous releases and generated two hierarchical datasets – one core dataset of experimentally verified VFs only and another full dataset including all known and predicted VFs and (ii) refined the gene annotation of the core dataset with controlled vocabularies. Our efforts enhanced the data quality of the VFDB and promoted the usability of the database in the big data era for the bioinformatic mining of the explosively growing data regarding bacterial VFs.",poster,cp19
Computer Science,p1149,d3,04163a7e819b37f7ed1c9031ea128132e1465975,c19,International Conference on Conceptual Structures,VFDB 2016: hierarchical and refined dataset for big data analysis—10 years on,"The virulence factor database (VFDB, http://www.mgc.ac.cn/VFs/) is dedicated to providing up-to-date knowledge of virulence factors (VFs) of various bacterial pathogens. Since its inception the VFDB has served as a comprehensive repository of bacterial VFs for over a decade. The exponential growth in the amount of biological data is challenging to the current database in regard to big data analysis. We recently improved two aspects of the infrastructural dataset of VFDB: (i) removed the redundancy introduced by previous releases and generated two hierarchical datasets – one core dataset of experimentally verified VFs only and another full dataset including all known and predicted VFs and (ii) refined the gene annotation of the core dataset with controlled vocabularies. Our efforts enhanced the data quality of the VFDB and promoted the usability of the database in the big data era for the bioinformatic mining of the explosively growing data regarding bacterial VFs.",poster,cp19
Biology,p1149,d5,04163a7e819b37f7ed1c9031ea128132e1465975,c19,International Conference on Conceptual Structures,VFDB 2016: hierarchical and refined dataset for big data analysis—10 years on,"The virulence factor database (VFDB, http://www.mgc.ac.cn/VFs/) is dedicated to providing up-to-date knowledge of virulence factors (VFs) of various bacterial pathogens. Since its inception the VFDB has served as a comprehensive repository of bacterial VFs for over a decade. The exponential growth in the amount of biological data is challenging to the current database in regard to big data analysis. We recently improved two aspects of the infrastructural dataset of VFDB: (i) removed the redundancy introduced by previous releases and generated two hierarchical datasets – one core dataset of experimentally verified VFs only and another full dataset including all known and predicted VFs and (ii) refined the gene annotation of the core dataset with controlled vocabularies. Our efforts enhanced the data quality of the VFDB and promoted the usability of the database in the big data era for the bioinformatic mining of the explosively growing data regarding bacterial VFs.",poster,cp19
Medicine,p1152,d1,179cf488917709be99fbab81158f520cfead41d3,j287,Current Opinion in Biotechnology,Big data analytics for personalized medicine.,Abstract,fullPaper,jv287
Computer Science,p1152,d3,179cf488917709be99fbab81158f520cfead41d3,j287,Current Opinion in Biotechnology,Big data analytics for personalized medicine.,Abstract,fullPaper,jv287
Medicine,p1160,d1,3b718eebc6f4469053dadb5737b67ee18b16722d,j104,Science,Beyond prediction: Using big data for policy problems,"Machine-learning prediction methods have been extremely productive in applications ranging from medicine to allocating fire and health inspectors in cities. However, there are a number of gaps between making a prediction and making a decision, and underlying assumptions need to be understood in order to optimize data-driven decision-making.",fullPaper,jv104
Computer Science,p1160,d3,3b718eebc6f4469053dadb5737b67ee18b16722d,j104,Science,Beyond prediction: Using big data for policy problems,"Machine-learning prediction methods have been extremely productive in applications ranging from medicine to allocating fire and health inspectors in cities. However, there are a number of gaps between making a prediction and making a decision, and underlying assumptions need to be understood in order to optimize data-driven decision-making.",fullPaper,jv104
Medicine,p1164,d1,433a6460a07ad6c2e24e8ef9c8d197b07303416d,j293,Kidney Research and Clinical Practice,Medical big data: promise and challenges,"The concept of big data, commonly characterized by volume, variety, velocity, and veracity, goes far beyond the data type and includes the aspects of data analysis, such as hypothesis-generating, rather than hypothesis-testing. Big data focuses on temporal stability of the association, rather than on causal relationship and underlying probability distribution assumptions are frequently not required. Medical big data as material to be analyzed has various features that are not only distinct from big data of other disciplines, but also distinct from traditional clinical epidemiology. Big data technology has many areas of application in healthcare, such as predictive modeling and clinical decision support, disease or safety surveillance, public health, and research. Big data analytics frequently exploits analytic methods developed in data mining, including classification, clustering, and regression. Medical big data analyses are complicated by many technical issues, such as missing values, curse of dimensionality, and bias control, and share the inherent limitations of observation study, namely the inability to test causality resulting from residual confounding and reverse causation. Recently, propensity score analysis and instrumental variable analysis have been introduced to overcome these limitations, and they have accomplished a great deal. Many challenges, such as the absence of evidence of practical benefits of big data, methodological issues including legal and ethical issues, and clinical integration and utility issues, must be overcome to realize the promise of medical big data as the fuel of a continuous learning healthcare system that will improve patient outcome and reduce waste in areas including nephrology.",fullPaper,jv293
Medicine,p1175,d1,6a028cc26d7eceada86e873a37e57a3e113e24af,j297,Annual Review of Public Health,"Big Data in Public Health: Terminology, Machine Learning, and Privacy.","The digital world is generating data at a staggering and still increasing rate. While these ""big data"" have unlocked novel opportunities to understand public health, they hold still greater potential for research and practice. This review explores several key issues that have arisen around big data. First, we propose a taxonomy of sources of big data to clarify terminology and identify threads common across some subtypes of big data. Next, we consider common public health research and practice uses for big data, including surveillance, hypothesis-generating research, and causal inference, while exploring the role that machine learning may play in each use. We then consider the ethical implications of the big data revolution with particular emphasis on maintaining appropriate care for privacy in a world in which technology is rapidly changing social norms regarding the need for (and even the meaning of) privacy. Finally, we make suggestions regarding structuring teams and training to succeed in working with big data in research and practice.",fullPaper,jv297
Computer Science,p1175,d3,6a028cc26d7eceada86e873a37e57a3e113e24af,j297,Annual Review of Public Health,"Big Data in Public Health: Terminology, Machine Learning, and Privacy.","The digital world is generating data at a staggering and still increasing rate. While these ""big data"" have unlocked novel opportunities to understand public health, they hold still greater potential for research and practice. This review explores several key issues that have arisen around big data. First, we propose a taxonomy of sources of big data to clarify terminology and identify threads common across some subtypes of big data. Next, we consider common public health research and practice uses for big data, including surveillance, hypothesis-generating research, and causal inference, while exploring the role that machine learning may play in each use. We then consider the ethical implications of the big data revolution with particular emphasis on maintaining appropriate care for privacy in a world in which technology is rapidly changing social norms regarding the need for (and even the meaning of) privacy. Finally, we make suggestions regarding structuring teams and training to succeed in working with big data in research and practice.",fullPaper,jv297
Medicine,p1177,d1,111e7daa3c579208ba80c23ac33ab7e7b5b1f099,j298,Journal of Integrative Bioinformatics,Big Data Analytics in Medicine and Healthcare,"Abstract This paper surveys big data with highlighting the big data analytics in medicine and healthcare. Big data characteristics: value, volume, velocity, variety, veracity and variability are described. Big data analytics in medicine and healthcare covers integration and analysis of large amount of complex heterogeneous data such as various – omics data (genomics, epigenomics, transcriptomics, proteomics, metabolomics, interactomics, pharmacogenomics, diseasomics), biomedical data and electronic health records data. We underline the challenging issues about big data privacy and security. Regarding big data characteristics, some directions of using suitable and promising open-source distributed data processing software platform are given.",fullPaper,jv298
Computer Science,p1177,d3,111e7daa3c579208ba80c23ac33ab7e7b5b1f099,j298,Journal of Integrative Bioinformatics,Big Data Analytics in Medicine and Healthcare,"Abstract This paper surveys big data with highlighting the big data analytics in medicine and healthcare. Big data characteristics: value, volume, velocity, variety, veracity and variability are described. Big data analytics in medicine and healthcare covers integration and analysis of large amount of complex heterogeneous data such as various – omics data (genomics, epigenomics, transcriptomics, proteomics, metabolomics, interactomics, pharmacogenomics, diseasomics), biomedical data and electronic health records data. We underline the challenging issues about big data privacy and security. Regarding big data characteristics, some directions of using suitable and promising open-source distributed data processing software platform are given.",fullPaper,jv298
Medicine,p1178,d1,efa5558bddd68abe4adc81adbbef6f739e648392,j64,PLoS Biology,Big Data: Astronomical or Genomical?,"Genomics is a Big Data science and is going to get much bigger, very soon, but it is not known whether the needs of genomics will exceed other Big Data domains. Projecting to the year 2025, we compared genomics with three other major generators of Big Data: astronomy, YouTube, and Twitter. Our estimates show that genomics is a “four-headed beast”—it is either on par with or the most demanding of the domains analyzed here in terms of data acquisition, storage, distribution, and analysis. We discuss aspects of new technologies that will need to be developed to rise up and meet the computational challenges that genomics poses for the near future. Now is the time for concerted, community-wide planning for the “genomical” challenges of the next decade.",fullPaper,jv64
Biology,p1178,d5,efa5558bddd68abe4adc81adbbef6f739e648392,j64,PLoS Biology,Big Data: Astronomical or Genomical?,"Genomics is a Big Data science and is going to get much bigger, very soon, but it is not known whether the needs of genomics will exceed other Big Data domains. Projecting to the year 2025, we compared genomics with three other major generators of Big Data: astronomy, YouTube, and Twitter. Our estimates show that genomics is a “four-headed beast”—it is either on par with or the most demanding of the domains analyzed here in terms of data acquisition, storage, distribution, and analysis. We discuss aspects of new technologies that will need to be developed to rise up and meet the computational challenges that genomics poses for the near future. Now is the time for concerted, community-wide planning for the “genomical” challenges of the next decade.",fullPaper,jv64
Medicine,p1191,d1,cff7f7f5aa2393a343444f469b2419c8770fd65d,j302,International Journal of Environmental Research and Public Health,Artificial Intelligence and Big Data in Public Health,"Artificial intelligence and automation are topics dominating global discussions on the future of professional employment, societal change, and economic performance. In this paper, we describe fundamental concepts underlying AI and Big Data and their significance to public health. We highlight issues involved and describe the potential impacts and challenges to medical professionals and diagnosticians. The possible benefits of advanced data analytics and machine learning are described in the context of recently reported research. Problems are identified and discussed with respect to ethical issues and the future roles of professionals and specialists in the age of artificial intelligence.",fullPaper,jv302
Psychology,p1191,d10,cff7f7f5aa2393a343444f469b2419c8770fd65d,j302,International Journal of Environmental Research and Public Health,Artificial Intelligence and Big Data in Public Health,"Artificial intelligence and automation are topics dominating global discussions on the future of professional employment, societal change, and economic performance. In this paper, we describe fundamental concepts underlying AI and Big Data and their significance to public health. We highlight issues involved and describe the potential impacts and challenges to medical professionals and diagnosticians. The possible benefits of advanced data analytics and machine learning are described in the context of recently reported research. Problems are identified and discussed with respect to ethical issues and the future roles of professionals and specialists in the age of artificial intelligence.",fullPaper,jv302
Medicine,p1199,d1,14911a345e4b223d1bd756e317a7410983c28c68,c77,Visualization for Computer Security,Big Data and Predictive Analytics: Recalibrating Expectations,"With the routine use of electronic health records (EHRs) in hospitals, health systems, and physician practices, there has been rapid growth in the availability of health care data over the last decade. In addition to the structured data in EHRs, new methods such as natural language processing can derive meaning from unstructured data, permitting the capture of substantial clinical information embedded in clinical notes. Furthermore, the growth in the availability of registries and claims data and the linkages between all these data sources have created a big data platform in health care, vast in both size and scope. Concurrently, new computational machine learning approaches promise ever-more-accurate prediction. The marvel of Google and of Watson, the inexorability of Moore’s law (ie, computing power doubles every 2 years for the same cost), suggest a future in which medicine will be transformed into an information science, and each clinical decision may be optimized based on a forecasting of outcomes under alternative treatment options, beyond the knowledge and understanding of the individual physician. Yet despite these innovations and those to come, quantitative risk prediction in medicine has been available for several decades, based on more classical",poster,cp77
Medicine,p1206,d1,9cacbc50b5a94e28b9de139c6c3abd1f00bd632f,j305,Healthcare Informatics Research,Medical Internet of Things and Big Data in Healthcare,"Objectives A number of technologies can reduce overall costs for the prevention or management of chronic illnesses. These include devices that constantly monitor health indicators, devices that auto-administer therapies, or devices that track real-time health data when a patient self-administers a therapy. Because they have increased access to high-speed Internet and smartphones, many patients have started to use mobile applications (apps) to manage various health needs. These devices and mobile apps are now increasingly used and integrated with telemedicine and telehealth via the medical Internet of Things (mIoT). This paper reviews mIoT and big data in healthcare fields. Methods mIoT is a critical piece of the digital transformation of healthcare, as it allows new business models to emerge and enables changes in work processes, productivity improvements, cost containment and enhanced customer experiences. Results Wearables and mobile apps today support fitness, health education, symptom tracking, and collaborative disease management and care coordination. All those platform analytics can raise the relevancy of data interpretations, reducing the amount of time that end users spend piecing together data outputs. Insights gained from big data analysis will drive the digital disruption of the healthcare world, business processes and real-time decision-making. Conclusions A new category of ""personalised preventative health coaches"" (Digital Health Advisors) will emerge. These workers will possess the skills and the ability to interpret and understand health and well-being data. They will help their clients avoid chronic and diet-related illness, improve cognitive function, achieve improved mental health and achieve improved lifestyles overall. As the global population ages, such roles will become increasingly important.",fullPaper,jv305
Medicine,p1207,d1,54e8764d47bc08bbb29cfe89547374fec21093eb,j306,Frontiers in Public Health,Big Data’s Role in Precision Public Health,"Precision public health is an emerging practice to more granularly predict and understand public health risks and customize treatments for more specific and homogeneous subpopulations, often using new data, technologies, and methods. Big data is one element that has consistently helped to achieve these goals, through its ability to deliver to practitioners a volume and variety of structured or unstructured data not previously possible. Big data has enabled more widespread and specific research and trials of stratifying and segmenting populations at risk for a variety of health problems. Examples of success using big data are surveyed in surveillance and signal detection, predicting future risk, targeted interventions, and understanding disease. Using novel big data or big data approaches has risks that remain to be resolved. The continued growth in volume and variety of available data, decreased costs of data capture, and emerging computational methods mean big data success will likely be a required pillar of precision public health into the future. This review article aims to identify the precision public health use cases where big data has added value, identify classes of value that big data may bring, and outline the risks inherent in using big data in precision public health efforts.",fullPaper,jv306
Computer Science,p1207,d3,54e8764d47bc08bbb29cfe89547374fec21093eb,j306,Frontiers in Public Health,Big Data’s Role in Precision Public Health,"Precision public health is an emerging practice to more granularly predict and understand public health risks and customize treatments for more specific and homogeneous subpopulations, often using new data, technologies, and methods. Big data is one element that has consistently helped to achieve these goals, through its ability to deliver to practitioners a volume and variety of structured or unstructured data not previously possible. Big data has enabled more widespread and specific research and trials of stratifying and segmenting populations at risk for a variety of health problems. Examples of success using big data are surveyed in surveillance and signal detection, predicting future risk, targeted interventions, and understanding disease. Using novel big data or big data approaches has risks that remain to be resolved. The continued growth in volume and variety of available data, decreased costs of data capture, and emerging computational methods mean big data success will likely be a required pillar of precision public health into the future. This review article aims to identify the precision public health use cases where big data has added value, identify classes of value that big data may bring, and outline the risks inherent in using big data in precision public health efforts.",fullPaper,jv306
Medicine,p1208,d1,c38bab2d1fcb823300b74b11c513e6a8aa7017de,c84,EUROCON Conference,Concurrence of big data analytics and healthcare: A systematic review,Abstract,poster,cp84
Computer Science,p1208,d3,c38bab2d1fcb823300b74b11c513e6a8aa7017de,c84,EUROCON Conference,Concurrence of big data analytics and healthcare: A systematic review,Abstract,poster,cp84
Medicine,p1217,d1,6a23f1aa4fa8201b603e6fa6eb6bdccaac7df0af,j107,Nucleic Acids Research,Database Resources of the BIG Data Center in 2019,"Abstract The BIG Data Center at Beijing Institute of Genomics (BIG) of the Chinese Academy of Sciences provides a suite of database resources in support of worldwide research activities in both academia and industry. With the vast amounts of multi-omics data generated at unprecedented scales and rates, the BIG Data Center is continually expanding, updating and enriching its core database resources through big data integration and value-added curation. Resources with significant updates in the past year include BioProject (a biological project library), BioSample (a biological sample library), Genome Sequence Archive (GSA, a data repository for archiving raw sequence reads), Genome Warehouse (GWH, a centralized resource housing genome-scale data), Genome Variation Map (GVM, a public repository of genome variations), Science Wikis (a catalog of biological knowledge wikis for community annotations) and IC4R (Information Commons for Rice). Newly released resources include EWAS Atlas (a knowledgebase of epigenome-wide association studies), iDog (an integrated omics data resource for dog) and RNA editing resources (for editome-disease associations and plant RNA editosome, respectively). To promote biodiversity and health big data sharing around the world, the Open Biodiversity and Health Big Data (BHBD) initiative is introduced. All of these resources are publicly accessible at http://bigd.big.ac.cn.",fullPaper,jv107
Biology,p1217,d5,6a23f1aa4fa8201b603e6fa6eb6bdccaac7df0af,j107,Nucleic Acids Research,Database Resources of the BIG Data Center in 2019,"Abstract The BIG Data Center at Beijing Institute of Genomics (BIG) of the Chinese Academy of Sciences provides a suite of database resources in support of worldwide research activities in both academia and industry. With the vast amounts of multi-omics data generated at unprecedented scales and rates, the BIG Data Center is continually expanding, updating and enriching its core database resources through big data integration and value-added curation. Resources with significant updates in the past year include BioProject (a biological project library), BioSample (a biological sample library), Genome Sequence Archive (GSA, a data repository for archiving raw sequence reads), Genome Warehouse (GWH, a centralized resource housing genome-scale data), Genome Variation Map (GVM, a public repository of genome variations), Science Wikis (a catalog of biological knowledge wikis for community annotations) and IC4R (Information Commons for Rice). Newly released resources include EWAS Atlas (a knowledgebase of epigenome-wide association studies), iDog (an integrated omics data resource for dog) and RNA editing resources (for editome-disease associations and plant RNA editosome, respectively). To promote biodiversity and health big data sharing around the world, the Open Biodiversity and Health Big Data (BHBD) initiative is introduced. All of these resources are publicly accessible at http://bigd.big.ac.cn.",fullPaper,jv107
Medicine,p1223,d1,7165864608ed8c0fc7343e8c2e90726936e3d980,j309,Neuron,Statistical Challenges in “Big Data” Human Neuroimaging,Abstract,fullPaper,jv309
Psychology,p1223,d10,7165864608ed8c0fc7343e8c2e90726936e3d980,j309,Neuron,Statistical Challenges in “Big Data” Human Neuroimaging,Abstract,fullPaper,jv309
Medicine,p1224,d1,89bd09cef9a6378685bfd67347c515d1b46e76ef,j310,British medical journal,Big data and medical research in China,Luxia Zhang and colleagues discuss the development of big data in Chinese healthcare and the opportunities for its use in medical research,fullPaper,jv310
Political Science,p1224,d15,89bd09cef9a6378685bfd67347c515d1b46e76ef,j310,British medical journal,Big data and medical research in China,Luxia Zhang and colleagues discuss the development of big data in Chinese healthcare and the opportunities for its use in medical research,fullPaper,jv310
Medicine,p1234,d1,442d88cde81ee9560d660d2dd66db7647bf15154,c43,European Conference on Machine Learning,Ten simple rules for responsible big data research,"The use of big data research methods has grown tremendously over the past five years in both academia and industry. As the size and complexity of available datasets has grown, so too have the ethical questions raised by big data research. These questions become increasingly urgent as data and research agendas move well beyond those typical of the computational and natural sciences, to more directly address sensitive aspects of human behavior, interaction, and health. The tools of big data research are increasingly woven into our daily lives, including mining digital medical records for scientific and economic insights, mapping relationships via social media, capturing individuals’ speech and action via sensors, tracking movement across space, shaping police and security policy via “predictive policing,” and much more.",poster,cp43
Computer Science,p1234,d3,442d88cde81ee9560d660d2dd66db7647bf15154,c43,European Conference on Machine Learning,Ten simple rules for responsible big data research,"The use of big data research methods has grown tremendously over the past five years in both academia and industry. As the size and complexity of available datasets has grown, so too have the ethical questions raised by big data research. These questions become increasingly urgent as data and research agendas move well beyond those typical of the computational and natural sciences, to more directly address sensitive aspects of human behavior, interaction, and health. The tools of big data research are increasingly woven into our daily lives, including mining digital medical records for scientific and economic insights, mapping relationships via social media, capturing individuals’ speech and action via sensors, tracking movement across space, shaping police and security policy via “predictive policing,” and much more.",poster,cp43
Medicine,p1251,d1,577564ac25a12b37972d77a35b589f6b2270a45f,j47,Chest,Big Data and Data Science in Critical Care.,Abstract,fullPaper,jv47
Medicine,p1255,d1,a7412ecf07ff540f9ede6eb6831803e115a16206,c21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,Recent Development in Big Data Analytics for Business Operations and Risk Management,"“Big data” is an emerging topic and has attracted the attention of many researchers and practitioners in industrial systems engineering and cybernetics. Big data analytics would definitely lead to valuable knowledge for many organizations. Business operations and risk management can be a beneficiary as there are many data collection channels in the related industrial systems (e.g., wireless sensor networks, Internet-based systems, etc.). Big data research, however, is still in its infancy. Its focus is rather unclear and related studies are not well amalgamated. This paper aims to present the challenges and opportunities of big data analytics in this unique application domain. Technological development and advances for industrial-based business systems, reliability and security of industrial systems, and their operational risk management are examined. Important areas for future research are also discussed and revealed.",poster,cp21
Computer Science,p1255,d3,a7412ecf07ff540f9ede6eb6831803e115a16206,c21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,Recent Development in Big Data Analytics for Business Operations and Risk Management,"“Big data” is an emerging topic and has attracted the attention of many researchers and practitioners in industrial systems engineering and cybernetics. Big data analytics would definitely lead to valuable knowledge for many organizations. Business operations and risk management can be a beneficiary as there are many data collection channels in the related industrial systems (e.g., wireless sensor networks, Internet-based systems, etc.). Big data research, however, is still in its infancy. Its focus is rather unclear and related studies are not well amalgamated. This paper aims to present the challenges and opportunities of big data analytics in this unique application domain. Technological development and advances for industrial-based business systems, reliability and security of industrial systems, and their operational risk management are examined. Important areas for future research are also discussed and revealed.",poster,cp21
Business,p1255,d9,a7412ecf07ff540f9ede6eb6831803e115a16206,c21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,Recent Development in Big Data Analytics for Business Operations and Risk Management,"“Big data” is an emerging topic and has attracted the attention of many researchers and practitioners in industrial systems engineering and cybernetics. Big data analytics would definitely lead to valuable knowledge for many organizations. Business operations and risk management can be a beneficiary as there are many data collection channels in the related industrial systems (e.g., wireless sensor networks, Internet-based systems, etc.). Big data research, however, is still in its infancy. Its focus is rather unclear and related studies are not well amalgamated. This paper aims to present the challenges and opportunities of big data analytics in this unique application domain. Technological development and advances for industrial-based business systems, reliability and security of industrial systems, and their operational risk management are examined. Important areas for future research are also discussed and revealed.",poster,cp21
Medicine,p1260,d1,8113771bcdb76cf7363c385dbc668dcf6cef2f2f,j318,BioMed Research International,Big Data Analytics in Healthcare,"The rapidly expanding field of big data analytics has started to play a pivotal role in the evolution of healthcare practices and research. It has provided tools to accumulate, manage, analyze, and assimilate large volumes of disparate, structured, and unstructured data produced by current healthcare systems. Big data analytics has been recently applied towards aiding the process of care delivery and disease exploration. However, the adoption rate and research development in this space is still hindered by some fundamental problems inherent within the big data paradigm. In this paper, we discuss some of these major challenges with a focus on three upcoming and promising areas of medical research: image, signal, and genomics based analytics. Recent research which targets utilization of large volumes of medical data while combining multimodal data from disparate sources is discussed. Potential areas of research within this field which have the ability to provide meaningful impact on healthcare delivery are also examined.",fullPaper,jv318
Computer Science,p1260,d3,8113771bcdb76cf7363c385dbc668dcf6cef2f2f,j318,BioMed Research International,Big Data Analytics in Healthcare,"The rapidly expanding field of big data analytics has started to play a pivotal role in the evolution of healthcare practices and research. It has provided tools to accumulate, manage, analyze, and assimilate large volumes of disparate, structured, and unstructured data produced by current healthcare systems. Big data analytics has been recently applied towards aiding the process of care delivery and disease exploration. However, the adoption rate and research development in this space is still hindered by some fundamental problems inherent within the big data paradigm. In this paper, we discuss some of these major challenges with a focus on three upcoming and promising areas of medical research: image, signal, and genomics based analytics. Recent research which targets utilization of large volumes of medical data while combining multimodal data from disparate sources is discussed. Potential areas of research within this field which have the ability to provide meaningful impact on healthcare delivery are also examined.",fullPaper,jv318
Medicine,p1290,d1,56e2e7643ac3a9832d14ade7d868892ed544fa90,j321,International Journal of Molecular Sciences,Big Data Analytics for Genomic Medicine,"Genomic medicine attempts to build individualized strategies for diagnostic or therapeutic decision-making by utilizing patients’ genomic information. Big Data analytics uncovers hidden patterns, unknown correlations, and other insights through examining large-scale various data sets. While integration and manipulation of diverse genomic data and comprehensive electronic health records (EHRs) on a Big Data infrastructure exhibit challenges, they also provide a feasible opportunity to develop an efficient and effective approach to identify clinically actionable genetic variants for individualized diagnosis and therapy. In this paper, we review the challenges of manipulating large-scale next-generation sequencing (NGS) data and diverse clinical data derived from the EHRs for genomic medicine. We introduce possible solutions for different challenges in manipulating, managing, and analyzing genomic and clinical data to implement genomic medicine. Additionally, we also present a practical Big Data toolset for identifying clinically actionable genetic variants using high-throughput NGS data and EHRs.",fullPaper,jv321
Medicine,p1304,d1,acce65a16ce75d2abd1fbf0578bdbbc04ebe644e,c86,International Conference on Big Data and Education,Database Resources of the BIG Data Center in 2018,"Abstract The BIG Data Center at Beijing Institute of Genomics (BIG) of the Chinese Academy of Sciences provides freely open access to a suite of database resources in support of worldwide research activities in both academia and industry. With the vast amounts of omics data generated at ever-greater scales and rates, the BIG Data Center is continually expanding, updating and enriching its core database resources through big-data integration and value-added curation, including BioCode (a repository archiving bioinformatics tool codes), BioProject (a biological project library), BioSample (a biological sample library), Genome Sequence Archive (GSA, a data repository for archiving raw sequence reads), Genome Warehouse (GWH, a centralized resource housing genome-scale data), Genome Variation Map (GVM, a public repository of genome variations), Gene Expression Nebulas (GEN, a database of gene expression profiles based on RNA-Seq data), Methylation Bank (MethBank, an integrated databank of DNA methylomes), and Science Wikis (a series of biological knowledge wikis for community annotations). In addition, three featured web services are provided, viz., BIG Search (search as a service; a scalable inter-domain text search engine), BIG SSO (single sign-on as a service; a user access control system to gain access to multiple independent systems with a single ID and password) and Gsub (submission as a service; a unified submission service for all relevant resources). All of these resources are publicly accessible through the home page of the BIG Data Center at http://bigd.big.ac.cn.",poster,cp86
Computer Science,p1304,d3,acce65a16ce75d2abd1fbf0578bdbbc04ebe644e,c86,International Conference on Big Data and Education,Database Resources of the BIG Data Center in 2018,"Abstract The BIG Data Center at Beijing Institute of Genomics (BIG) of the Chinese Academy of Sciences provides freely open access to a suite of database resources in support of worldwide research activities in both academia and industry. With the vast amounts of omics data generated at ever-greater scales and rates, the BIG Data Center is continually expanding, updating and enriching its core database resources through big-data integration and value-added curation, including BioCode (a repository archiving bioinformatics tool codes), BioProject (a biological project library), BioSample (a biological sample library), Genome Sequence Archive (GSA, a data repository for archiving raw sequence reads), Genome Warehouse (GWH, a centralized resource housing genome-scale data), Genome Variation Map (GVM, a public repository of genome variations), Gene Expression Nebulas (GEN, a database of gene expression profiles based on RNA-Seq data), Methylation Bank (MethBank, an integrated databank of DNA methylomes), and Science Wikis (a series of biological knowledge wikis for community annotations). In addition, three featured web services are provided, viz., BIG Search (search as a service; a scalable inter-domain text search engine), BIG SSO (single sign-on as a service; a user access control system to gain access to multiple independent systems with a single ID and password) and Gsub (submission as a service; a unified submission service for all relevant resources). All of these resources are publicly accessible through the home page of the BIG Data Center at http://bigd.big.ac.cn.",poster,cp86
Biology,p1304,d5,acce65a16ce75d2abd1fbf0578bdbbc04ebe644e,c86,International Conference on Big Data and Education,Database Resources of the BIG Data Center in 2018,"Abstract The BIG Data Center at Beijing Institute of Genomics (BIG) of the Chinese Academy of Sciences provides freely open access to a suite of database resources in support of worldwide research activities in both academia and industry. With the vast amounts of omics data generated at ever-greater scales and rates, the BIG Data Center is continually expanding, updating and enriching its core database resources through big-data integration and value-added curation, including BioCode (a repository archiving bioinformatics tool codes), BioProject (a biological project library), BioSample (a biological sample library), Genome Sequence Archive (GSA, a data repository for archiving raw sequence reads), Genome Warehouse (GWH, a centralized resource housing genome-scale data), Genome Variation Map (GVM, a public repository of genome variations), Gene Expression Nebulas (GEN, a database of gene expression profiles based on RNA-Seq data), Methylation Bank (MethBank, an integrated databank of DNA methylomes), and Science Wikis (a series of biological knowledge wikis for community annotations). In addition, three featured web services are provided, viz., BIG Search (search as a service; a scalable inter-domain text search engine), BIG SSO (single sign-on as a service; a user access control system to gain access to multiple independent systems with a single ID and password) and Gsub (submission as a service; a unified submission service for all relevant resources). All of these resources are publicly accessible through the home page of the BIG Data Center at http://bigd.big.ac.cn.",poster,cp86
Medicine,p1324,d1,32bebd75012f43840ada24c6ab3c917d6dff99bf,j285,Information Fusion,Social big data: Recent achievements and new challenges,Abstract,fullPaper,jv285
Computer Science,p1324,d3,32bebd75012f43840ada24c6ab3c917d6dff99bf,j285,Information Fusion,Social big data: Recent achievements and new challenges,Abstract,fullPaper,jv285
Medicine,p1330,d1,2302c56eb40f3339e910fa8197d7429e6ba8b032,j282,Science and Engineering Ethics,The Ethics of Big Data: Current and Foreseeable Issues in Biomedical Contexts,Abstract,fullPaper,jv282
Computer Science,p1330,d3,2302c56eb40f3339e910fa8197d7429e6ba8b032,j282,Science and Engineering Ethics,The Ethics of Big Data: Current and Foreseeable Issues in Biomedical Contexts,Abstract,fullPaper,jv282
Sociology,p1330,d4,2302c56eb40f3339e910fa8197d7429e6ba8b032,j282,Science and Engineering Ethics,The Ethics of Big Data: Current and Foreseeable Issues in Biomedical Contexts,Abstract,fullPaper,jv282
Medicine,p1336,d1,fd4879e9c17a5d6551dd9f3a0cb9df0fe1d04637,c46,Ideal,Big Data Application in Biomedical Research and Health Care: A Literature Review,"Big data technologies are increasingly used for biomedical and health-care informatics research. Large amounts of biological and clinical data have been generated and collected at an unprecedented speed and scale. For example, the new generation of sequencing technologies enables the processing of billions of DNA sequence data per day, and the application of electronic health records (EHRs) is documenting large amounts of patient data. The cost of acquiring and analyzing biomedical data is expected to decrease dramatically with the help of technology upgrades, such as the emergence of new sequencing machines, the development of novel hardware and software for parallel computing, and the extensive expansion of EHRs. Big data applications present new opportunities to discover new knowledge and create novel methods to improve the quality of health care. The application of big data in health care is a fast-growing field, with many new discoveries and methodologies published in the last five years. In this paper, we review and discuss big data application in four major biomedical subdisciplines: (1) bioinformatics, (2) clinical informatics, (3) imaging informatics, and (4) public health informatics. Specifically, in bioinformatics, high-throughput experiments facilitate the research of new genome-wide association studies of diseases, and with clinical informatics, the clinical field benefits from the vast amount of collected patient data for making intelligent decisions. Imaging informatics is now more rapidly integrated with cloud platforms to share medical image data and workflows, and public health informatics leverages big data techniques for predicting and monitoring infectious disease outbreaks, such as Ebola. In this paper, we review the recent progress and breakthroughs of big data applications in these health-care domains and summarize the challenges, gaps, and opportunities to improve and advance big data applications in health care.",poster,cp46
Medicine,p1345,d1,53834f0ee8df731cf0e629cd594dce0afaaa3d97,c84,EUROCON Conference,The inevitable application of big data to health care.,"THE AMOUNT OF DATA BEING DIGITALLY COLLECTED AND stored is vast and expanding rapidly. As a result, the science of data management and analysis is also advancing to enable organizations to convert this vast resource into information and knowledge that helps them achieve their objectives. Computer scientists have invented the term big data to describe this evolving technology. Big data has been successfully used in astronomy (eg, the Sloan Digital Sky Survey of telescopic information), retail sales (eg, Walmart’s expansive number of transactions), search engines (eg, Google’s customization of individual searches based on previous web data), and politics (eg, a campaign’s focus of political advertisements on people most likely to support their candidate based on web searches). In this Viewpoint, we discuss the application of big data to health care, using an economic framework to highlight the opportunities it will offer and the roadblocks to implementation. We suggest that leveraging the collection of patient and practitioner data could be an important way to improve quality and efficiency of health care delivery. Widespread uptake of electronic health records (EHRs) has generated massive data sets. A survey by the American Hospital Association showed that adoption of EHRs has doubled from 2009 to 2011, partly a result of funding provided by the Health Information Technology for Economic and Clinical Health Act of 2009. Most EHRs now contain quantitative data (eg, laboratory values), qualitative data (eg, text-based documents and demographics), and transactional data (eg, a record of medication delivery). However, much of this rich data set is currently perceived as a byproduct of health care delivery, rather than a central asset to improve its efficiency. The transition of data from refuse to riches has been key in the big data revolution of other industries. Advances in analytic techniques in the computer sciences, especially in machine learning, have been a major catalyst for dealing with these large information sets. These analytic techniques are in contrast to traditional statistical methods (derived from the social and physical sciences), which are largely not useful for analysis of unstructured data such as text-based documents that do not fit into relational tables. One estimate suggests that 80% of business-related data exist in an unstructured format. The same could probably be said for health care data, a large proportion of which is text-based. In contrast to most consumer service industries, medicine adopted a practice of generating evidence from experimental (randomized trials) and quasi-experimental studies to inform patients and clinicians. The evidence-based movement is founded on the belief that scientific inquiry is superior to expert opinion and testimonials. In this way, medicine was ahead of many other industries in terms of recognizing the value of data and information guiding rational decision making. However, health care has lagged in uptake of newer techniques to leverage the rich information contained in EHRs. There are 4 ways big data may advance the economic mission of health care delivery by improving quality and efficiency. First, big data may greatly expand the capacity to generate new knowledge. The cost of answering many clinical questions prospectively, and even retrospectively, by collecting structured data is prohibitive. Analyzing the unstructured data contained within EHRs using computational techniques (eg, natural language processing to extract medical concepts from free-text documents) permits finer data acquisition in an automated fashion. For instance, automated identification within EHRs using natural language processing was superior in detecting postoperative complications compared with patient safety indicators based on discharge coding. Big data offers the potential to create an observational evidence base for clinical questions that would otherwise not be possible and may be especially helpful with issues of generalizability. The latter issue limits the application of conclusions derived from randomized trials performed on a narrow spectrum of participants to patients who exhibit very different characteristics. Second, big data may help with knowledge dissemination. Most physicians struggle to stay current with the latest evidence guiding clinical practice. The digitization of medical literature has greatly improved access; however, the sheer",poster,cp84
Medicine,p1346,d1,0738f0d7dec95724fd2b58b51ef04b6bff3cf531,j326,Journal of Chemical Theory and Computation,Big Data Meets Quantum Chemistry Approximations: The Δ-Machine Learning Approach.,"Chemically accurate and comprehensive studies of the virtual space of all possible molecules are severely limited by the computational cost of quantum chemistry. We introduce a composite strategy that adds machine learning corrections to computationally inexpensive approximate legacy quantum methods. After training, highly accurate predictions of enthalpies, free energies, entropies, and electron correlation energies are possible, for significantly larger molecular sets than used for training. For thermochemical properties of up to 16k isomers of C7H10O2 we present numerical evidence that chemical accuracy can be reached. We also predict electron correlation energy in post Hartree-Fock methods, at the computational cost of Hartree-Fock, and we establish a qualitative relationship between molecular entropy and electron correlation. The transferability of our approach is demonstrated, using semiempirical quantum chemistry and machine learning models trained on 1 and 10% of 134k organic molecules, to reproduce enthalpies of all remaining molecules at density functional theory level of accuracy.",fullPaper,jv326
Physics,p1346,d2,0738f0d7dec95724fd2b58b51ef04b6bff3cf531,j326,Journal of Chemical Theory and Computation,Big Data Meets Quantum Chemistry Approximations: The Δ-Machine Learning Approach.,"Chemically accurate and comprehensive studies of the virtual space of all possible molecules are severely limited by the computational cost of quantum chemistry. We introduce a composite strategy that adds machine learning corrections to computationally inexpensive approximate legacy quantum methods. After training, highly accurate predictions of enthalpies, free energies, entropies, and electron correlation energies are possible, for significantly larger molecular sets than used for training. For thermochemical properties of up to 16k isomers of C7H10O2 we present numerical evidence that chemical accuracy can be reached. We also predict electron correlation energy in post Hartree-Fock methods, at the computational cost of Hartree-Fock, and we establish a qualitative relationship between molecular entropy and electron correlation. The transferability of our approach is demonstrated, using semiempirical quantum chemistry and machine learning models trained on 1 and 10% of 134k organic molecules, to reproduce enthalpies of all remaining molecules at density functional theory level of accuracy.",fullPaper,jv326
Chemistry,p1346,d8,0738f0d7dec95724fd2b58b51ef04b6bff3cf531,j326,Journal of Chemical Theory and Computation,Big Data Meets Quantum Chemistry Approximations: The Δ-Machine Learning Approach.,"Chemically accurate and comprehensive studies of the virtual space of all possible molecules are severely limited by the computational cost of quantum chemistry. We introduce a composite strategy that adds machine learning corrections to computationally inexpensive approximate legacy quantum methods. After training, highly accurate predictions of enthalpies, free energies, entropies, and electron correlation energies are possible, for significantly larger molecular sets than used for training. For thermochemical properties of up to 16k isomers of C7H10O2 we present numerical evidence that chemical accuracy can be reached. We also predict electron correlation energy in post Hartree-Fock methods, at the computational cost of Hartree-Fock, and we establish a qualitative relationship between molecular entropy and electron correlation. The transferability of our approach is demonstrated, using semiempirical quantum chemistry and machine learning models trained on 1 and 10% of 134k organic molecules, to reproduce enthalpies of all remaining molecules at density functional theory level of accuracy.",fullPaper,jv326
Medicine,p1350,d1,05425a0b1e8cd37c10250d520ae7cda14b611cf8,j327,Nature Reviews Cardiology,Big data analytics to improve cardiovascular care: promise and challenges,Abstract,fullPaper,jv327
Medicine,p1354,d1,aebe2eda5774f46d6413f45c11df58d6339a1003,j140,JMIR Medical Informatics,Challenges and Opportunities of Big Data in Health Care: A Systematic Review,"Background Big data analytics offers promise in many business sectors, and health care is looking at big data to provide answers to many age-related issues, particularly dementia and chronic disease management. Objective The purpose of this review was to summarize the challenges faced by big data analytics and the opportunities that big data opens in health care. Methods A total of 3 searches were performed for publications between January 1, 2010 and January 1, 2016 (PubMed/MEDLINE, CINAHL, and Google Scholar), and an assessment was made on content germane to big data in health care. From the results of the searches in research databases and Google Scholar (N=28), the authors summarized content and identified 9 and 14 themes under the categories Challenges and Opportunities, respectively. We rank-ordered and analyzed the themes based on the frequency of occurrence. Results The top challenges were issues of data structure, security, data standardization, storage and transfers, and managerial skills such as data governance. The top opportunities revealed were quality improvement, population management and health, early detection of disease, data quality, structure, and accessibility, improved decision making, and cost reduction. Conclusions Big data analytics has the potential for positive impact and global implications; however, it must overcome some legitimate obstacles.",fullPaper,jv140
Medicine,p1356,d1,5b023b7169217d965be03b6cdf196142bd507806,j328,Health Affairs,Big data in health care: using analytics to identify and manage high-risk and high-cost patients.,"The US health care system is rapidly adopting electronic health records, which will dramatically increase the quantity of clinical data that are available electronically. Simultaneously, rapid progress has been made in clinical analytics--techniques for analyzing large quantities of data and gleaning new insights from that analysis--which is part of what is known as big data. As a result, there are unprecedented opportunities to use big data to reduce the costs of health care in the United States. We present six use cases--that is, key examples--where some of the clearest opportunities exist to reduce costs through the use of big data: high-cost patients, readmissions, triage, decompensation (when a patient's condition worsens), adverse events, and treatment optimization for diseases affecting multiple organ systems. We discuss the types of insights that are likely to emerge from clinical analytics, the types of data needed to obtain such insights, and the infrastructure--analytics, algorithms, registries, assessment scores, monitoring devices, and so forth--that organizations will need to perform the necessary analyses and to implement changes that will improve care while reducing costs. Our findings have policy implications for regulatory oversight, ways to address privacy concerns, and the support of research on analytics.",fullPaper,jv328
Medicine,p1360,d1,69732dcf45024f28e5c43de68d1208f6e737eada,c72,Workshop on Research on Enterprise Networking,The BIG Data Center: from deposition to integration to translation,"Biological data are generated at unprecedentedly exponential rates, posing considerable challenges in big data deposition, integration and translation. The BIG Data Center, established at Beijing Institute of Genomics (BIG), Chinese Academy of Sciences, provides a suite of database resources, including (i) Genome Sequence Archive, a data repository specialized for archiving raw sequence reads, (ii) Gene Expression Nebulas, a data portal of gene expression profiles based entirely on RNA-Seq data, (iii) Genome Variation Map, a comprehensive collection of genome variations for featured species, (iv) Genome Warehouse, a centralized resource housing genome-scale data with particular focus on economically important animals and plants, (v) Methylation Bank, an integrated database of whole-genome single-base resolution methylomes and (vi) Science Wikis, a central access point for biological wikis developed for community annotations. The BIG Data Center is dedicated to constructing and maintaining biological databases through big data integration and value-added curation, conducting basic research to translate big data into big knowledge and providing freely open access to a variety of data resources in support of worldwide research activities in both academia and industry. All of these resources are publicly available and can be found at http://bigd.big.ac.cn.",poster,cp72
Computer Science,p1360,d3,69732dcf45024f28e5c43de68d1208f6e737eada,c72,Workshop on Research on Enterprise Networking,The BIG Data Center: from deposition to integration to translation,"Biological data are generated at unprecedentedly exponential rates, posing considerable challenges in big data deposition, integration and translation. The BIG Data Center, established at Beijing Institute of Genomics (BIG), Chinese Academy of Sciences, provides a suite of database resources, including (i) Genome Sequence Archive, a data repository specialized for archiving raw sequence reads, (ii) Gene Expression Nebulas, a data portal of gene expression profiles based entirely on RNA-Seq data, (iii) Genome Variation Map, a comprehensive collection of genome variations for featured species, (iv) Genome Warehouse, a centralized resource housing genome-scale data with particular focus on economically important animals and plants, (v) Methylation Bank, an integrated database of whole-genome single-base resolution methylomes and (vi) Science Wikis, a central access point for biological wikis developed for community annotations. The BIG Data Center is dedicated to constructing and maintaining biological databases through big data integration and value-added curation, conducting basic research to translate big data into big knowledge and providing freely open access to a variety of data resources in support of worldwide research activities in both academia and industry. All of these resources are publicly available and can be found at http://bigd.big.ac.cn.",poster,cp72
Medicine,p1387,d1,0faf86b14b22f6714d3ad524010d1129c364e4be,j60,Nature,"Neuroscience: Big brain, big data",Abstract,fullPaper,jv60
Computer Science,p1387,d3,0faf86b14b22f6714d3ad524010d1129c364e4be,j60,Nature,"Neuroscience: Big brain, big data",Abstract,fullPaper,jv60
Medicine,p1392,d1,cae80eca52369e50037cc761abc0c3d3395881ba,j333,BMC Medical Genomics,From big data analysis to personalized medicine for all: challenges and opportunities,Abstract,fullPaper,jv333
Computer Science,p1392,d3,cae80eca52369e50037cc761abc0c3d3395881ba,j333,BMC Medical Genomics,From big data analysis to personalized medicine for all: challenges and opportunities,Abstract,fullPaper,jv333
Medicine,p1408,d1,5f1cc2df59fbab055ce1ea64d667e0934e6baad3,j336,IEEE journal of biomedical and health informatics,Big Data for Health,"This paper provides an overview of recent developments in big data in the context of biomedical and health informatics. It outlines the key characteristics of big data and how medical and health informatics, translational bioinformatics, sensor informatics, and imaging informatics will benefit from an integrated approach of piecing together different aspects of personalized information from a diverse range of data sources, both structured and unstructured, covering genomics, proteomics, metabolomics, as well as imaging, clinical diagnosis, and long-term continuous physiological sensing of an individual. It is expected that recent advances in big data will expand our knowledge for testing new hypotheses about disease management from diagnosis to prevention to personalized treatment. The rise of big data, however, also raises challenges in terms of privacy, security, data ownership, data stewardship, and governance. This paper discusses some of the existing activities and future opportunities related to big data for health, outlining some of the key underlying issues that need to be tackled.",fullPaper,jv336
Computer Science,p1408,d3,5f1cc2df59fbab055ce1ea64d667e0934e6baad3,j336,IEEE journal of biomedical and health informatics,Big Data for Health,"This paper provides an overview of recent developments in big data in the context of biomedical and health informatics. It outlines the key characteristics of big data and how medical and health informatics, translational bioinformatics, sensor informatics, and imaging informatics will benefit from an integrated approach of piecing together different aspects of personalized information from a diverse range of data sources, both structured and unstructured, covering genomics, proteomics, metabolomics, as well as imaging, clinical diagnosis, and long-term continuous physiological sensing of an individual. It is expected that recent advances in big data will expand our knowledge for testing new hypotheses about disease management from diagnosis to prevention to personalized treatment. The rise of big data, however, also raises challenges in terms of privacy, security, data ownership, data stewardship, and governance. This paper discusses some of the existing activities and future opportunities related to big data for health, outlining some of the key underlying issues that need to be tackled.",fullPaper,jv336
Medicine,p1412,d1,bd1c1d5540f246090e740c0d5a0fa7f2c64059d1,c9,Big Data,Data Science and its Relationship to Big Data and Data-Driven Decision Making,"Companies have realized they need to hire data scientists, academic institutions are scrambling to put together data-science programs, and publications are touting data science as a hot-even ""sexy""-career choice. However, there is confusion about what exactly data science is, and this confusion could lead to disillusionment as the concept diffuses into meaningless buzz. In this article, we argue that there are good reasons why it has been hard to pin down exactly what is data science. One reason is that data science is intricately intertwined with other important concepts also of growing importance, such as big data and data-driven decision making. Another reason is the natural tendency to associate what a practitioner does with the definition of the practitioner's field; this can result in overlooking the fundamentals of the field. We believe that trying to define the boundaries of data science precisely is not of the utmost importance. We can debate the boundaries of the field in an academic setting, but in order for data science to serve business effectively, it is important (i) to understand its relationships to other important related concepts, and (ii) to begin to identify the fundamental principles underlying data science. Once we embrace (ii), we can much better understand and explain exactly what data science has to offer. Furthermore, only once we embrace (ii) should we be comfortable calling it data science. In this article, we present a perspective that addresses all these concepts. We close by offering, as examples, a partial list of fundamental principles underlying data science.",fullPaper,cp9
Computer Science,p1412,d3,bd1c1d5540f246090e740c0d5a0fa7f2c64059d1,c9,Big Data,Data Science and its Relationship to Big Data and Data-Driven Decision Making,"Companies have realized they need to hire data scientists, academic institutions are scrambling to put together data-science programs, and publications are touting data science as a hot-even ""sexy""-career choice. However, there is confusion about what exactly data science is, and this confusion could lead to disillusionment as the concept diffuses into meaningless buzz. In this article, we argue that there are good reasons why it has been hard to pin down exactly what is data science. One reason is that data science is intricately intertwined with other important concepts also of growing importance, such as big data and data-driven decision making. Another reason is the natural tendency to associate what a practitioner does with the definition of the practitioner's field; this can result in overlooking the fundamentals of the field. We believe that trying to define the boundaries of data science precisely is not of the utmost importance. We can debate the boundaries of the field in an academic setting, but in order for data science to serve business effectively, it is important (i) to understand its relationships to other important related concepts, and (ii) to begin to identify the fundamental principles underlying data science. Once we embrace (ii), we can much better understand and explain exactly what data science has to offer. Furthermore, only once we embrace (ii) should we be comfortable calling it data science. In this article, we present a perspective that addresses all these concepts. We close by offering, as examples, a partial list of fundamental principles underlying data science.",fullPaper,cp9
Psychology,p1412,d10,bd1c1d5540f246090e740c0d5a0fa7f2c64059d1,c9,Big Data,Data Science and its Relationship to Big Data and Data-Driven Decision Making,"Companies have realized they need to hire data scientists, academic institutions are scrambling to put together data-science programs, and publications are touting data science as a hot-even ""sexy""-career choice. However, there is confusion about what exactly data science is, and this confusion could lead to disillusionment as the concept diffuses into meaningless buzz. In this article, we argue that there are good reasons why it has been hard to pin down exactly what is data science. One reason is that data science is intricately intertwined with other important concepts also of growing importance, such as big data and data-driven decision making. Another reason is the natural tendency to associate what a practitioner does with the definition of the practitioner's field; this can result in overlooking the fundamentals of the field. We believe that trying to define the boundaries of data science precisely is not of the utmost importance. We can debate the boundaries of the field in an academic setting, but in order for data science to serve business effectively, it is important (i) to understand its relationships to other important related concepts, and (ii) to begin to identify the fundamental principles underlying data science. Once we embrace (ii), we can much better understand and explain exactly what data science has to offer. Furthermore, only once we embrace (ii) should we be comfortable calling it data science. In this article, we present a perspective that addresses all these concepts. We close by offering, as examples, a partial list of fundamental principles underlying data science.",fullPaper,cp9
Medicine,p1439,d1,6057eb2caf437b494731a4937cef01513d234500,j340,Journal of Infectious Diseases,Big Data for Infectious Disease Surveillance and Modeling.,"We devote a special issue of the Journal of Infectious Diseases to review the recent advances of big data in strengthening disease surveillance, monitoring medical adverse events, informing transmission models, and tracking patient sentiments and mobility. We consider a broad definition of big data for public health, one encompassing patient information gathered from high-volume electronic health records and participatory surveillance systems, as well as mining of digital traces such as social media, Internet searches, and cell-phone logs. We introduce nine independent contributions to this special issue and highlight several cross-cutting areas that require further research, including representativeness, biases, volatility, and validation, and the need for robust statistical and hypotheses-driven analyses. Overall, we are optimistic that the big-data revolution will vastly improve the granularity and timeliness of available epidemiological information, with hybrid systems augmenting rather than supplanting traditional surveillance systems, and better prospects for accurate infectious diseases models and forecasts.",fullPaper,jv340
Computer Science,p1439,d3,6057eb2caf437b494731a4937cef01513d234500,j340,Journal of Infectious Diseases,Big Data for Infectious Disease Surveillance and Modeling.,"We devote a special issue of the Journal of Infectious Diseases to review the recent advances of big data in strengthening disease surveillance, monitoring medical adverse events, informing transmission models, and tracking patient sentiments and mobility. We consider a broad definition of big data for public health, one encompassing patient information gathered from high-volume electronic health records and participatory surveillance systems, as well as mining of digital traces such as social media, Internet searches, and cell-phone logs. We introduce nine independent contributions to this special issue and highlight several cross-cutting areas that require further research, including representativeness, biases, volatility, and validation, and the need for robust statistical and hypotheses-driven analyses. Overall, we are optimistic that the big-data revolution will vastly improve the granularity and timeliness of available epidemiological information, with hybrid systems augmenting rather than supplanting traditional surveillance systems, and better prospects for accurate infectious diseases models and forecasts.",fullPaper,jv340
Medicine,p1446,d1,c3934f0d699e845fc9ee7816dfab7d91c72b33f8,j336,IEEE journal of biomedical and health informatics,"Big Data, Big Knowledge: Big Data for Personalized Healthcare","The idea that the purely phenomenological knowledge that we can extract by analyzing large amounts of data can be useful in healthcare seems to contradict the desire of VPH researchers to build detailed mechanistic models for individual patients. But in practice no model is ever entirely phenomenological or entirely mechanistic. We propose in this position paper that big data analytics can be successfully combined with VPH technologies to produce robust and effective in silico medicine solutions. In order to do this, big data technologies must be further developed to cope with some specific requirements that emerge from this application. Such requirements are: working with sensitive data; analytics of complex and heterogeneous data spaces, including nontextual information; distributed data management under security and performance constraints; specialized analytics to integrate bioinformatics and systems biology information with clinical observations at tissue, organ and organisms scales; and specialized analytics to define the “physiological envelope” during the daily life of each patient. These domain-specific requirements suggest a need for targeted funding, in which big data technologies for in silico medicine becomes the research priority.",fullPaper,jv336
Computer Science,p1446,d3,c3934f0d699e845fc9ee7816dfab7d91c72b33f8,j336,IEEE journal of biomedical and health informatics,"Big Data, Big Knowledge: Big Data for Personalized Healthcare","The idea that the purely phenomenological knowledge that we can extract by analyzing large amounts of data can be useful in healthcare seems to contradict the desire of VPH researchers to build detailed mechanistic models for individual patients. But in practice no model is ever entirely phenomenological or entirely mechanistic. We propose in this position paper that big data analytics can be successfully combined with VPH technologies to produce robust and effective in silico medicine solutions. In order to do this, big data technologies must be further developed to cope with some specific requirements that emerge from this application. Such requirements are: working with sensitive data; analytics of complex and heterogeneous data spaces, including nontextual information; distributed data management under security and performance constraints; specialized analytics to integrate bioinformatics and systems biology information with clinical observations at tissue, organ and organisms scales; and specialized analytics to define the “physiological envelope” during the daily life of each patient. These domain-specific requirements suggest a need for targeted funding, in which big data technologies for in silico medicine becomes the research priority.",fullPaper,jv336
Medicine,p1453,d1,2a58d13b5934eadc7b4d46869d7593059922e2a1,c87,International Conference on Big Data Research,Big data need big theory too,"The current interest in big data, machine learning and data analytics has generated the widespread impression that such methods are capable of solving most problems without the need for conventional scientific methods of inquiry. Interest in these methods is intensifying, accelerated by the ease with which digitized data can be acquired in virtually all fields of endeavour, from science, healthcare and cybersecurity to economics, social sciences and the humanities. In multiscale modelling, machine learning appears to provide a shortcut to reveal correlations of arbitrary complexity between processes at the atomic, molecular, meso- and macroscales. Here, we point out the weaknesses of pure big data approaches with particular focus on biology and medicine, which fail to provide conceptual accounts for the processes to which they are applied. No matter their ‘depth’ and the sophistication of data-driven methods, such as artificial neural nets, in the end they merely fit curves to existing data. Not only do these methods invariably require far larger quantities of data than anticipated by big data aficionados in order to produce statistically reliable results, but they can also fail in circumstances beyond the range of the data used to train them because they are not designed to model the structural characteristics of the underlying system. We argue that it is vital to use theory as a guide to experimental design for maximal efficiency of data collection and to produce reliable predictive models and conceptual knowledge. Rather than continuing to fund, pursue and promote ‘blind’ big data projects with massive budgets, we call for more funding to be allocated to the elucidation of the multiscale and stochastic processes controlling the behaviour of complex systems, including those of life, medicine and healthcare. This article is part of the themed issue ‘Multiscale modelling at the physics–chemistry–biology interface’.",poster,cp87
Mathematics,p1453,d6,2a58d13b5934eadc7b4d46869d7593059922e2a1,c87,International Conference on Big Data Research,Big data need big theory too,"The current interest in big data, machine learning and data analytics has generated the widespread impression that such methods are capable of solving most problems without the need for conventional scientific methods of inquiry. Interest in these methods is intensifying, accelerated by the ease with which digitized data can be acquired in virtually all fields of endeavour, from science, healthcare and cybersecurity to economics, social sciences and the humanities. In multiscale modelling, machine learning appears to provide a shortcut to reveal correlations of arbitrary complexity between processes at the atomic, molecular, meso- and macroscales. Here, we point out the weaknesses of pure big data approaches with particular focus on biology and medicine, which fail to provide conceptual accounts for the processes to which they are applied. No matter their ‘depth’ and the sophistication of data-driven methods, such as artificial neural nets, in the end they merely fit curves to existing data. Not only do these methods invariably require far larger quantities of data than anticipated by big data aficionados in order to produce statistically reliable results, but they can also fail in circumstances beyond the range of the data used to train them because they are not designed to model the structural characteristics of the underlying system. We argue that it is vital to use theory as a guide to experimental design for maximal efficiency of data collection and to produce reliable predictive models and conceptual knowledge. Rather than continuing to fund, pursue and promote ‘blind’ big data projects with massive budgets, we call for more funding to be allocated to the elucidation of the multiscale and stochastic processes controlling the behaviour of complex systems, including those of life, medicine and healthcare. This article is part of the themed issue ‘Multiscale modelling at the physics–chemistry–biology interface’.",poster,cp87
Medicine,p1454,d1,c660f1822cc133f6e48387a36af24747cd2fdeb2,j343,Proteomics,Integrative methods for analyzing big data in precision medicine,"We provide an overview of recent developments in big data analyses in the context of precision medicine and health informatics. With the advance in technologies capturing molecular and medical data, we entered the area of “Big Data” in biology and medicine. These data offer many opportunities to advance precision medicine. We outline key challenges in precision medicine and present recent advances in data integration‐based methods to uncover personalized information from big data produced by various omics studies. We survey recent integrative methods for disease subtyping, biomarkers discovery, and drug repurposing, and list the tools that are available to domain scientists. Given the ever‐growing nature of these big data, we highlight key issues that big data integration methods will face.",fullPaper,jv343
Medicine,p1465,d1,b5fb74dfc71c92113c84a0e8f0502e0e76b4dbda,j108,Social Science Research,The role of administrative data in the big data revolution in social science research.,Abstract,fullPaper,jv108
Computer Science,p1465,d3,b5fb74dfc71c92113c84a0e8f0502e0e76b4dbda,j108,Social Science Research,The role of administrative data in the big data revolution in social science research.,Abstract,fullPaper,jv108
Medicine,p1469,d1,8a5187780cf04dbe202d2a44cac524100a6c3ad8,j60,Nature,Perspective: Sustaining the big-data ecosystem,Abstract,fullPaper,jv60
Computer Science,p1469,d3,8a5187780cf04dbe202d2a44cac524100a6c3ad8,j60,Nature,Perspective: Sustaining the big-data ecosystem,Abstract,fullPaper,jv60
Business,p1469,d9,8a5187780cf04dbe202d2a44cac524100a6c3ad8,j60,Nature,Perspective: Sustaining the big-data ecosystem,Abstract,fullPaper,jv60
Medicine,p1491,d1,6f0ba62fb7f55cd9c17322d738afd43bb58ff869,j346,Statistics and its Interface,Statistical methods and computing for big data.,"Big data are data on a massive scale in terms of volume, intensity, and complexity that exceed the capacity of standard analytic tools. They present opportunities as well as challenges to statisticians. The role of computational statisticians in scientific discovery from big data analyses has been under-recognized even by peer statisticians. This article summarizes recent methodological and software developments in statistics that address the big data challenges. Methodologies are grouped into three classes: subsampling-based, divide and conquer, and online updating for stream data. As a new contribution, the online updating approach is extended to variable selection with commonly used criteria, and their performances are assessed in a simulation study with stream data. Software packages are summarized with focuses on the open source R and R packages, covering recent tools that help break the barriers of computer memory and computing power. Some of the tools are illustrated in a case study with a logistic regression for the chance of airline delay.",fullPaper,jv346
Computer Science,p1491,d3,6f0ba62fb7f55cd9c17322d738afd43bb58ff869,j346,Statistics and its Interface,Statistical methods and computing for big data.,"Big data are data on a massive scale in terms of volume, intensity, and complexity that exceed the capacity of standard analytic tools. They present opportunities as well as challenges to statisticians. The role of computational statisticians in scientific discovery from big data analyses has been under-recognized even by peer statisticians. This article summarizes recent methodological and software developments in statistics that address the big data challenges. Methodologies are grouped into three classes: subsampling-based, divide and conquer, and online updating for stream data. As a new contribution, the online updating approach is extended to variable selection with commonly used criteria, and their performances are assessed in a simulation study with stream data. Software packages are summarized with focuses on the open source R and R packages, covering recent tools that help break the barriers of computer memory and computing power. Some of the tools are illustrated in a case study with a logistic regression for the chance of airline delay.",fullPaper,jv346
Mathematics,p1491,d6,6f0ba62fb7f55cd9c17322d738afd43bb58ff869,j346,Statistics and its Interface,Statistical methods and computing for big data.,"Big data are data on a massive scale in terms of volume, intensity, and complexity that exceed the capacity of standard analytic tools. They present opportunities as well as challenges to statisticians. The role of computational statisticians in scientific discovery from big data analyses has been under-recognized even by peer statisticians. This article summarizes recent methodological and software developments in statistics that address the big data challenges. Methodologies are grouped into three classes: subsampling-based, divide and conquer, and online updating for stream data. As a new contribution, the online updating approach is extended to variable selection with commonly used criteria, and their performances are assessed in a simulation study with stream data. Software packages are summarized with focuses on the open source R and R packages, covering recent tools that help break the barriers of computer memory and computing power. Some of the tools are illustrated in a case study with a logistic regression for the chance of airline delay.",fullPaper,jv346
Medicine,p1500,d1,7fc9a268aeebfa25b77a784fb47d0959523cff00,c109,Computer Vision and Pattern Recognition,The Cambridge Structural Database,"This paper is the definitive article describing the creation, maintenance, information content and availability of the Cambridge Structural Database (CSD), the world’s repository of small molecule crystal structures.",poster,cp109
Chemistry,p1500,d8,7fc9a268aeebfa25b77a784fb47d0959523cff00,c109,Computer Vision and Pattern Recognition,The Cambridge Structural Database,"This paper is the definitive article describing the creation, maintenance, information content and availability of the Cambridge Structural Database (CSD), the world’s repository of small molecule crystal structures.",poster,cp109
Medicine,p1501,d1,948fd800ecdd3c99488dde36b41480ca1b8acce3,c121,International Conference on Interaction Sciences,The PRIDE database and related tools and resources in 2019: improving support for quantification data,"Abstract The PRoteomics IDEntifications (PRIDE) database (https://www.ebi.ac.uk/pride/) is the world’s largest data repository of mass spectrometry-based proteomics data, and is one of the founding members of the global ProteomeXchange (PX) consortium. In this manuscript, we summarize the developments in PRIDE resources and related tools since the previous update manuscript was published in Nucleic Acids Research in 2016. In the last 3 years, public data sharing through PRIDE (as part of PX) has definitely become the norm in the field. In parallel, data re-use of public proteomics data has increased enormously, with multiple applications. We first describe the new architecture of PRIDE Archive, the archival component of PRIDE. PRIDE Archive and the related data submission framework have been further developed to support the increase in submitted data volumes and additional data types. A new scalable and fault tolerant storage backend, Application Programming Interface and web interface have been implemented, as a part of an ongoing process. Additionally, we emphasize the improved support for quantitative proteomics data through the mzTab format. At last, we outline key statistics on the current data contents and volume of downloads, and how PRIDE data are starting to be disseminated to added-value resources including Ensembl, UniProt and Expression Atlas.",poster,cp121
Computer Science,p1501,d3,948fd800ecdd3c99488dde36b41480ca1b8acce3,c121,International Conference on Interaction Sciences,The PRIDE database and related tools and resources in 2019: improving support for quantification data,"Abstract The PRoteomics IDEntifications (PRIDE) database (https://www.ebi.ac.uk/pride/) is the world’s largest data repository of mass spectrometry-based proteomics data, and is one of the founding members of the global ProteomeXchange (PX) consortium. In this manuscript, we summarize the developments in PRIDE resources and related tools since the previous update manuscript was published in Nucleic Acids Research in 2016. In the last 3 years, public data sharing through PRIDE (as part of PX) has definitely become the norm in the field. In parallel, data re-use of public proteomics data has increased enormously, with multiple applications. We first describe the new architecture of PRIDE Archive, the archival component of PRIDE. PRIDE Archive and the related data submission framework have been further developed to support the increase in submitted data volumes and additional data types. A new scalable and fault tolerant storage backend, Application Programming Interface and web interface have been implemented, as a part of an ongoing process. Additionally, we emphasize the improved support for quantitative proteomics data through the mzTab format. At last, we outline key statistics on the current data contents and volume of downloads, and how PRIDE data are starting to be disseminated to added-value resources including Ensembl, UniProt and Expression Atlas.",poster,cp121
Biology,p1501,d5,948fd800ecdd3c99488dde36b41480ca1b8acce3,c121,International Conference on Interaction Sciences,The PRIDE database and related tools and resources in 2019: improving support for quantification data,"Abstract The PRoteomics IDEntifications (PRIDE) database (https://www.ebi.ac.uk/pride/) is the world’s largest data repository of mass spectrometry-based proteomics data, and is one of the founding members of the global ProteomeXchange (PX) consortium. In this manuscript, we summarize the developments in PRIDE resources and related tools since the previous update manuscript was published in Nucleic Acids Research in 2016. In the last 3 years, public data sharing through PRIDE (as part of PX) has definitely become the norm in the field. In parallel, data re-use of public proteomics data has increased enormously, with multiple applications. We first describe the new architecture of PRIDE Archive, the archival component of PRIDE. PRIDE Archive and the related data submission framework have been further developed to support the increase in submitted data volumes and additional data types. A new scalable and fault tolerant storage backend, Application Programming Interface and web interface have been implemented, as a part of an ongoing process. Additionally, we emphasize the improved support for quantitative proteomics data through the mzTab format. At last, we outline key statistics on the current data contents and volume of downloads, and how PRIDE data are starting to be disseminated to added-value resources including Ensembl, UniProt and Expression Atlas.",poster,cp121
Medicine,p1502,d1,95cd83603a0d2b6918a8e34a5637a8f382da96f5,j19,Scientific Data,"MIMIC-III, a freely accessible critical care database",Abstract,fullPaper,jv19
Computer Science,p1502,d3,95cd83603a0d2b6918a8e34a5637a8f382da96f5,j19,Scientific Data,"MIMIC-III, a freely accessible critical care database",Abstract,fullPaper,jv19
Medicine,p1503,d1,98128fd412ebfa90201a276f2c59020ccc696a75,c26,Decision Support Systems,DrugBank 5.0: a major update to the DrugBank database for 2018,"Abstract DrugBank (www.drugbank.ca) is a web-enabled database containing comprehensive molecular information about drugs, their mechanisms, their interactions and their targets. First described in 2006, DrugBank has continued to evolve over the past 12 years in response to marked improvements to web standards and changing needs for drug research and development. This year’s update, DrugBank 5.0, represents the most significant upgrade to the database in more than 10 years. In many cases, existing data content has grown by 100% or more over the last update. For instance, the total number of investigational drugs in the database has grown by almost 300%, the number of drug-drug interactions has grown by nearly 600% and the number of SNP-associated drug effects has grown more than 3000%. Significant improvements have been made to the quantity, quality and consistency of drug indications, drug binding data as well as drug-drug and drug-food interactions. A great deal of brand new data have also been added to DrugBank 5.0. This includes information on the influence of hundreds of drugs on metabolite levels (pharmacometabolomics), gene expression levels (pharmacotranscriptomics) and protein expression levels (pharmacoprotoemics). New data have also been added on the status of hundreds of new drug clinical trials and existing drug repurposing trials. Many other important improvements in the content, interface and performance of the DrugBank website have been made and these should greatly enhance its ease of use, utility and potential applications in many areas of pharmacological research, pharmaceutical science and drug education.",poster,cp26
Computer Science,p1503,d3,98128fd412ebfa90201a276f2c59020ccc696a75,c26,Decision Support Systems,DrugBank 5.0: a major update to the DrugBank database for 2018,"Abstract DrugBank (www.drugbank.ca) is a web-enabled database containing comprehensive molecular information about drugs, their mechanisms, their interactions and their targets. First described in 2006, DrugBank has continued to evolve over the past 12 years in response to marked improvements to web standards and changing needs for drug research and development. This year’s update, DrugBank 5.0, represents the most significant upgrade to the database in more than 10 years. In many cases, existing data content has grown by 100% or more over the last update. For instance, the total number of investigational drugs in the database has grown by almost 300%, the number of drug-drug interactions has grown by nearly 600% and the number of SNP-associated drug effects has grown more than 3000%. Significant improvements have been made to the quantity, quality and consistency of drug indications, drug binding data as well as drug-drug and drug-food interactions. A great deal of brand new data have also been added to DrugBank 5.0. This includes information on the influence of hundreds of drugs on metabolite levels (pharmacometabolomics), gene expression levels (pharmacotranscriptomics) and protein expression levels (pharmacoprotoemics). New data have also been added on the status of hundreds of new drug clinical trials and existing drug repurposing trials. Many other important improvements in the content, interface and performance of the DrugBank website have been made and these should greatly enhance its ease of use, utility and potential applications in many areas of pharmacological research, pharmaceutical science and drug education.",poster,cp26
Biology,p1503,d5,98128fd412ebfa90201a276f2c59020ccc696a75,c26,Decision Support Systems,DrugBank 5.0: a major update to the DrugBank database for 2018,"Abstract DrugBank (www.drugbank.ca) is a web-enabled database containing comprehensive molecular information about drugs, their mechanisms, their interactions and their targets. First described in 2006, DrugBank has continued to evolve over the past 12 years in response to marked improvements to web standards and changing needs for drug research and development. This year’s update, DrugBank 5.0, represents the most significant upgrade to the database in more than 10 years. In many cases, existing data content has grown by 100% or more over the last update. For instance, the total number of investigational drugs in the database has grown by almost 300%, the number of drug-drug interactions has grown by nearly 600% and the number of SNP-associated drug effects has grown more than 3000%. Significant improvements have been made to the quantity, quality and consistency of drug indications, drug binding data as well as drug-drug and drug-food interactions. A great deal of brand new data have also been added to DrugBank 5.0. This includes information on the influence of hundreds of drugs on metabolite levels (pharmacometabolomics), gene expression levels (pharmacotranscriptomics) and protein expression levels (pharmacoprotoemics). New data have also been added on the status of hundreds of new drug clinical trials and existing drug repurposing trials. Many other important improvements in the content, interface and performance of the DrugBank website have been made and these should greatly enhance its ease of use, utility and potential applications in many areas of pharmacological research, pharmaceutical science and drug education.",poster,cp26
Medicine,p1504,d1,da692ee969d9c33986196372c3f7cb87fa6b6f8f,c120,SIGSAND-Europe Symposium,Database resources of the National Center for Biotechnology Information,"Abstract The National Center for Biotechnology Information (NCBI) provides a large suite of online resources for biological information and data, including the GenBank® nucleic acid sequence database and the PubMed database of citations and abstracts for published life science journals. The Entrez system provides search and retrieval operations for most of these data from 39 distinct databases. The E-utilities serve as the programming interface for the Entrez system. Augmenting many of the Web applications are custom implementations of the BLAST program optimized to search specialized data sets. New resources released in the past year include PubMed Data Management, RefSeq Functional Elements, genome data download, variation services API, Magic-BLAST, QuickBLASTp, and Identical Protein Groups. Resources that were updated in the past year include the genome data viewer, a human genome resources page, Gene, virus variation, OSIRIS, and PubChem. All of these resources can be accessed through the NCBI home page at www.ncbi.nlm.nih.gov.",poster,cp120
Computer Science,p1504,d3,da692ee969d9c33986196372c3f7cb87fa6b6f8f,c120,SIGSAND-Europe Symposium,Database resources of the National Center for Biotechnology Information,"Abstract The National Center for Biotechnology Information (NCBI) provides a large suite of online resources for biological information and data, including the GenBank® nucleic acid sequence database and the PubMed database of citations and abstracts for published life science journals. The Entrez system provides search and retrieval operations for most of these data from 39 distinct databases. The E-utilities serve as the programming interface for the Entrez system. Augmenting many of the Web applications are custom implementations of the BLAST program optimized to search specialized data sets. New resources released in the past year include PubMed Data Management, RefSeq Functional Elements, genome data download, variation services API, Magic-BLAST, QuickBLASTp, and Identical Protein Groups. Resources that were updated in the past year include the genome data viewer, a human genome resources page, Gene, virus variation, OSIRIS, and PubChem. All of these resources can be accessed through the NCBI home page at www.ncbi.nlm.nih.gov.",poster,cp120
Medicine,p1505,d1,6e1e6afb314f9c5a24d744252a30aa5efc313571,c1,International Conference on Human Factors in Computing Systems,"The STRING database in 2017: quality-controlled protein–protein association networks, made broadly accessible","A system-wide understanding of cellular function requires knowledge of all functional interactions between the expressed proteins. The STRING database aims to collect and integrate this information, by consolidating known and predicted protein–protein association data for a large number of organisms. The associations in STRING include direct (physical) interactions, as well as indirect (functional) interactions, as long as both are specific and biologically meaningful. Apart from collecting and reassessing available experimental data on protein–protein interactions, and importing known pathways and protein complexes from curated databases, interaction predictions are derived from the following sources: (i) systematic co-expression analysis, (ii) detection of shared selective signals across genomes, (iii) automated text-mining of the scientific literature and (iv) computational transfer of interaction knowledge between organisms based on gene orthology. In the latest version 10.5 of STRING, the biggest changes are concerned with data dissemination: the web frontend has been completely redesigned to reduce dependency on outdated browser technologies, and the database can now also be queried from inside the popular Cytoscape software framework. Further improvements include automated background analysis of user inputs for functional enrichments, and streamlined download options. The STRING resource is available online, at http://string-db.org/.",poster,cp1
Computer Science,p1505,d3,6e1e6afb314f9c5a24d744252a30aa5efc313571,c1,International Conference on Human Factors in Computing Systems,"The STRING database in 2017: quality-controlled protein–protein association networks, made broadly accessible","A system-wide understanding of cellular function requires knowledge of all functional interactions between the expressed proteins. The STRING database aims to collect and integrate this information, by consolidating known and predicted protein–protein association data for a large number of organisms. The associations in STRING include direct (physical) interactions, as well as indirect (functional) interactions, as long as both are specific and biologically meaningful. Apart from collecting and reassessing available experimental data on protein–protein interactions, and importing known pathways and protein complexes from curated databases, interaction predictions are derived from the following sources: (i) systematic co-expression analysis, (ii) detection of shared selective signals across genomes, (iii) automated text-mining of the scientific literature and (iv) computational transfer of interaction knowledge between organisms based on gene orthology. In the latest version 10.5 of STRING, the biggest changes are concerned with data dissemination: the web frontend has been completely redesigned to reduce dependency on outdated browser technologies, and the database can now also be queried from inside the popular Cytoscape software framework. Further improvements include automated background analysis of user inputs for functional enrichments, and streamlined download options. The STRING resource is available online, at http://string-db.org/.",poster,cp1
Biology,p1505,d5,6e1e6afb314f9c5a24d744252a30aa5efc313571,c1,International Conference on Human Factors in Computing Systems,"The STRING database in 2017: quality-controlled protein–protein association networks, made broadly accessible","A system-wide understanding of cellular function requires knowledge of all functional interactions between the expressed proteins. The STRING database aims to collect and integrate this information, by consolidating known and predicted protein–protein association data for a large number of organisms. The associations in STRING include direct (physical) interactions, as well as indirect (functional) interactions, as long as both are specific and biologically meaningful. Apart from collecting and reassessing available experimental data on protein–protein interactions, and importing known pathways and protein complexes from curated databases, interaction predictions are derived from the following sources: (i) systematic co-expression analysis, (ii) detection of shared selective signals across genomes, (iii) automated text-mining of the scientific literature and (iv) computational transfer of interaction knowledge between organisms based on gene orthology. In the latest version 10.5 of STRING, the biggest changes are concerned with data dissemination: the web frontend has been completely redesigned to reduce dependency on outdated browser technologies, and the database can now also be queried from inside the popular Cytoscape software framework. Further improvements include automated background analysis of user inputs for functional enrichments, and streamlined download options. The STRING resource is available online, at http://string-db.org/.",poster,cp1
Medicine,p1506,d1,51da1eab2d350b5aa0eeebf83fba7caae3a3bc29,j347,International Journal of Systematic and Evolutionary Microbiology,Introducing EzBioCloud: a taxonomically united database of 16S rRNA gene sequences and whole-genome assemblies,"The recent advent of DNA sequencing technologies facilitates the use of genome sequencing data that provide means for more informative and precise classification and identification of members of the Bacteria and Archaea. Because the current species definition is based on the comparison of genome sequences between type and other strains in a given species, building a genome database with correct taxonomic information is of paramount need to enhance our efforts in exploring prokaryotic diversity and discovering novel species as well as for routine identifications. Here we introduce an integrated database, called EzBioCloud, that holds the taxonomic hierarchy of the Bacteria and Archaea, which is represented by quality-controlled 16S rRNA gene and genome sequences. Whole-genome assemblies in the NCBI Assembly Database were screened for low quality and subjected to a composite identification bioinformatics pipeline that employs gene-based searches followed by the calculation of average nucleotide identity. As a result, the database is made of 61 700 species/phylotypes, including 13 132 with validly published names, and 62 362 whole-genome assemblies that were identified taxonomically at the genus, species and subspecies levels. Genomic properties, such as genome size and DNA G+C content, and the occurrence in human microbiome data were calculated for each genus or higher taxa. This united database of taxonomy, 16S rRNA gene and genome sequences, with accompanying bioinformatics tools, should accelerate genome-based classification and identification of members of the Bacteria and Archaea. The database and related search tools are available at www.ezbiocloud.net/.",fullPaper,jv347
Biology,p1506,d5,51da1eab2d350b5aa0eeebf83fba7caae3a3bc29,j347,International Journal of Systematic and Evolutionary Microbiology,Introducing EzBioCloud: a taxonomically united database of 16S rRNA gene sequences and whole-genome assemblies,"The recent advent of DNA sequencing technologies facilitates the use of genome sequencing data that provide means for more informative and precise classification and identification of members of the Bacteria and Archaea. Because the current species definition is based on the comparison of genome sequences between type and other strains in a given species, building a genome database with correct taxonomic information is of paramount need to enhance our efforts in exploring prokaryotic diversity and discovering novel species as well as for routine identifications. Here we introduce an integrated database, called EzBioCloud, that holds the taxonomic hierarchy of the Bacteria and Archaea, which is represented by quality-controlled 16S rRNA gene and genome sequences. Whole-genome assemblies in the NCBI Assembly Database were screened for low quality and subjected to a composite identification bioinformatics pipeline that employs gene-based searches followed by the calculation of average nucleotide identity. As a result, the database is made of 61 700 species/phylotypes, including 13 132 with validly published names, and 62 362 whole-genome assemblies that were identified taxonomically at the genus, species and subspecies levels. Genomic properties, such as genome size and DNA G+C content, and the occurrence in human microbiome data were calculated for each genus or higher taxa. This united database of taxonomy, 16S rRNA gene and genome sequences, with accompanying bioinformatics tools, should accelerate genome-based classification and identification of members of the Bacteria and Archaea. The database and related search tools are available at www.ezbiocloud.net/.",fullPaper,jv347
Medicine,p1507,d1,0f5c63182b5d40850c741888a89e6c055a3593af,c58,Extreme Science and Engineering Discovery Environment,The Pfam protein families database: towards a more sustainable future,"In the last two years the Pfam database (http://pfam.xfam.org) has undergone a substantial reorganisation to reduce the effort involved in making a release, thereby permitting more frequent releases. Arguably the most significant of these changes is that Pfam is now primarily based on the UniProtKB reference proteomes, with the counts of matched sequences and species reported on the website restricted to this smaller set. Building families on reference proteomes sequences brings greater stability, which decreases the amount of manual curation required to maintain them. It also reduces the number of sequences displayed on the website, whilst still providing access to many important model organisms. Matches to the full UniProtKB database are, however, still available and Pfam annotations for individual UniProtKB sequences can still be retrieved. Some Pfam entries (1.6%) which have no matches to reference proteomes remain; we are working with UniProt to see if sequences from them can be incorporated into reference proteomes. Pfam-B, the automatically-generated supplement to Pfam, has been removed. The current release (Pfam 29.0) includes 16 295 entries and 559 clans. The facility to view the relationship between families within a clan has been improved by the introduction of a new tool.",poster,cp58
Computer Science,p1507,d3,0f5c63182b5d40850c741888a89e6c055a3593af,c58,Extreme Science and Engineering Discovery Environment,The Pfam protein families database: towards a more sustainable future,"In the last two years the Pfam database (http://pfam.xfam.org) has undergone a substantial reorganisation to reduce the effort involved in making a release, thereby permitting more frequent releases. Arguably the most significant of these changes is that Pfam is now primarily based on the UniProtKB reference proteomes, with the counts of matched sequences and species reported on the website restricted to this smaller set. Building families on reference proteomes sequences brings greater stability, which decreases the amount of manual curation required to maintain them. It also reduces the number of sequences displayed on the website, whilst still providing access to many important model organisms. Matches to the full UniProtKB database are, however, still available and Pfam annotations for individual UniProtKB sequences can still be retrieved. Some Pfam entries (1.6%) which have no matches to reference proteomes remain; we are working with UniProt to see if sequences from them can be incorporated into reference proteomes. Pfam-B, the automatically-generated supplement to Pfam, has been removed. The current release (Pfam 29.0) includes 16 295 entries and 559 clans. The facility to view the relationship between families within a clan has been improved by the introduction of a new tool.",poster,cp58
Biology,p1507,d5,0f5c63182b5d40850c741888a89e6c055a3593af,c58,Extreme Science and Engineering Discovery Environment,The Pfam protein families database: towards a more sustainable future,"In the last two years the Pfam database (http://pfam.xfam.org) has undergone a substantial reorganisation to reduce the effort involved in making a release, thereby permitting more frequent releases. Arguably the most significant of these changes is that Pfam is now primarily based on the UniProtKB reference proteomes, with the counts of matched sequences and species reported on the website restricted to this smaller set. Building families on reference proteomes sequences brings greater stability, which decreases the amount of manual curation required to maintain them. It also reduces the number of sequences displayed on the website, whilst still providing access to many important model organisms. Matches to the full UniProtKB database are, however, still available and Pfam annotations for individual UniProtKB sequences can still be retrieved. Some Pfam entries (1.6%) which have no matches to reference proteomes remain; we are working with UniProt to see if sequences from them can be incorporated into reference proteomes. Pfam-B, the automatically-generated supplement to Pfam, has been removed. The current release (Pfam 29.0) includes 16 295 entries and 559 clans. The facility to view the relationship between families within a clan has been improved by the introduction of a new tool.",poster,cp58
Medicine,p1508,d1,16b0744424f02e01fe2f01b3ea03e2862f1359fc,c22,Grid Computing Environments,"Reference sequence (RefSeq) database at NCBI: current status, taxonomic expansion, and functional annotation","The RefSeq project at the National Center for Biotechnology Information (NCBI) maintains and curates a publicly available database of annotated genomic, transcript, and protein sequence records (http://www.ncbi.nlm.nih.gov/refseq/). The RefSeq project leverages the data submitted to the International Nucleotide Sequence Database Collaboration (INSDC) against a combination of computation, manual curation, and collaboration to produce a standard set of stable, non-redundant reference sequences. The RefSeq project augments these reference sequences with current knowledge including publications, functional features and informative nomenclature. The database currently represents sequences from more than 55 000 organisms (>4800 viruses, >40 000 prokaryotes and >10 000 eukaryotes; RefSeq release 71), ranging from a single record to complete genomes. This paper summarizes the current status of the viral, prokaryotic, and eukaryotic branches of the RefSeq project, reports on improvements to data access and details efforts to further expand the taxonomic representation of the collection. We also highlight diverse functional curation initiatives that support multiple uses of RefSeq data including taxonomic validation, genome annotation, comparative genomics, and clinical testing. We summarize our approach to utilizing available RNA-Seq and other data types in our manual curation process for vertebrate, plant, and other species, and describe a new direction for prokaryotic genomes and protein name management.",poster,cp22
Computer Science,p1508,d3,16b0744424f02e01fe2f01b3ea03e2862f1359fc,c22,Grid Computing Environments,"Reference sequence (RefSeq) database at NCBI: current status, taxonomic expansion, and functional annotation","The RefSeq project at the National Center for Biotechnology Information (NCBI) maintains and curates a publicly available database of annotated genomic, transcript, and protein sequence records (http://www.ncbi.nlm.nih.gov/refseq/). The RefSeq project leverages the data submitted to the International Nucleotide Sequence Database Collaboration (INSDC) against a combination of computation, manual curation, and collaboration to produce a standard set of stable, non-redundant reference sequences. The RefSeq project augments these reference sequences with current knowledge including publications, functional features and informative nomenclature. The database currently represents sequences from more than 55 000 organisms (>4800 viruses, >40 000 prokaryotes and >10 000 eukaryotes; RefSeq release 71), ranging from a single record to complete genomes. This paper summarizes the current status of the viral, prokaryotic, and eukaryotic branches of the RefSeq project, reports on improvements to data access and details efforts to further expand the taxonomic representation of the collection. We also highlight diverse functional curation initiatives that support multiple uses of RefSeq data including taxonomic validation, genome annotation, comparative genomics, and clinical testing. We summarize our approach to utilizing available RNA-Seq and other data types in our manual curation process for vertebrate, plant, and other species, and describe a new direction for prokaryotic genomes and protein name management.",poster,cp22
Biology,p1508,d5,16b0744424f02e01fe2f01b3ea03e2862f1359fc,c22,Grid Computing Environments,"Reference sequence (RefSeq) database at NCBI: current status, taxonomic expansion, and functional annotation","The RefSeq project at the National Center for Biotechnology Information (NCBI) maintains and curates a publicly available database of annotated genomic, transcript, and protein sequence records (http://www.ncbi.nlm.nih.gov/refseq/). The RefSeq project leverages the data submitted to the International Nucleotide Sequence Database Collaboration (INSDC) against a combination of computation, manual curation, and collaboration to produce a standard set of stable, non-redundant reference sequences. The RefSeq project augments these reference sequences with current knowledge including publications, functional features and informative nomenclature. The database currently represents sequences from more than 55 000 organisms (>4800 viruses, >40 000 prokaryotes and >10 000 eukaryotes; RefSeq release 71), ranging from a single record to complete genomes. This paper summarizes the current status of the viral, prokaryotic, and eukaryotic branches of the RefSeq project, reports on improvements to data access and details efforts to further expand the taxonomic representation of the collection. We also highlight diverse functional curation initiatives that support multiple uses of RefSeq data including taxonomic validation, genome annotation, comparative genomics, and clinical testing. We summarize our approach to utilizing available RNA-Seq and other data types in our manual curation process for vertebrate, plant, and other species, and describe a new direction for prokaryotic genomes and protein name management.",poster,cp22
Medicine,p1509,d1,f986968735459e789890f24b6b277b0920a9725d,j173,IEEE Transactions on Pattern Analysis and Machine Intelligence,Places: A 10 Million Image Database for Scene Recognition,"The rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach near-human semantic classification performance at tasks such as visual object and scene recognition. Here we describe the Places Database, a repository of 10 million scene photographs, labeled with scene semantic categories, comprising a large and diverse list of the types of environments encountered in the world. Using the state-of-the-art Convolutional Neural Networks (CNNs), we provide scene classification CNNs (Places-CNNs) as baselines, that significantly outperform the previous approaches. Visualization of the CNNs trained on Places shows that object detectors emerge as an intermediate representation of scene classification. With its high-coverage and high-diversity of exemplars, the Places Database along with the Places-CNNs offer a novel resource to guide future progress on scene recognition problems.",fullPaper,jv173
Computer Science,p1509,d3,f986968735459e789890f24b6b277b0920a9725d,j173,IEEE Transactions on Pattern Analysis and Machine Intelligence,Places: A 10 Million Image Database for Scene Recognition,"The rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach near-human semantic classification performance at tasks such as visual object and scene recognition. Here we describe the Places Database, a repository of 10 million scene photographs, labeled with scene semantic categories, comprising a large and diverse list of the types of environments encountered in the world. Using the state-of-the-art Convolutional Neural Networks (CNNs), we provide scene classification CNNs (Places-CNNs) as baselines, that significantly outperform the previous approaches. Visualization of the CNNs trained on Places shows that object detectors emerge as an intermediate representation of scene classification. With its high-coverage and high-diversity of exemplars, the Places Database along with the Places-CNNs offer a novel resource to guide future progress on scene recognition problems.",fullPaper,jv173
Medicine,p1512,d1,02613d6e3ecf67ed9ae8ce67a35a92f3986bc4cf,j107,Nucleic Acids Research,Gapped BLAST and PSI-BLAST: a new generation of protein database search programs.,"The BLAST programs are widely used tools for searching protein and DNA databases for sequence similarities. For protein comparisons, a variety of definitional, algorithmic and statistical refinements described here permits the execution time of the BLAST programs to be decreased substantially while enhancing their sensitivity to weak similarities. A new criterion for triggering the extension of word hits, combined with a new heuristic for generating gapped alignments, yields a gapped BLAST program that runs at approximately three times the speed of the original. In addition, a method is introduced for automatically combining statistically significant alignments produced by BLAST into a position-specific score matrix, and searching the database using this matrix. The resulting Position-Specific Iterated BLAST (PSI-BLAST) program runs at approximately the same speed per iteration as gapped BLAST, but in many cases is much more sensitive to weak but biologically relevant sequence similarities. PSI-BLAST is used to uncover several new and interesting members of the BRCT superfamily.",fullPaper,jv107
Biology,p1512,d5,02613d6e3ecf67ed9ae8ce67a35a92f3986bc4cf,j107,Nucleic Acids Research,Gapped BLAST and PSI-BLAST: a new generation of protein database search programs.,"The BLAST programs are widely used tools for searching protein and DNA databases for sequence similarities. For protein comparisons, a variety of definitional, algorithmic and statistical refinements described here permits the execution time of the BLAST programs to be decreased substantially while enhancing their sensitivity to weak similarities. A new criterion for triggering the extension of word hits, combined with a new heuristic for generating gapped alignments, yields a gapped BLAST program that runs at approximately three times the speed of the original. In addition, a method is introduced for automatically combining statistically significant alignments produced by BLAST into a position-specific score matrix, and searching the database using this matrix. The resulting Position-Specific Iterated BLAST (PSI-BLAST) program runs at approximately the same speed per iteration as gapped BLAST, but in many cases is much more sensitive to weak but biologically relevant sequence similarities. PSI-BLAST is used to uncover several new and interesting members of the BRCT superfamily.",fullPaper,jv107
Medicine,p1513,d1,7f19972754ac0c15329666b3a6efbf569b27d8d5,c46,Ideal,The Pfam protein families database in 2019,"Abstract The last few years have witnessed significant changes in Pfam (https://pfam.xfam.org). The number of families has grown substantially to a total of 17,929 in release 32.0. New additions have been coupled with efforts to improve existing families, including refinement of domain boundaries, their classification into Pfam clans, as well as their functional annotation. We recently began to collaborate with the RepeatsDB resource to improve the definition of tandem repeat families within Pfam. We carried out a significant comparison to the structural classification database, namely the Evolutionary Classification of Protein Domains (ECOD) that led to the creation of 825 new families based on their set of uncharacterized families (EUFs). Furthermore, we also connected Pfam entries to the Sequence Ontology (SO) through mapping of the Pfam type definitions to SO terms. Since Pfam has many community contributors, we recently enabled the linking between authorship of all Pfam entries with the corresponding authors’ ORCID identifiers. This effectively permits authors to claim credit for their Pfam curation and link them to their ORCID record.",poster,cp46
Computer Science,p1513,d3,7f19972754ac0c15329666b3a6efbf569b27d8d5,c46,Ideal,The Pfam protein families database in 2019,"Abstract The last few years have witnessed significant changes in Pfam (https://pfam.xfam.org). The number of families has grown substantially to a total of 17,929 in release 32.0. New additions have been coupled with efforts to improve existing families, including refinement of domain boundaries, their classification into Pfam clans, as well as their functional annotation. We recently began to collaborate with the RepeatsDB resource to improve the definition of tandem repeat families within Pfam. We carried out a significant comparison to the structural classification database, namely the Evolutionary Classification of Protein Domains (ECOD) that led to the creation of 825 new families based on their set of uncharacterized families (EUFs). Furthermore, we also connected Pfam entries to the Sequence Ontology (SO) through mapping of the Pfam type definitions to SO terms. Since Pfam has many community contributors, we recently enabled the linking between authorship of all Pfam entries with the corresponding authors’ ORCID identifiers. This effectively permits authors to claim credit for their Pfam curation and link them to their ORCID record.",poster,cp46
Biology,p1513,d5,7f19972754ac0c15329666b3a6efbf569b27d8d5,c46,Ideal,The Pfam protein families database in 2019,"Abstract The last few years have witnessed significant changes in Pfam (https://pfam.xfam.org). The number of families has grown substantially to a total of 17,929 in release 32.0. New additions have been coupled with efforts to improve existing families, including refinement of domain boundaries, their classification into Pfam clans, as well as their functional annotation. We recently began to collaborate with the RepeatsDB resource to improve the definition of tandem repeat families within Pfam. We carried out a significant comparison to the structural classification database, namely the Evolutionary Classification of Protein Domains (ECOD) that led to the creation of 825 new families based on their set of uncharacterized families (EUFs). Furthermore, we also connected Pfam entries to the Sequence Ontology (SO) through mapping of the Pfam type definitions to SO terms. Since Pfam has many community contributors, we recently enabled the linking between authorship of all Pfam entries with the corresponding authors’ ORCID identifiers. This effectively permits authors to claim credit for their Pfam curation and link them to their ORCID record.",poster,cp46
Medicine,p1514,d1,3283f9c33e4b3fbd51d58a54dc236a92f5a98f80,c49,ACM/SIGCOMM Internet Measurement Conference,The PRIDE database resources in 2022: a hub for mass spectrometry-based proteomics evidences,"Abstract The PRoteomics IDEntifications (PRIDE) database (https://www.ebi.ac.uk/pride/) is the world's largest data repository of mass spectrometry-based proteomics data. PRIDE is one of the founding members of the global ProteomeXchange (PX) consortium and an ELIXIR core data resource. In this manuscript, we summarize the developments in PRIDE resources and related tools since the previous update manuscript was published in Nucleic Acids Research in 2019. The number of submitted datasets to PRIDE Archive (the archival component of PRIDE) has reached on average around 500 datasets per month during 2021. In addition to continuous improvements in PRIDE Archive data pipelines and infrastructure, the PRIDE Spectra Archive has been developed to provide direct access to the submitted mass spectra using Universal Spectrum Identifiers. As a key point, the file format MAGE-TAB for proteomics has been developed to enable the improvement of sample metadata annotation. Additionally, the resource PRIDE Peptidome provides access to aggregated peptide/protein evidences across PRIDE Archive. Furthermore, we will describe how PRIDE has increased its efforts to reuse and disseminate high-quality proteomics data into other added-value resources such as UniProt, Ensembl and Expression Atlas.",poster,cp49
Computer Science,p1514,d3,3283f9c33e4b3fbd51d58a54dc236a92f5a98f80,c49,ACM/SIGCOMM Internet Measurement Conference,The PRIDE database resources in 2022: a hub for mass spectrometry-based proteomics evidences,"Abstract The PRoteomics IDEntifications (PRIDE) database (https://www.ebi.ac.uk/pride/) is the world's largest data repository of mass spectrometry-based proteomics data. PRIDE is one of the founding members of the global ProteomeXchange (PX) consortium and an ELIXIR core data resource. In this manuscript, we summarize the developments in PRIDE resources and related tools since the previous update manuscript was published in Nucleic Acids Research in 2019. The number of submitted datasets to PRIDE Archive (the archival component of PRIDE) has reached on average around 500 datasets per month during 2021. In addition to continuous improvements in PRIDE Archive data pipelines and infrastructure, the PRIDE Spectra Archive has been developed to provide direct access to the submitted mass spectra using Universal Spectrum Identifiers. As a key point, the file format MAGE-TAB for proteomics has been developed to enable the improvement of sample metadata annotation. Additionally, the resource PRIDE Peptidome provides access to aggregated peptide/protein evidences across PRIDE Archive. Furthermore, we will describe how PRIDE has increased its efforts to reuse and disseminate high-quality proteomics data into other added-value resources such as UniProt, Ensembl and Expression Atlas.",poster,cp49
Medicine,p1515,d1,9f7626c7af925b7b69f1ba86ceb916d21bc03dbe,c80,International Symposium on Software Engineering for Adaptive and Self-Managing Systems,Pfam: The protein families database in 2021,"Abstract The Pfam database is a widely used resource for classifying protein sequences into families and domains. Since Pfam was last described in this journal, over 350 new families have been added in Pfam 33.1 and numerous improvements have been made to existing entries. To facilitate research on COVID-19, we have revised the Pfam entries that cover the SARS-CoV-2 proteome, and built new entries for regions that were not covered by Pfam. We have reintroduced Pfam-B which provides an automatically generated supplement to Pfam and contains 136 730 novel clusters of sequences that are not yet matched by a Pfam family. The new Pfam-B is based on a clustering by the MMseqs2 software. We have compared all of the regions in the RepeatsDB to those in Pfam and have started to use the results to build and refine Pfam repeat families. Pfam is freely available for browsing and download at http://pfam.xfam.org/.",poster,cp80
Computer Science,p1515,d3,9f7626c7af925b7b69f1ba86ceb916d21bc03dbe,c80,International Symposium on Software Engineering for Adaptive and Self-Managing Systems,Pfam: The protein families database in 2021,"Abstract The Pfam database is a widely used resource for classifying protein sequences into families and domains. Since Pfam was last described in this journal, over 350 new families have been added in Pfam 33.1 and numerous improvements have been made to existing entries. To facilitate research on COVID-19, we have revised the Pfam entries that cover the SARS-CoV-2 proteome, and built new entries for regions that were not covered by Pfam. We have reintroduced Pfam-B which provides an automatically generated supplement to Pfam and contains 136 730 novel clusters of sequences that are not yet matched by a Pfam family. The new Pfam-B is based on a clustering by the MMseqs2 software. We have compared all of the regions in the RepeatsDB to those in Pfam and have started to use the results to build and refine Pfam repeat families. Pfam is freely available for browsing and download at http://pfam.xfam.org/.",poster,cp80
Biology,p1515,d5,9f7626c7af925b7b69f1ba86ceb916d21bc03dbe,c80,International Symposium on Software Engineering for Adaptive and Self-Managing Systems,Pfam: The protein families database in 2021,"Abstract The Pfam database is a widely used resource for classifying protein sequences into families and domains. Since Pfam was last described in this journal, over 350 new families have been added in Pfam 33.1 and numerous improvements have been made to existing entries. To facilitate research on COVID-19, we have revised the Pfam entries that cover the SARS-CoV-2 proteome, and built new entries for regions that were not covered by Pfam. We have reintroduced Pfam-B which provides an automatically generated supplement to Pfam and contains 136 730 novel clusters of sequences that are not yet matched by a Pfam family. The new Pfam-B is based on a clustering by the MMseqs2 software. We have compared all of the regions in the RepeatsDB to those in Pfam and have started to use the results to build and refine Pfam repeat families. Pfam is freely available for browsing and download at http://pfam.xfam.org/.",poster,cp80
Medicine,p1516,d1,b204970b0503a923359bff532726666f5e0e971b,c26,Decision Support Systems,The SILVA ribosomal RNA gene database project: improved data processing and web-based tools,"SILVA (from Latin silva, forest, http://www.arb-silva.de) is a comprehensive web resource for up to date, quality-controlled databases of aligned ribosomal RNA (rRNA) gene sequences from the Bacteria, Archaea and Eukaryota domains and supplementary online services. The referred database release 111 (July 2012) contains 3 194 778 small subunit and 288 717 large subunit rRNA gene sequences. Since the initial description of the project, substantial new features have been introduced, including advanced quality control procedures, an improved rRNA gene aligner, online tools for probe and primer evaluation and optimized browsing, searching and downloading on the website. Furthermore, the extensively curated SILVA taxonomy and the new non-redundant SILVA datasets provide an ideal reference for high-throughput classification of data from next-generation sequencing approaches.",poster,cp26
Computer Science,p1516,d3,b204970b0503a923359bff532726666f5e0e971b,c26,Decision Support Systems,The SILVA ribosomal RNA gene database project: improved data processing and web-based tools,"SILVA (from Latin silva, forest, http://www.arb-silva.de) is a comprehensive web resource for up to date, quality-controlled databases of aligned ribosomal RNA (rRNA) gene sequences from the Bacteria, Archaea and Eukaryota domains and supplementary online services. The referred database release 111 (July 2012) contains 3 194 778 small subunit and 288 717 large subunit rRNA gene sequences. Since the initial description of the project, substantial new features have been introduced, including advanced quality control procedures, an improved rRNA gene aligner, online tools for probe and primer evaluation and optimized browsing, searching and downloading on the website. Furthermore, the extensively curated SILVA taxonomy and the new non-redundant SILVA datasets provide an ideal reference for high-throughput classification of data from next-generation sequencing approaches.",poster,cp26
Biology,p1516,d5,b204970b0503a923359bff532726666f5e0e971b,c26,Decision Support Systems,The SILVA ribosomal RNA gene database project: improved data processing and web-based tools,"SILVA (from Latin silva, forest, http://www.arb-silva.de) is a comprehensive web resource for up to date, quality-controlled databases of aligned ribosomal RNA (rRNA) gene sequences from the Bacteria, Archaea and Eukaryota domains and supplementary online services. The referred database release 111 (July 2012) contains 3 194 778 small subunit and 288 717 large subunit rRNA gene sequences. Since the initial description of the project, substantial new features have been introduced, including advanced quality control procedures, an improved rRNA gene aligner, online tools for probe and primer evaluation and optimized browsing, searching and downloading on the website. Furthermore, the extensively curated SILVA taxonomy and the new non-redundant SILVA datasets provide an ideal reference for high-throughput classification of data from next-generation sequencing approaches.",poster,cp26
Medicine,p1517,d1,3aca912f21d54b3931fa1fdfac0c199c557374a4,c31,Information Security Solutions Europe,GTDB-Tk: a toolkit to classify genomes with the Genome Taxonomy Database,Abstract Summary The Genome Taxonomy Database Toolkit (GTDB-Tk) provides objective taxonomic assignments for bacterial and archaeal genomes based on the GTDB. GTDB-Tk is computationally efficient and able to classify thousands of draft genomes in parallel. Here we demonstrate the accuracy of the GTDB-Tk taxonomic assignments by evaluating its performance on a phylogenetically diverse set of 10 156 bacterial and archaeal metagenome-assembled genomes. Availability and implementation GTDB-Tk is implemented in Python and licenced under the GNU General Public Licence v3.0. Source code and documentation are available at: https://github.com/ecogenomics/gtdbtk. Supplementary information Supplementary data are available at Bioinformatics online.,poster,cp31
Computer Science,p1517,d3,3aca912f21d54b3931fa1fdfac0c199c557374a4,c31,Information Security Solutions Europe,GTDB-Tk: a toolkit to classify genomes with the Genome Taxonomy Database,Abstract Summary The Genome Taxonomy Database Toolkit (GTDB-Tk) provides objective taxonomic assignments for bacterial and archaeal genomes based on the GTDB. GTDB-Tk is computationally efficient and able to classify thousands of draft genomes in parallel. Here we demonstrate the accuracy of the GTDB-Tk taxonomic assignments by evaluating its performance on a phylogenetically diverse set of 10 156 bacterial and archaeal metagenome-assembled genomes. Availability and implementation GTDB-Tk is implemented in Python and licenced under the GNU General Public Licence v3.0. Source code and documentation are available at: https://github.com/ecogenomics/gtdbtk. Supplementary information Supplementary data are available at Bioinformatics online.,poster,cp31
Medicine,p1518,d1,633f318876c41fed36b3905b8af5fdc27f734615,j348,Cell Systems,The Molecular Signatures Database (MSigDB) hallmark gene set collection.,"The Molecular Signatures Database (MSigDB) is one of the most widely used and comprehensive databases of gene sets for performing gene set enrichment analysis. Since its creation, MSigDB has grown beyond its roots in metabolic disease and cancer to include >10,000 gene sets. These better represent a wider range of biological processes and diseases, but the utility of the database is reduced by increased redundancy across, and heterogeneity within, gene sets. To address this challenge, here we use a combination of automated approaches and expert curation to develop a collection of ""hallmark"" gene sets as part of MSigDB. Each hallmark in this collection consists of a ""refined"" gene set, derived from multiple ""founder"" sets, that conveys a specific biological state or process and displays coherent expression. The hallmarks effectively summarize most of the relevant information of the original founder sets and, by reducing both variation and redundancy, provide more refined and concise inputs for gene set enrichment analysis.",fullPaper,jv348
Medicine,p1519,d1,a98753021c6a076a5307f4dfb7fd1fcb14089910,c42,IEEE Working Conference on Mining Software Repositories,The Pfam protein families database,"Pfam is a widely used database of protein families and domains. This article describes a set of major updates that we have implemented in the latest release (version 24.0). The most important change is that we now use HMMER3, the latest version of the popular profile hidden Markov model package. This software is ∼100 times faster than HMMER2 and is more sensitive due to the routine use of the forward algorithm. The move to HMMER3 has necessitated numerous changes to Pfam that are described in detail. Pfam release 24.0 contains 11 912 families, of which a large number have been significantly updated during the past two years. Pfam is available via servers in the UK (http://pfam.sanger.ac.uk/), the USA (http://pfam.janelia.org/) and Sweden (http://pfam.sbc.su.se/).",poster,cp42
Computer Science,p1519,d3,a98753021c6a076a5307f4dfb7fd1fcb14089910,c42,IEEE Working Conference on Mining Software Repositories,The Pfam protein families database,"Pfam is a widely used database of protein families and domains. This article describes a set of major updates that we have implemented in the latest release (version 24.0). The most important change is that we now use HMMER3, the latest version of the popular profile hidden Markov model package. This software is ∼100 times faster than HMMER2 and is more sensitive due to the routine use of the forward algorithm. The move to HMMER3 has necessitated numerous changes to Pfam that are described in detail. Pfam release 24.0 contains 11 912 families, of which a large number have been significantly updated during the past two years. Pfam is available via servers in the UK (http://pfam.sanger.ac.uk/), the USA (http://pfam.janelia.org/) and Sweden (http://pfam.sbc.su.se/).",poster,cp42
Medicine,p1522,d1,54cc1f2e86d1913521b466cef19d72ed02b6c800,j82,BMC Bioinformatics,Argonaute—a database for gene regulation by mammalian microRNAs,Abstract,fullPaper,jv82
Computer Science,p1522,d3,54cc1f2e86d1913521b466cef19d72ed02b6c800,j82,BMC Bioinformatics,Argonaute—a database for gene regulation by mammalian microRNAs,Abstract,fullPaper,jv82
Biology,p1522,d5,54cc1f2e86d1913521b466cef19d72ed02b6c800,j82,BMC Bioinformatics,Argonaute—a database for gene regulation by mammalian microRNAs,Abstract,fullPaper,jv82
Medicine,p1523,d1,0e5bccdedb82fbafece8ca71d64b16ff05ec9145,c38,IEEE Global Engineering Education Conference,The carbohydrate-active enzymes database (CAZy) in 2013,"The Carbohydrate-Active Enzymes database (CAZy; http://www.cazy.org) provides online and continuously updated access to a sequence-based family classification linking the sequence to the specificity and 3D structure of the enzymes that assemble, modify and breakdown oligo- and polysaccharides. Functional and 3D structural information is added and curated on a regular basis based on the available literature. In addition to the use of the database by enzymologists seeking curated information on CAZymes, the dissemination of a stable nomenclature for these enzymes is probably a major contribution of CAZy. The past few years have seen the expansion of the CAZy classification scheme to new families, the development of subfamilies in several families and the power of CAZy for the analysis of genomes and metagenomes. This article outlines the changes that have occurred in CAZy during the past 5 years and presents our novel effort to display the resolution and the carbohydrate ligands in crystallographic complexes of CAZymes.",poster,cp38
Computer Science,p1523,d3,0e5bccdedb82fbafece8ca71d64b16ff05ec9145,c38,IEEE Global Engineering Education Conference,The carbohydrate-active enzymes database (CAZy) in 2013,"The Carbohydrate-Active Enzymes database (CAZy; http://www.cazy.org) provides online and continuously updated access to a sequence-based family classification linking the sequence to the specificity and 3D structure of the enzymes that assemble, modify and breakdown oligo- and polysaccharides. Functional and 3D structural information is added and curated on a regular basis based on the available literature. In addition to the use of the database by enzymologists seeking curated information on CAZymes, the dissemination of a stable nomenclature for these enzymes is probably a major contribution of CAZy. The past few years have seen the expansion of the CAZy classification scheme to new families, the development of subfamilies in several families and the power of CAZy for the analysis of genomes and metagenomes. This article outlines the changes that have occurred in CAZy during the past 5 years and presents our novel effort to display the resolution and the carbohydrate ligands in crystallographic complexes of CAZymes.",poster,cp38
Biology,p1523,d5,0e5bccdedb82fbafece8ca71d64b16ff05ec9145,c38,IEEE Global Engineering Education Conference,The carbohydrate-active enzymes database (CAZy) in 2013,"The Carbohydrate-Active Enzymes database (CAZy; http://www.cazy.org) provides online and continuously updated access to a sequence-based family classification linking the sequence to the specificity and 3D structure of the enzymes that assemble, modify and breakdown oligo- and polysaccharides. Functional and 3D structural information is added and curated on a regular basis based on the available literature. In addition to the use of the database by enzymologists seeking curated information on CAZymes, the dissemination of a stable nomenclature for these enzymes is probably a major contribution of CAZy. The past few years have seen the expansion of the CAZy classification scheme to new families, the development of subfamilies in several families and the power of CAZy for the analysis of genomes and metagenomes. This article outlines the changes that have occurred in CAZy during the past 5 years and presents our novel effort to display the resolution and the carbohydrate ligands in crystallographic complexes of CAZymes.",poster,cp38
Medicine,p1525,d1,57dfc18815bba1c3737dbc2e5497fd1fc595edb5,j347,International Journal of Systematic and Evolutionary Microbiology,Introducing EzTaxon-e: a prokaryotic 16S rRNA gene sequence database with phylotypes that represent uncultured species.,"Despite recent advances in commercially optimized identification systems, bacterial identification remains a challenging task in many routine microbiological laboratories, especially in situations where taxonomically novel isolates are involved. The 16S rRNA gene has been used extensively for this task when coupled with a well-curated database, such as EzTaxon, containing sequences of type strains of prokaryotic species with validly published names. Although the EzTaxon database has been widely used for routine identification of prokaryotic isolates, sequences from uncultured prokaryotes have not been considered. Here, the next generation database, named EzTaxon-e, is formally introduced. This new database covers not only species within the formal nomenclatural system but also phylotypes that may represent species in nature. In addition to an identification function based on Basic Local Alignment Search Tool (blast) searches and pairwise global sequence alignments, a new objective method of assessing the degree of completeness in sequencing is proposed. All sequences that are held in the EzTaxon-e database have been subjected to phylogenetic analysis and this has resulted in a complete hierarchical classification system. It is concluded that the EzTaxon-e database provides a useful taxonomic backbone for the identification of cultured and uncultured prokaryotes and offers a valuable means of communication among microbiologists who routinely encounter taxonomically novel isolates. The database and its analytical functions can be found at http://eztaxon-e.ezbiocloud.net/.",fullPaper,jv347
Biology,p1525,d5,57dfc18815bba1c3737dbc2e5497fd1fc595edb5,j347,International Journal of Systematic and Evolutionary Microbiology,Introducing EzTaxon-e: a prokaryotic 16S rRNA gene sequence database with phylotypes that represent uncultured species.,"Despite recent advances in commercially optimized identification systems, bacterial identification remains a challenging task in many routine microbiological laboratories, especially in situations where taxonomically novel isolates are involved. The 16S rRNA gene has been used extensively for this task when coupled with a well-curated database, such as EzTaxon, containing sequences of type strains of prokaryotic species with validly published names. Although the EzTaxon database has been widely used for routine identification of prokaryotic isolates, sequences from uncultured prokaryotes have not been considered. Here, the next generation database, named EzTaxon-e, is formally introduced. This new database covers not only species within the formal nomenclatural system but also phylotypes that may represent species in nature. In addition to an identification function based on Basic Local Alignment Search Tool (blast) searches and pairwise global sequence alignments, a new objective method of assessing the degree of completeness in sequencing is proposed. All sequences that are held in the EzTaxon-e database have been subjected to phylogenetic analysis and this has resulted in a complete hierarchical classification system. It is concluded that the EzTaxon-e database provides a useful taxonomic backbone for the identification of cultured and uncultured prokaryotes and offers a valuable means of communication among microbiologists who routinely encounter taxonomically novel isolates. The database and its analytical functions can be found at http://eztaxon-e.ezbiocloud.net/.",fullPaper,jv347
Medicine,p1526,d1,66470cf9df2f932f80094a309abcc14bcc1b9373,c75,International Conference on Predictive Models in Software Engineering,2016 update of the PRIDE database and its related tools,"The PRoteomics IDEntifications (PRIDE) database is one of the world-leading data repositories of mass spectrometry (MS)-based proteomics data. Since the beginning of 2014, PRIDE Archive (http://www.ebi.ac.uk/pride/archive/) is the new PRIDE archival system, replacing the original PRIDE database. Here we summarize the developments in PRIDE resources and related tools since the previous update manuscript in the Database Issue in 2013. PRIDE Archive constitutes a complete redevelopment of the original PRIDE, comprising a new storage backend, data submission system and web interface, among other components. PRIDE Archive supports the most-widely used PSI (Proteomics Standards Initiative) data standard formats (mzML and mzIdentML) and implements the data requirements and guidelines of the ProteomeXchange Consortium. The wide adoption of ProteomeXchange within the community has triggered an unprecedented increase in the number of submitted data sets (around 150 data sets per month). We outline some statistics on the current PRIDE Archive data contents. We also report on the status of the PRIDE related stand-alone tools: PRIDE Inspector, PRIDE Converter 2 and the ProteomeXchange submission tool. Finally, we will give a brief update on the resources under development ‘PRIDE Cluster’ and ‘PRIDE Proteomes’, which provide a complementary view and quality-scored information of the peptide and protein identification data available in PRIDE Archive.",poster,cp75
Computer Science,p1526,d3,66470cf9df2f932f80094a309abcc14bcc1b9373,c75,International Conference on Predictive Models in Software Engineering,2016 update of the PRIDE database and its related tools,"The PRoteomics IDEntifications (PRIDE) database is one of the world-leading data repositories of mass spectrometry (MS)-based proteomics data. Since the beginning of 2014, PRIDE Archive (http://www.ebi.ac.uk/pride/archive/) is the new PRIDE archival system, replacing the original PRIDE database. Here we summarize the developments in PRIDE resources and related tools since the previous update manuscript in the Database Issue in 2013. PRIDE Archive constitutes a complete redevelopment of the original PRIDE, comprising a new storage backend, data submission system and web interface, among other components. PRIDE Archive supports the most-widely used PSI (Proteomics Standards Initiative) data standard formats (mzML and mzIdentML) and implements the data requirements and guidelines of the ProteomeXchange Consortium. The wide adoption of ProteomeXchange within the community has triggered an unprecedented increase in the number of submitted data sets (around 150 data sets per month). We outline some statistics on the current PRIDE Archive data contents. We also report on the status of the PRIDE related stand-alone tools: PRIDE Inspector, PRIDE Converter 2 and the ProteomeXchange submission tool. Finally, we will give a brief update on the resources under development ‘PRIDE Cluster’ and ‘PRIDE Proteomes’, which provide a complementary view and quality-scored information of the peptide and protein identification data available in PRIDE Archive.",poster,cp75
Biology,p1526,d5,66470cf9df2f932f80094a309abcc14bcc1b9373,c75,International Conference on Predictive Models in Software Engineering,2016 update of the PRIDE database and its related tools,"The PRoteomics IDEntifications (PRIDE) database is one of the world-leading data repositories of mass spectrometry (MS)-based proteomics data. Since the beginning of 2014, PRIDE Archive (http://www.ebi.ac.uk/pride/archive/) is the new PRIDE archival system, replacing the original PRIDE database. Here we summarize the developments in PRIDE resources and related tools since the previous update manuscript in the Database Issue in 2013. PRIDE Archive constitutes a complete redevelopment of the original PRIDE, comprising a new storage backend, data submission system and web interface, among other components. PRIDE Archive supports the most-widely used PSI (Proteomics Standards Initiative) data standard formats (mzML and mzIdentML) and implements the data requirements and guidelines of the ProteomeXchange Consortium. The wide adoption of ProteomeXchange within the community has triggered an unprecedented increase in the number of submitted data sets (around 150 data sets per month). We outline some statistics on the current PRIDE Archive data contents. We also report on the status of the PRIDE related stand-alone tools: PRIDE Inspector, PRIDE Converter 2 and the ProteomeXchange submission tool. Finally, we will give a brief update on the resources under development ‘PRIDE Cluster’ and ‘PRIDE Proteomes’, which provide a complementary view and quality-scored information of the peptide and protein identification data available in PRIDE Archive.",poster,cp75
Medicine,p1527,d1,bbe6e5fcc96e685db714d6aa11ffe6f49567c585,j55,Nature Human Behaviour,A global database of COVID-19 vaccinations,Abstract,fullPaper,jv55
Medicine,p1528,d1,e6ce8255f48e3736f0a5fa0d85fb43c700d4f743,j350,Acta Crystallographica Section B Structural Science,The Cambridge Structural Database: a quarter of a million crystal structures and rising.,"The Cambridge Structural Database (CSD) now contains data for more than a quarter of a million small-molecule crystal structures. The information content of the CSD, together with methods for data acquisition, processing and validation, are summarized, with particular emphasis on the chemical information added by CSD editors. Nearly 80% of new structural data arrives electronically, mostly in CIF format, and the CCDC acts as the official crystal structure data depository for 51 major journals. The CCDC now maintains both a CIF archive (more than 73,000 CIFs dating from 1996), as well as the distributed binary CSD archive; the availability of data in both archives is discussed. A statistical survey of the CSD is also presented and projections concerning future accession rates indicate that the CSD will contain at least 500,000 crystal structures by the year 2010.",fullPaper,jv350
Materials Science,p1528,d7,e6ce8255f48e3736f0a5fa0d85fb43c700d4f743,j350,Acta Crystallographica Section B Structural Science,The Cambridge Structural Database: a quarter of a million crystal structures and rising.,"The Cambridge Structural Database (CSD) now contains data for more than a quarter of a million small-molecule crystal structures. The information content of the CSD, together with methods for data acquisition, processing and validation, are summarized, with particular emphasis on the chemical information added by CSD editors. Nearly 80% of new structural data arrives electronically, mostly in CIF format, and the CCDC acts as the official crystal structure data depository for 51 major journals. The CCDC now maintains both a CIF archive (more than 73,000 CIFs dating from 1996), as well as the distributed binary CSD archive; the availability of data in both archives is discussed. A statistical survey of the CSD is also presented and projections concerning future accession rates indicate that the CSD will contain at least 500,000 crystal structures by the year 2010.",fullPaper,jv350
Chemistry,p1528,d8,e6ce8255f48e3736f0a5fa0d85fb43c700d4f743,j350,Acta Crystallographica Section B Structural Science,The Cambridge Structural Database: a quarter of a million crystal structures and rising.,"The Cambridge Structural Database (CSD) now contains data for more than a quarter of a million small-molecule crystal structures. The information content of the CSD, together with methods for data acquisition, processing and validation, are summarized, with particular emphasis on the chemical information added by CSD editors. Nearly 80% of new structural data arrives electronically, mostly in CIF format, and the CCDC acts as the official crystal structure data depository for 51 major journals. The CCDC now maintains both a CIF archive (more than 73,000 CIFs dating from 1996), as well as the distributed binary CSD archive; the availability of data in both archives is discussed. A statistical survey of the CSD is also presented and projections concerning future accession rates indicate that the CSD will contain at least 500,000 crystal structures by the year 2010.",fullPaper,jv350
Medicine,p1530,d1,76eb8e5688ee2951e5f04fb14956abf93a890149,c19,International Conference on Conceptual Structures,The Carbohydrate-Active EnZymes database (CAZy): an expert resource for Glycogenomics,"The Carbohydrate-Active Enzyme (CAZy) database is a knowledge-based resource specialized in the enzymes that build and breakdown complex carbohydrates and glycoconjugates. As of September 2008, the database describes the present knowledge on 113 glycoside hydrolase, 91 glycosyltransferase, 19 polysaccharide lyase, 15 carbohydrate esterase and 52 carbohydrate-binding module families. These families are created based on experimentally characterized proteins and are populated by sequences from public databases with significant similarity. Protein biochemical information is continuously curated based on the available literature and structural information. Over 6400 proteins have assigned EC numbers and 700 proteins have a PDB structure. The classification (i) reflects the structural features of these enzymes better than their sole substrate specificity, (ii) helps to reveal the evolutionary relationships between these enzymes and (iii) provides a convenient framework to understand mechanistic properties. This resource has been available for over 10 years to the scientific community, contributing to information dissemination and providing a transversal nomenclature to glycobiologists. More recently, this resource has been used to improve the quality of functional predictions of a number genome projects by providing expert annotation. The CAZy resource resides at URL: http://www.cazy.org/.",poster,cp19
Computer Science,p1530,d3,76eb8e5688ee2951e5f04fb14956abf93a890149,c19,International Conference on Conceptual Structures,The Carbohydrate-Active EnZymes database (CAZy): an expert resource for Glycogenomics,"The Carbohydrate-Active Enzyme (CAZy) database is a knowledge-based resource specialized in the enzymes that build and breakdown complex carbohydrates and glycoconjugates. As of September 2008, the database describes the present knowledge on 113 glycoside hydrolase, 91 glycosyltransferase, 19 polysaccharide lyase, 15 carbohydrate esterase and 52 carbohydrate-binding module families. These families are created based on experimentally characterized proteins and are populated by sequences from public databases with significant similarity. Protein biochemical information is continuously curated based on the available literature and structural information. Over 6400 proteins have assigned EC numbers and 700 proteins have a PDB structure. The classification (i) reflects the structural features of these enzymes better than their sole substrate specificity, (ii) helps to reveal the evolutionary relationships between these enzymes and (iii) provides a convenient framework to understand mechanistic properties. This resource has been available for over 10 years to the scientific community, contributing to information dissemination and providing a transversal nomenclature to glycobiologists. More recently, this resource has been used to improve the quality of functional predictions of a number genome projects by providing expert annotation. The CAZy resource resides at URL: http://www.cazy.org/.",poster,cp19
Biology,p1530,d5,76eb8e5688ee2951e5f04fb14956abf93a890149,c19,International Conference on Conceptual Structures,The Carbohydrate-Active EnZymes database (CAZy): an expert resource for Glycogenomics,"The Carbohydrate-Active Enzyme (CAZy) database is a knowledge-based resource specialized in the enzymes that build and breakdown complex carbohydrates and glycoconjugates. As of September 2008, the database describes the present knowledge on 113 glycoside hydrolase, 91 glycosyltransferase, 19 polysaccharide lyase, 15 carbohydrate esterase and 52 carbohydrate-binding module families. These families are created based on experimentally characterized proteins and are populated by sequences from public databases with significant similarity. Protein biochemical information is continuously curated based on the available literature and structural information. Over 6400 proteins have assigned EC numbers and 700 proteins have a PDB structure. The classification (i) reflects the structural features of these enzymes better than their sole substrate specificity, (ii) helps to reveal the evolutionary relationships between these enzymes and (iii) provides a convenient framework to understand mechanistic properties. This resource has been available for over 10 years to the scientific community, contributing to information dissemination and providing a transversal nomenclature to glycobiologists. More recently, this resource has been used to improve the quality of functional predictions of a number genome projects by providing expert annotation. The CAZy resource resides at URL: http://www.cazy.org/.",poster,cp19
Medicine,p1531,d1,7b1d8dfb9e6260685d9fbb8c41bfc0a35710fe41,c7,International Symposium on Intelligent Data Analysis,CARD 2020: antibiotic resistome surveillance with the comprehensive antibiotic resistance database,"Abstract The Comprehensive Antibiotic Resistance Database (CARD; https://card.mcmaster.ca) is a curated resource providing reference DNA and protein sequences, detection models and bioinformatics tools on the molecular basis of bacterial antimicrobial resistance (AMR). CARD focuses on providing high-quality reference data and molecular sequences within a controlled vocabulary, the Antibiotic Resistance Ontology (ARO), designed by the CARD biocuration team to integrate with software development efforts for resistome analysis and prediction, such as CARD’s Resistance Gene Identifier (RGI) software. Since 2017, CARD has expanded through extensive curation of reference sequences, revision of the ontological structure, curation of over 500 new AMR detection models, development of a new classification paradigm and expansion of analytical tools. Most notably, a new Resistomes & Variants module provides analysis and statistical summary of in silico predicted resistance variants from 82 pathogens and over 100 000 genomes. By adding these resistance variants to CARD, we are able to summarize predicted resistance using the information included in CARD, identify trends in AMR mobility and determine previously undescribed and novel resistance variants. Here, we describe updates and recent expansions to CARD and its biocuration process, including new resources for community biocuration of AMR molecular reference data.",poster,cp7
Computer Science,p1531,d3,7b1d8dfb9e6260685d9fbb8c41bfc0a35710fe41,c7,International Symposium on Intelligent Data Analysis,CARD 2020: antibiotic resistome surveillance with the comprehensive antibiotic resistance database,"Abstract The Comprehensive Antibiotic Resistance Database (CARD; https://card.mcmaster.ca) is a curated resource providing reference DNA and protein sequences, detection models and bioinformatics tools on the molecular basis of bacterial antimicrobial resistance (AMR). CARD focuses on providing high-quality reference data and molecular sequences within a controlled vocabulary, the Antibiotic Resistance Ontology (ARO), designed by the CARD biocuration team to integrate with software development efforts for resistome analysis and prediction, such as CARD’s Resistance Gene Identifier (RGI) software. Since 2017, CARD has expanded through extensive curation of reference sequences, revision of the ontological structure, curation of over 500 new AMR detection models, development of a new classification paradigm and expansion of analytical tools. Most notably, a new Resistomes & Variants module provides analysis and statistical summary of in silico predicted resistance variants from 82 pathogens and over 100 000 genomes. By adding these resistance variants to CARD, we are able to summarize predicted resistance using the information included in CARD, identify trends in AMR mobility and determine previously undescribed and novel resistance variants. Here, we describe updates and recent expansions to CARD and its biocuration process, including new resources for community biocuration of AMR molecular reference data.",poster,cp7
Biology,p1531,d5,7b1d8dfb9e6260685d9fbb8c41bfc0a35710fe41,c7,International Symposium on Intelligent Data Analysis,CARD 2020: antibiotic resistome surveillance with the comprehensive antibiotic resistance database,"Abstract The Comprehensive Antibiotic Resistance Database (CARD; https://card.mcmaster.ca) is a curated resource providing reference DNA and protein sequences, detection models and bioinformatics tools on the molecular basis of bacterial antimicrobial resistance (AMR). CARD focuses on providing high-quality reference data and molecular sequences within a controlled vocabulary, the Antibiotic Resistance Ontology (ARO), designed by the CARD biocuration team to integrate with software development efforts for resistome analysis and prediction, such as CARD’s Resistance Gene Identifier (RGI) software. Since 2017, CARD has expanded through extensive curation of reference sequences, revision of the ontological structure, curation of over 500 new AMR detection models, development of a new classification paradigm and expansion of analytical tools. Most notably, a new Resistomes & Variants module provides analysis and statistical summary of in silico predicted resistance variants from 82 pathogens and over 100 000 genomes. By adding these resistance variants to CARD, we are able to summarize predicted resistance using the information included in CARD, identify trends in AMR mobility and determine previously undescribed and novel resistance variants. Here, we describe updates and recent expansions to CARD and its biocuration process, including new resources for community biocuration of AMR molecular reference data.",poster,cp7
Medicine,p1533,d1,317325439a0ce543d7629848a35adea04b6e7d12,c62,International Conference on Advanced Data and Information Engineering,The InterPro protein families and domains database: 20 years on,"Abstract The InterPro database (https://www.ebi.ac.uk/interpro/) provides an integrative classification of protein sequences into families, and identifies functionally important domains and conserved sites. InterProScan is the underlying software that allows protein and nucleic acid sequences to be searched against InterPro's signatures. Signatures are predictive models which describe protein families, domains or sites, and are provided by multiple databases. InterPro combines signatures representing equivalent families, domains or sites, and provides additional information such as descriptions, literature references and Gene Ontology (GO) terms, to produce a comprehensive resource for protein classification. Founded in 1999, InterPro has become one of the most widely used resources for protein family annotation. Here, we report the status of InterPro (version 81.0) in its 20th year of operation, and its associated software, including updates to database content, the release of a new website and REST API, and performance improvements in InterProScan.",poster,cp62
Computer Science,p1533,d3,317325439a0ce543d7629848a35adea04b6e7d12,c62,International Conference on Advanced Data and Information Engineering,The InterPro protein families and domains database: 20 years on,"Abstract The InterPro database (https://www.ebi.ac.uk/interpro/) provides an integrative classification of protein sequences into families, and identifies functionally important domains and conserved sites. InterProScan is the underlying software that allows protein and nucleic acid sequences to be searched against InterPro's signatures. Signatures are predictive models which describe protein families, domains or sites, and are provided by multiple databases. InterPro combines signatures representing equivalent families, domains or sites, and provides additional information such as descriptions, literature references and Gene Ontology (GO) terms, to produce a comprehensive resource for protein classification. Founded in 1999, InterPro has become one of the most widely used resources for protein family annotation. Here, we report the status of InterPro (version 81.0) in its 20th year of operation, and its associated software, including updates to database content, the release of a new website and REST API, and performance improvements in InterProScan.",poster,cp62
Biology,p1533,d5,317325439a0ce543d7629848a35adea04b6e7d12,c62,International Conference on Advanced Data and Information Engineering,The InterPro protein families and domains database: 20 years on,"Abstract The InterPro database (https://www.ebi.ac.uk/interpro/) provides an integrative classification of protein sequences into families, and identifies functionally important domains and conserved sites. InterProScan is the underlying software that allows protein and nucleic acid sequences to be searched against InterPro's signatures. Signatures are predictive models which describe protein families, domains or sites, and are provided by multiple databases. InterPro combines signatures representing equivalent families, domains or sites, and provides additional information such as descriptions, literature references and Gene Ontology (GO) terms, to produce a comprehensive resource for protein classification. Founded in 1999, InterPro has become one of the most widely used resources for protein family annotation. Here, we report the status of InterPro (version 81.0) in its 20th year of operation, and its associated software, including updates to database content, the release of a new website and REST API, and performance improvements in InterProScan.",poster,cp62
Medicine,p1534,d1,f80a6ab4b0cfae0d00747f0f41f3e643f22f33ee,c53,International Conference on Learning Representations,Molecular signatures database (MSigDB) 3.0,"MOTIVATION
Well-annotated gene sets representing the universe of the biological processes are critical for meaningful and insightful interpretation of large-scale genomic data. The Molecular Signatures Database (MSigDB) is one of the most widely used repositories of such sets.


RESULTS
We report the availability of a new version of the database, MSigDB 3.0, with over 6700 gene sets, a complete revision of the collection of canonical pathways and experimental signatures from publications, enhanced annotations and upgrades to the web site.


AVAILABILITY AND IMPLEMENTATION
MSigDB is freely available for non-commercial use at http://www.broadinstitute.org/msigdb.",poster,cp53
Computer Science,p1534,d3,f80a6ab4b0cfae0d00747f0f41f3e643f22f33ee,c53,International Conference on Learning Representations,Molecular signatures database (MSigDB) 3.0,"MOTIVATION
Well-annotated gene sets representing the universe of the biological processes are critical for meaningful and insightful interpretation of large-scale genomic data. The Molecular Signatures Database (MSigDB) is one of the most widely used repositories of such sets.


RESULTS
We report the availability of a new version of the database, MSigDB 3.0, with over 6700 gene sets, a complete revision of the collection of canonical pathways and experimental signatures from publications, enhanced annotations and upgrades to the web site.


AVAILABILITY AND IMPLEMENTATION
MSigDB is freely available for non-commercial use at http://www.broadinstitute.org/msigdb.",poster,cp53
Medicine,p1538,d1,c07eca9862e0144aa4c0c29c7978caa6eff60e2f,c0,International Conference on Machine Learning,The Ribosomal Database Project: improved alignments and new tools for rRNA analysis,"The Ribosomal Database Project (RDP) provides researchers with quality-controlled bacterial and archaeal small subunit rRNA alignments and analysis tools. An improved alignment strategy uses the Infernal secondary structure aware aligner to provide a more consistent higher quality alignment and faster processing of user sequences. Substantial new analysis features include a new Pyrosequencing Pipeline that provides tools to support analysis of ultra high-throughput rRNA sequencing data. This pipeline offers a collection of tools that automate the data processing and simplify the computationally intensive analysis of large sequencing libraries. In addition, a new Taxomatic visualization tool allows rapid visualization of taxonomic inconsistencies and suggests corrections, and a new class Assignment Generator provides instructors with a lesson plan and individualized teaching materials. Details about RDP data and analytical functions can be found at http://rdp.cme.msu.edu/.",poster,cp0
Computer Science,p1538,d3,c07eca9862e0144aa4c0c29c7978caa6eff60e2f,c0,International Conference on Machine Learning,The Ribosomal Database Project: improved alignments and new tools for rRNA analysis,"The Ribosomal Database Project (RDP) provides researchers with quality-controlled bacterial and archaeal small subunit rRNA alignments and analysis tools. An improved alignment strategy uses the Infernal secondary structure aware aligner to provide a more consistent higher quality alignment and faster processing of user sequences. Substantial new analysis features include a new Pyrosequencing Pipeline that provides tools to support analysis of ultra high-throughput rRNA sequencing data. This pipeline offers a collection of tools that automate the data processing and simplify the computationally intensive analysis of large sequencing libraries. In addition, a new Taxomatic visualization tool allows rapid visualization of taxonomic inconsistencies and suggests corrections, and a new class Assignment Generator provides instructors with a lesson plan and individualized teaching materials. Details about RDP data and analytical functions can be found at http://rdp.cme.msu.edu/.",poster,cp0
Biology,p1538,d5,c07eca9862e0144aa4c0c29c7978caa6eff60e2f,c0,International Conference on Machine Learning,The Ribosomal Database Project: improved alignments and new tools for rRNA analysis,"The Ribosomal Database Project (RDP) provides researchers with quality-controlled bacterial and archaeal small subunit rRNA alignments and analysis tools. An improved alignment strategy uses the Infernal secondary structure aware aligner to provide a more consistent higher quality alignment and faster processing of user sequences. Substantial new analysis features include a new Pyrosequencing Pipeline that provides tools to support analysis of ultra high-throughput rRNA sequencing data. This pipeline offers a collection of tools that automate the data processing and simplify the computationally intensive analysis of large sequencing libraries. In addition, a new Taxomatic visualization tool allows rapid visualization of taxonomic inconsistencies and suggests corrections, and a new class Assignment Generator provides instructors with a lesson plan and individualized teaching materials. Details about RDP data and analytical functions can be found at http://rdp.cme.msu.edu/.",poster,cp0
Medicine,p1539,d1,8c1a1e761b715b23668b4f850e2bcc958fa21ad2,j351,Analytical Chemistry,Empirical statistical model to estimate the accuracy of peptide identifications made by MS/MS and database search.,"We present a statistical model to estimate the accuracy of peptide assignments to tandem mass (MS/MS) spectra made by database search applications such as SEQUEST. Employing the expectation maximization algorithm, the analysis learns to distinguish correct from incorrect database search results, computing probabilities that peptide assignments to spectra are correct based upon database search scores and the number of tryptic termini of peptides. Using SEQUEST search results for spectra generated from a sample of known protein components, we demonstrate that the computed probabilities are accurate and have high power to discriminate between correctly and incorrectly assigned peptides. This analysis makes it possible to filter large volumes of MS/MS database search results with predictable false identification error rates and can serve as a common standard by which the results of different research groups are compared.",fullPaper,jv351
Computer Science,p1539,d3,8c1a1e761b715b23668b4f850e2bcc958fa21ad2,j351,Analytical Chemistry,Empirical statistical model to estimate the accuracy of peptide identifications made by MS/MS and database search.,"We present a statistical model to estimate the accuracy of peptide assignments to tandem mass (MS/MS) spectra made by database search applications such as SEQUEST. Employing the expectation maximization algorithm, the analysis learns to distinguish correct from incorrect database search results, computing probabilities that peptide assignments to spectra are correct based upon database search scores and the number of tryptic termini of peptides. Using SEQUEST search results for spectra generated from a sample of known protein components, we demonstrate that the computed probabilities are accurate and have high power to discriminate between correctly and incorrectly assigned peptides. This analysis makes it possible to filter large volumes of MS/MS database search results with predictable false identification error rates and can serve as a common standard by which the results of different research groups are compared.",fullPaper,jv351
Chemistry,p1539,d8,8c1a1e761b715b23668b4f850e2bcc958fa21ad2,j351,Analytical Chemistry,Empirical statistical model to estimate the accuracy of peptide identifications made by MS/MS and database search.,"We present a statistical model to estimate the accuracy of peptide assignments to tandem mass (MS/MS) spectra made by database search applications such as SEQUEST. Employing the expectation maximization algorithm, the analysis learns to distinguish correct from incorrect database search results, computing probabilities that peptide assignments to spectra are correct based upon database search scores and the number of tryptic termini of peptides. Using SEQUEST search results for spectra generated from a sample of known protein components, we demonstrate that the computed probabilities are accurate and have high power to discriminate between correctly and incorrectly assigned peptides. This analysis makes it possible to filter large volumes of MS/MS database search results with predictable false identification error rates and can serve as a common standard by which the results of different research groups are compared.",fullPaper,jv351
Medicine,p1542,d1,80777d42513103bede188b2eebbdce7fb6f91390,c86,International Conference on Big Data and Education,HMDB 4.0: the human metabolome database for 2018,"Abstract The Human Metabolome Database or HMDB (www.hmdb.ca) is a web-enabled metabolomic database containing comprehensive information about human metabolites along with their biological roles, physiological concentrations, disease associations, chemical reactions, metabolic pathways, and reference spectra. First described in 2007, the HMDB is now considered the standard metabolomic resource for human metabolic studies. Over the past decade the HMDB has continued to grow and evolve in response to emerging needs for metabolomics researchers and continuing changes in web standards. This year's update, HMDB 4.0, represents the most significant upgrade to the database in its history. For instance, the number of fully annotated metabolites has increased by nearly threefold, the number of experimental spectra has grown by almost fourfold and the number of illustrated metabolic pathways has grown by a factor of almost 60. Significant improvements have also been made to the HMDB’s chemical taxonomy, chemical ontology, spectral viewing, and spectral/text searching tools. A great deal of brand new data has also been added to HMDB 4.0. This includes large quantities of predicted MS/MS and GC–MS reference spectral data as well as predicted (physiologically feasible) metabolite structures to facilitate novel metabolite identification. Additional information on metabolite-SNP interactions and the influence of drugs on metabolite levels (pharmacometabolomics) has also been added. Many other important improvements in the content, the interface, and the performance of the HMDB website have been made and these should greatly enhance its ease of use and its potential applications in nutrition, biochemistry, clinical chemistry, clinical genetics, medicine, and metabolomics science.",poster,cp86
Computer Science,p1542,d3,80777d42513103bede188b2eebbdce7fb6f91390,c86,International Conference on Big Data and Education,HMDB 4.0: the human metabolome database for 2018,"Abstract The Human Metabolome Database or HMDB (www.hmdb.ca) is a web-enabled metabolomic database containing comprehensive information about human metabolites along with their biological roles, physiological concentrations, disease associations, chemical reactions, metabolic pathways, and reference spectra. First described in 2007, the HMDB is now considered the standard metabolomic resource for human metabolic studies. Over the past decade the HMDB has continued to grow and evolve in response to emerging needs for metabolomics researchers and continuing changes in web standards. This year's update, HMDB 4.0, represents the most significant upgrade to the database in its history. For instance, the number of fully annotated metabolites has increased by nearly threefold, the number of experimental spectra has grown by almost fourfold and the number of illustrated metabolic pathways has grown by a factor of almost 60. Significant improvements have also been made to the HMDB’s chemical taxonomy, chemical ontology, spectral viewing, and spectral/text searching tools. A great deal of brand new data has also been added to HMDB 4.0. This includes large quantities of predicted MS/MS and GC–MS reference spectral data as well as predicted (physiologically feasible) metabolite structures to facilitate novel metabolite identification. Additional information on metabolite-SNP interactions and the influence of drugs on metabolite levels (pharmacometabolomics) has also been added. Many other important improvements in the content, the interface, and the performance of the HMDB website have been made and these should greatly enhance its ease of use and its potential applications in nutrition, biochemistry, clinical chemistry, clinical genetics, medicine, and metabolomics science.",poster,cp86
Biology,p1542,d5,80777d42513103bede188b2eebbdce7fb6f91390,c86,International Conference on Big Data and Education,HMDB 4.0: the human metabolome database for 2018,"Abstract The Human Metabolome Database or HMDB (www.hmdb.ca) is a web-enabled metabolomic database containing comprehensive information about human metabolites along with their biological roles, physiological concentrations, disease associations, chemical reactions, metabolic pathways, and reference spectra. First described in 2007, the HMDB is now considered the standard metabolomic resource for human metabolic studies. Over the past decade the HMDB has continued to grow and evolve in response to emerging needs for metabolomics researchers and continuing changes in web standards. This year's update, HMDB 4.0, represents the most significant upgrade to the database in its history. For instance, the number of fully annotated metabolites has increased by nearly threefold, the number of experimental spectra has grown by almost fourfold and the number of illustrated metabolic pathways has grown by a factor of almost 60. Significant improvements have also been made to the HMDB’s chemical taxonomy, chemical ontology, spectral viewing, and spectral/text searching tools. A great deal of brand new data has also been added to HMDB 4.0. This includes large quantities of predicted MS/MS and GC–MS reference spectral data as well as predicted (physiologically feasible) metabolite structures to facilitate novel metabolite identification. Additional information on metabolite-SNP interactions and the influence of drugs on metabolite levels (pharmacometabolomics) has also been added. Many other important improvements in the content, the interface, and the performance of the HMDB website have been made and these should greatly enhance its ease of use and its potential applications in nutrition, biochemistry, clinical chemistry, clinical genetics, medicine, and metabolomics science.",poster,cp86
Medicine,p1543,d1,5ef2cf7b7aa6f7e44488d5db5409ef7f76b9ef9a,j352,Journal of Molecular Biology,SCOP: a structural classification of proteins database for the investigation of sequences and structures.,Abstract,fullPaper,jv352
Computer Science,p1543,d3,5ef2cf7b7aa6f7e44488d5db5409ef7f76b9ef9a,j352,Journal of Molecular Biology,SCOP: a structural classification of proteins database for the investigation of sequences and structures.,Abstract,fullPaper,jv352
Medicine,p1544,d1,ceee6447b291f8052a28c9eb00ca360d6f39f9b1,c37,International Workshop on the Semantic Web,"The STRING database in 2011: functional interaction networks of proteins, globally integrated and scored","An essential prerequisite for any systems-level understanding of cellular functions is to correctly uncover and annotate all functional interactions among proteins in the cell. Toward this goal, remarkable progress has been made in recent years, both in terms of experimental measurements and computational prediction techniques. However, public efforts to collect and present protein interaction information have struggled to keep up with the pace of interaction discovery, partly because protein–protein interaction information can be error-prone and require considerable effort to annotate. Here, we present an update on the online database resource Search Tool for the Retrieval of Interacting Genes (STRING); it provides uniquely comprehensive coverage and ease of access to both experimental as well as predicted interaction information. Interactions in STRING are provided with a confidence score, and accessory information such as protein domains and 3D structures is made available, all within a stable and consistent identifier space. New features in STRING include an interactive network viewer that can cluster networks on demand, updated on-screen previews of structural information including homology models, extensive data updates and strongly improved connectivity and integration with third-party resources. Version 9.0 of STRING covers more than 1100 completely sequenced organisms; the resource can be reached at http://string-db.org.",poster,cp37
Computer Science,p1544,d3,ceee6447b291f8052a28c9eb00ca360d6f39f9b1,c37,International Workshop on the Semantic Web,"The STRING database in 2011: functional interaction networks of proteins, globally integrated and scored","An essential prerequisite for any systems-level understanding of cellular functions is to correctly uncover and annotate all functional interactions among proteins in the cell. Toward this goal, remarkable progress has been made in recent years, both in terms of experimental measurements and computational prediction techniques. However, public efforts to collect and present protein interaction information have struggled to keep up with the pace of interaction discovery, partly because protein–protein interaction information can be error-prone and require considerable effort to annotate. Here, we present an update on the online database resource Search Tool for the Retrieval of Interacting Genes (STRING); it provides uniquely comprehensive coverage and ease of access to both experimental as well as predicted interaction information. Interactions in STRING are provided with a confidence score, and accessory information such as protein domains and 3D structures is made available, all within a stable and consistent identifier space. New features in STRING include an interactive network viewer that can cluster networks on demand, updated on-screen previews of structural information including homology models, extensive data updates and strongly improved connectivity and integration with third-party resources. Version 9.0 of STRING covers more than 1100 completely sequenced organisms; the resource can be reached at http://string-db.org.",poster,cp37
Biology,p1544,d5,ceee6447b291f8052a28c9eb00ca360d6f39f9b1,c37,International Workshop on the Semantic Web,"The STRING database in 2011: functional interaction networks of proteins, globally integrated and scored","An essential prerequisite for any systems-level understanding of cellular functions is to correctly uncover and annotate all functional interactions among proteins in the cell. Toward this goal, remarkable progress has been made in recent years, both in terms of experimental measurements and computational prediction techniques. However, public efforts to collect and present protein interaction information have struggled to keep up with the pace of interaction discovery, partly because protein–protein interaction information can be error-prone and require considerable effort to annotate. Here, we present an update on the online database resource Search Tool for the Retrieval of Interacting Genes (STRING); it provides uniquely comprehensive coverage and ease of access to both experimental as well as predicted interaction information. Interactions in STRING are provided with a confidence score, and accessory information such as protein domains and 3D structures is made available, all within a stable and consistent identifier space. New features in STRING include an interactive network viewer that can cluster networks on demand, updated on-screen previews of structural information including homology models, extensive data updates and strongly improved connectivity and integration with third-party resources. Version 9.0 of STRING covers more than 1100 completely sequenced organisms; the resource can be reached at http://string-db.org.",poster,cp37
Medicine,p1545,d1,cdad2f8ca559f425ab7fa402535354a86b0a370a,c64,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",CDD/SPARCLE: the conserved domain database in 2020,"As NLM's Conserved Domain Database (CDD) enters its 20th year of operations as a publicly available resource, CDD curation staff continues to develop hierarchical classifications of widely distributed protein domain families, and to record conserved sites associated with molecular function, so that they can be mapped onto user queries in support of hypothesis-driven biomolecular research. CDD offers both an archive of pre-computed domain annotations as well as live search services for both single protein or nucleotide queries and larger sets of protein query sequences. CDD staff has continued to characterize protein families via conserved domain architectures and has built up a significant corpus of curated domain architectures in support of naming bacterial proteins in RefSeq. These architecture definitions are available via SPARCLE, the Subfamily Protein Architecture Labeling Engine. CDD can be accessed at https://www.ncbi.nlm.nih.gov/Structure/cdd/cdd.shtml.",poster,cp64
Computer Science,p1545,d3,cdad2f8ca559f425ab7fa402535354a86b0a370a,c64,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",CDD/SPARCLE: the conserved domain database in 2020,"As NLM's Conserved Domain Database (CDD) enters its 20th year of operations as a publicly available resource, CDD curation staff continues to develop hierarchical classifications of widely distributed protein domain families, and to record conserved sites associated with molecular function, so that they can be mapped onto user queries in support of hypothesis-driven biomolecular research. CDD offers both an archive of pre-computed domain annotations as well as live search services for both single protein or nucleotide queries and larger sets of protein query sequences. CDD staff has continued to characterize protein families via conserved domain architectures and has built up a significant corpus of curated domain architectures in support of naming bacterial proteins in RefSeq. These architecture definitions are available via SPARCLE, the Subfamily Protein Architecture Labeling Engine. CDD can be accessed at https://www.ncbi.nlm.nih.gov/Structure/cdd/cdd.shtml.",poster,cp64
Biology,p1545,d5,cdad2f8ca559f425ab7fa402535354a86b0a370a,c64,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",CDD/SPARCLE: the conserved domain database in 2020,"As NLM's Conserved Domain Database (CDD) enters its 20th year of operations as a publicly available resource, CDD curation staff continues to develop hierarchical classifications of widely distributed protein domain families, and to record conserved sites associated with molecular function, so that they can be mapped onto user queries in support of hypothesis-driven biomolecular research. CDD offers both an archive of pre-computed domain annotations as well as live search services for both single protein or nucleotide queries and larger sets of protein query sequences. CDD staff has continued to characterize protein families via conserved domain architectures and has built up a significant corpus of curated domain architectures in support of naming bacterial proteins in RefSeq. These architecture definitions are available via SPARCLE, the Subfamily Protein Architecture Labeling Engine. CDD can be accessed at https://www.ncbi.nlm.nih.gov/Structure/cdd/cdd.shtml.",poster,cp64
Medicine,p1548,d1,6dd9508b8311852afec88bce55283551da5aa7b7,c99,Symposium on the Theory of Computing,The IPD-IMGT/HLA Database,"Abstract It is 24 years since the IPD-IMGT/HLA Database, http://www.ebi.ac.uk/ipd/imgt/hla/, was first released, providing the HLA community with a searchable repository of highly curated HLA sequences. The database now contains over 35 000 alleles of the human Major Histocompatibility Complex (MHC) named by the WHO Nomenclature Committee for Factors of the HLA System. This complex contains the most polymorphic genes in the human genome and is now considered hyperpolymorphic. The IPD-IMGT/HLA Database provides a stable and user-friendly repository for this information. Uptake of Next Generation Sequencing technology in recent years has driven an increase in the number of alleles and the length of sequences submitted. As the size of the database has grown the traditional methods of accessing and presenting this data have been challenged, in response, we have developed a suite of tools providing an enhanced user experience to our traditional web-based users while creating new programmatic access for our bioinformatics user base. This suite of tools is powered by the IPD-API, an Application Programming Interface (API), providing scalable and flexible access to the database. The IPD-API provides a stable platform for our future development allowing us to meet the future challenges of the HLA field and needs of the community.",poster,cp99
Computer Science,p1548,d3,6dd9508b8311852afec88bce55283551da5aa7b7,c99,Symposium on the Theory of Computing,The IPD-IMGT/HLA Database,"Abstract It is 24 years since the IPD-IMGT/HLA Database, http://www.ebi.ac.uk/ipd/imgt/hla/, was first released, providing the HLA community with a searchable repository of highly curated HLA sequences. The database now contains over 35 000 alleles of the human Major Histocompatibility Complex (MHC) named by the WHO Nomenclature Committee for Factors of the HLA System. This complex contains the most polymorphic genes in the human genome and is now considered hyperpolymorphic. The IPD-IMGT/HLA Database provides a stable and user-friendly repository for this information. Uptake of Next Generation Sequencing technology in recent years has driven an increase in the number of alleles and the length of sequences submitted. As the size of the database has grown the traditional methods of accessing and presenting this data have been challenged, in response, we have developed a suite of tools providing an enhanced user experience to our traditional web-based users while creating new programmatic access for our bioinformatics user base. This suite of tools is powered by the IPD-API, an Application Programming Interface (API), providing scalable and flexible access to the database. The IPD-API provides a stable platform for our future development allowing us to meet the future challenges of the HLA field and needs of the community.",poster,cp99
Medicine,p1549,d1,11f647b95a7c9a94c346cd8dc53987105cb0f7c1,c89,Conference on Uncertainty in Artificial Intelligence,dbSNP: the NCBI database of genetic variation,"In response to a need for a general catalog of genome variation to address the large-scale sampling designs required by association studies, gene mapping and evolutionary biology, the National Center for Biotechnology Information (NCBI) has established the dbSNP database [S.T.Sherry, M.Ward and K. Sirotkin (1999) Genome Res., 9, 677-679]. Submissions to dbSNP will be integrated with other sources of information at NCBI such as GenBank, PubMed, LocusLink and the Human Genome Project data. The complete contents of dbSNP are available to the public at website: http://www.ncbi.nlm.nih.gov/SNP. The complete contents of dbSNP can also be downloaded in multiple formats via anonymous FTP at ftp://ncbi.nlm.nih.gov/snp/.",poster,cp89
Computer Science,p1549,d3,11f647b95a7c9a94c346cd8dc53987105cb0f7c1,c89,Conference on Uncertainty in Artificial Intelligence,dbSNP: the NCBI database of genetic variation,"In response to a need for a general catalog of genome variation to address the large-scale sampling designs required by association studies, gene mapping and evolutionary biology, the National Center for Biotechnology Information (NCBI) has established the dbSNP database [S.T.Sherry, M.Ward and K. Sirotkin (1999) Genome Res., 9, 677-679]. Submissions to dbSNP will be integrated with other sources of information at NCBI such as GenBank, PubMed, LocusLink and the Human Genome Project data. The complete contents of dbSNP are available to the public at website: http://www.ncbi.nlm.nih.gov/SNP. The complete contents of dbSNP can also be downloaded in multiple formats via anonymous FTP at ftp://ncbi.nlm.nih.gov/snp/.",poster,cp89
Biology,p1549,d5,11f647b95a7c9a94c346cd8dc53987105cb0f7c1,c89,Conference on Uncertainty in Artificial Intelligence,dbSNP: the NCBI database of genetic variation,"In response to a need for a general catalog of genome variation to address the large-scale sampling designs required by association studies, gene mapping and evolutionary biology, the National Center for Biotechnology Information (NCBI) has established the dbSNP database [S.T.Sherry, M.Ward and K. Sirotkin (1999) Genome Res., 9, 677-679]. Submissions to dbSNP will be integrated with other sources of information at NCBI such as GenBank, PubMed, LocusLink and the Human Genome Project data. The complete contents of dbSNP are available to the public at website: http://www.ncbi.nlm.nih.gov/SNP. The complete contents of dbSNP can also be downloaded in multiple formats via anonymous FTP at ftp://ncbi.nlm.nih.gov/snp/.",poster,cp89
Medicine,p1550,d1,d7c78b7071ea150346320e5b43a03824263e0fa9,c112,British Machine Vision Conference,Ribosomal Database Project: data and tools for high throughput rRNA analysis,"Ribosomal Database Project (RDP; http://rdp.cme.msu.edu/) provides the research community with aligned and annotated rRNA gene sequence data, along with tools to allow researchers to analyze their own rRNA gene sequences in the RDP framework. RDP data and tools are utilized in fields as diverse as human health, microbial ecology, environmental microbiology, nucleic acid chemistry, taxonomy and phylogenetics. In addition to aligned and annotated collections of bacterial and archaeal small subunit rRNA genes, RDP now includes a collection of fungal large subunit rRNA genes. RDP tools, including Classifier and Aligner, have been updated to work with this new fungal collection. The use of high-throughput sequencing to characterize environmental microbial populations has exploded in the past several years, and as sequence technologies have improved, the sizes of environmental datasets have increased. With release 11, RDP is providing an expanded set of tools to facilitate analysis of high-throughput data, including both single-stranded and paired-end reads. In addition, most tools are now available as open source packages for download and local use by researchers with high-volume needs or who would like to develop custom analysis pipelines.",poster,cp112
Computer Science,p1550,d3,d7c78b7071ea150346320e5b43a03824263e0fa9,c112,British Machine Vision Conference,Ribosomal Database Project: data and tools for high throughput rRNA analysis,"Ribosomal Database Project (RDP; http://rdp.cme.msu.edu/) provides the research community with aligned and annotated rRNA gene sequence data, along with tools to allow researchers to analyze their own rRNA gene sequences in the RDP framework. RDP data and tools are utilized in fields as diverse as human health, microbial ecology, environmental microbiology, nucleic acid chemistry, taxonomy and phylogenetics. In addition to aligned and annotated collections of bacterial and archaeal small subunit rRNA genes, RDP now includes a collection of fungal large subunit rRNA genes. RDP tools, including Classifier and Aligner, have been updated to work with this new fungal collection. The use of high-throughput sequencing to characterize environmental microbial populations has exploded in the past several years, and as sequence technologies have improved, the sizes of environmental datasets have increased. With release 11, RDP is providing an expanded set of tools to facilitate analysis of high-throughput data, including both single-stranded and paired-end reads. In addition, most tools are now available as open source packages for download and local use by researchers with high-volume needs or who would like to develop custom analysis pipelines.",poster,cp112
Biology,p1550,d5,d7c78b7071ea150346320e5b43a03824263e0fa9,c112,British Machine Vision Conference,Ribosomal Database Project: data and tools for high throughput rRNA analysis,"Ribosomal Database Project (RDP; http://rdp.cme.msu.edu/) provides the research community with aligned and annotated rRNA gene sequence data, along with tools to allow researchers to analyze their own rRNA gene sequences in the RDP framework. RDP data and tools are utilized in fields as diverse as human health, microbial ecology, environmental microbiology, nucleic acid chemistry, taxonomy and phylogenetics. In addition to aligned and annotated collections of bacterial and archaeal small subunit rRNA genes, RDP now includes a collection of fungal large subunit rRNA genes. RDP tools, including Classifier and Aligner, have been updated to work with this new fungal collection. The use of high-throughput sequencing to characterize environmental microbial populations has exploded in the past several years, and as sequence technologies have improved, the sizes of environmental datasets have increased. With release 11, RDP is providing an expanded set of tools to facilitate analysis of high-throughput data, including both single-stranded and paired-end reads. In addition, most tools are now available as open source packages for download and local use by researchers with high-volume needs or who would like to develop custom analysis pipelines.",poster,cp112
Medicine,p1551,d1,62f5ffb09a4c9543509c38f005b9c6eb308c6974,c75,International Conference on Predictive Models in Software Engineering,The COG database: a tool for genome-scale analysis of protein functions and evolution,"Rational classification of proteins encoded in sequenced genomes is critical for making the genome sequences maximally useful for functional and evolutionary studies. The database of Clusters of Orthologous Groups of proteins (COGs) is an attempt on a phylogenetic classification of the proteins encoded in 21 complete genomes of bacteria, archaea and eukaryotes (http://www. ncbi.nlm. nih.gov/COG). The COGs were constructed by applying the criterion of consistency of genome-specific best hits to the results of an exhaustive comparison of all protein sequences from these genomes. The database comprises 2091 COGs that include 56-83% of the gene products from each of the complete bacterial and archaeal genomes and approximately 35% of those from the yeast Saccharomyces cerevisiae genome. The COG database is accompanied by the COGNITOR program that is used to fit new proteins into the COGs and can be applied to functional and phylogenetic annotation of newly sequenced genomes.",poster,cp75
Computer Science,p1551,d3,62f5ffb09a4c9543509c38f005b9c6eb308c6974,c75,International Conference on Predictive Models in Software Engineering,The COG database: a tool for genome-scale analysis of protein functions and evolution,"Rational classification of proteins encoded in sequenced genomes is critical for making the genome sequences maximally useful for functional and evolutionary studies. The database of Clusters of Orthologous Groups of proteins (COGs) is an attempt on a phylogenetic classification of the proteins encoded in 21 complete genomes of bacteria, archaea and eukaryotes (http://www. ncbi.nlm. nih.gov/COG). The COGs were constructed by applying the criterion of consistency of genome-specific best hits to the results of an exhaustive comparison of all protein sequences from these genomes. The database comprises 2091 COGs that include 56-83% of the gene products from each of the complete bacterial and archaeal genomes and approximately 35% of those from the yeast Saccharomyces cerevisiae genome. The COG database is accompanied by the COGNITOR program that is used to fit new proteins into the COGs and can be applied to functional and phylogenetic annotation of newly sequenced genomes.",poster,cp75
Biology,p1551,d5,62f5ffb09a4c9543509c38f005b9c6eb308c6974,c75,International Conference on Predictive Models in Software Engineering,The COG database: a tool for genome-scale analysis of protein functions and evolution,"Rational classification of proteins encoded in sequenced genomes is critical for making the genome sequences maximally useful for functional and evolutionary studies. The database of Clusters of Orthologous Groups of proteins (COGs) is an attempt on a phylogenetic classification of the proteins encoded in 21 complete genomes of bacteria, archaea and eukaryotes (http://www. ncbi.nlm. nih.gov/COG). The COGs were constructed by applying the criterion of consistency of genome-specific best hits to the results of an exhaustive comparison of all protein sequences from these genomes. The database comprises 2091 COGs that include 56-83% of the gene products from each of the complete bacterial and archaeal genomes and approximately 35% of those from the yeast Saccharomyces cerevisiae genome. The COG database is accompanied by the COGNITOR program that is used to fit new proteins into the COGs and can be applied to functional and phylogenetic annotation of newly sequenced genomes.",poster,cp75
Medicine,p1553,d1,d364903a626ad70e6ce057209d9b7e004dafd4be,c52,Workshop on Applied Computational Geometry,"PlantCARE, a database of plant cis-acting regulatory elements and a portal to tools for in silico analysis of promoter sequences","PlantCARE is a database of plant cis-acting regulatory elements, enhancers and repressors. Regulatory elements are represented by positional matrices, consensus sequences and individual sites on particular promoter sequences. Links to the EMBL, TRANSFAC and MEDLINE databases are provided when available. Data about the transcription sites are extracted mainly from the literature, supplemented with an increasing number of in silico predicted data. Apart from a general description for specific transcription factor sites, levels of confidence for the experimental evidence, functional information and the position on the promoter are given as well. New features have been implemented to search for plant cis-acting regulatory elements in a query sequence. Furthermore, links are now provided to a new clustering and motif search method to investigate clusters of co-expressed genes. New regulatory elements can be sent automatically and will be added to the database after curation. The PlantCARE relational database is available via the World Wide Web at http://sphinx.rug.ac.be:8080/PlantCARE/.",poster,cp52
Computer Science,p1553,d3,d364903a626ad70e6ce057209d9b7e004dafd4be,c52,Workshop on Applied Computational Geometry,"PlantCARE, a database of plant cis-acting regulatory elements and a portal to tools for in silico analysis of promoter sequences","PlantCARE is a database of plant cis-acting regulatory elements, enhancers and repressors. Regulatory elements are represented by positional matrices, consensus sequences and individual sites on particular promoter sequences. Links to the EMBL, TRANSFAC and MEDLINE databases are provided when available. Data about the transcription sites are extracted mainly from the literature, supplemented with an increasing number of in silico predicted data. Apart from a general description for specific transcription factor sites, levels of confidence for the experimental evidence, functional information and the position on the promoter are given as well. New features have been implemented to search for plant cis-acting regulatory elements in a query sequence. Furthermore, links are now provided to a new clustering and motif search method to investigate clusters of co-expressed genes. New regulatory elements can be sent automatically and will be added to the database after curation. The PlantCARE relational database is available via the World Wide Web at http://sphinx.rug.ac.be:8080/PlantCARE/.",poster,cp52
Biology,p1553,d5,d364903a626ad70e6ce057209d9b7e004dafd4be,c52,Workshop on Applied Computational Geometry,"PlantCARE, a database of plant cis-acting regulatory elements and a portal to tools for in silico analysis of promoter sequences","PlantCARE is a database of plant cis-acting regulatory elements, enhancers and repressors. Regulatory elements are represented by positional matrices, consensus sequences and individual sites on particular promoter sequences. Links to the EMBL, TRANSFAC and MEDLINE databases are provided when available. Data about the transcription sites are extracted mainly from the literature, supplemented with an increasing number of in silico predicted data. Apart from a general description for specific transcription factor sites, levels of confidence for the experimental evidence, functional information and the position on the promoter are given as well. New features have been implemented to search for plant cis-acting regulatory elements in a query sequence. Furthermore, links are now provided to a new clustering and motif search method to investigate clusters of co-expressed genes. New regulatory elements can be sent automatically and will be added to the database after curation. The PlantCARE relational database is available via the World Wide Web at http://sphinx.rug.ac.be:8080/PlantCARE/.",poster,cp52
Medicine,p1554,d1,b7599c8ba88e7c93edbce57df513152e8f5693e7,j82,BMC Bioinformatics,The COG database: an updated version includes eukaryotes,Abstract,fullPaper,jv82
Computer Science,p1554,d3,b7599c8ba88e7c93edbce57df513152e8f5693e7,j82,BMC Bioinformatics,The COG database: an updated version includes eukaryotes,Abstract,fullPaper,jv82
Biology,p1554,d5,b7599c8ba88e7c93edbce57df513152e8f5693e7,j82,BMC Bioinformatics,The COG database: an updated version includes eukaryotes,Abstract,fullPaper,jv82
Medicine,p1555,d1,1f53996347086be3bd3a32da0976ba2db7687988,c91,International Symposium on High-Performance Computer Architecture,miRDB: an online database for prediction of functional microRNA targets,"Abstract MicroRNAs (miRNAs) are small noncoding RNAs that act as master regulators in many biological processes. miRNAs function mainly by downregulating the expression of their gene targets. Thus, accurate prediction of miRNA targets is critical for characterization of miRNA functions. To this end, we have developed an online database, miRDB, for miRNA target prediction and functional annotations. Recently, we have performed major updates for miRDB. Specifically, by employing an improved algorithm for miRNA target prediction, we now present updated transcriptome-wide target prediction data in miRDB, including 3.5 million predicted targets regulated by 7000 miRNAs in five species. Further, we have implemented the new prediction algorithm into a web server, allowing custom target prediction with user-provided sequences. Another new database feature is the prediction of cell-specific miRNA targets. miRDB now hosts the expression profiles of over 1000 cell lines and presents target prediction data that are tailored for specific cell models. At last, a new web query interface has been added to miRDB for prediction of miRNA functions by integrative analysis of target prediction and Gene Ontology data. All data in miRDB are freely accessible at http://mirdb.org.",poster,cp91
Computer Science,p1555,d3,1f53996347086be3bd3a32da0976ba2db7687988,c91,International Symposium on High-Performance Computer Architecture,miRDB: an online database for prediction of functional microRNA targets,"Abstract MicroRNAs (miRNAs) are small noncoding RNAs that act as master regulators in many biological processes. miRNAs function mainly by downregulating the expression of their gene targets. Thus, accurate prediction of miRNA targets is critical for characterization of miRNA functions. To this end, we have developed an online database, miRDB, for miRNA target prediction and functional annotations. Recently, we have performed major updates for miRDB. Specifically, by employing an improved algorithm for miRNA target prediction, we now present updated transcriptome-wide target prediction data in miRDB, including 3.5 million predicted targets regulated by 7000 miRNAs in five species. Further, we have implemented the new prediction algorithm into a web server, allowing custom target prediction with user-provided sequences. Another new database feature is the prediction of cell-specific miRNA targets. miRDB now hosts the expression profiles of over 1000 cell lines and presents target prediction data that are tailored for specific cell models. At last, a new web query interface has been added to miRDB for prediction of miRNA functions by integrative analysis of target prediction and Gene Ontology data. All data in miRDB are freely accessible at http://mirdb.org.",poster,cp91
Biology,p1555,d5,1f53996347086be3bd3a32da0976ba2db7687988,c91,International Symposium on High-Performance Computer Architecture,miRDB: an online database for prediction of functional microRNA targets,"Abstract MicroRNAs (miRNAs) are small noncoding RNAs that act as master regulators in many biological processes. miRNAs function mainly by downregulating the expression of their gene targets. Thus, accurate prediction of miRNA targets is critical for characterization of miRNA functions. To this end, we have developed an online database, miRDB, for miRNA target prediction and functional annotations. Recently, we have performed major updates for miRDB. Specifically, by employing an improved algorithm for miRNA target prediction, we now present updated transcriptome-wide target prediction data in miRDB, including 3.5 million predicted targets regulated by 7000 miRNAs in five species. Further, we have implemented the new prediction algorithm into a web server, allowing custom target prediction with user-provided sequences. Another new database feature is the prediction of cell-specific miRNA targets. miRDB now hosts the expression profiles of over 1000 cell lines and presents target prediction data that are tailored for specific cell models. At last, a new web query interface has been added to miRDB for prediction of miRNA functions by integrative analysis of target prediction and Gene Ontology data. All data in miRDB are freely accessible at http://mirdb.org.",poster,cp91
Medicine,p1556,d1,0e466ea033b982519f351022304dccb64a46b93c,c36,International Conference on Information Technology Based Higher Education and Training,IPD-IMGT/HLA Database,"Abstract The IPD-IMGT/HLA Database, http://www.ebi.ac.uk/ipd/imgt/hla/, currently contains over 25 000 allele sequence for 45 genes, which are located within the Major Histocompatibility Complex (MHC) of the human genome. This region is the most polymorphic region of the human genome, and the levels of polymorphism seen exceed most other genes. Some of the genes have several thousand variants and are now termed hyperpolymorphic, rather than just simply polymorphic. The IPD-IMGT/HLA Database has provided a stable, highly accessible, user-friendly repository for this information, providing the scientific and medical community access to the many variant sequences of this gene system, that are critical for the successful outcome of transplantation. The number of currently known variants, and dramatic increase in the number of new variants being identified has necessitated a dedicated resource with custom tools for curation and publication. The challenge for the database is to continue to provide a highly curated database of sequence variants, while supporting the increased number of submissions and complexity of sequences. In order to do this, traditional methods of accessing and presenting data will be challenged, and new methods will need to be utilized to keep pace with new discoveries.",poster,cp36
Computer Science,p1556,d3,0e466ea033b982519f351022304dccb64a46b93c,c36,International Conference on Information Technology Based Higher Education and Training,IPD-IMGT/HLA Database,"Abstract The IPD-IMGT/HLA Database, http://www.ebi.ac.uk/ipd/imgt/hla/, currently contains over 25 000 allele sequence for 45 genes, which are located within the Major Histocompatibility Complex (MHC) of the human genome. This region is the most polymorphic region of the human genome, and the levels of polymorphism seen exceed most other genes. Some of the genes have several thousand variants and are now termed hyperpolymorphic, rather than just simply polymorphic. The IPD-IMGT/HLA Database has provided a stable, highly accessible, user-friendly repository for this information, providing the scientific and medical community access to the many variant sequences of this gene system, that are critical for the successful outcome of transplantation. The number of currently known variants, and dramatic increase in the number of new variants being identified has necessitated a dedicated resource with custom tools for curation and publication. The challenge for the database is to continue to provide a highly curated database of sequence variants, while supporting the increased number of submissions and complexity of sequences. In order to do this, traditional methods of accessing and presenting data will be challenged, and new methods will need to be utilized to keep pace with new discoveries.",poster,cp36
Biology,p1556,d5,0e466ea033b982519f351022304dccb64a46b93c,c36,International Conference on Information Technology Based Higher Education and Training,IPD-IMGT/HLA Database,"Abstract The IPD-IMGT/HLA Database, http://www.ebi.ac.uk/ipd/imgt/hla/, currently contains over 25 000 allele sequence for 45 genes, which are located within the Major Histocompatibility Complex (MHC) of the human genome. This region is the most polymorphic region of the human genome, and the levels of polymorphism seen exceed most other genes. Some of the genes have several thousand variants and are now termed hyperpolymorphic, rather than just simply polymorphic. The IPD-IMGT/HLA Database has provided a stable, highly accessible, user-friendly repository for this information, providing the scientific and medical community access to the many variant sequences of this gene system, that are critical for the successful outcome of transplantation. The number of currently known variants, and dramatic increase in the number of new variants being identified has necessitated a dedicated resource with custom tools for curation and publication. The challenge for the database is to continue to provide a highly curated database of sequence variants, while supporting the increased number of submissions and complexity of sequences. In order to do this, traditional methods of accessing and presenting data will be challenged, and new methods will need to be utilized to keep pace with new discoveries.",poster,cp36
Medicine,p1559,d1,6ad9053940676fca029dabdc7937e5d854df61e0,c111,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,The Pfam protein families database,"Pfam is a widely used database of protein families, currently containing more than 13 000 manually curated protein families as of release 26.0. Pfam is available via servers in the UK (http://pfam.sanger.ac.uk/), the USA (http://pfam.janelia.org/) and Sweden (http://pfam.sbc.su.se/). Here, we report on changes that have occurred since our 2010 NAR paper (release 24.0). Over the last 2 years, we have generated 1840 new families and increased coverage of the UniProt Knowledgebase (UniProtKB) to nearly 80%. Notably, we have taken the step of opening up the annotation of our families to the Wikipedia community, by linking Pfam families to relevant Wikipedia pages and encouraging the Pfam and Wikipedia communities to improve and expand those pages. We continue to improve the Pfam website and add new visualizations, such as the ‘sunburst’ representation of taxonomic distribution of families. In this work we additionally address two topics that will be of particular interest to the Pfam community. First, we explain the definition and use of family-specific, manually curated gathering thresholds. Second, we discuss some of the features of domains of unknown function (also known as DUFs), which constitute a rapidly growing class of families within Pfam.",poster,cp111
Computer Science,p1559,d3,6ad9053940676fca029dabdc7937e5d854df61e0,c111,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,The Pfam protein families database,"Pfam is a widely used database of protein families, currently containing more than 13 000 manually curated protein families as of release 26.0. Pfam is available via servers in the UK (http://pfam.sanger.ac.uk/), the USA (http://pfam.janelia.org/) and Sweden (http://pfam.sbc.su.se/). Here, we report on changes that have occurred since our 2010 NAR paper (release 24.0). Over the last 2 years, we have generated 1840 new families and increased coverage of the UniProt Knowledgebase (UniProtKB) to nearly 80%. Notably, we have taken the step of opening up the annotation of our families to the Wikipedia community, by linking Pfam families to relevant Wikipedia pages and encouraging the Pfam and Wikipedia communities to improve and expand those pages. We continue to improve the Pfam website and add new visualizations, such as the ‘sunburst’ representation of taxonomic distribution of families. In this work we additionally address two topics that will be of particular interest to the Pfam community. First, we explain the definition and use of family-specific, manually curated gathering thresholds. Second, we discuss some of the features of domains of unknown function (also known as DUFs), which constitute a rapidly growing class of families within Pfam.",poster,cp111
Biology,p1559,d5,6ad9053940676fca029dabdc7937e5d854df61e0,c111,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,The Pfam protein families database,"Pfam is a widely used database of protein families, currently containing more than 13 000 manually curated protein families as of release 26.0. Pfam is available via servers in the UK (http://pfam.sanger.ac.uk/), the USA (http://pfam.janelia.org/) and Sweden (http://pfam.sbc.su.se/). Here, we report on changes that have occurred since our 2010 NAR paper (release 24.0). Over the last 2 years, we have generated 1840 new families and increased coverage of the UniProt Knowledgebase (UniProtKB) to nearly 80%. Notably, we have taken the step of opening up the annotation of our families to the Wikipedia community, by linking Pfam families to relevant Wikipedia pages and encouraging the Pfam and Wikipedia communities to improve and expand those pages. We continue to improve the Pfam website and add new visualizations, such as the ‘sunburst’ representation of taxonomic distribution of families. In this work we additionally address two topics that will be of particular interest to the Pfam community. First, we explain the definition and use of family-specific, manually curated gathering thresholds. Second, we discuss some of the features of domains of unknown function (also known as DUFs), which constitute a rapidly growing class of families within Pfam.",poster,cp111
Medicine,p1560,d1,288b317e427c6bf4c94d455049bd1368ff2071eb,c105,International Conference on Automatic Face and Gesture Recognition,The Immune Epitope Database (IEDB): 2018 update,"Abstract The Immune Epitope Database (IEDB, iedb.org) captures experimental data confined in figures, text and tables of the scientific literature, making it freely available and easily searchable to the public. The scope of the IEDB extends across immune epitope data related to all species studied and includes antibody, T cell, and MHC binding contexts associated with infectious, allergic, autoimmune, and transplant related diseases. Having been publicly accessible for >10 years, the recent focus of the IEDB has been improved query and reporting functionality to meet the needs of our users to access and summarize data that continues to grow in quantity and complexity. Here we present an update on our current efforts and future goals.",poster,cp105
Computer Science,p1560,d3,288b317e427c6bf4c94d455049bd1368ff2071eb,c105,International Conference on Automatic Face and Gesture Recognition,The Immune Epitope Database (IEDB): 2018 update,"Abstract The Immune Epitope Database (IEDB, iedb.org) captures experimental data confined in figures, text and tables of the scientific literature, making it freely available and easily searchable to the public. The scope of the IEDB extends across immune epitope data related to all species studied and includes antibody, T cell, and MHC binding contexts associated with infectious, allergic, autoimmune, and transplant related diseases. Having been publicly accessible for >10 years, the recent focus of the IEDB has been improved query and reporting functionality to meet the needs of our users to access and summarize data that continues to grow in quantity and complexity. Here we present an update on our current efforts and future goals.",poster,cp105
Biology,p1560,d5,288b317e427c6bf4c94d455049bd1368ff2071eb,c105,International Conference on Automatic Face and Gesture Recognition,The Immune Epitope Database (IEDB): 2018 update,"Abstract The Immune Epitope Database (IEDB, iedb.org) captures experimental data confined in figures, text and tables of the scientific literature, making it freely available and easily searchable to the public. The scope of the IEDB extends across immune epitope data related to all species studied and includes antibody, T cell, and MHC binding contexts associated with infectious, allergic, autoimmune, and transplant related diseases. Having been publicly accessible for >10 years, the recent focus of the IEDB has been improved query and reporting functionality to meet the needs of our users to access and summarize data that continues to grow in quantity and complexity. Here we present an update on our current efforts and future goals.",poster,cp105
Medicine,p1561,d1,95162f20fa22a8cfe84b74aa118f18a6f04eb1ab,j353,Mobile DNA,"Repbase Update, a database of repetitive elements in eukaryotic genomes",Abstract,fullPaper,jv353
Biology,p1561,d5,95162f20fa22a8cfe84b74aa118f18a6f04eb1ab,j353,Mobile DNA,"Repbase Update, a database of repetitive elements in eukaryotic genomes",Abstract,fullPaper,jv353
Medicine,p1562,d1,41abf43dc718e271299457bce65bccfe3feeb9d6,j354,Applied and Environmental Microbiology,"Greengenes, a Chimera-Checked 16S rRNA Gene Database and Workbench Compatible with ARB","ABSTRACT A 16S rRNA gene database (http://greengenes.lbl.gov ) addresses limitations of public repositories by providing chimera screening, standard alignment, and taxonomic classification using multiple published taxonomies. It was found that there is incongruent taxonomic nomenclature among curators even at the phylum level. Putative chimeras were identified in 3% of environmental sequences and in 0.2% of records derived from isolates. Environmental sequences were classified into 100 phylum-level lineages in the Archaea and Bacteria.",fullPaper,jv354
Biology,p1562,d5,41abf43dc718e271299457bce65bccfe3feeb9d6,j354,Applied and Environmental Microbiology,"Greengenes, a Chimera-Checked 16S rRNA Gene Database and Workbench Compatible with ARB","ABSTRACT A 16S rRNA gene database (http://greengenes.lbl.gov ) addresses limitations of public repositories by providing chimera screening, standard alignment, and taxonomic classification using multiple published taxonomies. It was found that there is incongruent taxonomic nomenclature among curators even at the phylum level. Putative chimeras were identified in 3% of environmental sequences and in 0.2% of records derived from isolates. Environmental sequences were classified into 100 phylum-level lineages in the Archaea and Bacteria.",fullPaper,jv354
Medicine,p1567,d1,3b073a5e7de5513705a7e2a7b1c88d3acbeed82c,j356,Journal of Cheminformatics,TCMSP: a database of systems pharmacology for drug discovery from herbal medicines,Abstract,fullPaper,jv356
Computer Science,p1567,d3,3b073a5e7de5513705a7e2a7b1c88d3acbeed82c,j356,Journal of Cheminformatics,TCMSP: a database of systems pharmacology for drug discovery from herbal medicines,Abstract,fullPaper,jv356
Medicine,p1569,d1,fc1e3ed87c15d62148f03ff99677e6be0fc6f5b1,c107,Annual Haifa Experimental Systems Conference,The carbohydrate-active enzyme database: functions and literature,"Abstract Thirty years have elapsed since the emergence of the classification of carbohydrate-active enzymes in sequence-based families that became the CAZy database over 20 years ago, freely available for browsing and download at www.cazy.org. In the era of large scale sequencing and high-throughput Biology, it is important to examine the position of this specialist database that is deeply rooted in human curation. The three primary tasks of the CAZy curators are (i) to maintain and update the family classification of this class of enzymes, (ii) to classify sequences newly released by GenBank and the Protein Data Bank and (iii) to capture and present functional information for each family. The CAZy website is updated once a month. Here we briefly summarize the increase in novel families and the annotations conducted during the last 8 years. We present several important changes that facilitate taxonomic navigation, and allow to download the entirety of the annotations. Most importantly we highlight the considerable amount of work that accompanies the analysis and report of biochemical data from the literature.",poster,cp107
Computer Science,p1569,d3,fc1e3ed87c15d62148f03ff99677e6be0fc6f5b1,c107,Annual Haifa Experimental Systems Conference,The carbohydrate-active enzyme database: functions and literature,"Abstract Thirty years have elapsed since the emergence of the classification of carbohydrate-active enzymes in sequence-based families that became the CAZy database over 20 years ago, freely available for browsing and download at www.cazy.org. In the era of large scale sequencing and high-throughput Biology, it is important to examine the position of this specialist database that is deeply rooted in human curation. The three primary tasks of the CAZy curators are (i) to maintain and update the family classification of this class of enzymes, (ii) to classify sequences newly released by GenBank and the Protein Data Bank and (iii) to capture and present functional information for each family. The CAZy website is updated once a month. Here we briefly summarize the increase in novel families and the annotations conducted during the last 8 years. We present several important changes that facilitate taxonomic navigation, and allow to download the entirety of the annotations. Most importantly we highlight the considerable amount of work that accompanies the analysis and report of biochemical data from the literature.",poster,cp107
Medicine,p1571,d1,90bc0ca3feebe0215079cf575b90017170a0089f,c53,International Conference on Learning Representations,CDD: NCBI's conserved domain database,"NCBI's CDD, the Conserved Domain Database, enters its 15th year as a public resource for the annotation of proteins with the location of conserved domain footprints. Going forward, we strive to improve the coverage and consistency of domain annotation provided by CDD. We maintain a live search system as well as an archive of pre-computed domain annotation for sequences tracked in NCBI's Entrez protein database, which can be retrieved for single sequences or in bulk. We also maintain import procedures so that CDD contains domain models and domain definitions provided by several collections available in the public domain, as well as those produced by an in-house curation effort. The curation effort aims at increasing coverage and providing finer-grained classifications of common protein domains, for which a wealth of functional and structural data has become available. CDD curation generates alignment models of representative sequence fragments, which are in agreement with domain boundaries as observed in protein 3D structure, and which model the structurally conserved cores of domain families as well as annotate conserved features. CDD can be accessed at http://www.ncbi.nlm.nih.gov/Structure/cdd/cdd.shtml.",poster,cp53
Computer Science,p1571,d3,90bc0ca3feebe0215079cf575b90017170a0089f,c53,International Conference on Learning Representations,CDD: NCBI's conserved domain database,"NCBI's CDD, the Conserved Domain Database, enters its 15th year as a public resource for the annotation of proteins with the location of conserved domain footprints. Going forward, we strive to improve the coverage and consistency of domain annotation provided by CDD. We maintain a live search system as well as an archive of pre-computed domain annotation for sequences tracked in NCBI's Entrez protein database, which can be retrieved for single sequences or in bulk. We also maintain import procedures so that CDD contains domain models and domain definitions provided by several collections available in the public domain, as well as those produced by an in-house curation effort. The curation effort aims at increasing coverage and providing finer-grained classifications of common protein domains, for which a wealth of functional and structural data has become available. CDD curation generates alignment models of representative sequence fragments, which are in agreement with domain boundaries as observed in protein 3D structure, and which model the structurally conserved cores of domain families as well as annotate conserved features. CDD can be accessed at http://www.ncbi.nlm.nih.gov/Structure/cdd/cdd.shtml.",poster,cp53
Biology,p1571,d5,90bc0ca3feebe0215079cf575b90017170a0089f,c53,International Conference on Learning Representations,CDD: NCBI's conserved domain database,"NCBI's CDD, the Conserved Domain Database, enters its 15th year as a public resource for the annotation of proteins with the location of conserved domain footprints. Going forward, we strive to improve the coverage and consistency of domain annotation provided by CDD. We maintain a live search system as well as an archive of pre-computed domain annotation for sequences tracked in NCBI's Entrez protein database, which can be retrieved for single sequences or in bulk. We also maintain import procedures so that CDD contains domain models and domain definitions provided by several collections available in the public domain, as well as those produced by an in-house curation effort. The curation effort aims at increasing coverage and providing finer-grained classifications of common protein domains, for which a wealth of functional and structural data has become available. CDD curation generates alignment models of representative sequence fragments, which are in agreement with domain boundaries as observed in protein 3D structure, and which model the structurally conserved cores of domain families as well as annotate conserved features. CDD can be accessed at http://www.ncbi.nlm.nih.gov/Structure/cdd/cdd.shtml.",poster,cp53
Medicine,p1573,d1,d71af418eeb9f5a68062929bae12af74773ffcb2,c11,European Conference on Modelling and Simulation,The ChEMBL database in 2017,"ChEMBL is an open large-scale bioactivity database (https://www.ebi.ac.uk/chembl), previously described in the 2012 and 2014 Nucleic Acids Research Database Issues. Since then, alongside the continued extraction of data from the medicinal chemistry literature, new sources of bioactivity data have also been added to the database. These include: deposited data sets from neglected disease screening; crop protection data; drug metabolism and disposition data and bioactivity data from patents. A number of improvements and new features have also been incorporated. These include the annotation of assays and targets using ontologies, the inclusion of targets and indications for clinical candidates, addition of metabolic pathways for drugs and calculation of structural alerts. The ChEMBL data can be accessed via a web-interface, RDF distribution, data downloads and RESTful web-services.",poster,cp11
Computer Science,p1573,d3,d71af418eeb9f5a68062929bae12af74773ffcb2,c11,European Conference on Modelling and Simulation,The ChEMBL database in 2017,"ChEMBL is an open large-scale bioactivity database (https://www.ebi.ac.uk/chembl), previously described in the 2012 and 2014 Nucleic Acids Research Database Issues. Since then, alongside the continued extraction of data from the medicinal chemistry literature, new sources of bioactivity data have also been added to the database. These include: deposited data sets from neglected disease screening; crop protection data; drug metabolism and disposition data and bioactivity data from patents. A number of improvements and new features have also been incorporated. These include the annotation of assays and targets using ontologies, the inclusion of targets and indications for clinical candidates, addition of metabolic pathways for drugs and calculation of structural alerts. The ChEMBL data can be accessed via a web-interface, RDF distribution, data downloads and RESTful web-services.",poster,cp11
Biology,p1573,d5,d71af418eeb9f5a68062929bae12af74773ffcb2,c11,European Conference on Modelling and Simulation,The ChEMBL database in 2017,"ChEMBL is an open large-scale bioactivity database (https://www.ebi.ac.uk/chembl), previously described in the 2012 and 2014 Nucleic Acids Research Database Issues. Since then, alongside the continued extraction of data from the medicinal chemistry literature, new sources of bioactivity data have also been added to the database. These include: deposited data sets from neglected disease screening; crop protection data; drug metabolism and disposition data and bioactivity data from patents. A number of improvements and new features have also been incorporated. These include the annotation of assays and targets using ontologies, the inclusion of targets and indications for clinical candidates, addition of metabolic pathways for drugs and calculation of structural alerts. The ChEMBL data can be accessed via a web-interface, RDF distribution, data downloads and RESTful web-services.",poster,cp11
Medicine,p1574,d1,cb56121bc38e0f4b44bcb5296a12038626152e96,c15,Pacific Symposium on Biocomputing,CARD 2017: expansion and model-centric curation of the comprehensive antibiotic resistance database,"The Comprehensive Antibiotic Resistance Database (CARD; http://arpcard.mcmaster.ca) is a manually curated resource containing high quality reference data on the molecular basis of antimicrobial resistance (AMR), with an emphasis on the genes, proteins and mutations involved in AMR. CARD is ontologically structured, model centric, and spans the breadth of AMR drug classes and resistance mechanisms, including intrinsic, mutation-driven and acquired resistance. It is built upon the Antibiotic Resistance Ontology (ARO), a custom built, interconnected and hierarchical controlled vocabulary allowing advanced data sharing and organization. Its design allows the development of novel genome analysis tools, such as the Resistance Gene Identifier (RGI) for resistome prediction from raw genome sequence. Recent improvements include extensive curation of additional reference sequences and mutations, development of a unique Model Ontology and accompanying AMR detection models to power sequence analysis, new visualization tools, and expansion of the RGI for detection of emergent AMR threats. CARD curation is updated monthly based on an interplay of manual literature curation, computational text mining, and genome analysis.",poster,cp15
Computer Science,p1574,d3,cb56121bc38e0f4b44bcb5296a12038626152e96,c15,Pacific Symposium on Biocomputing,CARD 2017: expansion and model-centric curation of the comprehensive antibiotic resistance database,"The Comprehensive Antibiotic Resistance Database (CARD; http://arpcard.mcmaster.ca) is a manually curated resource containing high quality reference data on the molecular basis of antimicrobial resistance (AMR), with an emphasis on the genes, proteins and mutations involved in AMR. CARD is ontologically structured, model centric, and spans the breadth of AMR drug classes and resistance mechanisms, including intrinsic, mutation-driven and acquired resistance. It is built upon the Antibiotic Resistance Ontology (ARO), a custom built, interconnected and hierarchical controlled vocabulary allowing advanced data sharing and organization. Its design allows the development of novel genome analysis tools, such as the Resistance Gene Identifier (RGI) for resistome prediction from raw genome sequence. Recent improvements include extensive curation of additional reference sequences and mutations, development of a unique Model Ontology and accompanying AMR detection models to power sequence analysis, new visualization tools, and expansion of the RGI for detection of emergent AMR threats. CARD curation is updated monthly based on an interplay of manual literature curation, computational text mining, and genome analysis.",poster,cp15
Biology,p1574,d5,cb56121bc38e0f4b44bcb5296a12038626152e96,c15,Pacific Symposium on Biocomputing,CARD 2017: expansion and model-centric curation of the comprehensive antibiotic resistance database,"The Comprehensive Antibiotic Resistance Database (CARD; http://arpcard.mcmaster.ca) is a manually curated resource containing high quality reference data on the molecular basis of antimicrobial resistance (AMR), with an emphasis on the genes, proteins and mutations involved in AMR. CARD is ontologically structured, model centric, and spans the breadth of AMR drug classes and resistance mechanisms, including intrinsic, mutation-driven and acquired resistance. It is built upon the Antibiotic Resistance Ontology (ARO), a custom built, interconnected and hierarchical controlled vocabulary allowing advanced data sharing and organization. Its design allows the development of novel genome analysis tools, such as the Resistance Gene Identifier (RGI) for resistome prediction from raw genome sequence. Recent improvements include extensive curation of additional reference sequences and mutations, development of a unique Model Ontology and accompanying AMR detection models to power sequence analysis, new visualization tools, and expansion of the RGI for detection of emergent AMR threats. CARD curation is updated monthly based on an interplay of manual literature curation, computational text mining, and genome analysis.",poster,cp15
Medicine,p1575,d1,960e7494ef4ec5964407488080f249104cd218f0,c59,Australian Software Engineering Conference,Database resources of the National Center for Biotechnology Information,"The National Center for Biotechnology Information (NCBI) provides a large suite of online resources for biological information and data, including the GenBank® nucleic acid sequence database and the PubMed® database of citations and abstracts published in life science journals. The Entrez system provides search and retrieval operations for most of these data from 34 distinct databases. The E-utilities serve as the programming interface for the Entrez system. Custom implementations of the BLAST program provide sequence-based searching of many specialized datasets. New resources released in the past year include a new PubMed interface and NCBI datasets. Additional resources that were updated in the past year include PMC, Bookshelf, Genome Data Viewer, SRA, ClinVar, dbSNP, dbVar, Pathogen Detection, BLAST, Primer-BLAST, IgBLAST, iCn3D and PubChem. All of these resources can be accessed through the NCBI home page at https://www.ncbi.nlm.nih.gov.",poster,cp59
Computer Science,p1575,d3,960e7494ef4ec5964407488080f249104cd218f0,c59,Australian Software Engineering Conference,Database resources of the National Center for Biotechnology Information,"The National Center for Biotechnology Information (NCBI) provides a large suite of online resources for biological information and data, including the GenBank® nucleic acid sequence database and the PubMed® database of citations and abstracts published in life science journals. The Entrez system provides search and retrieval operations for most of these data from 34 distinct databases. The E-utilities serve as the programming interface for the Entrez system. Custom implementations of the BLAST program provide sequence-based searching of many specialized datasets. New resources released in the past year include a new PubMed interface and NCBI datasets. Additional resources that were updated in the past year include PMC, Bookshelf, Genome Data Viewer, SRA, ClinVar, dbSNP, dbVar, Pathogen Detection, BLAST, Primer-BLAST, IgBLAST, iCn3D and PubChem. All of these resources can be accessed through the NCBI home page at https://www.ncbi.nlm.nih.gov.",poster,cp59
Medicine,p1577,d1,3a2b869533620d2dfa076522321983c537b3c175,c112,British Machine Vision Conference,Gene Ontology Consortium: The Gene Ontology (GO) database and informatics resource,"The Gene Ontology (GO) project (http://www. geneontology.org/) provides structured, controlled vocabularies and classifications that cover several domains of molecular and cellular biology and are freely available for community use in the annotation of genes, gene products and sequences. Many model organism databases and genome annotation groups use the GO and contribute their annotation sets to the GO resource. The GO database integrates the vocabularies and contributed annotations and provides full access to this information in several formats. Members of the GO Consortium continually work collectively, involving outside experts as needed, to expand and update the GO vocabularies. The GO Web resource also provides access to extensive documentation about the GO project and links to applications that use GO data for functional analyses.",poster,cp112
Computer Science,p1577,d3,3a2b869533620d2dfa076522321983c537b3c175,c112,British Machine Vision Conference,Gene Ontology Consortium: The Gene Ontology (GO) database and informatics resource,"The Gene Ontology (GO) project (http://www. geneontology.org/) provides structured, controlled vocabularies and classifications that cover several domains of molecular and cellular biology and are freely available for community use in the annotation of genes, gene products and sequences. Many model organism databases and genome annotation groups use the GO and contribute their annotation sets to the GO resource. The GO database integrates the vocabularies and contributed annotations and provides full access to this information in several formats. Members of the GO Consortium continually work collectively, involving outside experts as needed, to expand and update the GO vocabularies. The GO Web resource also provides access to extensive documentation about the GO project and links to applications that use GO data for functional analyses.",poster,cp112
Biology,p1577,d5,3a2b869533620d2dfa076522321983c537b3c175,c112,British Machine Vision Conference,Gene Ontology Consortium: The Gene Ontology (GO) database and informatics resource,"The Gene Ontology (GO) project (http://www. geneontology.org/) provides structured, controlled vocabularies and classifications that cover several domains of molecular and cellular biology and are freely available for community use in the annotation of genes, gene products and sequences. Many model organism databases and genome annotation groups use the GO and contribute their annotation sets to the GO resource. The GO database integrates the vocabularies and contributed annotations and provides full access to this information in several formats. Members of the GO Consortium continually work collectively, involving outside experts as needed, to expand and update the GO vocabularies. The GO Web resource also provides access to extensive documentation about the GO project and links to applications that use GO data for functional analyses.",poster,cp112
Medicine,p1578,d1,a8db50edfe26a6ae33a6787e2049de5bacd18666,c101,Interspeech,ChEMBL: a large-scale bioactivity database for drug discovery,"ChEMBL is an Open Data database containing binding, functional and ADMET information for a large number of drug-like bioactive compounds. These data are manually abstracted from the primary published literature on a regular basis, then further curated and standardized to maximize their quality and utility across a wide range of chemical biology and drug-discovery research problems. Currently, the database contains 5.4 million bioactivity measurements for more than 1 million compounds and 5200 protein targets. Access is available through a web-based interface, data downloads and web services at: https://www.ebi.ac.uk/chembldb.",poster,cp101
Computer Science,p1578,d3,a8db50edfe26a6ae33a6787e2049de5bacd18666,c101,Interspeech,ChEMBL: a large-scale bioactivity database for drug discovery,"ChEMBL is an Open Data database containing binding, functional and ADMET information for a large number of drug-like bioactive compounds. These data are manually abstracted from the primary published literature on a regular basis, then further curated and standardized to maximize their quality and utility across a wide range of chemical biology and drug-discovery research problems. Currently, the database contains 5.4 million bioactivity measurements for more than 1 million compounds and 5200 protein targets. Access is available through a web-based interface, data downloads and web services at: https://www.ebi.ac.uk/chembldb.",poster,cp101
Biology,p1578,d5,a8db50edfe26a6ae33a6787e2049de5bacd18666,c101,Interspeech,ChEMBL: a large-scale bioactivity database for drug discovery,"ChEMBL is an Open Data database containing binding, functional and ADMET information for a large number of drug-like bioactive compounds. These data are manually abstracted from the primary published literature on a regular basis, then further curated and standardized to maximize their quality and utility across a wide range of chemical biology and drug-discovery research problems. Currently, the database contains 5.4 million bioactivity measurements for more than 1 million compounds and 5200 protein targets. Access is available through a web-based interface, data downloads and web services at: https://www.ebi.ac.uk/chembldb.",poster,cp101
Medicine,p1579,d1,bc744742f1644c9cab6b9535ab0bd6f2eed320bb,c9,Big Data,CDD: a Conserved Domain Database for the functional annotation of proteins,"NCBI’s Conserved Domain Database (CDD) is a resource for the annotation of protein sequences with the location of conserved domain footprints, and functional sites inferred from these footprints. CDD includes manually curated domain models that make use of protein 3D structure to refine domain models and provide insights into sequence/structure/function relationships. Manually curated models are organized hierarchically if they describe domain families that are clearly related by common descent. As CDD also imports domain family models from a variety of external sources, it is a partially redundant collection. To simplify protein annotation, redundant models and models describing homologous families are clustered into superfamilies. By default, domain footprints are annotated with the corresponding superfamily designation, on top of which specific annotation may indicate high-confidence assignment of family membership. Pre-computed domain annotation is available for proteins in the Entrez/Protein dataset, and a novel interface, Batch CD-Search, allows the computation and download of annotation for large sets of protein queries. CDD can be accessed via http://www.ncbi.nlm.nih.gov/Structure/cdd/cdd.shtml.",poster,cp9
Computer Science,p1579,d3,bc744742f1644c9cab6b9535ab0bd6f2eed320bb,c9,Big Data,CDD: a Conserved Domain Database for the functional annotation of proteins,"NCBI’s Conserved Domain Database (CDD) is a resource for the annotation of protein sequences with the location of conserved domain footprints, and functional sites inferred from these footprints. CDD includes manually curated domain models that make use of protein 3D structure to refine domain models and provide insights into sequence/structure/function relationships. Manually curated models are organized hierarchically if they describe domain families that are clearly related by common descent. As CDD also imports domain family models from a variety of external sources, it is a partially redundant collection. To simplify protein annotation, redundant models and models describing homologous families are clustered into superfamilies. By default, domain footprints are annotated with the corresponding superfamily designation, on top of which specific annotation may indicate high-confidence assignment of family membership. Pre-computed domain annotation is available for proteins in the Entrez/Protein dataset, and a novel interface, Batch CD-Search, allows the computation and download of annotation for large sets of protein queries. CDD can be accessed via http://www.ncbi.nlm.nih.gov/Structure/cdd/cdd.shtml.",poster,cp9
Biology,p1579,d5,bc744742f1644c9cab6b9535ab0bd6f2eed320bb,c9,Big Data,CDD: a Conserved Domain Database for the functional annotation of proteins,"NCBI’s Conserved Domain Database (CDD) is a resource for the annotation of protein sequences with the location of conserved domain footprints, and functional sites inferred from these footprints. CDD includes manually curated domain models that make use of protein 3D structure to refine domain models and provide insights into sequence/structure/function relationships. Manually curated models are organized hierarchically if they describe domain families that are clearly related by common descent. As CDD also imports domain family models from a variety of external sources, it is a partially redundant collection. To simplify protein annotation, redundant models and models describing homologous families are clustered into superfamilies. By default, domain footprints are annotated with the corresponding superfamily designation, on top of which specific annotation may indicate high-confidence assignment of family membership. Pre-computed domain annotation is available for proteins in the Entrez/Protein dataset, and a novel interface, Batch CD-Search, allows the computation and download of annotation for large sets of protein queries. CDD can be accessed via http://www.ncbi.nlm.nih.gov/Structure/cdd/cdd.shtml.",poster,cp9
Medicine,p1580,d1,b307d55ba07058d6183991d2d2a81b340d558186,c104,North American Chapter of the Association for Computational Linguistics,"NCBI Reference Sequence (RefSeq): a curated non-redundant sequence database of genomes, transcripts and proteins","The National Center for Biotechnology Information (NCBI) Reference Sequence (RefSeq) database (http://www.ncbi.nlm.nih.gov/RefSeq/) provides a non-redundant collection of sequences representing genomic data, transcripts and proteins. Although the goal is to provide a comprehensive dataset representing the complete sequence information for any given species, the database pragmatically includes sequence data that are currently publicly available in the archival databases. The database incorporates data from over 2400 organisms and includes over one million proteins representing significant taxonomic diversity spanning prokaryotes, eukaryotes and viruses. Nucleotide and protein sequences are explicitly linked, and the sequences are linked to other resources including the NCBI Map Viewer and Gene. Sequences are annotated to include coding regions, conserved domains, variation, references, names, database cross-references, and other features using a combined approach of collaboration and other input from the scientific community, automated annotation, propagation from GenBank and curation by NCBI staff.",poster,cp104
Computer Science,p1580,d3,b307d55ba07058d6183991d2d2a81b340d558186,c104,North American Chapter of the Association for Computational Linguistics,"NCBI Reference Sequence (RefSeq): a curated non-redundant sequence database of genomes, transcripts and proteins","The National Center for Biotechnology Information (NCBI) Reference Sequence (RefSeq) database (http://www.ncbi.nlm.nih.gov/RefSeq/) provides a non-redundant collection of sequences representing genomic data, transcripts and proteins. Although the goal is to provide a comprehensive dataset representing the complete sequence information for any given species, the database pragmatically includes sequence data that are currently publicly available in the archival databases. The database incorporates data from over 2400 organisms and includes over one million proteins representing significant taxonomic diversity spanning prokaryotes, eukaryotes and viruses. Nucleotide and protein sequences are explicitly linked, and the sequences are linked to other resources including the NCBI Map Viewer and Gene. Sequences are annotated to include coding regions, conserved domains, variation, references, names, database cross-references, and other features using a combined approach of collaboration and other input from the scientific community, automated annotation, propagation from GenBank and curation by NCBI staff.",poster,cp104
Medicine,p1581,d1,50819fdfb666fdabb12f00e93e0d7e43ba4ba4bd,j359,Journal of Chemical Information and Modeling,ZINC - A Free Database of Commercially Available Compounds for Virtual Screening,"A critical barrier to entry into structure-based virtual screening is the lack of a suitable, easy to access database of purchasable compounds. We have therefore prepared a library of 727,842 molecules, each with 3D structure, using catalogs of compounds from vendors (the size of this library continues to grow). The molecules have been assigned biologically relevant protonation states and are annotated with properties such as molecular weight, calculated LogP, and number of rotatable bonds. Each molecule in the library contains vendor and purchasing information and is ready for docking using a number of popular docking programs. Within certain limits, the molecules are prepared in multiple protonation states and multiple tautomeric forms. In one format, multiple conformations are available for the molecules. This database is available for free download (http://zinc.docking.org) in several common file formats including SMILES, mol2, 3D SDF, and DOCK flexibase format. A Web-based query tool incorporating a molecular drawing interface enables the database to be searched and browsed and subsets to be created. Users can process their own molecules by uploading them to a server. Our hope is that this database will bring virtual screening libraries to a wide community of structural biologists and medicinal chemists.",fullPaper,jv359
Computer Science,p1581,d3,50819fdfb666fdabb12f00e93e0d7e43ba4ba4bd,j359,Journal of Chemical Information and Modeling,ZINC - A Free Database of Commercially Available Compounds for Virtual Screening,"A critical barrier to entry into structure-based virtual screening is the lack of a suitable, easy to access database of purchasable compounds. We have therefore prepared a library of 727,842 molecules, each with 3D structure, using catalogs of compounds from vendors (the size of this library continues to grow). The molecules have been assigned biologically relevant protonation states and are annotated with properties such as molecular weight, calculated LogP, and number of rotatable bonds. Each molecule in the library contains vendor and purchasing information and is ready for docking using a number of popular docking programs. Within certain limits, the molecules are prepared in multiple protonation states and multiple tautomeric forms. In one format, multiple conformations are available for the molecules. This database is available for free download (http://zinc.docking.org) in several common file formats including SMILES, mol2, 3D SDF, and DOCK flexibase format. A Web-based query tool incorporating a molecular drawing interface enables the database to be searched and browsed and subsets to be created. Users can process their own molecules by uploading them to a server. Our hope is that this database will bring virtual screening libraries to a wide community of structural biologists and medicinal chemists.",fullPaper,jv359
Medicine,p1582,d1,61533dd9e41f20e2f5deaf22afb04c94b4071eac,j360,Cytogenetic and Genome Research,"Repbase Update, a database of eukaryotic repetitive elements","Repbase Update is a comprehensive database of repetitive elements from diverse eukaryotic organisms. Currently, it contains over 3600 annotated sequences representing different families and subfamilies of repeats, many of which are unreported anywhere else. Each sequence is accompanied by a short description and references to the original contributors. Repbase Update includes Repbase Reports, an electronic journal publishing newly discovered transposable elements, and the Transposon Pub, a web-based browser of selected chromosomal maps of transposable elements. Sequences from Repbase Update are used to screen and annotate repetitive elements using programs such as Censor and RepeatMasker. Repbase Update is available on the worldwide web at http://www.girinst.org/Repbase_Update.html.",fullPaper,jv360
Biology,p1582,d5,61533dd9e41f20e2f5deaf22afb04c94b4071eac,j360,Cytogenetic and Genome Research,"Repbase Update, a database of eukaryotic repetitive elements","Repbase Update is a comprehensive database of repetitive elements from diverse eukaryotic organisms. Currently, it contains over 3600 annotated sequences representing different families and subfamilies of repeats, many of which are unreported anywhere else. Each sequence is accompanied by a short description and references to the original contributors. Repbase Update includes Repbase Reports, an electronic journal publishing newly discovered transposable elements, and the Transposon Pub, a web-based browser of selected chromosomal maps of transposable elements. Sequences from Repbase Update are used to screen and annotate repetitive elements using programs such as Censor and RepeatMasker. Repbase Update is available on the worldwide web at http://www.girinst.org/Repbase_Update.html.",fullPaper,jv360
Medicine,p1585,d1,3d1ba71a1c3b7302e12ab3d07bf4a8451db5aad0,c68,Symposium on Advances in Databases and Information Systems,Plant cis-acting regulatory DNA elements (PLACE) database: 1999,"PLACE (http://www.dna.affrc.go.jp/htdocs/PLACE/) is a database of nucleotide sequence motifs found in plant cis-acting regulatory DNA elements. Motifs were extracted from previously published reports on genes in vascular plants. In addition to the motifs originally reported, their variations in other genes or in other plant species in later reports are also compiled. Documents for each motif in the PLACE database contains, in addition to a motif sequence, a brief definition and description of each motif, and relevant literature with PubMed ID numbers and GenBank accession numbers where available. Users can search their query sequences for cis-elements using the Signal Scan program at our web site. The results will be reported in one of the three forms. Clicking the PLACE accession numbers in the result report will open the pertinent motif document. Clicking the PubMed or GenBank accession number in the document will allow users to access to these databases, and to read the of the literature or the annotation in the DNA database. This report summarizes the present status of this database and available tools.",poster,cp68
Computer Science,p1585,d3,3d1ba71a1c3b7302e12ab3d07bf4a8451db5aad0,c68,Symposium on Advances in Databases and Information Systems,Plant cis-acting regulatory DNA elements (PLACE) database: 1999,"PLACE (http://www.dna.affrc.go.jp/htdocs/PLACE/) is a database of nucleotide sequence motifs found in plant cis-acting regulatory DNA elements. Motifs were extracted from previously published reports on genes in vascular plants. In addition to the motifs originally reported, their variations in other genes or in other plant species in later reports are also compiled. Documents for each motif in the PLACE database contains, in addition to a motif sequence, a brief definition and description of each motif, and relevant literature with PubMed ID numbers and GenBank accession numbers where available. Users can search their query sequences for cis-elements using the Signal Scan program at our web site. The results will be reported in one of the three forms. Clicking the PLACE accession numbers in the result report will open the pertinent motif document. Clicking the PubMed or GenBank accession number in the document will allow users to access to these databases, and to read the of the literature or the annotation in the DNA database. This report summarizes the present status of this database and available tools.",poster,cp68
Biology,p1585,d5,3d1ba71a1c3b7302e12ab3d07bf4a8451db5aad0,c68,Symposium on Advances in Databases and Information Systems,Plant cis-acting regulatory DNA elements (PLACE) database: 1999,"PLACE (http://www.dna.affrc.go.jp/htdocs/PLACE/) is a database of nucleotide sequence motifs found in plant cis-acting regulatory DNA elements. Motifs were extracted from previously published reports on genes in vascular plants. In addition to the motifs originally reported, their variations in other genes or in other plant species in later reports are also compiled. Documents for each motif in the PLACE database contains, in addition to a motif sequence, a brief definition and description of each motif, and relevant literature with PubMed ID numbers and GenBank accession numbers where available. Users can search their query sequences for cis-elements using the Signal Scan program at our web site. The results will be reported in one of the three forms. Clicking the PLACE accession numbers in the result report will open the pertinent motif document. Clicking the PubMed or GenBank accession number in the document will allow users to access to these databases, and to read the of the literature or the annotation in the DNA database. This report summarizes the present status of this database and available tools.",poster,cp68
Medicine,p1587,d1,e274c1b6e17825feab52de205fd0bc4917d5be6c,j362,Journal of Biomolecular NMR,Protein backbone angle restraints from searching a database for chemical shift and sequence homology,Abstract,fullPaper,jv362
Chemistry,p1587,d8,e274c1b6e17825feab52de205fd0bc4917d5be6c,j362,Journal of Biomolecular NMR,Protein backbone angle restraints from searching a database for chemical shift and sequence homology,Abstract,fullPaper,jv362
Medicine,p1588,d1,ba90ae48b30594b57a5ca7bfd37cae150458ecfa,c10,Americas Conference on Information Systems,TRRUST v2: an expanded reference database of human and mouse transcriptional regulatory interactions,"Abstract Transcription factors (TFs) are major trans-acting factors in transcriptional regulation. Therefore, elucidating TF–target interactions is a key step toward understanding the regulatory circuitry underlying complex traits such as human diseases. We previously published a reference TF–target interaction database for humans—TRRUST (Transcriptional Regulatory Relationships Unraveled by Sentence-based Text mining)—which was constructed using sentence-based text mining, followed by manual curation. Here, we present TRRUST v2 (www.grnpedia.org/trrust) with a significant improvement from the previous version, including a significantly increased size of the database consisting of 8444 regulatory interactions for 800 TFs in humans. More importantly, TRRUST v2 also contains a database for TF–target interactions in mice, including 6552 TF–target interactions for 828 mouse TFs. TRRUST v2 is also substantially more comprehensive and less biased than other TF–target interaction databases. We also improved the web interface, which now enables prioritization of key TFs for a physiological condition depicted by a set of user-input transcriptional responsive genes. With the significant expansion in the database size and inclusion of the new web tool for TF prioritization, we believe that TRRUST v2 will be a versatile database for the study of the transcriptional regulation involved in human diseases.",poster,cp10
Computer Science,p1588,d3,ba90ae48b30594b57a5ca7bfd37cae150458ecfa,c10,Americas Conference on Information Systems,TRRUST v2: an expanded reference database of human and mouse transcriptional regulatory interactions,"Abstract Transcription factors (TFs) are major trans-acting factors in transcriptional regulation. Therefore, elucidating TF–target interactions is a key step toward understanding the regulatory circuitry underlying complex traits such as human diseases. We previously published a reference TF–target interaction database for humans—TRRUST (Transcriptional Regulatory Relationships Unraveled by Sentence-based Text mining)—which was constructed using sentence-based text mining, followed by manual curation. Here, we present TRRUST v2 (www.grnpedia.org/trrust) with a significant improvement from the previous version, including a significantly increased size of the database consisting of 8444 regulatory interactions for 800 TFs in humans. More importantly, TRRUST v2 also contains a database for TF–target interactions in mice, including 6552 TF–target interactions for 828 mouse TFs. TRRUST v2 is also substantially more comprehensive and less biased than other TF–target interaction databases. We also improved the web interface, which now enables prioritization of key TFs for a physiological condition depicted by a set of user-input transcriptional responsive genes. With the significant expansion in the database size and inclusion of the new web tool for TF prioritization, we believe that TRRUST v2 will be a versatile database for the study of the transcriptional regulation involved in human diseases.",poster,cp10
Biology,p1588,d5,ba90ae48b30594b57a5ca7bfd37cae150458ecfa,c10,Americas Conference on Information Systems,TRRUST v2: an expanded reference database of human and mouse transcriptional regulatory interactions,"Abstract Transcription factors (TFs) are major trans-acting factors in transcriptional regulation. Therefore, elucidating TF–target interactions is a key step toward understanding the regulatory circuitry underlying complex traits such as human diseases. We previously published a reference TF–target interaction database for humans—TRRUST (Transcriptional Regulatory Relationships Unraveled by Sentence-based Text mining)—which was constructed using sentence-based text mining, followed by manual curation. Here, we present TRRUST v2 (www.grnpedia.org/trrust) with a significant improvement from the previous version, including a significantly increased size of the database consisting of 8444 regulatory interactions for 800 TFs in humans. More importantly, TRRUST v2 also contains a database for TF–target interactions in mice, including 6552 TF–target interactions for 828 mouse TFs. TRRUST v2 is also substantially more comprehensive and less biased than other TF–target interaction databases. We also improved the web interface, which now enables prioritization of key TFs for a physiological condition depicted by a set of user-input transcriptional responsive genes. With the significant expansion in the database size and inclusion of the new web tool for TF prioritization, we believe that TRRUST v2 will be a versatile database for the study of the transcriptional regulation involved in human diseases.",poster,cp10
Medicine,p1591,d1,8c8b7c1adb6f077bb3045928767b8bc6763e0c06,c105,International Conference on Automatic Face and Gesture Recognition,HMDB 3.0—The Human Metabolome Database in 2013,"The Human Metabolome Database (HMDB) (www.hmdb.ca) is a resource dedicated to providing scientists with the most current and comprehensive coverage of the human metabolome. Since its first release in 2007, the HMDB has been used to facilitate research for nearly 1000 published studies in metabolomics, clinical biochemistry and systems biology. The most recent release of HMDB (version 3.0) has been significantly expanded and enhanced over the 2009 release (version 2.0). In particular, the number of annotated metabolite entries has grown from 6500 to more than 40 000 (a 600% increase). This enormous expansion is a result of the inclusion of both ‘detected’ metabolites (those with measured concentrations or experimental confirmation of their existence) and ‘expected’ metabolites (those for which biochemical pathways are known or human intake/exposure is frequent but the compound has yet to be detected in the body). The latest release also has greatly increased the number of metabolites with biofluid or tissue concentration data, the number of compounds with reference spectra and the number of data fields per entry. In addition to this expansion in data quantity, new database visualization tools and new data content have been added or enhanced. These include better spectral viewing tools, more powerful chemical substructure searches, an improved chemical taxonomy and better, more interactive pathway maps. This article describes these enhancements to the HMDB, which was previously featured in the 2009 NAR Database Issue. (Note to referees, HMDB 3.0 will go live on 18 September 2012.).",poster,cp105
Computer Science,p1591,d3,8c8b7c1adb6f077bb3045928767b8bc6763e0c06,c105,International Conference on Automatic Face and Gesture Recognition,HMDB 3.0—The Human Metabolome Database in 2013,"The Human Metabolome Database (HMDB) (www.hmdb.ca) is a resource dedicated to providing scientists with the most current and comprehensive coverage of the human metabolome. Since its first release in 2007, the HMDB has been used to facilitate research for nearly 1000 published studies in metabolomics, clinical biochemistry and systems biology. The most recent release of HMDB (version 3.0) has been significantly expanded and enhanced over the 2009 release (version 2.0). In particular, the number of annotated metabolite entries has grown from 6500 to more than 40 000 (a 600% increase). This enormous expansion is a result of the inclusion of both ‘detected’ metabolites (those with measured concentrations or experimental confirmation of their existence) and ‘expected’ metabolites (those for which biochemical pathways are known or human intake/exposure is frequent but the compound has yet to be detected in the body). The latest release also has greatly increased the number of metabolites with biofluid or tissue concentration data, the number of compounds with reference spectra and the number of data fields per entry. In addition to this expansion in data quantity, new database visualization tools and new data content have been added or enhanced. These include better spectral viewing tools, more powerful chemical substructure searches, an improved chemical taxonomy and better, more interactive pathway maps. This article describes these enhancements to the HMDB, which was previously featured in the 2009 NAR Database Issue. (Note to referees, HMDB 3.0 will go live on 18 September 2012.).",poster,cp105
Biology,p1591,d5,8c8b7c1adb6f077bb3045928767b8bc6763e0c06,c105,International Conference on Automatic Face and Gesture Recognition,HMDB 3.0—The Human Metabolome Database in 2013,"The Human Metabolome Database (HMDB) (www.hmdb.ca) is a resource dedicated to providing scientists with the most current and comprehensive coverage of the human metabolome. Since its first release in 2007, the HMDB has been used to facilitate research for nearly 1000 published studies in metabolomics, clinical biochemistry and systems biology. The most recent release of HMDB (version 3.0) has been significantly expanded and enhanced over the 2009 release (version 2.0). In particular, the number of annotated metabolite entries has grown from 6500 to more than 40 000 (a 600% increase). This enormous expansion is a result of the inclusion of both ‘detected’ metabolites (those with measured concentrations or experimental confirmation of their existence) and ‘expected’ metabolites (those for which biochemical pathways are known or human intake/exposure is frequent but the compound has yet to be detected in the body). The latest release also has greatly increased the number of metabolites with biofluid or tissue concentration data, the number of compounds with reference spectra and the number of data fields per entry. In addition to this expansion in data quantity, new database visualization tools and new data content have been added or enhanced. These include better spectral viewing tools, more powerful chemical substructure searches, an improved chemical taxonomy and better, more interactive pathway maps. This article describes these enhancements to the HMDB, which was previously featured in the 2009 NAR Database Issue. (Note to referees, HMDB 3.0 will go live on 18 September 2012.).",poster,cp105
Medicine,p1592,d1,73a254f05fa694dc11a5efc5a033a8f1a4c84fd0,c92,International Symposium on Computer Architecture,The Gene Expression Omnibus Database,Abstract,poster,cp92
Computer Science,p1592,d3,73a254f05fa694dc11a5efc5a033a8f1a4c84fd0,c92,International Symposium on Computer Architecture,The Gene Expression Omnibus Database,Abstract,poster,cp92
Medicine,p1593,d1,8035e5002b7b0898ca7fa8263d09fe4454c6e4fd,c70,Annual Meeting of the Association for Computational Linguistics,The BioGRID interaction database: 2019 update,"Abstract The Biological General Repository for Interaction Datasets (BioGRID: https://thebiogrid.org) is an open access database dedicated to the curation and archival storage of protein, genetic and chemical interactions for all major model organism species and humans. As of September 2018 (build 3.4.164), BioGRID contains records for 1 598 688 biological interactions manually annotated from 55 809 publications for 71 species, as classified by an updated set of controlled vocabularies for experimental detection methods. BioGRID also houses records for >700 000 post-translational modification sites. BioGRID now captures chemical interaction data, including chemical–protein interactions for human drug targets drawn from the DrugBank database and manually curated bioactive compounds reported in the literature. A new dedicated aspect of BioGRID annotates genome-wide CRISPR/Cas9-based screens that report gene–phenotype and gene–gene relationships. An extension of the BioGRID resource called the Open Repository for CRISPR Screens (ORCS) database (https://orcs.thebiogrid.org) currently contains over 500 genome-wide screens carried out in human or mouse cell lines. All data in BioGRID is made freely available without restriction, is directly downloadable in standard formats and can be readily incorporated into existing applications via our web service platforms. BioGRID data are also freely distributed through partner model organism databases and meta-databases.",poster,cp70
Computer Science,p1593,d3,8035e5002b7b0898ca7fa8263d09fe4454c6e4fd,c70,Annual Meeting of the Association for Computational Linguistics,The BioGRID interaction database: 2019 update,"Abstract The Biological General Repository for Interaction Datasets (BioGRID: https://thebiogrid.org) is an open access database dedicated to the curation and archival storage of protein, genetic and chemical interactions for all major model organism species and humans. As of September 2018 (build 3.4.164), BioGRID contains records for 1 598 688 biological interactions manually annotated from 55 809 publications for 71 species, as classified by an updated set of controlled vocabularies for experimental detection methods. BioGRID also houses records for >700 000 post-translational modification sites. BioGRID now captures chemical interaction data, including chemical–protein interactions for human drug targets drawn from the DrugBank database and manually curated bioactive compounds reported in the literature. A new dedicated aspect of BioGRID annotates genome-wide CRISPR/Cas9-based screens that report gene–phenotype and gene–gene relationships. An extension of the BioGRID resource called the Open Repository for CRISPR Screens (ORCS) database (https://orcs.thebiogrid.org) currently contains over 500 genome-wide screens carried out in human or mouse cell lines. All data in BioGRID is made freely available without restriction, is directly downloadable in standard formats and can be readily incorporated into existing applications via our web service platforms. BioGRID data are also freely distributed through partner model organism databases and meta-databases.",poster,cp70
Biology,p1593,d5,8035e5002b7b0898ca7fa8263d09fe4454c6e4fd,c70,Annual Meeting of the Association for Computational Linguistics,The BioGRID interaction database: 2019 update,"Abstract The Biological General Repository for Interaction Datasets (BioGRID: https://thebiogrid.org) is an open access database dedicated to the curation and archival storage of protein, genetic and chemical interactions for all major model organism species and humans. As of September 2018 (build 3.4.164), BioGRID contains records for 1 598 688 biological interactions manually annotated from 55 809 publications for 71 species, as classified by an updated set of controlled vocabularies for experimental detection methods. BioGRID also houses records for >700 000 post-translational modification sites. BioGRID now captures chemical interaction data, including chemical–protein interactions for human drug targets drawn from the DrugBank database and manually curated bioactive compounds reported in the literature. A new dedicated aspect of BioGRID annotates genome-wide CRISPR/Cas9-based screens that report gene–phenotype and gene–gene relationships. An extension of the BioGRID resource called the Open Repository for CRISPR Screens (ORCS) database (https://orcs.thebiogrid.org) currently contains over 500 genome-wide screens carried out in human or mouse cell lines. All data in BioGRID is made freely available without restriction, is directly downloadable in standard formats and can be readily incorporated into existing applications via our web service platforms. BioGRID data are also freely distributed through partner model organism databases and meta-databases.",poster,cp70
Medicine,p1594,d1,ea35cd7fd2c46f86232f21ef73239f34f2d180a6,j107,Nucleic Acids Research,Database resources of the National Center for Biotechnology Information.,"The National Center for Biotechnology Information (NCBI) provides a large suite of online resources for biological information and data, including the GenBank® nucleic acid sequence database and the PubMed database of citations and abstracts published in life science journals. The Entrez system provides search and retrieval operations for most of these data from 35 distinct databases. The E-utilities serve as the programming interface for the Entrez system. Custom implementations of the BLAST program provide sequence-based searching of many specialized datasets. New resources released in the past year include a new PubMed interface, a sequence database search and a gene orthologs page. Additional resources that were updated in the past year include PMC, Bookshelf, My Bibliography, Assembly, RefSeq, viral genomes, the prokaryotic genome annotation pipeline, Genome Workbench, dbSNP, BLAST, Primer-BLAST, IgBLAST and PubChem. All of these resources can be accessed through the NCBI home page at www.ncbi.nlm.nih.gov.",fullPaper,jv107
Medicine,p1595,d1,4097c531036fcda230e0174a93565e54684e0e54,c33,Workshop on Python for High-Performance and Scientific Computing,The MetaCyc database of metabolic pathways and enzymes - a 2019 update,"Abstract MetaCyc (MetaCyc.org) is a comprehensive reference database of metabolic pathways and enzymes from all domains of life. It contains 2749 pathways derived from more than 60 000 publications, making it the largest curated collection of metabolic pathways. The data in MetaCyc are evidence-based and richly curated, resulting in an encyclopedic reference tool for metabolism. MetaCyc is also used as a knowledge base for generating thousands of organism-specific Pathway/Genome Databases (PGDBs), which are available in BioCyc.org and other genomic portals. This article provides an update on the developments in MetaCyc during September 2017 to August 2019, up to version 23.1. Some of the topics that received intensive curation during this period include cobamides biosynthesis, sterol metabolism, fatty acid biosynthesis, lipid metabolism, carotenoid metabolism, protein glycosylation, antibiotics and cytotoxins biosynthesis, siderophore biosynthesis, bioluminescence, vitamin K metabolism, brominated compound metabolism, plant secondary metabolism and human metabolism. Other additions include modifications to the GlycanBuilder software that enable displaying glycans using symbolic representation, improved graphics and fonts for web displays, improvements in the PathoLogic component of Pathway Tools, and the optional addition of regulatory information to pathway diagrams.",poster,cp33
Computer Science,p1595,d3,4097c531036fcda230e0174a93565e54684e0e54,c33,Workshop on Python for High-Performance and Scientific Computing,The MetaCyc database of metabolic pathways and enzymes - a 2019 update,"Abstract MetaCyc (MetaCyc.org) is a comprehensive reference database of metabolic pathways and enzymes from all domains of life. It contains 2749 pathways derived from more than 60 000 publications, making it the largest curated collection of metabolic pathways. The data in MetaCyc are evidence-based and richly curated, resulting in an encyclopedic reference tool for metabolism. MetaCyc is also used as a knowledge base for generating thousands of organism-specific Pathway/Genome Databases (PGDBs), which are available in BioCyc.org and other genomic portals. This article provides an update on the developments in MetaCyc during September 2017 to August 2019, up to version 23.1. Some of the topics that received intensive curation during this period include cobamides biosynthesis, sterol metabolism, fatty acid biosynthesis, lipid metabolism, carotenoid metabolism, protein glycosylation, antibiotics and cytotoxins biosynthesis, siderophore biosynthesis, bioluminescence, vitamin K metabolism, brominated compound metabolism, plant secondary metabolism and human metabolism. Other additions include modifications to the GlycanBuilder software that enable displaying glycans using symbolic representation, improved graphics and fonts for web displays, improvements in the PathoLogic component of Pathway Tools, and the optional addition of regulatory information to pathway diagrams.",poster,cp33
Biology,p1595,d5,4097c531036fcda230e0174a93565e54684e0e54,c33,Workshop on Python for High-Performance and Scientific Computing,The MetaCyc database of metabolic pathways and enzymes - a 2019 update,"Abstract MetaCyc (MetaCyc.org) is a comprehensive reference database of metabolic pathways and enzymes from all domains of life. It contains 2749 pathways derived from more than 60 000 publications, making it the largest curated collection of metabolic pathways. The data in MetaCyc are evidence-based and richly curated, resulting in an encyclopedic reference tool for metabolism. MetaCyc is also used as a knowledge base for generating thousands of organism-specific Pathway/Genome Databases (PGDBs), which are available in BioCyc.org and other genomic portals. This article provides an update on the developments in MetaCyc during September 2017 to August 2019, up to version 23.1. Some of the topics that received intensive curation during this period include cobamides biosynthesis, sterol metabolism, fatty acid biosynthesis, lipid metabolism, carotenoid metabolism, protein glycosylation, antibiotics and cytotoxins biosynthesis, siderophore biosynthesis, bioluminescence, vitamin K metabolism, brominated compound metabolism, plant secondary metabolism and human metabolism. Other additions include modifications to the GlycanBuilder software that enable displaying glycans using symbolic representation, improved graphics and fonts for web displays, improvements in the PathoLogic component of Pathway Tools, and the optional addition of regulatory information to pathway diagrams.",poster,cp33
Medicine,p1596,d1,eb960b5d56ed1368991eaa4f40cb7afee66edb1f,c73,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,ONCOMINE: a cancer microarray database and integrated data-mining platform.,Abstract,poster,cp73
Biology,p1596,d5,eb960b5d56ed1368991eaa4f40cb7afee66edb1f,c73,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,ONCOMINE: a cancer microarray database and integrated data-mining platform.,Abstract,poster,cp73
Medicine,p1597,d1,26c075104d0ea1177cce4bd2d5c5d9eef93b8a3b,c61,"International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems","The MEROPS database of proteolytic enzymes, their substrates and inhibitors in 2017 and a comparison with peptidases in the PANTHER database","Abstract The MEROPS database (http://www.ebi.ac.uk/merops/) is an integrated source of information about peptidases, their substrates and inhibitors. The hierarchical classification is: protein-species, family, clan, with an identifier at each level. The MEROPS website moved to the EMBL-EBI in 2017, requiring refactoring of the code-base and services provided. The interface to sequence searching has changed and the MEROPS protein sequence libraries can be searched at the EMBL-EBI with HMMER, FastA and BLASTP. Cross-references have been established between MEROPS and the PANTHER database at both the family and protein-species level, which will help to improve curation and coverage between the resources. Because of the increasing size of the MEROPS sequence collection, in future only sequences of characterized proteins, and from completely sequenced genomes of organisms of evolutionary, medical or commercial significance will be added. As an example, peptidase homologues in four proteomes from the Asgard superphylum of Archaea have been identified and compared to other archaean, bacterial and eukaryote proteomes. This has given insights into the origins and evolution of peptidase families, including an expansion in the number of proteasome components in Asgard archaeotes and as organisms increase in complexity. Novel structures for proteasome complexes in archaea are postulated.",poster,cp61
Computer Science,p1597,d3,26c075104d0ea1177cce4bd2d5c5d9eef93b8a3b,c61,"International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems","The MEROPS database of proteolytic enzymes, their substrates and inhibitors in 2017 and a comparison with peptidases in the PANTHER database","Abstract The MEROPS database (http://www.ebi.ac.uk/merops/) is an integrated source of information about peptidases, their substrates and inhibitors. The hierarchical classification is: protein-species, family, clan, with an identifier at each level. The MEROPS website moved to the EMBL-EBI in 2017, requiring refactoring of the code-base and services provided. The interface to sequence searching has changed and the MEROPS protein sequence libraries can be searched at the EMBL-EBI with HMMER, FastA and BLASTP. Cross-references have been established between MEROPS and the PANTHER database at both the family and protein-species level, which will help to improve curation and coverage between the resources. Because of the increasing size of the MEROPS sequence collection, in future only sequences of characterized proteins, and from completely sequenced genomes of organisms of evolutionary, medical or commercial significance will be added. As an example, peptidase homologues in four proteomes from the Asgard superphylum of Archaea have been identified and compared to other archaean, bacterial and eukaryote proteomes. This has given insights into the origins and evolution of peptidase families, including an expansion in the number of proteasome components in Asgard archaeotes and as organisms increase in complexity. Novel structures for proteasome complexes in archaea are postulated.",poster,cp61
Biology,p1597,d5,26c075104d0ea1177cce4bd2d5c5d9eef93b8a3b,c61,"International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems","The MEROPS database of proteolytic enzymes, their substrates and inhibitors in 2017 and a comparison with peptidases in the PANTHER database","Abstract The MEROPS database (http://www.ebi.ac.uk/merops/) is an integrated source of information about peptidases, their substrates and inhibitors. The hierarchical classification is: protein-species, family, clan, with an identifier at each level. The MEROPS website moved to the EMBL-EBI in 2017, requiring refactoring of the code-base and services provided. The interface to sequence searching has changed and the MEROPS protein sequence libraries can be searched at the EMBL-EBI with HMMER, FastA and BLASTP. Cross-references have been established between MEROPS and the PANTHER database at both the family and protein-species level, which will help to improve curation and coverage between the resources. Because of the increasing size of the MEROPS sequence collection, in future only sequences of characterized proteins, and from completely sequenced genomes of organisms of evolutionary, medical or commercial significance will be added. As an example, peptidase homologues in four proteomes from the Asgard superphylum of Archaea have been identified and compared to other archaean, bacterial and eukaryote proteomes. This has given insights into the origins and evolution of peptidase families, including an expansion in the number of proteasome components in Asgard archaeotes and as organisms increase in complexity. Novel structures for proteasome complexes in archaea are postulated.",poster,cp61
Medicine,p1599,d1,f89df7381ea8febb419fae473725e44931f6b22c,j364,BMC Genomics,"Generation and analysis of a 29,745 unique Expressed Sequence Tags from the Pacific oyster (Crassostrea gigas) assembled into a publicly accessible database: the GigasDatabase",Abstract,fullPaper,jv364
Biology,p1599,d5,f89df7381ea8febb419fae473725e44931f6b22c,j364,BMC Genomics,"Generation and analysis of a 29,745 unique Expressed Sequence Tags from the Pacific oyster (Crassostrea gigas) assembled into a publicly accessible database: the GigasDatabase",Abstract,fullPaper,jv364
Medicine,p1600,d1,2ecbb9d6c6e698dd51134e081fa836319801ae27,j19,Scientific Data,"The eICU Collaborative Research Database, a freely available multi-center database for critical care research",Abstract,fullPaper,jv19
Computer Science,p1600,d3,2ecbb9d6c6e698dd51134e081fa836319801ae27,j19,Scientific Data,"The eICU Collaborative Research Database, a freely available multi-center database for critical care research",Abstract,fullPaper,jv19
Medicine,p1601,d1,12d8a9991ee7aecc65bc0991959c5b58a367b2ae,c115,International Conference on Information Integration and Web-based Applications & Services,APD3: the antimicrobial peptide database as a tool for research and education,"The antimicrobial peptide database (APD, http://aps.unmc.edu/AP/) is an original database initially online in 2003. The APD2 (2009 version) has been regularly updated and further expanded into the APD3. This database currently focuses on natural antimicrobial peptides (AMPs) with defined sequence and activity. It includes a total of 2619 AMPs with 261 bacteriocins from bacteria, 4 AMPs from archaea, 7 from protists, 13 from fungi, 321 from plants and 1972 animal host defense peptides. The APD3 contains 2169 antibacterial, 172 antiviral, 105 anti-HIV, 959 antifungal, 80 antiparasitic and 185 anticancer peptides. Newly annotated are AMPs with antibiofilm, antimalarial, anti-protist, insecticidal, spermicidal, chemotactic, wound healing, antioxidant and protease inhibiting properties. We also describe other searchable annotations, including target pathogens, molecule-binding partners, post-translational modifications and animal models. Amino acid profiles or signatures of natural AMPs are important for peptide classification, prediction and design. Finally, we summarize various database applications in research and education.",poster,cp115
Computer Science,p1601,d3,12d8a9991ee7aecc65bc0991959c5b58a367b2ae,c115,International Conference on Information Integration and Web-based Applications & Services,APD3: the antimicrobial peptide database as a tool for research and education,"The antimicrobial peptide database (APD, http://aps.unmc.edu/AP/) is an original database initially online in 2003. The APD2 (2009 version) has been regularly updated and further expanded into the APD3. This database currently focuses on natural antimicrobial peptides (AMPs) with defined sequence and activity. It includes a total of 2619 AMPs with 261 bacteriocins from bacteria, 4 AMPs from archaea, 7 from protists, 13 from fungi, 321 from plants and 1972 animal host defense peptides. The APD3 contains 2169 antibacterial, 172 antiviral, 105 anti-HIV, 959 antifungal, 80 antiparasitic and 185 anticancer peptides. Newly annotated are AMPs with antibiofilm, antimalarial, anti-protist, insecticidal, spermicidal, chemotactic, wound healing, antioxidant and protease inhibiting properties. We also describe other searchable annotations, including target pathogens, molecule-binding partners, post-translational modifications and animal models. Amino acid profiles or signatures of natural AMPs are important for peptide classification, prediction and design. Finally, we summarize various database applications in research and education.",poster,cp115
Biology,p1601,d5,12d8a9991ee7aecc65bc0991959c5b58a367b2ae,c115,International Conference on Information Integration and Web-based Applications & Services,APD3: the antimicrobial peptide database as a tool for research and education,"The antimicrobial peptide database (APD, http://aps.unmc.edu/AP/) is an original database initially online in 2003. The APD2 (2009 version) has been regularly updated and further expanded into the APD3. This database currently focuses on natural antimicrobial peptides (AMPs) with defined sequence and activity. It includes a total of 2619 AMPs with 261 bacteriocins from bacteria, 4 AMPs from archaea, 7 from protists, 13 from fungi, 321 from plants and 1972 animal host defense peptides. The APD3 contains 2169 antibacterial, 172 antiviral, 105 anti-HIV, 959 antifungal, 80 antiparasitic and 185 anticancer peptides. Newly annotated are AMPs with antibiofilm, antimalarial, anti-protist, insecticidal, spermicidal, chemotactic, wound healing, antioxidant and protease inhibiting properties. We also describe other searchable annotations, including target pathogens, molecule-binding partners, post-translational modifications and animal models. Amino acid profiles or signatures of natural AMPs are important for peptide classification, prediction and design. Finally, we summarize various database applications in research and education.",poster,cp115
Medicine,p1603,d1,3f376a9b2d659e52c98d911a1fb3aa3a834f66e5,c10,Americas Conference on Information Systems,Database resources of the National Center for Biotechnology Information,"The National Center for Biotechnology Information (NCBI) provides a large suite of online resources for biological information and data, including the GenBank® nucleic acid sequence database and the PubMed database of citations and abstracts for published life science journals. Additional NCBI resources focus on literature (Bookshelf, PubMed Central (PMC) and PubReader); medical genetics (ClinVar, dbMHC, the Genetic Testing Registry, HIV-1/Human Protein Interaction Database and MedGen); genes and genomics (BioProject, BioSample, dbSNP, dbVar, Epigenomics, Gene, Gene Expression Omnibus (GEO), Genome, HomoloGene, the Map Viewer, Nucleotide, PopSet, Probe, RefSeq, Sequence Read Archive, the Taxonomy Browser, Trace Archive and UniGene); and proteins and chemicals (Biosystems, COBALT, the Conserved Domain Database (CDD), the Conserved Domain Architecture Retrieval Tool (CDART), the Molecular Modeling Database (MMDB), Protein Clusters, Protein and the PubChem suite of small molecule databases). The Entrez system provides search and retrieval operations for many of these databases. Augmenting many of the Web applications are custom implementations of the BLAST program optimized to search specialized data sets. All of these resources can be accessed through the NCBI home page at http://www.ncbi.nlm.nih.gov.",poster,cp10
Computer Science,p1603,d3,3f376a9b2d659e52c98d911a1fb3aa3a834f66e5,c10,Americas Conference on Information Systems,Database resources of the National Center for Biotechnology Information,"The National Center for Biotechnology Information (NCBI) provides a large suite of online resources for biological information and data, including the GenBank® nucleic acid sequence database and the PubMed database of citations and abstracts for published life science journals. Additional NCBI resources focus on literature (Bookshelf, PubMed Central (PMC) and PubReader); medical genetics (ClinVar, dbMHC, the Genetic Testing Registry, HIV-1/Human Protein Interaction Database and MedGen); genes and genomics (BioProject, BioSample, dbSNP, dbVar, Epigenomics, Gene, Gene Expression Omnibus (GEO), Genome, HomoloGene, the Map Viewer, Nucleotide, PopSet, Probe, RefSeq, Sequence Read Archive, the Taxonomy Browser, Trace Archive and UniGene); and proteins and chemicals (Biosystems, COBALT, the Conserved Domain Database (CDD), the Conserved Domain Architecture Retrieval Tool (CDART), the Molecular Modeling Database (MMDB), Protein Clusters, Protein and the PubChem suite of small molecule databases). The Entrez system provides search and retrieval operations for many of these databases. Augmenting many of the Web applications are custom implementations of the BLAST program optimized to search specialized data sets. All of these resources can be accessed through the NCBI home page at http://www.ncbi.nlm.nih.gov.",poster,cp10
Medicine,p1605,d1,2c0aaeb420e1cd2d767a1797b2ded62e0d2ee426,j365,Plant Physiology,GENEVESTIGATOR. Arabidopsis Microarray Database and Analysis Toolbox1[w],"High-throughput gene expression analysis has become a frequent and powerful research tool in biology. At present, however, few software applications have been developed for biologists to query large microarray gene expression databases using a Web-browser interface. We present GENEVESTIGATOR, a database and Web-browser data mining interface for Affymetrix GeneChip data. Users can query the database to retrieve the expression patterns of individual genes throughout chosen environmental conditions, growth stages, or organs. Reversely, mining tools allow users to identify genes specifically expressed during selected stresses, growth stages, or in particular organs. Using GENEVESTIGATOR, the gene expression profiles of more than 22,000 Arabidopsis genes can be obtained, including those of 10,600 currently uncharacterized genes. The objective of this software application is to direct gene functional discovery and design of new experiments by providing plant biologists with contextual information on the expression of genes. The database and analysis toolbox is available as a community resource at https://www.genevestigator.ethz.ch.",fullPaper,jv365
Biology,p1605,d5,2c0aaeb420e1cd2d767a1797b2ded62e0d2ee426,j365,Plant Physiology,GENEVESTIGATOR. Arabidopsis Microarray Database and Analysis Toolbox1[w],"High-throughput gene expression analysis has become a frequent and powerful research tool in biology. At present, however, few software applications have been developed for biologists to query large microarray gene expression databases using a Web-browser interface. We present GENEVESTIGATOR, a database and Web-browser data mining interface for Affymetrix GeneChip data. Users can query the database to retrieve the expression patterns of individual genes throughout chosen environmental conditions, growth stages, or organs. Reversely, mining tools allow users to identify genes specifically expressed during selected stresses, growth stages, or in particular organs. Using GENEVESTIGATOR, the gene expression profiles of more than 22,000 Arabidopsis genes can be obtained, including those of 10,600 currently uncharacterized genes. The objective of this software application is to direct gene functional discovery and design of new experiments by providing plant biologists with contextual information on the expression of genes. The database and analysis toolbox is available as a community resource at https://www.genevestigator.ethz.ch.",fullPaper,jv365
Medicine,p1607,d1,41e2692d9ac1434d1841d5a29e7ccb927b82b677,c14,Hawaii International Conference on System Sciences,The Comparative Toxicogenomics Database: update 2019,"Abstract The Comparative Toxicogenomics Database (CTD; http://ctdbase.org/) is a premier public resource for literature-based, manually curated associations between chemicals, gene products, phenotypes, diseases, and environmental exposures. In this biennial update, we present our new chemical–phenotype module that codes chemical-induced effects on phenotypes, curated using controlled vocabularies for chemicals, phenotypes, taxa, and anatomical descriptors; this module provides unique opportunities to explore cellular and system-level phenotypes of the pre-disease state and allows users to construct predictive adverse outcome pathways (linking chemical–gene molecular initiating events with phenotypic key events, diseases, and population-level health outcomes). We also report a 46% increase in CTD manually curated content, which when integrated with other datasets yields more than 38 million toxicogenomic relationships. We describe new querying and display features for our enhanced chemical–exposure science module, providing greater scope of content and utility. As well, we discuss an updated MEDIC disease vocabulary with over 1700 new terms and accession identifiers. To accommodate these increases in data content and functionality, CTD has upgraded its computational infrastructure. These updates continue to improve CTD and help inform new testable hypotheses about the etiology and mechanisms underlying environmentally influenced diseases.",poster,cp14
Computer Science,p1607,d3,41e2692d9ac1434d1841d5a29e7ccb927b82b677,c14,Hawaii International Conference on System Sciences,The Comparative Toxicogenomics Database: update 2019,"Abstract The Comparative Toxicogenomics Database (CTD; http://ctdbase.org/) is a premier public resource for literature-based, manually curated associations between chemicals, gene products, phenotypes, diseases, and environmental exposures. In this biennial update, we present our new chemical–phenotype module that codes chemical-induced effects on phenotypes, curated using controlled vocabularies for chemicals, phenotypes, taxa, and anatomical descriptors; this module provides unique opportunities to explore cellular and system-level phenotypes of the pre-disease state and allows users to construct predictive adverse outcome pathways (linking chemical–gene molecular initiating events with phenotypic key events, diseases, and population-level health outcomes). We also report a 46% increase in CTD manually curated content, which when integrated with other datasets yields more than 38 million toxicogenomic relationships. We describe new querying and display features for our enhanced chemical–exposure science module, providing greater scope of content and utility. As well, we discuss an updated MEDIC disease vocabulary with over 1700 new terms and accession identifiers. To accommodate these increases in data content and functionality, CTD has upgraded its computational infrastructure. These updates continue to improve CTD and help inform new testable hypotheses about the etiology and mechanisms underlying environmentally influenced diseases.",poster,cp14
Biology,p1607,d5,41e2692d9ac1434d1841d5a29e7ccb927b82b677,c14,Hawaii International Conference on System Sciences,The Comparative Toxicogenomics Database: update 2019,"Abstract The Comparative Toxicogenomics Database (CTD; http://ctdbase.org/) is a premier public resource for literature-based, manually curated associations between chemicals, gene products, phenotypes, diseases, and environmental exposures. In this biennial update, we present our new chemical–phenotype module that codes chemical-induced effects on phenotypes, curated using controlled vocabularies for chemicals, phenotypes, taxa, and anatomical descriptors; this module provides unique opportunities to explore cellular and system-level phenotypes of the pre-disease state and allows users to construct predictive adverse outcome pathways (linking chemical–gene molecular initiating events with phenotypic key events, diseases, and population-level health outcomes). We also report a 46% increase in CTD manually curated content, which when integrated with other datasets yields more than 38 million toxicogenomic relationships. We describe new querying and display features for our enhanced chemical–exposure science module, providing greater scope of content and utility. As well, we discuss an updated MEDIC disease vocabulary with over 1700 new terms and accession identifiers. To accommodate these increases in data content and functionality, CTD has upgraded its computational infrastructure. These updates continue to improve CTD and help inform new testable hypotheses about the etiology and mechanisms underlying environmentally influenced diseases.",poster,cp14
Medicine,p1611,d1,58a93e9cd60ce331606d31ebed62599a2b7db805,c61,"International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems",The SWISS-PROT protein sequence database and its supplement TrEMBL in 2000,"SWISS-PROT is a curated protein sequence database which strives to provide a high level of annotation (such as the description of the function of a protein, its domains structure, post-translational modifications, variants, etc.), a minimal level of redundancy and high level of integration with other databases. Recent developments of the database include format and content enhancements, cross-references to additional databases, new documentation files and improvements to TrEMBL, a computer-annotated supplement to SWISS-PROT. TrEMBL consists of entries in SWISS-PROT-like format derived from the translation of all coding sequences (CDSs) in the EMBL Nucleotide Sequence Database, except the CDSs already included in SWISS-PROT. We also describe the Human Proteomics Initiative (HPI), a major project to annotate all known human sequences according to the quality standards of SWISS-PROT. SWISS-PROT is available at: http://www.expasy.ch/sprot/ and http://www.ebi.ac.uk/swissprot/",poster,cp61
Computer Science,p1611,d3,58a93e9cd60ce331606d31ebed62599a2b7db805,c61,"International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems",The SWISS-PROT protein sequence database and its supplement TrEMBL in 2000,"SWISS-PROT is a curated protein sequence database which strives to provide a high level of annotation (such as the description of the function of a protein, its domains structure, post-translational modifications, variants, etc.), a minimal level of redundancy and high level of integration with other databases. Recent developments of the database include format and content enhancements, cross-references to additional databases, new documentation files and improvements to TrEMBL, a computer-annotated supplement to SWISS-PROT. TrEMBL consists of entries in SWISS-PROT-like format derived from the translation of all coding sequences (CDSs) in the EMBL Nucleotide Sequence Database, except the CDSs already included in SWISS-PROT. We also describe the Human Proteomics Initiative (HPI), a major project to annotate all known human sequences according to the quality standards of SWISS-PROT. SWISS-PROT is available at: http://www.expasy.ch/sprot/ and http://www.ebi.ac.uk/swissprot/",poster,cp61
Biology,p1611,d5,58a93e9cd60ce331606d31ebed62599a2b7db805,c61,"International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems",The SWISS-PROT protein sequence database and its supplement TrEMBL in 2000,"SWISS-PROT is a curated protein sequence database which strives to provide a high level of annotation (such as the description of the function of a protein, its domains structure, post-translational modifications, variants, etc.), a minimal level of redundancy and high level of integration with other databases. Recent developments of the database include format and content enhancements, cross-references to additional databases, new documentation files and improvements to TrEMBL, a computer-annotated supplement to SWISS-PROT. TrEMBL consists of entries in SWISS-PROT-like format derived from the translation of all coding sequences (CDSs) in the EMBL Nucleotide Sequence Database, except the CDSs already included in SWISS-PROT. We also describe the Human Proteomics Initiative (HPI), a major project to annotate all known human sequences according to the quality standards of SWISS-PROT. SWISS-PROT is available at: http://www.expasy.ch/sprot/ and http://www.ebi.ac.uk/swissprot/",poster,cp61
Medicine,p1612,d1,137832dd10d669a300b7751e0ed3e3b91172372a,c104,North American Chapter of the Association for Computational Linguistics,Human protein reference database—2006 update,"Human Protein Reference Database (HPRD) () was developed to serve as a comprehensive collection of protein features, post-translational modifications (PTMs) and protein–protein interactions. Since the original report, this database has increased to >20 000 proteins entries and has become the largest database for literature-derived protein–protein interactions (>30 000) and PTMs (>8000) for human proteins. We have also introduced several new features in HPRD including: (i) protein isoforms, (ii) enhanced search options, (iii) linking of pathway annotations and (iv) integration of a novel browser, GenProt Viewer (), developed by us that allows integration of genomic and proteomic information. With the continued support and active participation by the biomedical community, we expect HPRD to become a unique source of curated information for the human proteome and spur biomedical discoveries based on integration of genomic, transcriptomic and proteomic data.",poster,cp104
Computer Science,p1612,d3,137832dd10d669a300b7751e0ed3e3b91172372a,c104,North American Chapter of the Association for Computational Linguistics,Human protein reference database—2006 update,"Human Protein Reference Database (HPRD) () was developed to serve as a comprehensive collection of protein features, post-translational modifications (PTMs) and protein–protein interactions. Since the original report, this database has increased to >20 000 proteins entries and has become the largest database for literature-derived protein–protein interactions (>30 000) and PTMs (>8000) for human proteins. We have also introduced several new features in HPRD including: (i) protein isoforms, (ii) enhanced search options, (iii) linking of pathway annotations and (iv) integration of a novel browser, GenProt Viewer (), developed by us that allows integration of genomic and proteomic information. With the continued support and active participation by the biomedical community, we expect HPRD to become a unique source of curated information for the human proteome and spur biomedical discoveries based on integration of genomic, transcriptomic and proteomic data.",poster,cp104
Biology,p1612,d5,137832dd10d669a300b7751e0ed3e3b91172372a,c104,North American Chapter of the Association for Computational Linguistics,Human protein reference database—2006 update,"Human Protein Reference Database (HPRD) () was developed to serve as a comprehensive collection of protein features, post-translational modifications (PTMs) and protein–protein interactions. Since the original report, this database has increased to >20 000 proteins entries and has become the largest database for literature-derived protein–protein interactions (>30 000) and PTMs (>8000) for human proteins. We have also introduced several new features in HPRD including: (i) protein isoforms, (ii) enhanced search options, (iii) linking of pathway annotations and (iv) integration of a novel browser, GenProt Viewer (), developed by us that allows integration of genomic and proteomic information. With the continued support and active participation by the biomedical community, we expect HPRD to become a unique source of curated information for the human proteome and spur biomedical discoveries based on integration of genomic, transcriptomic and proteomic data.",poster,cp104
Medicine,p1613,d1,aca92bbbe7ebc8b6b0f272aca209ac5a027222bd,c28,International Conference on Contemporary Computing,The IPD and IMGT/HLA database: allele variant databases,"The Immuno Polymorphism Database (IPD) was developed to provide a centralized system for the study of polymorphism in genes of the immune system. Through the IPD project we have established a central platform for the curation and publication of locus-specific databases involved either directly or related to the function of the Major Histocompatibility Complex in a number of different species. We have collaborated with specialist groups or nomenclature committees that curate the individual sections before they are submitted to IPD for online publication. IPD consists of five core databases, with the IMGT/HLA Database as the primary database. Through the work of the various nomenclature committees, the HLA Informatics Group and in collaboration with the European Bioinformatics Institute we are able to provide public access to this data through the website http://www.ebi.ac.uk/ipd/. The IPD project continues to develop with new tools being added to address scientific developments, such as Next Generation Sequencing, and to address user feedback and requests. Regular updates to the website ensure that new and confirmatory sequences are dispersed to the immunogenetics community, and the wider research and clinical communities.",poster,cp28
Computer Science,p1613,d3,aca92bbbe7ebc8b6b0f272aca209ac5a027222bd,c28,International Conference on Contemporary Computing,The IPD and IMGT/HLA database: allele variant databases,"The Immuno Polymorphism Database (IPD) was developed to provide a centralized system for the study of polymorphism in genes of the immune system. Through the IPD project we have established a central platform for the curation and publication of locus-specific databases involved either directly or related to the function of the Major Histocompatibility Complex in a number of different species. We have collaborated with specialist groups or nomenclature committees that curate the individual sections before they are submitted to IPD for online publication. IPD consists of five core databases, with the IMGT/HLA Database as the primary database. Through the work of the various nomenclature committees, the HLA Informatics Group and in collaboration with the European Bioinformatics Institute we are able to provide public access to this data through the website http://www.ebi.ac.uk/ipd/. The IPD project continues to develop with new tools being added to address scientific developments, such as Next Generation Sequencing, and to address user feedback and requests. Regular updates to the website ensure that new and confirmatory sequences are dispersed to the immunogenetics community, and the wider research and clinical communities.",poster,cp28
Biology,p1613,d5,aca92bbbe7ebc8b6b0f272aca209ac5a027222bd,c28,International Conference on Contemporary Computing,The IPD and IMGT/HLA database: allele variant databases,"The Immuno Polymorphism Database (IPD) was developed to provide a centralized system for the study of polymorphism in genes of the immune system. Through the IPD project we have established a central platform for the curation and publication of locus-specific databases involved either directly or related to the function of the Major Histocompatibility Complex in a number of different species. We have collaborated with specialist groups or nomenclature committees that curate the individual sections before they are submitted to IPD for online publication. IPD consists of five core databases, with the IMGT/HLA Database as the primary database. Through the work of the various nomenclature committees, the HLA Informatics Group and in collaboration with the European Bioinformatics Institute we are able to provide public access to this data through the website http://www.ebi.ac.uk/ipd/. The IPD project continues to develop with new tools being added to address scientific developments, such as Next Generation Sequencing, and to address user feedback and requests. Regular updates to the website ensure that new and confirmatory sequences are dispersed to the immunogenetics community, and the wider research and clinical communities.",poster,cp28
Medicine,p1618,d1,4c987ffb492e44acc010cbeb2347b92e257d7b59,c56,International Conference on Automated Software Engineering,HMDB: the Human Metabolome Database,"The Human Metabolome Database (HMDB) is currently the most complete and comprehensive curated collection of human metabolite and human metabolism data in the world. It contains records for more than 2180 endogenous metabolites with information gathered from thousands of books, journal articles and electronic databases. In addition to its comprehensive literature-derived data, the HMDB also contains an extensive collection of experimental metabolite concentration data compiled from hundreds of mass spectra (MS) and Nuclear Magnetic resonance (NMR) metabolomic analyses performed on urine, blood and cerebrospinal fluid samples. This is further supplemented with thousands of NMR and MS spectra collected on purified, reference metabolites. Each metabolite entry in the HMDB contains an average of 90 separate data fields including a comprehensive compound description, names and synonyms, structural information, physico-chemical data, reference NMR and MS spectra, biofluid concentrations, disease associations, pathway information, enzyme data, gene sequence data, SNP and mutation data as well as extensive links to images, references and other public databases. Extensive searching, relational querying and data browsing tools are also provided. The HMDB is designed to address the broad needs of biochemists, clinical chemists, physicians, medical geneticists, nutritionists and members of the metabolomics community. The HMDB is available at:",poster,cp56
Computer Science,p1618,d3,4c987ffb492e44acc010cbeb2347b92e257d7b59,c56,International Conference on Automated Software Engineering,HMDB: the Human Metabolome Database,"The Human Metabolome Database (HMDB) is currently the most complete and comprehensive curated collection of human metabolite and human metabolism data in the world. It contains records for more than 2180 endogenous metabolites with information gathered from thousands of books, journal articles and electronic databases. In addition to its comprehensive literature-derived data, the HMDB also contains an extensive collection of experimental metabolite concentration data compiled from hundreds of mass spectra (MS) and Nuclear Magnetic resonance (NMR) metabolomic analyses performed on urine, blood and cerebrospinal fluid samples. This is further supplemented with thousands of NMR and MS spectra collected on purified, reference metabolites. Each metabolite entry in the HMDB contains an average of 90 separate data fields including a comprehensive compound description, names and synonyms, structural information, physico-chemical data, reference NMR and MS spectra, biofluid concentrations, disease associations, pathway information, enzyme data, gene sequence data, SNP and mutation data as well as extensive links to images, references and other public databases. Extensive searching, relational querying and data browsing tools are also provided. The HMDB is designed to address the broad needs of biochemists, clinical chemists, physicians, medical geneticists, nutritionists and members of the metabolomics community. The HMDB is available at:",poster,cp56
Biology,p1618,d5,4c987ffb492e44acc010cbeb2347b92e257d7b59,c56,International Conference on Automated Software Engineering,HMDB: the Human Metabolome Database,"The Human Metabolome Database (HMDB) is currently the most complete and comprehensive curated collection of human metabolite and human metabolism data in the world. It contains records for more than 2180 endogenous metabolites with information gathered from thousands of books, journal articles and electronic databases. In addition to its comprehensive literature-derived data, the HMDB also contains an extensive collection of experimental metabolite concentration data compiled from hundreds of mass spectra (MS) and Nuclear Magnetic resonance (NMR) metabolomic analyses performed on urine, blood and cerebrospinal fluid samples. This is further supplemented with thousands of NMR and MS spectra collected on purified, reference metabolites. Each metabolite entry in the HMDB contains an average of 90 separate data fields including a comprehensive compound description, names and synonyms, structural information, physico-chemical data, reference NMR and MS spectra, biofluid concentrations, disease associations, pathway information, enzyme data, gene sequence data, SNP and mutation data as well as extensive links to images, references and other public databases. Extensive searching, relational querying and data browsing tools are also provided. The HMDB is designed to address the broad needs of biochemists, clinical chemists, physicians, medical geneticists, nutritionists and members of the metabolomics community. The HMDB is available at:",poster,cp56
Medicine,p1619,d1,3ef0c7784bf446de5ce5977a35f86c8b30fd668f,j367,Systematic Reviews,Optimal database combinations for literature searches in systematic reviews: a prospective exploratory study,Abstract,fullPaper,jv367
Medicine,p1624,d1,b92ed5d8103faf9ccdfd5d3c4f60b722866038d0,c91,International Symposium on High-Performance Computer Architecture,PhenoScanner: a database of human genotype–phenotype associations,"Abstract Summary: PhenoScanner is a curated database of publicly available results from large-scale genetic association studies. This tool aims to facilitate ‘phenome scans’, the cross-referencing of genetic variants with many phenotypes, to help aid understanding of disease pathways and biology. The database currently contains over 350 million association results and over 10 million unique genetic variants, mostly single nucleotide polymorphisms. It is accompanied by a web-based tool that queries the database for associations with user-specified variants, providing results according to the same effect and non-effect alleles for each input variant. The tool provides the option of searching for trait associations with proxies of the input variants, calculated using the European samples from 1000 Genomes and Hapmap. Availability and Implementation: PhenoScanner is available at www.phenoscanner.medschl.cam.ac.uk. Contact: jrs95@medschl.cam.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.",poster,cp91
Computer Science,p1624,d3,b92ed5d8103faf9ccdfd5d3c4f60b722866038d0,c91,International Symposium on High-Performance Computer Architecture,PhenoScanner: a database of human genotype–phenotype associations,"Abstract Summary: PhenoScanner is a curated database of publicly available results from large-scale genetic association studies. This tool aims to facilitate ‘phenome scans’, the cross-referencing of genetic variants with many phenotypes, to help aid understanding of disease pathways and biology. The database currently contains over 350 million association results and over 10 million unique genetic variants, mostly single nucleotide polymorphisms. It is accompanied by a web-based tool that queries the database for associations with user-specified variants, providing results according to the same effect and non-effect alleles for each input variant. The tool provides the option of searching for trait associations with proxies of the input variants, calculated using the European samples from 1000 Genomes and Hapmap. Availability and Implementation: PhenoScanner is available at www.phenoscanner.medschl.cam.ac.uk. Contact: jrs95@medschl.cam.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.",poster,cp91
Political Science,p1624,d15,b92ed5d8103faf9ccdfd5d3c4f60b722866038d0,c91,International Symposium on High-Performance Computer Architecture,PhenoScanner: a database of human genotype–phenotype associations,"Abstract Summary: PhenoScanner is a curated database of publicly available results from large-scale genetic association studies. This tool aims to facilitate ‘phenome scans’, the cross-referencing of genetic variants with many phenotypes, to help aid understanding of disease pathways and biology. The database currently contains over 350 million association results and over 10 million unique genetic variants, mostly single nucleotide polymorphisms. It is accompanied by a web-based tool that queries the database for associations with user-specified variants, providing results according to the same effect and non-effect alleles for each input variant. The tool provides the option of searching for trait associations with proxies of the input variants, calculated using the European samples from 1000 Genomes and Hapmap. Availability and Implementation: PhenoScanner is available at www.phenoscanner.medschl.cam.ac.uk. Contact: jrs95@medschl.cam.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.",poster,cp91
Medicine,p1625,d1,0414f6ffc086bf6c2015176d4b46a051d436cd2b,c63,International Conference on Evaluation & Assessment in Software Engineering,Mouse Genome Database (MGD) 2019,"Abstract The Mouse Genome Database (MGD; http://www.informatics.jax.org) is the community model organism genetic and genome resource for the laboratory mouse. MGD is the authoritative source for biological reference data sets related to mouse genes, gene functions, phenotypes, and mouse models of human disease. MGD is the primary outlet for official gene, allele and mouse strain nomenclature based on the guidelines set by the International Committee on Standardized Nomenclature for Mice. In this report we describe significant enhancements to MGD, including two new graphical user interfaces: (i) the Multi Genome Viewer for exploring the genomes of multiple mouse strains and (ii) the Phenotype-Gene Expression matrix which was developed in collaboration with the Gene Expression Database (GXD) and allows researchers to compare gene expression and phenotype annotations for mouse genes. Other recent improvements include enhanced efficiency of our literature curation processes and the incorporation of Transcriptional Start Site (TSS) annotations from RIKEN’s FANTOM 5 initiative.",poster,cp63
Computer Science,p1625,d3,0414f6ffc086bf6c2015176d4b46a051d436cd2b,c63,International Conference on Evaluation & Assessment in Software Engineering,Mouse Genome Database (MGD) 2019,"Abstract The Mouse Genome Database (MGD; http://www.informatics.jax.org) is the community model organism genetic and genome resource for the laboratory mouse. MGD is the authoritative source for biological reference data sets related to mouse genes, gene functions, phenotypes, and mouse models of human disease. MGD is the primary outlet for official gene, allele and mouse strain nomenclature based on the guidelines set by the International Committee on Standardized Nomenclature for Mice. In this report we describe significant enhancements to MGD, including two new graphical user interfaces: (i) the Multi Genome Viewer for exploring the genomes of multiple mouse strains and (ii) the Phenotype-Gene Expression matrix which was developed in collaboration with the Gene Expression Database (GXD) and allows researchers to compare gene expression and phenotype annotations for mouse genes. Other recent improvements include enhanced efficiency of our literature curation processes and the incorporation of Transcriptional Start Site (TSS) annotations from RIKEN’s FANTOM 5 initiative.",poster,cp63
Biology,p1625,d5,0414f6ffc086bf6c2015176d4b46a051d436cd2b,c63,International Conference on Evaluation & Assessment in Software Engineering,Mouse Genome Database (MGD) 2019,"Abstract The Mouse Genome Database (MGD; http://www.informatics.jax.org) is the community model organism genetic and genome resource for the laboratory mouse. MGD is the authoritative source for biological reference data sets related to mouse genes, gene functions, phenotypes, and mouse models of human disease. MGD is the primary outlet for official gene, allele and mouse strain nomenclature based on the guidelines set by the International Committee on Standardized Nomenclature for Mice. In this report we describe significant enhancements to MGD, including two new graphical user interfaces: (i) the Multi Genome Viewer for exploring the genomes of multiple mouse strains and (ii) the Phenotype-Gene Expression matrix which was developed in collaboration with the Gene Expression Database (GXD) and allows researchers to compare gene expression and phenotype annotations for mouse genes. Other recent improvements include enhanced efficiency of our literature curation processes and the incorporation of Transcriptional Start Site (TSS) annotations from RIKEN’s FANTOM 5 initiative.",poster,cp63
Medicine,p1626,d1,118cdcf302328a5896de4adbb1e2554641212091,c89,Conference on Uncertainty in Artificial Intelligence,The BioGRID interaction database: 2017 update,"The Biological General Repository for Interaction Datasets (BioGRID: https://thebiogrid.org) is an open access database dedicated to the annotation and archival of protein, genetic and chemical interactions for all major model organism species and humans. As of September 2016 (build 3.4.140), the BioGRID contains 1 072 173 genetic and protein interactions, and 38 559 post-translational modifications, as manually annotated from 48 114 publications. This dataset represents interaction records for 66 model organisms and represents a 30% increase compared to the previous 2015 BioGRID update. BioGRID curates the biomedical literature for major model organism species, including humans, with a recent emphasis on central biological processes and specific human diseases. To facilitate network-based approaches to drug discovery, BioGRID now incorporates 27 501 chemical–protein interactions for human drug targets, as drawn from the DrugBank database. A new dynamic interaction network viewer allows the easy navigation and filtering of all genetic and protein interaction data, as well as for bioactive compounds and their established targets. BioGRID data are directly downloadable without restriction in a variety of standardized formats and are freely distributed through partner model organism databases and meta-databases.",poster,cp89
Computer Science,p1626,d3,118cdcf302328a5896de4adbb1e2554641212091,c89,Conference on Uncertainty in Artificial Intelligence,The BioGRID interaction database: 2017 update,"The Biological General Repository for Interaction Datasets (BioGRID: https://thebiogrid.org) is an open access database dedicated to the annotation and archival of protein, genetic and chemical interactions for all major model organism species and humans. As of September 2016 (build 3.4.140), the BioGRID contains 1 072 173 genetic and protein interactions, and 38 559 post-translational modifications, as manually annotated from 48 114 publications. This dataset represents interaction records for 66 model organisms and represents a 30% increase compared to the previous 2015 BioGRID update. BioGRID curates the biomedical literature for major model organism species, including humans, with a recent emphasis on central biological processes and specific human diseases. To facilitate network-based approaches to drug discovery, BioGRID now incorporates 27 501 chemical–protein interactions for human drug targets, as drawn from the DrugBank database. A new dynamic interaction network viewer allows the easy navigation and filtering of all genetic and protein interaction data, as well as for bioactive compounds and their established targets. BioGRID data are directly downloadable without restriction in a variety of standardized formats and are freely distributed through partner model organism databases and meta-databases.",poster,cp89
Biology,p1626,d5,118cdcf302328a5896de4adbb1e2554641212091,c89,Conference on Uncertainty in Artificial Intelligence,The BioGRID interaction database: 2017 update,"The Biological General Repository for Interaction Datasets (BioGRID: https://thebiogrid.org) is an open access database dedicated to the annotation and archival of protein, genetic and chemical interactions for all major model organism species and humans. As of September 2016 (build 3.4.140), the BioGRID contains 1 072 173 genetic and protein interactions, and 38 559 post-translational modifications, as manually annotated from 48 114 publications. This dataset represents interaction records for 66 model organisms and represents a 30% increase compared to the previous 2015 BioGRID update. BioGRID curates the biomedical literature for major model organism species, including humans, with a recent emphasis on central biological processes and specific human diseases. To facilitate network-based approaches to drug discovery, BioGRID now incorporates 27 501 chemical–protein interactions for human drug targets, as drawn from the DrugBank database. A new dynamic interaction network viewer allows the easy navigation and filtering of all genetic and protein interaction data, as well as for bioactive compounds and their established targets. BioGRID data are directly downloadable without restriction in a variety of standardized formats and are freely distributed through partner model organism databases and meta-databases.",poster,cp89
Medicine,p1627,d1,8257167186837ff6840d6c2f552b4d23ff26ec81,j368,Medical Physics (Lancaster),The Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI): a completed reference database of lung nodules on CT scans.,"PURPOSE
The development of computer-aided diagnostic (CAD) methods for lung nodule detection, classification, and quantitative assessment can be facilitated through a well-characterized repository of computed tomography (CT) scans. The Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI) completed such a database, establishing a publicly available reference for the medical imaging research community. Initiated by the National Cancer Institute (NCI), further advanced by the Foundation for the National Institutes of Health (FNIH), and accompanied by the Food and Drug Administration (FDA) through active participation, this public-private partnership demonstrates the success of a consortium founded on a consensus-based process.


METHODS
Seven academic centers and eight medical imaging companies collaborated to identify, address, and resolve challenging organizational, technical, and clinical issues to provide a solid foundation for a robust database. The LIDC/IDRI Database contains 1018 cases, each of which includes images from a clinical thoracic CT scan and an associated XML file that records the results of a two-phase image annotation process performed by four experienced thoracic radiologists. In the initial blinded-read phase, each radiologist independently reviewed each CT scan and marked lesions belonging to one of three categories (""nodule > or =3 mm,"" ""nodule <3 mm,"" and ""non-nodule > or =3 mm""). In the subsequent unblinded-read phase, each radiologist independently reviewed their own marks along with the anonymized marks of the three other radiologists to render a final opinion. The goal of this process was to identify as completely as possible all lung nodules in each CT scan without requiring forced consensus.


RESULTS
The Database contains 7371 lesions marked ""nodule"" by at least one radiologist. 2669 of these lesions were marked ""nodule > or =3 mm"" by at least one radiologist, of which 928 (34.7%) received such marks from all four radiologists. These 2669 lesions include nodule outlines and subjective nodule characteristic ratings.


CONCLUSIONS
The LIDC/IDRI Database is expected to provide an essential medical imaging research resource to spur CAD development, validation, and dissemination in clinical practice.",fullPaper,jv368
Medicine,p1629,d1,c1e62b537f3d30018e7979a89b0e0f15e2b6eecc,c38,IEEE Global Engineering Education Conference,The SIDER database of drugs and side effects,"Unwanted side effects of drugs are a burden on patients and a severe impediment in the development of new drugs. At the same time, adverse drug reactions (ADRs) recorded during clinical trials are an important source of human phenotypic data. It is therefore essential to combine data on drugs, targets and side effects into a more complete picture of the therapeutic mechanism of actions of drugs and the ways in which they cause adverse reactions. To this end, we have created the SIDER (‘Side Effect Resource’, http://sideeffects.embl.de) database of drugs and ADRs. The current release, SIDER 4, contains data on 1430 drugs, 5880 ADRs and 140 064 drug–ADR pairs, which is an increase of 40% compared to the previous version. For more fine-grained analyses, we extracted the frequency with which side effects occur from the package inserts. This information is available for 39% of drug–ADR pairs, 19% of which can be compared to the frequency under placebo treatment. SIDER furthermore contains a data set of drug indications, extracted from the package inserts using Natural Language Processing. These drug indications are used to reduce the rate of false positives by identifying medical terms that do not correspond to ADRs.",poster,cp38
Computer Science,p1629,d3,c1e62b537f3d30018e7979a89b0e0f15e2b6eecc,c38,IEEE Global Engineering Education Conference,The SIDER database of drugs and side effects,"Unwanted side effects of drugs are a burden on patients and a severe impediment in the development of new drugs. At the same time, adverse drug reactions (ADRs) recorded during clinical trials are an important source of human phenotypic data. It is therefore essential to combine data on drugs, targets and side effects into a more complete picture of the therapeutic mechanism of actions of drugs and the ways in which they cause adverse reactions. To this end, we have created the SIDER (‘Side Effect Resource’, http://sideeffects.embl.de) database of drugs and ADRs. The current release, SIDER 4, contains data on 1430 drugs, 5880 ADRs and 140 064 drug–ADR pairs, which is an increase of 40% compared to the previous version. For more fine-grained analyses, we extracted the frequency with which side effects occur from the package inserts. This information is available for 39% of drug–ADR pairs, 19% of which can be compared to the frequency under placebo treatment. SIDER furthermore contains a data set of drug indications, extracted from the package inserts using Natural Language Processing. These drug indications are used to reduce the rate of false positives by identifying medical terms that do not correspond to ADRs.",poster,cp38
Biology,p1629,d5,c1e62b537f3d30018e7979a89b0e0f15e2b6eecc,c38,IEEE Global Engineering Education Conference,The SIDER database of drugs and side effects,"Unwanted side effects of drugs are a burden on patients and a severe impediment in the development of new drugs. At the same time, adverse drug reactions (ADRs) recorded during clinical trials are an important source of human phenotypic data. It is therefore essential to combine data on drugs, targets and side effects into a more complete picture of the therapeutic mechanism of actions of drugs and the ways in which they cause adverse reactions. To this end, we have created the SIDER (‘Side Effect Resource’, http://sideeffects.embl.de) database of drugs and ADRs. The current release, SIDER 4, contains data on 1430 drugs, 5880 ADRs and 140 064 drug–ADR pairs, which is an increase of 40% compared to the previous version. For more fine-grained analyses, we extracted the frequency with which side effects occur from the package inserts. This information is available for 39% of drug–ADR pairs, 19% of which can be compared to the frequency under placebo treatment. SIDER furthermore contains a data set of drug indications, extracted from the package inserts using Natural Language Processing. These drug indications are used to reduce the rate of false positives by identifying medical terms that do not correspond to ADRs.",poster,cp38
Medicine,p1630,d1,839c069fc576c816b89d84aa7c18849874de486a,c71,International Joint Conference on Artificial Intelligence,The MetaCyc database of metabolic pathways and enzymes and the BioCyc collection of pathway/genome databases,"The MetaCyc database (MetaCyc.org) is a comprehensive and freely accessible resource for metabolic pathways and enzymes from all domains of life. The pathways in MetaCyc are experimentally determined, small-molecule metabolic pathways and are curated from the primary scientific literature. With more than 1400 pathways, MetaCyc is the largest collection of metabolic pathways currently available. Pathways reactions are linked to one or more well-characterized enzymes, and both pathways and enzymes are annotated with reviews, evidence codes, and literature citations. BioCyc (BioCyc.org) is a collection of more than 500 organism-specific Pathway/Genome Databases (PGDBs). Each BioCyc PGDB contains the full genome and predicted metabolic network of one organism. The network, which is predicted by the Pathway Tools software using MetaCyc as a reference, consists of metabolites, enzymes, reactions and metabolic pathways. BioCyc PGDBs also contain additional features, such as predicted operons, transport systems, and pathway hole-fillers. The BioCyc Web site offers several tools for the analysis of the PGDBs, including Omics Viewers that enable visualization of omics datasets on two different genome-scale diagrams and tools for comparative analysis. The BioCyc PGDBs generated by SRI are offered for adoption by any party interested in curation of metabolic, regulatory, and genome-related information about an organism.",poster,cp71
Computer Science,p1630,d3,839c069fc576c816b89d84aa7c18849874de486a,c71,International Joint Conference on Artificial Intelligence,The MetaCyc database of metabolic pathways and enzymes and the BioCyc collection of pathway/genome databases,"The MetaCyc database (MetaCyc.org) is a comprehensive and freely accessible resource for metabolic pathways and enzymes from all domains of life. The pathways in MetaCyc are experimentally determined, small-molecule metabolic pathways and are curated from the primary scientific literature. With more than 1400 pathways, MetaCyc is the largest collection of metabolic pathways currently available. Pathways reactions are linked to one or more well-characterized enzymes, and both pathways and enzymes are annotated with reviews, evidence codes, and literature citations. BioCyc (BioCyc.org) is a collection of more than 500 organism-specific Pathway/Genome Databases (PGDBs). Each BioCyc PGDB contains the full genome and predicted metabolic network of one organism. The network, which is predicted by the Pathway Tools software using MetaCyc as a reference, consists of metabolites, enzymes, reactions and metabolic pathways. BioCyc PGDBs also contain additional features, such as predicted operons, transport systems, and pathway hole-fillers. The BioCyc Web site offers several tools for the analysis of the PGDBs, including Omics Viewers that enable visualization of omics datasets on two different genome-scale diagrams and tools for comparative analysis. The BioCyc PGDBs generated by SRI are offered for adoption by any party interested in curation of metabolic, regulatory, and genome-related information about an organism.",poster,cp71
Medicine,p1631,d1,7071b85c83035ff86c8ed3c1f3319a304bfb7fb5,c84,EUROCON Conference,The MetaCyc database of metabolic pathways and enzymes,"Abstract MetaCyc (https://MetaCyc.org) is a comprehensive reference database of metabolic pathways and enzymes from all domains of life. It contains more than 2570 pathways derived from >54 000 publications, making it the largest curated collection of metabolic pathways. The data in MetaCyc is strictly evidence-based and richly curated, resulting in an encyclopedic reference tool for metabolism. MetaCyc is also used as a knowledge base for generating thousands of organism-specific Pathway/Genome Databases (PGDBs), which are available in the BioCyc (https://BioCyc.org) and other PGDB collections. This article provides an update on the developments in MetaCyc during the past two years, including the expansion of data and addition of new features.",poster,cp84
Computer Science,p1631,d3,7071b85c83035ff86c8ed3c1f3319a304bfb7fb5,c84,EUROCON Conference,The MetaCyc database of metabolic pathways and enzymes,"Abstract MetaCyc (https://MetaCyc.org) is a comprehensive reference database of metabolic pathways and enzymes from all domains of life. It contains more than 2570 pathways derived from >54 000 publications, making it the largest curated collection of metabolic pathways. The data in MetaCyc is strictly evidence-based and richly curated, resulting in an encyclopedic reference tool for metabolism. MetaCyc is also used as a knowledge base for generating thousands of organism-specific Pathway/Genome Databases (PGDBs), which are available in the BioCyc (https://BioCyc.org) and other PGDB collections. This article provides an update on the developments in MetaCyc during the past two years, including the expansion of data and addition of new features.",poster,cp84
Biology,p1631,d5,7071b85c83035ff86c8ed3c1f3319a304bfb7fb5,c84,EUROCON Conference,The MetaCyc database of metabolic pathways and enzymes,"Abstract MetaCyc (https://MetaCyc.org) is a comprehensive reference database of metabolic pathways and enzymes from all domains of life. It contains more than 2570 pathways derived from >54 000 publications, making it the largest curated collection of metabolic pathways. The data in MetaCyc is strictly evidence-based and richly curated, resulting in an encyclopedic reference tool for metabolism. MetaCyc is also used as a knowledge base for generating thousands of organism-specific Pathway/Genome Databases (PGDBs), which are available in the BioCyc (https://BioCyc.org) and other PGDB collections. This article provides an update on the developments in MetaCyc during the past two years, including the expansion of data and addition of new features.",poster,cp84
Medicine,p1632,d1,e4b52a1a00e9db941326fc857b95245cbfb60bce,c19,International Conference on Conceptual Structures,Reactome graph database: Efficient access to complex pathway data,"Reactome is a free, open-source, open-data, curated and peer-reviewed knowledgebase of biomolecular pathways. One of its main priorities is to provide easy and efficient access to its high quality curated data. At present, biological pathway databases typically store their contents in relational databases. This limits access efficiency because there are performance issues associated with queries traversing highly interconnected data. The same data in a graph database can be queried more efficiently. Here we present the rationale behind the adoption of a graph database (Neo4j) as well as the new ContentService (REST API) that provides access to these data. The Neo4j graph database and its query language, Cypher, provide efficient access to the complex Reactome data model, facilitating easy traversal and knowledge discovery. The adoption of this technology greatly improved query efficiency, reducing the average query time by 93%. The web service built on top of the graph database provides programmatic access to Reactome data by object oriented queries, but also supports more complex queries that take advantage of the new underlying graph-based data storage. By adopting graph database technology we are providing a high performance pathway data resource to the community. The Reactome graph database use case shows the power of NoSQL database engines for complex biological data types.",poster,cp19
Computer Science,p1632,d3,e4b52a1a00e9db941326fc857b95245cbfb60bce,c19,International Conference on Conceptual Structures,Reactome graph database: Efficient access to complex pathway data,"Reactome is a free, open-source, open-data, curated and peer-reviewed knowledgebase of biomolecular pathways. One of its main priorities is to provide easy and efficient access to its high quality curated data. At present, biological pathway databases typically store their contents in relational databases. This limits access efficiency because there are performance issues associated with queries traversing highly interconnected data. The same data in a graph database can be queried more efficiently. Here we present the rationale behind the adoption of a graph database (Neo4j) as well as the new ContentService (REST API) that provides access to these data. The Neo4j graph database and its query language, Cypher, provide efficient access to the complex Reactome data model, facilitating easy traversal and knowledge discovery. The adoption of this technology greatly improved query efficiency, reducing the average query time by 93%. The web service built on top of the graph database provides programmatic access to Reactome data by object oriented queries, but also supports more complex queries that take advantage of the new underlying graph-based data storage. By adopting graph database technology we are providing a high performance pathway data resource to the community. The Reactome graph database use case shows the power of NoSQL database engines for complex biological data types.",poster,cp19
Medicine,p1633,d1,f4b3598f49fbb81c0ea21b8130bbdd5133403efc,j369,JAMA Oncology,Using the National Cancer Database for Outcomes Research: A Review,"Importance The National Cancer Database (NCDB), a joint quality improvement initiative of the American College of Surgeons Commission on Cancer and the American Cancer Society, has created a shared research file that has changed the study of cancer care in the United States. A thorough understanding of the nuances, strengths, and limitations of the database by both readers and investigators is of critical importance. This review describes the use of the NCDB to study cancer care, with a focus on the advantages of using the database and important considerations that affect the interpretation of NCDB studies. Observations The NCDB is one of the largest cancer registries in the world and has rapidly become one of the most commonly used data resources to study the care of cancer in the United States. The NCDB paints a comprehensive picture of cancer care, including a number of less commonly available details that enable subtle nuances of treatment to be studied. On the other hand, several potentially important patient and treatment attributes are not collected in the NCDB, which may affect the extent to which comparisons can be adjusted. Finally, the NCDB has undergone several significant changes during the past decade that may affect its completeness and the types of available data. Conclusions and Relevance The NCDB offers a critically important perspective on cancer care in the United States. To capitalize on its strengths and adjust for its limitations, investigators and their audiences should familiarize themselves with the advantages and shortcomings of the NCDB, as well as its evolution over time.",fullPaper,jv369
Medicine,p1634,d1,2b20c0d15c4ec8a48bfd916f73b163a2decd0852,c57,IEEE International Geoscience and Remote Sensing Symposium,The InterPro protein families database: the classification resource after 15 years,"The InterPro database (http://www.ebi.ac.uk/interpro/) is a freely available resource that can be used to classify sequences into protein families and to predict the presence of important domains and sites. Central to the InterPro database are predictive models, known as signatures, from a range of different protein family databases that have different biological focuses and use different methodological approaches to classify protein families and domains. InterPro integrates these signatures, capitalizing on the respective strengths of the individual databases, to produce a powerful protein classification resource. Here, we report on the status of InterPro as it enters its 15th year of operation, and give an overview of new developments with the database and its associated Web interfaces and software. In particular, the new domain architecture search tool is described and the process of mapping of Gene Ontology terms to InterPro is outlined. We also discuss the challenges faced by the resource given the explosive growth in sequence data in recent years. InterPro (version 48.0) contains 36 766 member database signatures integrated into 26 238 InterPro entries, an increase of over 3993 entries (5081 signatures), since 2012.",poster,cp57
Computer Science,p1634,d3,2b20c0d15c4ec8a48bfd916f73b163a2decd0852,c57,IEEE International Geoscience and Remote Sensing Symposium,The InterPro protein families database: the classification resource after 15 years,"The InterPro database (http://www.ebi.ac.uk/interpro/) is a freely available resource that can be used to classify sequences into protein families and to predict the presence of important domains and sites. Central to the InterPro database are predictive models, known as signatures, from a range of different protein family databases that have different biological focuses and use different methodological approaches to classify protein families and domains. InterPro integrates these signatures, capitalizing on the respective strengths of the individual databases, to produce a powerful protein classification resource. Here, we report on the status of InterPro as it enters its 15th year of operation, and give an overview of new developments with the database and its associated Web interfaces and software. In particular, the new domain architecture search tool is described and the process of mapping of Gene Ontology terms to InterPro is outlined. We also discuss the challenges faced by the resource given the explosive growth in sequence data in recent years. InterPro (version 48.0) contains 36 766 member database signatures integrated into 26 238 InterPro entries, an increase of over 3993 entries (5081 signatures), since 2012.",poster,cp57
Biology,p1634,d5,2b20c0d15c4ec8a48bfd916f73b163a2decd0852,c57,IEEE International Geoscience and Remote Sensing Symposium,The InterPro protein families database: the classification resource after 15 years,"The InterPro database (http://www.ebi.ac.uk/interpro/) is a freely available resource that can be used to classify sequences into protein families and to predict the presence of important domains and sites. Central to the InterPro database are predictive models, known as signatures, from a range of different protein family databases that have different biological focuses and use different methodological approaches to classify protein families and domains. InterPro integrates these signatures, capitalizing on the respective strengths of the individual databases, to produce a powerful protein classification resource. Here, we report on the status of InterPro as it enters its 15th year of operation, and give an overview of new developments with the database and its associated Web interfaces and software. In particular, the new domain architecture search tool is described and the process of mapping of Gene Ontology terms to InterPro is outlined. We also discuss the challenges faced by the resource given the explosive growth in sequence data in recent years. InterPro (version 48.0) contains 36 766 member database signatures integrated into 26 238 InterPro entries, an increase of over 3993 entries (5081 signatures), since 2012.",poster,cp57
Medicine,p1635,d1,ca6ac75d2408d9fbc3ff49e8d62341821574337b,j370,Immunogenetics,SYFPEITHI: database for MHC ligands and peptide motifs,Abstract,fullPaper,jv370
Biology,p1635,d5,ca6ac75d2408d9fbc3ff49e8d62341821574337b,j370,Immunogenetics,SYFPEITHI: database for MHC ligands and peptide motifs,Abstract,fullPaper,jv370
Medicine,p1636,d1,c75ba6ef724c0c3a9c9510a70da4cc8729b59a35,j371,IEEE Transactions on Visualization and Computer Graphics,FaceWarehouse: A 3D Facial Expression Database for Visual Computing,"We present FaceWarehouse, a database of 3D facial expressions for visual computing applications. We use Kinect, an off-the-shelf RGBD camera, to capture 150 individuals aged 7-80 from various ethnic backgrounds. For each person, we captured the RGBD data of her different expressions, including the neutral expression and 19 other expressions such as mouth-opening, smile, kiss, etc. For every RGBD raw data record, a set of facial feature points on the color image such as eye corners, mouth contour, and the nose tip are automatically localized, and manually adjusted if better accuracy is required. We then deform a template facial mesh to fit the depth data as closely as possible while matching the feature points on the color image to their corresponding points on the mesh. Starting from these fitted face meshes, we construct a set of individual-specific expression blendshapes for each person. These meshes with consistent topology are assembled as a rank-3 tensor to build a bilinear face model with two attributes: identity and expression. Compared with previous 3D facial databases, for every person in our database, there is a much richer matching collection of expressions, enabling depiction of most human facial actions. We demonstrate the potential of FaceWarehouse for visual computing with four applications: facial image manipulation, face component transfer, real-time performance-based facial image animation, and facial animation retargeting from video to image.",fullPaper,jv371
Computer Science,p1636,d3,c75ba6ef724c0c3a9c9510a70da4cc8729b59a35,j371,IEEE Transactions on Visualization and Computer Graphics,FaceWarehouse: A 3D Facial Expression Database for Visual Computing,"We present FaceWarehouse, a database of 3D facial expressions for visual computing applications. We use Kinect, an off-the-shelf RGBD camera, to capture 150 individuals aged 7-80 from various ethnic backgrounds. For each person, we captured the RGBD data of her different expressions, including the neutral expression and 19 other expressions such as mouth-opening, smile, kiss, etc. For every RGBD raw data record, a set of facial feature points on the color image such as eye corners, mouth contour, and the nose tip are automatically localized, and manually adjusted if better accuracy is required. We then deform a template facial mesh to fit the depth data as closely as possible while matching the feature points on the color image to their corresponding points on the mesh. Starting from these fitted face meshes, we construct a set of individual-specific expression blendshapes for each person. These meshes with consistent topology are assembled as a rank-3 tensor to build a bilinear face model with two attributes: identity and expression. Compared with previous 3D facial databases, for every person in our database, there is a much richer matching collection of expressions, enabling depiction of most human facial actions. We demonstrate the potential of FaceWarehouse for visual computing with four applications: facial image manipulation, face component transfer, real-time performance-based facial image animation, and facial animation retargeting from video to image.",fullPaper,jv371
Medicine,p1639,d1,d90293c06067d1a3718b4fb8c167285bfd099702,j372,RNA: A publication of the RNA Society,circBase: a database for circular RNAs,"Recently, several laboratories have reported thousands of circular RNAs (circRNAs) in animals. Numerous circRNAs are highly stable and have specific spatiotemporal expression patterns. Even though a function for circRNAs is unknown, these features make circRNAs an interesting class of RNAs as possible biomarkers and for further research. We developed a database and website, “circBase,” where merged and unified data sets of circRNAs and the evidence supporting their expression can be accessed, downloaded, and browsed within the genomic context. circBase also provides scripts to identify known and novel circRNAs in sequencing data. The database is freely accessible through the web server at http://www.circbase.org/.",fullPaper,jv372
Biology,p1639,d5,d90293c06067d1a3718b4fb8c167285bfd099702,j372,RNA: A publication of the RNA Society,circBase: a database for circular RNAs,"Recently, several laboratories have reported thousands of circular RNAs (circRNAs) in animals. Numerous circRNAs are highly stable and have specific spatiotemporal expression patterns. Even though a function for circRNAs is unknown, these features make circRNAs an interesting class of RNAs as possible biomarkers and for further research. We developed a database and website, “circBase,” where merged and unified data sets of circRNAs and the evidence supporting their expression can be accessed, downloaded, and browsed within the genomic context. circBase also provides scripts to identify known and novel circRNAs in sequencing data. The database is freely accessible through the web server at http://www.circbase.org/.",fullPaper,jv372
Medicine,p1640,d1,3bbc9400429ad3d6bda6d12e4449053afa1114a2,j373,Antimicrobial Agents and Chemotherapy,The Comprehensive Antibiotic Resistance Database,"ABSTRACT The field of antibiotic drug discovery and the monitoring of new antibiotic resistance elements have yet to fully exploit the power of the genome revolution. Despite the fact that the first genomes sequenced of free living organisms were those of bacteria, there have been few specialized bioinformatic tools developed to mine the growing amount of genomic data associated with pathogens. In particular, there are few tools to study the genetics and genomics of antibiotic resistance and how it impacts bacterial populations, ecology, and the clinic. We have initiated development of such tools in the form of the Comprehensive Antibiotic Research Database (CARD; http://arpcard.mcmaster.ca). The CARD integrates disparate molecular and sequence data, provides a unique organizing principle in the form of the Antibiotic Resistance Ontology (ARO), and can quickly identify putative antibiotic resistance genes in new unannotated genome sequences. This unique platform provides an informatic tool that bridges antibiotic resistance concerns in health care, agriculture, and the environment.",fullPaper,jv373
Biology,p1640,d5,3bbc9400429ad3d6bda6d12e4449053afa1114a2,j373,Antimicrobial Agents and Chemotherapy,The Comprehensive Antibiotic Resistance Database,"ABSTRACT The field of antibiotic drug discovery and the monitoring of new antibiotic resistance elements have yet to fully exploit the power of the genome revolution. Despite the fact that the first genomes sequenced of free living organisms were those of bacteria, there have been few specialized bioinformatic tools developed to mine the growing amount of genomic data associated with pathogens. In particular, there are few tools to study the genetics and genomics of antibiotic resistance and how it impacts bacterial populations, ecology, and the clinic. We have initiated development of such tools in the form of the Comprehensive Antibiotic Research Database (CARD; http://arpcard.mcmaster.ca). The CARD integrates disparate molecular and sequence data, provides a unique organizing principle in the form of the Antibiotic Resistance Ontology (ARO), and can quickly identify putative antibiotic resistance genes in new unannotated genome sequences. This unique platform provides an informatic tool that bridges antibiotic resistance concerns in health care, agriculture, and the environment.",fullPaper,jv373
Medicine,p1643,d1,bca0c491e3758ae2973d3374b62e1d417a3dc9c7,c29,ACM-SIAM Symposium on Discrete Algorithms,Rfam 12.0: updates to the RNA families database,"The Rfam database (available at http://rfam.xfam.org) is a collection of non-coding RNA families represented by manually curated sequence alignments, consensus secondary structures and annotation gathered from corresponding Wikipedia, taxonomy and ontology resources. In this article, we detail updates and improvements to the Rfam data and website for the Rfam 12.0 release. We describe the upgrade of our search pipeline to use Infernal 1.1 and demonstrate its improved homology detection ability by comparison with the previous version. The new pipeline is easier for users to apply to their own data sets, and we illustrate its ability to annotate RNAs in genomic and metagenomic data sets of various sizes. Rfam has been expanded to include 260 new families, including the well-studied large subunit ribosomal RNA family, and for the first time includes information on short sequence- and structure-based RNA motifs present within families.",poster,cp29
Computer Science,p1643,d3,bca0c491e3758ae2973d3374b62e1d417a3dc9c7,c29,ACM-SIAM Symposium on Discrete Algorithms,Rfam 12.0: updates to the RNA families database,"The Rfam database (available at http://rfam.xfam.org) is a collection of non-coding RNA families represented by manually curated sequence alignments, consensus secondary structures and annotation gathered from corresponding Wikipedia, taxonomy and ontology resources. In this article, we detail updates and improvements to the Rfam data and website for the Rfam 12.0 release. We describe the upgrade of our search pipeline to use Infernal 1.1 and demonstrate its improved homology detection ability by comparison with the previous version. The new pipeline is easier for users to apply to their own data sets, and we illustrate its ability to annotate RNAs in genomic and metagenomic data sets of various sizes. Rfam has been expanded to include 260 new families, including the well-studied large subunit ribosomal RNA family, and for the first time includes information on short sequence- and structure-based RNA motifs present within families.",poster,cp29
Biology,p1643,d5,bca0c491e3758ae2973d3374b62e1d417a3dc9c7,c29,ACM-SIAM Symposium on Discrete Algorithms,Rfam 12.0: updates to the RNA families database,"The Rfam database (available at http://rfam.xfam.org) is a collection of non-coding RNA families represented by manually curated sequence alignments, consensus secondary structures and annotation gathered from corresponding Wikipedia, taxonomy and ontology resources. In this article, we detail updates and improvements to the Rfam data and website for the Rfam 12.0 release. We describe the upgrade of our search pipeline to use Infernal 1.1 and demonstrate its improved homology detection ability by comparison with the previous version. The new pipeline is easier for users to apply to their own data sets, and we illustrate its ability to annotate RNAs in genomic and metagenomic data sets of various sizes. Rfam has been expanded to include 260 new families, including the well-studied large subunit ribosomal RNA family, and for the first time includes information on short sequence- and structure-based RNA motifs present within families.",poster,cp29
Medicine,p1646,d1,9a7508c7aa498cb738d3c16d35e06a662168106b,c63,International Conference on Evaluation & Assessment in Software Engineering,The UCSC Genome Browser database: 2015 update,"Launched in 2001 to showcase the draft human genome assembly, the UCSC Genome Browser database (http://genome.ucsc.edu) and associated tools continue to grow, providing a comprehensive resource of genome assemblies and annotations to scientists and students worldwide. Highlights of the past year include the release of a browser for the first new human genome reference assembly in 4 years in December 2013 (GRCh38, UCSC hg38), a watershed comparative genomics annotation (100-species multiple alignment and conservation) and a novel distribution mechanism for the browser (GBiB: Genome Browser in a Box). We created browsers for new species (Chinese hamster, elephant shark, minke whale), ‘mined the web’ for DNA sequences and expanded the browser display with stacked color graphs and region highlighting. As our user community increasingly adopts the UCSC track hub and assembly hub representations for sharing large-scale genomic annotation data sets and genome sequencing projects, our menu of public data hubs has tripled.",poster,cp63
Computer Science,p1646,d3,9a7508c7aa498cb738d3c16d35e06a662168106b,c63,International Conference on Evaluation & Assessment in Software Engineering,The UCSC Genome Browser database: 2015 update,"Launched in 2001 to showcase the draft human genome assembly, the UCSC Genome Browser database (http://genome.ucsc.edu) and associated tools continue to grow, providing a comprehensive resource of genome assemblies and annotations to scientists and students worldwide. Highlights of the past year include the release of a browser for the first new human genome reference assembly in 4 years in December 2013 (GRCh38, UCSC hg38), a watershed comparative genomics annotation (100-species multiple alignment and conservation) and a novel distribution mechanism for the browser (GBiB: Genome Browser in a Box). We created browsers for new species (Chinese hamster, elephant shark, minke whale), ‘mined the web’ for DNA sequences and expanded the browser display with stacked color graphs and region highlighting. As our user community increasingly adopts the UCSC track hub and assembly hub representations for sharing large-scale genomic annotation data sets and genome sequencing projects, our menu of public data hubs has tripled.",poster,cp63
Biology,p1646,d5,9a7508c7aa498cb738d3c16d35e06a662168106b,c63,International Conference on Evaluation & Assessment in Software Engineering,The UCSC Genome Browser database: 2015 update,"Launched in 2001 to showcase the draft human genome assembly, the UCSC Genome Browser database (http://genome.ucsc.edu) and associated tools continue to grow, providing a comprehensive resource of genome assemblies and annotations to scientists and students worldwide. Highlights of the past year include the release of a browser for the first new human genome reference assembly in 4 years in December 2013 (GRCh38, UCSC hg38), a watershed comparative genomics annotation (100-species multiple alignment and conservation) and a novel distribution mechanism for the browser (GBiB: Genome Browser in a Box). We created browsers for new species (Chinese hamster, elephant shark, minke whale), ‘mined the web’ for DNA sequences and expanded the browser display with stacked color graphs and region highlighting. As our user community increasingly adopts the UCSC track hub and assembly hub representations for sharing large-scale genomic annotation data sets and genome sequencing projects, our menu of public data hubs has tripled.",poster,cp63
Medicine,p1647,d1,e45a8d7176bd738c1e63de1f6791a88e704f8b4b,j121,Nature Communications,Universal database search tool for proteomics,Abstract,fullPaper,jv121
Computer Science,p1647,d3,e45a8d7176bd738c1e63de1f6791a88e704f8b4b,j121,Nature Communications,Universal database search tool for proteomics,Abstract,fullPaper,jv121
Medicine,p1648,d1,9c15ba340c2a44a7b2066d8ded94ce18ea68fe2f,c73,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,The BioGRID interaction database: 2015 update,"The Biological General Repository for Interaction Datasets (BioGRID: http://thebiogrid.org) is an open access database that houses genetic and protein interactions curated from the primary biomedical literature for all major model organism species and humans. As of September 2014, the BioGRID contains 749 912 interactions as drawn from 43 149 publications that represent 30 model organisms. This interaction count represents a 50% increase compared to our previous 2013 BioGRID update. BioGRID data are freely distributed through partner model organism databases and meta-databases and are directly downloadable in a variety of formats. In addition to general curation of the published literature for the major model species, BioGRID undertakes themed curation projects in areas of particular relevance for biomedical sciences, such as the ubiquitin-proteasome system and various human disease-associated interaction networks. BioGRID curation is coordinated through an Interaction Management System (IMS) that facilitates the compilation interaction records through structured evidence codes, phenotype ontologies, and gene annotation. The BioGRID architecture has been improved in order to support a broader range of interaction and post-translational modification types, to allow the representation of more complex multi-gene/protein interactions, to account for cellular phenotypes through structured ontologies, to expedite curation through semi-automated text-mining approaches, and to enhance curation quality control.",poster,cp73
Computer Science,p1648,d3,9c15ba340c2a44a7b2066d8ded94ce18ea68fe2f,c73,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,The BioGRID interaction database: 2015 update,"The Biological General Repository for Interaction Datasets (BioGRID: http://thebiogrid.org) is an open access database that houses genetic and protein interactions curated from the primary biomedical literature for all major model organism species and humans. As of September 2014, the BioGRID contains 749 912 interactions as drawn from 43 149 publications that represent 30 model organisms. This interaction count represents a 50% increase compared to our previous 2013 BioGRID update. BioGRID data are freely distributed through partner model organism databases and meta-databases and are directly downloadable in a variety of formats. In addition to general curation of the published literature for the major model species, BioGRID undertakes themed curation projects in areas of particular relevance for biomedical sciences, such as the ubiquitin-proteasome system and various human disease-associated interaction networks. BioGRID curation is coordinated through an Interaction Management System (IMS) that facilitates the compilation interaction records through structured evidence codes, phenotype ontologies, and gene annotation. The BioGRID architecture has been improved in order to support a broader range of interaction and post-translational modification types, to allow the representation of more complex multi-gene/protein interactions, to account for cellular phenotypes through structured ontologies, to expedite curation through semi-automated text-mining approaches, and to enhance curation quality control.",poster,cp73
Biology,p1648,d5,9c15ba340c2a44a7b2066d8ded94ce18ea68fe2f,c73,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,The BioGRID interaction database: 2015 update,"The Biological General Repository for Interaction Datasets (BioGRID: http://thebiogrid.org) is an open access database that houses genetic and protein interactions curated from the primary biomedical literature for all major model organism species and humans. As of September 2014, the BioGRID contains 749 912 interactions as drawn from 43 149 publications that represent 30 model organisms. This interaction count represents a 50% increase compared to our previous 2013 BioGRID update. BioGRID data are freely distributed through partner model organism databases and meta-databases and are directly downloadable in a variety of formats. In addition to general curation of the published literature for the major model species, BioGRID undertakes themed curation projects in areas of particular relevance for biomedical sciences, such as the ubiquitin-proteasome system and various human disease-associated interaction networks. BioGRID curation is coordinated through an Interaction Management System (IMS) that facilitates the compilation interaction records through structured evidence codes, phenotype ontologies, and gene annotation. The BioGRID architecture has been improved in order to support a broader range of interaction and post-translational modification types, to allow the representation of more complex multi-gene/protein interactions, to account for cellular phenotypes through structured ontologies, to expedite curation through semi-automated text-mining approaches, and to enhance curation quality control.",poster,cp73
Medicine,p1649,d1,e049bd1ef18a0f1cdb45477d957393cf9ef41c6d,c59,Australian Software Engineering Conference,The UCSC Genome Browser Database,"The University of California Santa Cruz (UCSC) Genome Browser Database is an up to date source for genome sequence data integrated with a large collection of related annotations. The database is optimized to support fast interactive performance with the web-based UCSC Genome Browser, a tool built on top of the database for rapid visualization and querying of the data at many levels. The annotations for a given genome are displayed in the browser as a series of tracks aligned with the genomic sequence. Sequence data and annotations may also be viewed in a text-based tabular format or downloaded as tab-delimited flat files. The Genome Browser Database, browsing tools and downloadable data files can all be found on the UCSC Genome Bioinformatics website (http://genome.ucsc.edu), which also contains links to documentation and related technical information.",poster,cp59
Computer Science,p1649,d3,e049bd1ef18a0f1cdb45477d957393cf9ef41c6d,c59,Australian Software Engineering Conference,The UCSC Genome Browser Database,"The University of California Santa Cruz (UCSC) Genome Browser Database is an up to date source for genome sequence data integrated with a large collection of related annotations. The database is optimized to support fast interactive performance with the web-based UCSC Genome Browser, a tool built on top of the database for rapid visualization and querying of the data at many levels. The annotations for a given genome are displayed in the browser as a series of tracks aligned with the genomic sequence. Sequence data and annotations may also be viewed in a text-based tabular format or downloaded as tab-delimited flat files. The Genome Browser Database, browsing tools and downloadable data files can all be found on the UCSC Genome Bioinformatics website (http://genome.ucsc.edu), which also contains links to documentation and related technical information.",poster,cp59
Biology,p1649,d5,e049bd1ef18a0f1cdb45477d957393cf9ef41c6d,c59,Australian Software Engineering Conference,The UCSC Genome Browser Database,"The University of California Santa Cruz (UCSC) Genome Browser Database is an up to date source for genome sequence data integrated with a large collection of related annotations. The database is optimized to support fast interactive performance with the web-based UCSC Genome Browser, a tool built on top of the database for rapid visualization and querying of the data at many levels. The annotations for a given genome are displayed in the browser as a series of tracks aligned with the genomic sequence. Sequence data and annotations may also be viewed in a text-based tabular format or downloaded as tab-delimited flat files. The Genome Browser Database, browsing tools and downloadable data files can all be found on the UCSC Genome Bioinformatics website (http://genome.ucsc.edu), which also contains links to documentation and related technical information.",poster,cp59
Medicine,p1650,d1,fa69e5e6d21650daa286ed936af9c53d1aca00df,c51,International Conference on Engineering Education,STRING: a database of predicted functional associations between proteins,"Functional links between proteins can often be inferred from genomic associations between the genes that encode them: groups of genes that are required for the same function tend to show similar species coverage, are often located in close proximity on the genome (in prokaryotes), and tend to be involved in gene-fusion events. The database STRING is a precomputed global resource for the exploration and analysis of these associations. Since the three types of evidence differ conceptually, and the number of predicted interactions is very large, it is essential to be able to assess and compare the significance of individual predictions. Thus, STRING contains a unique scoring-framework based on benchmarks of the different types of associations against a common reference set, integrated in a single confidence score per prediction. The graphical representation of the network of inferred, weighted protein interactions provides a high-level view of functional linkage, facilitating the analysis of modularity in biological processes. STRING is updated continuously, and currently contains 261 033 orthologs in 89 fully sequenced genomes. The database predicts functional interactions at an expected level of accuracy of at least 80% for more than half of the genes; it is online at http://www.bork.embl-heidelberg.de/STRING/.",poster,cp51
Computer Science,p1650,d3,fa69e5e6d21650daa286ed936af9c53d1aca00df,c51,International Conference on Engineering Education,STRING: a database of predicted functional associations between proteins,"Functional links between proteins can often be inferred from genomic associations between the genes that encode them: groups of genes that are required for the same function tend to show similar species coverage, are often located in close proximity on the genome (in prokaryotes), and tend to be involved in gene-fusion events. The database STRING is a precomputed global resource for the exploration and analysis of these associations. Since the three types of evidence differ conceptually, and the number of predicted interactions is very large, it is essential to be able to assess and compare the significance of individual predictions. Thus, STRING contains a unique scoring-framework based on benchmarks of the different types of associations against a common reference set, integrated in a single confidence score per prediction. The graphical representation of the network of inferred, weighted protein interactions provides a high-level view of functional linkage, facilitating the analysis of modularity in biological processes. STRING is updated continuously, and currently contains 261 033 orthologs in 89 fully sequenced genomes. The database predicts functional interactions at an expected level of accuracy of at least 80% for more than half of the genes; it is online at http://www.bork.embl-heidelberg.de/STRING/.",poster,cp51
Biology,p1650,d5,fa69e5e6d21650daa286ed936af9c53d1aca00df,c51,International Conference on Engineering Education,STRING: a database of predicted functional associations between proteins,"Functional links between proteins can often be inferred from genomic associations between the genes that encode them: groups of genes that are required for the same function tend to show similar species coverage, are often located in close proximity on the genome (in prokaryotes), and tend to be involved in gene-fusion events. The database STRING is a precomputed global resource for the exploration and analysis of these associations. Since the three types of evidence differ conceptually, and the number of predicted interactions is very large, it is essential to be able to assess and compare the significance of individual predictions. Thus, STRING contains a unique scoring-framework based on benchmarks of the different types of associations against a common reference set, integrated in a single confidence score per prediction. The graphical representation of the network of inferred, weighted protein interactions provides a high-level view of functional linkage, facilitating the analysis of modularity in biological processes. STRING is updated continuously, and currently contains 261 033 orthologs in 89 fully sequenced genomes. The database predicts functional interactions at an expected level of accuracy of at least 80% for more than half of the genes; it is online at http://www.bork.embl-heidelberg.de/STRING/.",poster,cp51
Medicine,p1651,d1,537ab55f4bdfcbe83ab68b2032d83ae9c7d55d31,c38,IEEE Global Engineering Education Conference,The Transporter Classification Database (TCDB): recent advances,"The Transporter Classification Database (TCDB; http://www.tcdb.org) is a freely accessible reference database for transport protein research, which provides structural, functional, mechanistic, evolutionary and disease/medical information about transporters from organisms of all types. TCDB is the only transport protein classification database adopted by the International Union of Biochemistry and Molecular Biology (IUBMB). It consists of more than 10 000 non-redundant transport systems with more than 11 000 reference citations, classified into over 1000 transporter families. Transporters in TCDB can be single or multi-component systems, categorized in a functional/phylogenetic hierarchical system of classes, subclasses, families, subfamilies and transport systems. TCDB also includes updated software designed to analyze the distinctive features of transport proteins, extending its usefulness. Here we present a comprehensive update of the database contents and features and summarize recent discoveries recorded in TCDB.",poster,cp38
Computer Science,p1651,d3,537ab55f4bdfcbe83ab68b2032d83ae9c7d55d31,c38,IEEE Global Engineering Education Conference,The Transporter Classification Database (TCDB): recent advances,"The Transporter Classification Database (TCDB; http://www.tcdb.org) is a freely accessible reference database for transport protein research, which provides structural, functional, mechanistic, evolutionary and disease/medical information about transporters from organisms of all types. TCDB is the only transport protein classification database adopted by the International Union of Biochemistry and Molecular Biology (IUBMB). It consists of more than 10 000 non-redundant transport systems with more than 11 000 reference citations, classified into over 1000 transporter families. Transporters in TCDB can be single or multi-component systems, categorized in a functional/phylogenetic hierarchical system of classes, subclasses, families, subfamilies and transport systems. TCDB also includes updated software designed to analyze the distinctive features of transport proteins, extending its usefulness. Here we present a comprehensive update of the database contents and features and summarize recent discoveries recorded in TCDB.",poster,cp38
Biology,p1651,d5,537ab55f4bdfcbe83ab68b2032d83ae9c7d55d31,c38,IEEE Global Engineering Education Conference,The Transporter Classification Database (TCDB): recent advances,"The Transporter Classification Database (TCDB; http://www.tcdb.org) is a freely accessible reference database for transport protein research, which provides structural, functional, mechanistic, evolutionary and disease/medical information about transporters from organisms of all types. TCDB is the only transport protein classification database adopted by the International Union of Biochemistry and Molecular Biology (IUBMB). It consists of more than 10 000 non-redundant transport systems with more than 11 000 reference citations, classified into over 1000 transporter families. Transporters in TCDB can be single or multi-component systems, categorized in a functional/phylogenetic hierarchical system of classes, subclasses, families, subfamilies and transport systems. TCDB also includes updated software designed to analyze the distinctive features of transport proteins, extending its usefulness. Here we present a comprehensive update of the database contents and features and summarize recent discoveries recorded in TCDB.",poster,cp38
Medicine,p1653,d1,2e5701b71ccf3352b30b584c2e48fdc307376385,c70,Annual Meeting of the Association for Computational Linguistics,The immune epitope database (IEDB) 3.0,"The IEDB, www.iedb.org, contains information on immune epitopes—the molecular targets of adaptive immune responses—curated from the published literature and submitted by National Institutes of Health funded epitope discovery efforts. From 2004 to 2012 the IEDB curation of journal articles published since 1960 has caught up to the present day, with >95% of relevant published literature manually curated amounting to more than 15 000 journal articles and more than 704 000 experiments to date. The revised curation target since 2012 has been to make recent research findings quickly available in the IEDB and thereby ensure that it continues to be an up-to-date resource. Having gathered a comprehensive dataset in the IEDB, a complete redesign of the query and reporting interface has been performed in the IEDB 3.0 release to improve how end users can access this information in an intuitive and biologically accurate manner. We here present this most recent release of the IEDB and describe the user testing procedures as well as the use of external ontologies that have enabled it.",poster,cp70
Computer Science,p1653,d3,2e5701b71ccf3352b30b584c2e48fdc307376385,c70,Annual Meeting of the Association for Computational Linguistics,The immune epitope database (IEDB) 3.0,"The IEDB, www.iedb.org, contains information on immune epitopes—the molecular targets of adaptive immune responses—curated from the published literature and submitted by National Institutes of Health funded epitope discovery efforts. From 2004 to 2012 the IEDB curation of journal articles published since 1960 has caught up to the present day, with >95% of relevant published literature manually curated amounting to more than 15 000 journal articles and more than 704 000 experiments to date. The revised curation target since 2012 has been to make recent research findings quickly available in the IEDB and thereby ensure that it continues to be an up-to-date resource. Having gathered a comprehensive dataset in the IEDB, a complete redesign of the query and reporting interface has been performed in the IEDB 3.0 release to improve how end users can access this information in an intuitive and biologically accurate manner. We here present this most recent release of the IEDB and describe the user testing procedures as well as the use of external ontologies that have enabled it.",poster,cp70
Biology,p1653,d5,2e5701b71ccf3352b30b584c2e48fdc307376385,c70,Annual Meeting of the Association for Computational Linguistics,The immune epitope database (IEDB) 3.0,"The IEDB, www.iedb.org, contains information on immune epitopes—the molecular targets of adaptive immune responses—curated from the published literature and submitted by National Institutes of Health funded epitope discovery efforts. From 2004 to 2012 the IEDB curation of journal articles published since 1960 has caught up to the present day, with >95% of relevant published literature manually curated amounting to more than 15 000 journal articles and more than 704 000 experiments to date. The revised curation target since 2012 has been to make recent research findings quickly available in the IEDB and thereby ensure that it continues to be an up-to-date resource. Having gathered a comprehensive dataset in the IEDB, a complete redesign of the query and reporting interface has been performed in the IEDB 3.0 release to improve how end users can access this information in an intuitive and biologically accurate manner. We here present this most recent release of the IEDB and describe the user testing procedures as well as the use of external ontologies that have enabled it.",poster,cp70
Medicine,p1654,d1,66fb37fee3250f68d598e45b3097b50045edc499,j350,Acta Crystallographica Section B Structural Science,New software for searching the Cambridge Structural Database and visualizing crystal structures.,"Two new programs have been developed for searching the Cambridge Structural Database (CSD) and visualizing database entries: ConQuest and Mercury. The former is a new search interface to the CSD, the latter is a high-performance crystal-structure visualizer with extensive facilities for exploring networks of intermolecular contacts. Particular emphasis has been placed on making the programs as intuitive as possible. Both ConQuest and Mercury run under Windows and various types of Unix, including Linux.",fullPaper,jv350
Chemistry,p1654,d8,66fb37fee3250f68d598e45b3097b50045edc499,j350,Acta Crystallographica Section B Structural Science,New software for searching the Cambridge Structural Database and visualizing crystal structures.,"Two new programs have been developed for searching the Cambridge Structural Database (CSD) and visualizing database entries: ConQuest and Mercury. The former is a new search interface to the CSD, the latter is a high-performance crystal-structure visualizer with extensive facilities for exploring networks of intermolecular contacts. Particular emphasis has been placed on making the programs as intuitive as possible. Both ConQuest and Mercury run under Windows and various types of Unix, including Linux.",fullPaper,jv350
Medicine,p1655,d1,00bc156bac2b39fab1689fe047b23c9f216d7f29,j107,Nucleic Acids Research,Database resources of the National Center for Biotechnology Information: update,"In addition to maintaining the GenBank(R) nucleic acid sequence database, the National Center for Biotechnology Information (NCBI) provides data analysis and retrieval resources for the data in GenBank and other biological data made available through NCBI’s website. NCBI resources include Entrez, PubMed, PubMed Central, LocusLink, the NCBI Taxonomy Browser, BLAST, BLAST Link (BLink), Electronic PCR, OrfFinder, Spidey, RefSeq, UniGene, HomoloGene, ProtEST, dbMHC, dbSNP, Cancer Chromosome Aberration Project (CCAP), Entrez Genomes and related tools, the Map Viewer, Model Maker, Evidence Viewer, Clusters of Orthologous Groups (COGs) database, Retroviral Genotyping Tools, SARS Coronavirus Resource, SAGEmap, Gene Expression Omnibus (GEO), Online Mendelian Inheritance in Man (OMIM), the Molecular Modeling Database (MMDB), the Conserved Domain Database (CDD) and the Conserved Domain Architecture Retrieval Tool (CDART). Augmenting many of the web applications are custom implementations of the BLAST program optimized to search specialized data sets. All of the resources can be accessed through the NCBI home page at: http://www.ncbi.nlm.nih.gov.",fullPaper,jv107
Biology,p1655,d5,00bc156bac2b39fab1689fe047b23c9f216d7f29,j107,Nucleic Acids Research,Database resources of the National Center for Biotechnology Information: update,"In addition to maintaining the GenBank(R) nucleic acid sequence database, the National Center for Biotechnology Information (NCBI) provides data analysis and retrieval resources for the data in GenBank and other biological data made available through NCBI’s website. NCBI resources include Entrez, PubMed, PubMed Central, LocusLink, the NCBI Taxonomy Browser, BLAST, BLAST Link (BLink), Electronic PCR, OrfFinder, Spidey, RefSeq, UniGene, HomoloGene, ProtEST, dbMHC, dbSNP, Cancer Chromosome Aberration Project (CCAP), Entrez Genomes and related tools, the Map Viewer, Model Maker, Evidence Viewer, Clusters of Orthologous Groups (COGs) database, Retroviral Genotyping Tools, SARS Coronavirus Resource, SAGEmap, Gene Expression Omnibus (GEO), Online Mendelian Inheritance in Man (OMIM), the Molecular Modeling Database (MMDB), the Conserved Domain Database (CDD) and the Conserved Domain Architecture Retrieval Tool (CDART). Augmenting many of the web applications are custom implementations of the BLAST program optimized to search specialized data sets. All of the resources can be accessed through the NCBI home page at: http://www.ncbi.nlm.nih.gov.",fullPaper,jv107
Medicine,p1656,d1,269b9e182815a5d805cfa1f5c1137763b0b2b7cf,c116,International Society for Music Information Retrieval Conference,Genevestigator V3: A Reference Expression Database for the Meta-Analysis of Transcriptomes,"The Web-based software tool Genevestigator provides powerful tools for biologists to explore gene expression across a wide variety of biological contexts. Its first releases, however, were limited by the scaling ability of the system architecture, multiorganism data storage and analysis capability, and availability of computationally intensive analysis methods. Genevestigator V3 is a novel meta-analysis system resulting from new algorithmic and software development using a client/server architecture, large-scale manual curation and quality control of microarray data for several organisms, and curation of pathway data for mouse and Arabidopsis. In addition to improved querying features, Genevestigator V3 provides new tools to analyze the expression of genes in many different contexts, to identify biomarker genes, to cluster genes into expression modules, and to model expression responses in the context of metabolic and regulatory networks. Being a reference expression database with user-friendly tools, Genevestigator V3 facilitates discovery research and hypothesis validation.",poster,cp116
Computer Science,p1656,d3,269b9e182815a5d805cfa1f5c1137763b0b2b7cf,c116,International Society for Music Information Retrieval Conference,Genevestigator V3: A Reference Expression Database for the Meta-Analysis of Transcriptomes,"The Web-based software tool Genevestigator provides powerful tools for biologists to explore gene expression across a wide variety of biological contexts. Its first releases, however, were limited by the scaling ability of the system architecture, multiorganism data storage and analysis capability, and availability of computationally intensive analysis methods. Genevestigator V3 is a novel meta-analysis system resulting from new algorithmic and software development using a client/server architecture, large-scale manual curation and quality control of microarray data for several organisms, and curation of pathway data for mouse and Arabidopsis. In addition to improved querying features, Genevestigator V3 provides new tools to analyze the expression of genes in many different contexts, to identify biomarker genes, to cluster genes into expression modules, and to model expression responses in the context of metabolic and regulatory networks. Being a reference expression database with user-friendly tools, Genevestigator V3 facilitates discovery research and hypothesis validation.",poster,cp116
Medicine,p1657,d1,89c433d6a7160bba49ef164fa93c3036d9f7ac5e,c57,IEEE International Geoscience and Remote Sensing Symposium,The MetaCyc database of metabolic pathways and enzymes and the BioCyc collection of pathway/genome databases,"The MetaCyc database (MetaCyc.org) is a freely accessible comprehensive database describing metabolic pathways and enzymes from all domains of life. The majority of MetaCyc pathways are small-molecule metabolic pathways that have been experimentally determined. MetaCyc contains more than 2400 pathways derived from >46 000 publications, and is the largest curated collection of metabolic pathways. BioCyc (BioCyc.org) is a collection of 5700 organism-specific Pathway/Genome Databases (PGDBs), each containing the full genome and predicted metabolic network of one organism, including metabolites, enzymes, reactions, metabolic pathways, predicted operons, transport systems, and pathway-hole fillers. The BioCyc website offers a variety of tools for querying and analyzing PGDBs, including Omics Viewers and tools for comparative analysis. This article provides an update of new developments in MetaCyc and BioCyc during the last two years, including addition of Gibbs free energy values for compounds and reactions; redesign of the primary gene/protein page; addition of a tool for creating diagrams containing multiple linked pathways; several new search capabilities, including searching for genes based on sequence patterns, searching for databases based on an organism's phenotypes, and a cross-organism search; and a metabolite identifier translation service.",poster,cp57
Computer Science,p1657,d3,89c433d6a7160bba49ef164fa93c3036d9f7ac5e,c57,IEEE International Geoscience and Remote Sensing Symposium,The MetaCyc database of metabolic pathways and enzymes and the BioCyc collection of pathway/genome databases,"The MetaCyc database (MetaCyc.org) is a freely accessible comprehensive database describing metabolic pathways and enzymes from all domains of life. The majority of MetaCyc pathways are small-molecule metabolic pathways that have been experimentally determined. MetaCyc contains more than 2400 pathways derived from >46 000 publications, and is the largest curated collection of metabolic pathways. BioCyc (BioCyc.org) is a collection of 5700 organism-specific Pathway/Genome Databases (PGDBs), each containing the full genome and predicted metabolic network of one organism, including metabolites, enzymes, reactions, metabolic pathways, predicted operons, transport systems, and pathway-hole fillers. The BioCyc website offers a variety of tools for querying and analyzing PGDBs, including Omics Viewers and tools for comparative analysis. This article provides an update of new developments in MetaCyc and BioCyc during the last two years, including addition of Gibbs free energy values for compounds and reactions; redesign of the primary gene/protein page; addition of a tool for creating diagrams containing multiple linked pathways; several new search capabilities, including searching for genes based on sequence patterns, searching for databases based on an organism's phenotypes, and a cross-organism search; and a metabolite identifier translation service.",poster,cp57
Medicine,p1659,d1,0be6f2f0329230040e2a2dbd4b9e321ef33a2c11,c97,International Conference on Computational Logic,The UCSC Genome Browser database: update 2011,"The University of California, Santa Cruz Genome Browser (http://genome.ucsc.edu) offers online access to a database of genomic sequence and annotation data for a wide variety of organisms. The Browser also has many tools for visualizing, comparing and analyzing both publicly available and user-generated genomic data sets, aligning sequences and uploading user data. Among the features released this year are a gene search tool and annotation track drag-reorder functionality as well as support for BAM and BigWig/BigBed file formats. New display enhancements include overlay of multiple wiggle tracks through use of transparent coloring, options for displaying transformed wiggle data, a ‘mean+whiskers’ windowing function for display of wiggle data at high zoom levels, and more color schemes for microarray data. New data highlights include seven new genome assemblies, a Neandertal genome data portal, phenotype and disease association data, a human RNA editing track, and a zebrafish Conservation track. We also describe updates to existing tracks.",poster,cp97
Computer Science,p1659,d3,0be6f2f0329230040e2a2dbd4b9e321ef33a2c11,c97,International Conference on Computational Logic,The UCSC Genome Browser database: update 2011,"The University of California, Santa Cruz Genome Browser (http://genome.ucsc.edu) offers online access to a database of genomic sequence and annotation data for a wide variety of organisms. The Browser also has many tools for visualizing, comparing and analyzing both publicly available and user-generated genomic data sets, aligning sequences and uploading user data. Among the features released this year are a gene search tool and annotation track drag-reorder functionality as well as support for BAM and BigWig/BigBed file formats. New display enhancements include overlay of multiple wiggle tracks through use of transparent coloring, options for displaying transformed wiggle data, a ‘mean+whiskers’ windowing function for display of wiggle data at high zoom levels, and more color schemes for microarray data. New data highlights include seven new genome assemblies, a Neandertal genome data portal, phenotype and disease association data, a human RNA editing track, and a zebrafish Conservation track. We also describe updates to existing tracks.",poster,cp97
Biology,p1659,d5,0be6f2f0329230040e2a2dbd4b9e321ef33a2c11,c97,International Conference on Computational Logic,The UCSC Genome Browser database: update 2011,"The University of California, Santa Cruz Genome Browser (http://genome.ucsc.edu) offers online access to a database of genomic sequence and annotation data for a wide variety of organisms. The Browser also has many tools for visualizing, comparing and analyzing both publicly available and user-generated genomic data sets, aligning sequences and uploading user data. Among the features released this year are a gene search tool and annotation track drag-reorder functionality as well as support for BAM and BigWig/BigBed file formats. New display enhancements include overlay of multiple wiggle tracks through use of transparent coloring, options for displaying transformed wiggle data, a ‘mean+whiskers’ windowing function for display of wiggle data at high zoom levels, and more color schemes for microarray data. New data highlights include seven new genome assemblies, a Neandertal genome data portal, phenotype and disease association data, a human RNA editing track, and a zebrafish Conservation track. We also describe updates to existing tracks.",poster,cp97
Medicine,p1660,d1,b7e54a5d6149cde03fd5144d38a0647d25a795f6,c2,International Conference on Software Engineering,The ChEMBL bioactivity database: an update,"ChEMBL is an open large-scale bioactivity database (https://www.ebi.ac.uk/chembl), previously described in the 2012 Nucleic Acids Research Database Issue. Since then, a variety of new data sources and improvements in functionality have contributed to the growth and utility of the resource. In particular, more comprehensive tracking of compounds from research stages through clinical development to market is provided through the inclusion of data from United States Adopted Name applications; a new richer data model for representing drug targets has been developed; and a number of methods have been put in place to allow users to more easily identify reliable data. Finally, access to ChEMBL is now available via a new Resource Description Framework format, in addition to the web-based interface, data downloads and web services.",poster,cp2
Computer Science,p1660,d3,b7e54a5d6149cde03fd5144d38a0647d25a795f6,c2,International Conference on Software Engineering,The ChEMBL bioactivity database: an update,"ChEMBL is an open large-scale bioactivity database (https://www.ebi.ac.uk/chembl), previously described in the 2012 Nucleic Acids Research Database Issue. Since then, a variety of new data sources and improvements in functionality have contributed to the growth and utility of the resource. In particular, more comprehensive tracking of compounds from research stages through clinical development to market is provided through the inclusion of data from United States Adopted Name applications; a new richer data model for representing drug targets has been developed; and a number of methods have been put in place to allow users to more easily identify reliable data. Finally, access to ChEMBL is now available via a new Resource Description Framework format, in addition to the web-based interface, data downloads and web services.",poster,cp2
Biology,p1660,d5,b7e54a5d6149cde03fd5144d38a0647d25a795f6,c2,International Conference on Software Engineering,The ChEMBL bioactivity database: an update,"ChEMBL is an open large-scale bioactivity database (https://www.ebi.ac.uk/chembl), previously described in the 2012 Nucleic Acids Research Database Issue. Since then, a variety of new data sources and improvements in functionality have contributed to the growth and utility of the resource. In particular, more comprehensive tracking of compounds from research stages through clinical development to market is provided through the inclusion of data from United States Adopted Name applications; a new richer data model for representing drug targets has been developed; and a number of methods have been put in place to allow users to more easily identify reliable data. Finally, access to ChEMBL is now available via a new Resource Description Framework format, in addition to the web-based interface, data downloads and web services.",poster,cp2
Medicine,p1661,d1,20740b17fd91394cfdf17ebf1c227312d6bb56cb,c64,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",The Database of Interacting Proteins: 2004 update,"The Database of Interacting Proteins (http://dip.doe-mbi.ucla.edu) aims to integrate the diverse body of experimental evidence on protein-protein interactions into a single, easily accessible online database. Because the reliability of experimental evidence varies widely, methods of quality assessment have been developed and utilized to identify the most reliable subset of the interactions. This CORE set can be used as a reference when evaluating the reliability of high-throughput protein-protein interaction data sets, for development of prediction methods, as well as in the studies of the properties of protein interaction networks.",poster,cp64
Computer Science,p1661,d3,20740b17fd91394cfdf17ebf1c227312d6bb56cb,c64,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",The Database of Interacting Proteins: 2004 update,"The Database of Interacting Proteins (http://dip.doe-mbi.ucla.edu) aims to integrate the diverse body of experimental evidence on protein-protein interactions into a single, easily accessible online database. Because the reliability of experimental evidence varies widely, methods of quality assessment have been developed and utilized to identify the most reliable subset of the interactions. This CORE set can be used as a reference when evaluating the reliability of high-throughput protein-protein interaction data sets, for development of prediction methods, as well as in the studies of the properties of protein interaction networks.",poster,cp64
Medicine,p1663,d1,6c20fecf959174309f8c9ddc1b21787513842af9,c47,EUROMICRO Conference on Software Engineering and Advanced Applications,Saccharomyces Genome Database: the genomics resource of budding yeast,"The Saccharomyces Genome Database (SGD, http://www.yeastgenome.org) is the community resource for the budding yeast Saccharomyces cerevisiae. The SGD project provides the highest-quality manually curated information from peer-reviewed literature. The experimental results reported in the literature are extracted and integrated within a well-developed database. These data are combined with quality high-throughput results and provided through Locus Summary pages, a powerful query engine and rich genome browser. The acquisition, integration and retrieval of these data allow SGD to facilitate experimental design and analysis by providing an encyclopedia of the yeast genome, its chromosomal features, their functions and interactions. Public access to these data is provided to researchers and educators via web pages designed for optimal ease of use.",poster,cp47
Computer Science,p1663,d3,6c20fecf959174309f8c9ddc1b21787513842af9,c47,EUROMICRO Conference on Software Engineering and Advanced Applications,Saccharomyces Genome Database: the genomics resource of budding yeast,"The Saccharomyces Genome Database (SGD, http://www.yeastgenome.org) is the community resource for the budding yeast Saccharomyces cerevisiae. The SGD project provides the highest-quality manually curated information from peer-reviewed literature. The experimental results reported in the literature are extracted and integrated within a well-developed database. These data are combined with quality high-throughput results and provided through Locus Summary pages, a powerful query engine and rich genome browser. The acquisition, integration and retrieval of these data allow SGD to facilitate experimental design and analysis by providing an encyclopedia of the yeast genome, its chromosomal features, their functions and interactions. Public access to these data is provided to researchers and educators via web pages designed for optimal ease of use.",poster,cp47
Biology,p1663,d5,6c20fecf959174309f8c9ddc1b21787513842af9,c47,EUROMICRO Conference on Software Engineering and Advanced Applications,Saccharomyces Genome Database: the genomics resource of budding yeast,"The Saccharomyces Genome Database (SGD, http://www.yeastgenome.org) is the community resource for the budding yeast Saccharomyces cerevisiae. The SGD project provides the highest-quality manually curated information from peer-reviewed literature. The experimental results reported in the literature are extracted and integrated within a well-developed database. These data are combined with quality high-throughput results and provided through Locus Summary pages, a powerful query engine and rich genome browser. The acquisition, integration and retrieval of these data allow SGD to facilitate experimental design and analysis by providing an encyclopedia of the yeast genome, its chromosomal features, their functions and interactions. Public access to these data is provided to researchers and educators via web pages designed for optimal ease of use.",poster,cp47
Medicine,p1668,d1,475bbf493d8246031a5152c8005a5c567231307c,j359,Journal of Chemical Information and Modeling,Basis Set Exchange: A Community Database for Computational Sciences,"Basis sets are some of the most important input data for computational models in the chemistry, materials, biology, and other science domains that utilize computational quantum mechanics methods. Providing a shared, Web-accessible environment where researchers can not only download basis sets in their required format but browse the data, contribute new basis sets, and ultimately curate and manage the data as a community will facilitate growth of this resource and encourage sharing both data and knowledge. We describe the Basis Set Exchange (BSE), a Web portal that provides advanced browsing and download capabilities, facilities for contributing basis set data, and an environment that incorporates tools to foster development and interaction of communities. The BSE leverages and enables continued development of the basis set library originally assembled at the Environmental Molecular Sciences Laboratory.",fullPaper,jv359
Computer Science,p1668,d3,475bbf493d8246031a5152c8005a5c567231307c,j359,Journal of Chemical Information and Modeling,Basis Set Exchange: A Community Database for Computational Sciences,"Basis sets are some of the most important input data for computational models in the chemistry, materials, biology, and other science domains that utilize computational quantum mechanics methods. Providing a shared, Web-accessible environment where researchers can not only download basis sets in their required format but browse the data, contribute new basis sets, and ultimately curate and manage the data as a community will facilitate growth of this resource and encourage sharing both data and knowledge. We describe the Basis Set Exchange (BSE), a Web portal that provides advanced browsing and download capabilities, facilities for contributing basis set data, and an environment that incorporates tools to foster development and interaction of communities. The BSE leverages and enables continued development of the basis set library originally assembled at the Environmental Molecular Sciences Laboratory.",fullPaper,jv359
Medicine,p1672,d1,3b3ad5eaddd5a970519b8c9b4097816fe374e8ec,c23,International Conference on Open and Big Data,The Proteomics Identifications (PRIDE) database and associated tools: status in 2013,"The PRoteomics IDEntifications (PRIDE, http://www.ebi.ac.uk/pride) database at the European Bioinformatics Institute is one of the most prominent data repositories of mass spectrometry (MS)-based proteomics data. Here, we summarize recent developments in the PRIDE database and related tools. First, we provide up-to-date statistics in data content, splitting the figures by groups of organisms and species, including peptide and protein identifications, and post-translational modifications. We then describe the tools that are part of the PRIDE submission pipeline, especially the recently developed PRIDE Converter 2 (new submission tool) and PRIDE Inspector (visualization and analysis tool). We also give an update about the integration of PRIDE with other MS proteomics resources in the context of the ProteomeXchange consortium. Finally, we briefly review the quality control efforts that are ongoing at present and outline our future plans.",poster,cp23
Computer Science,p1672,d3,3b3ad5eaddd5a970519b8c9b4097816fe374e8ec,c23,International Conference on Open and Big Data,The Proteomics Identifications (PRIDE) database and associated tools: status in 2013,"The PRoteomics IDEntifications (PRIDE, http://www.ebi.ac.uk/pride) database at the European Bioinformatics Institute is one of the most prominent data repositories of mass spectrometry (MS)-based proteomics data. Here, we summarize recent developments in the PRIDE database and related tools. First, we provide up-to-date statistics in data content, splitting the figures by groups of organisms and species, including peptide and protein identifications, and post-translational modifications. We then describe the tools that are part of the PRIDE submission pipeline, especially the recently developed PRIDE Converter 2 (new submission tool) and PRIDE Inspector (visualization and analysis tool). We also give an update about the integration of PRIDE with other MS proteomics resources in the context of the ProteomeXchange consortium. Finally, we briefly review the quality control efforts that are ongoing at present and outline our future plans.",poster,cp23
Biology,p1672,d5,3b3ad5eaddd5a970519b8c9b4097816fe374e8ec,c23,International Conference on Open and Big Data,The Proteomics Identifications (PRIDE) database and associated tools: status in 2013,"The PRoteomics IDEntifications (PRIDE, http://www.ebi.ac.uk/pride) database at the European Bioinformatics Institute is one of the most prominent data repositories of mass spectrometry (MS)-based proteomics data. Here, we summarize recent developments in the PRIDE database and related tools. First, we provide up-to-date statistics in data content, splitting the figures by groups of organisms and species, including peptide and protein identifications, and post-translational modifications. We then describe the tools that are part of the PRIDE submission pipeline, especially the recently developed PRIDE Converter 2 (new submission tool) and PRIDE Inspector (visualization and analysis tool). We also give an update about the integration of PRIDE with other MS proteomics resources in the context of the ProteomeXchange consortium. Finally, we briefly review the quality control efforts that are ongoing at present and outline our future plans.",poster,cp23
Medicine,p1675,d1,16cf42b0481042514a983e48b9994d60145553f1,c28,International Conference on Contemporary Computing,InterPro: the integrative protein signature database,"The InterPro database (http://www.ebi.ac.uk/interpro/) integrates together predictive models or ‘signatures’ representing protein domains, families and functional sites from multiple, diverse source databases: Gene3D, PANTHER, Pfam, PIRSF, PRINTS, ProDom, PROSITE, SMART, SUPERFAMILY and TIGRFAMs. Integration is performed manually and approximately half of the total ∼58 000 signatures available in the source databases belong to an InterPro entry. Recently, we have started to also display the remaining un-integrated signatures via our web interface. Other developments include the provision of non-signature data, such as structural data, in new XML files on our FTP site, as well as the inclusion of matchless UniProtKB proteins in the existing match XML files. The web interface has been extended and now links out to the ADAN predicted protein–protein interaction database and the SPICE and Dasty viewers. The latest public release (v18.0) covers 79.8% of UniProtKB (v14.1) and consists of 16 549 entries. InterPro data may be accessed either via the web address above, via web services, by downloading files by anonymous FTP or by using the InterProScan search software (http://www.ebi.ac.uk/Tools/InterProScan/).",poster,cp28
Computer Science,p1675,d3,16cf42b0481042514a983e48b9994d60145553f1,c28,International Conference on Contemporary Computing,InterPro: the integrative protein signature database,"The InterPro database (http://www.ebi.ac.uk/interpro/) integrates together predictive models or ‘signatures’ representing protein domains, families and functional sites from multiple, diverse source databases: Gene3D, PANTHER, Pfam, PIRSF, PRINTS, ProDom, PROSITE, SMART, SUPERFAMILY and TIGRFAMs. Integration is performed manually and approximately half of the total ∼58 000 signatures available in the source databases belong to an InterPro entry. Recently, we have started to also display the remaining un-integrated signatures via our web interface. Other developments include the provision of non-signature data, such as structural data, in new XML files on our FTP site, as well as the inclusion of matchless UniProtKB proteins in the existing match XML files. The web interface has been extended and now links out to the ADAN predicted protein–protein interaction database and the SPICE and Dasty viewers. The latest public release (v18.0) covers 79.8% of UniProtKB (v14.1) and consists of 16 549 entries. InterPro data may be accessed either via the web address above, via web services, by downloading files by anonymous FTP or by using the InterProScan search software (http://www.ebi.ac.uk/Tools/InterProScan/).",poster,cp28
Biology,p1675,d5,16cf42b0481042514a983e48b9994d60145553f1,c28,International Conference on Contemporary Computing,InterPro: the integrative protein signature database,"The InterPro database (http://www.ebi.ac.uk/interpro/) integrates together predictive models or ‘signatures’ representing protein domains, families and functional sites from multiple, diverse source databases: Gene3D, PANTHER, Pfam, PIRSF, PRINTS, ProDom, PROSITE, SMART, SUPERFAMILY and TIGRFAMs. Integration is performed manually and approximately half of the total ∼58 000 signatures available in the source databases belong to an InterPro entry. Recently, we have started to also display the remaining un-integrated signatures via our web interface. Other developments include the provision of non-signature data, such as structural data, in new XML files on our FTP site, as well as the inclusion of matchless UniProtKB proteins in the existing match XML files. The web interface has been extended and now links out to the ADAN predicted protein–protein interaction database and the SPICE and Dasty viewers. The latest public release (v18.0) covers 79.8% of UniProtKB (v14.1) and consists of 16 549 entries. InterPro data may be accessed either via the web address above, via web services, by downloading files by anonymous FTP or by using the InterProScan search software (http://www.ebi.ac.uk/Tools/InterProScan/).",poster,cp28
Medicine,p1677,d1,f49052eeb5607062941137c9468ff1122a19e016,c77,Visualization for Computer Security,Cochrane Database of Systematic Reviews,"Here, we bring readers extracts from the latest issues of The Cochrane Database of Systematic Reviews relevant to the fields of psychiatry and neurology. We feature a summary of the results, reviewers' conclusions, and implications for clinical practice and research, from selected new reviews featured in issues 6 and 7, 2011. For further information, visit www.thecochranelibrary.com. Copyright © 2011 Wiley Interface Ltd",poster,cp77
Medicine,p1680,d1,7400d5e1d211294ee2147355e863287f3cbad0a5,c99,Symposium on the Theory of Computing,PhagesDB: the actinobacteriophage database,"The Actinobacteriophage Database (PhagesDB) is a comprehensive, interactive, database-backed website that collects and shares information related to the discovery, characterization and genomics of viruses that infect Actinobacterial hosts. To date, more than 8000 bacteriophages-including over 1600 with sequenced genomes-have been entered into the database. PhagesDB plays a crucial role in organizing the discoveries of phage biologists around the world-including students in the SEA-PHAGES program-and has been cited in over 50 peer-reviewed articles.


Availability and Implementation
http://phagesdb.org/.


Contact
gfh@pitt.edu.",poster,cp99
Computer Science,p1680,d3,7400d5e1d211294ee2147355e863287f3cbad0a5,c99,Symposium on the Theory of Computing,PhagesDB: the actinobacteriophage database,"The Actinobacteriophage Database (PhagesDB) is a comprehensive, interactive, database-backed website that collects and shares information related to the discovery, characterization and genomics of viruses that infect Actinobacterial hosts. To date, more than 8000 bacteriophages-including over 1600 with sequenced genomes-have been entered into the database. PhagesDB plays a crucial role in organizing the discoveries of phage biologists around the world-including students in the SEA-PHAGES program-and has been cited in over 50 peer-reviewed articles.


Availability and Implementation
http://phagesdb.org/.


Contact
gfh@pitt.edu.",poster,cp99
Biology,p1680,d5,7400d5e1d211294ee2147355e863287f3cbad0a5,c99,Symposium on the Theory of Computing,PhagesDB: the actinobacteriophage database,"The Actinobacteriophage Database (PhagesDB) is a comprehensive, interactive, database-backed website that collects and shares information related to the discovery, characterization and genomics of viruses that infect Actinobacterial hosts. To date, more than 8000 bacteriophages-including over 1600 with sequenced genomes-have been entered into the database. PhagesDB plays a crucial role in organizing the discoveries of phage biologists around the world-including students in the SEA-PHAGES program-and has been cited in over 50 peer-reviewed articles.


Availability and Implementation
http://phagesdb.org/.


Contact
gfh@pitt.edu.",poster,cp99
Medicine,p1681,d1,3a19b36d7f41ba20a507dee2585fd56fa019167a,j193,Nature Genetics,Systematic meta-analyses of Alzheimer disease genetic association studies: the AlzGene database,Abstract,fullPaper,jv193
Biology,p1681,d5,3a19b36d7f41ba20a507dee2585fd56fa019167a,j193,Nature Genetics,Systematic meta-analyses of Alzheimer disease genetic association studies: the AlzGene database,Abstract,fullPaper,jv193
Medicine,p1683,d1,5fec3cda0d994c217c737c10b0b7e56d4574edca,c3,Knowledge Discovery and Data Mining,"MEROPS: the database of proteolytic enzymes, their substrates and inhibitors","Peptidases, their substrates and inhibitors are of great relevance to biology, medicine and biotechnology. The MEROPS database (http://merops.sanger.ac.uk) aims to fulfill the need for an integrated source of information about these. The database has hierarchical classifications in which homologous sets of peptidases and protein inhibitors are grouped into protein species, which are grouped into families, which are in turn grouped into clans. Recent developments include the following. A community annotation project has been instigated in which acknowledged experts are invited to contribute summaries for peptidases. Software has been written to provide an Internet-based data entry form. Contributors are acknowledged on the relevant web page. A new display showing the intron/exon structures of eukaryote peptidase genes and the phasing of the junctions has been implemented. It is now possible to filter the list of peptidases from a completely sequenced bacterial genome for a particular strain of the organism. The MEROPS filing pipeline has been altered to circumvent the restrictions imposed on non-interactive blastp searches, and a HMMER search using specially generated alignments to maximize the distribution of organisms returned in the search results has been added.",poster,cp3
Computer Science,p1683,d3,5fec3cda0d994c217c737c10b0b7e56d4574edca,c3,Knowledge Discovery and Data Mining,"MEROPS: the database of proteolytic enzymes, their substrates and inhibitors","Peptidases, their substrates and inhibitors are of great relevance to biology, medicine and biotechnology. The MEROPS database (http://merops.sanger.ac.uk) aims to fulfill the need for an integrated source of information about these. The database has hierarchical classifications in which homologous sets of peptidases and protein inhibitors are grouped into protein species, which are grouped into families, which are in turn grouped into clans. Recent developments include the following. A community annotation project has been instigated in which acknowledged experts are invited to contribute summaries for peptidases. Software has been written to provide an Internet-based data entry form. Contributors are acknowledged on the relevant web page. A new display showing the intron/exon structures of eukaryote peptidase genes and the phasing of the junctions has been implemented. It is now possible to filter the list of peptidases from a completely sequenced bacterial genome for a particular strain of the organism. The MEROPS filing pipeline has been altered to circumvent the restrictions imposed on non-interactive blastp searches, and a HMMER search using specially generated alignments to maximize the distribution of organisms returned in the search results has been added.",poster,cp3
Medicine,p1684,d1,b43ffeb049adb791a82521eaaf83c367d651da4c,c69,Neural Information Processing Systems,The BioGRID interaction database: 2013 update,"The Biological General Repository for Interaction Datasets (BioGRID: http//thebiogrid.org) is an open access archive of genetic and protein interactions that are curated from the primary biomedical literature for all major model organism species. As of September 2012, BioGRID houses more than 500 000 manually annotated interactions from more than 30 model organisms. BioGRID maintains complete curation coverage of the literature for the budding yeast Saccharomyces cerevisiae, the fission yeast Schizosaccharomyces pombe and the model plant Arabidopsis thaliana. A number of themed curation projects in areas of biomedical importance are also supported. BioGRID has established collaborations and/or shares data records for the annotation of interactions and phenotypes with most major model organism databases, including Saccharomyces Genome Database, PomBase, WormBase, FlyBase and The Arabidopsis Information Resource. BioGRID also actively engages with the text-mining community to benchmark and deploy automated tools to expedite curation workflows. BioGRID data are freely accessible through both a user-defined interactive interface and in batch downloads in a wide variety of formats, including PSI-MI2.5 and tab-delimited files. BioGRID records can also be interrogated and analyzed with a series of new bioinformatics tools, which include a post-translational modification viewer, a graphical viewer, a REST service and a Cytoscape plugin.",poster,cp69
Computer Science,p1684,d3,b43ffeb049adb791a82521eaaf83c367d651da4c,c69,Neural Information Processing Systems,The BioGRID interaction database: 2013 update,"The Biological General Repository for Interaction Datasets (BioGRID: http//thebiogrid.org) is an open access archive of genetic and protein interactions that are curated from the primary biomedical literature for all major model organism species. As of September 2012, BioGRID houses more than 500 000 manually annotated interactions from more than 30 model organisms. BioGRID maintains complete curation coverage of the literature for the budding yeast Saccharomyces cerevisiae, the fission yeast Schizosaccharomyces pombe and the model plant Arabidopsis thaliana. A number of themed curation projects in areas of biomedical importance are also supported. BioGRID has established collaborations and/or shares data records for the annotation of interactions and phenotypes with most major model organism databases, including Saccharomyces Genome Database, PomBase, WormBase, FlyBase and The Arabidopsis Information Resource. BioGRID also actively engages with the text-mining community to benchmark and deploy automated tools to expedite curation workflows. BioGRID data are freely accessible through both a user-defined interactive interface and in batch downloads in a wide variety of formats, including PSI-MI2.5 and tab-delimited files. BioGRID records can also be interrogated and analyzed with a series of new bioinformatics tools, which include a post-translational modification viewer, a graphical viewer, a REST service and a Cytoscape plugin.",poster,cp69
Biology,p1684,d5,b43ffeb049adb791a82521eaaf83c367d651da4c,c69,Neural Information Processing Systems,The BioGRID interaction database: 2013 update,"The Biological General Repository for Interaction Datasets (BioGRID: http//thebiogrid.org) is an open access archive of genetic and protein interactions that are curated from the primary biomedical literature for all major model organism species. As of September 2012, BioGRID houses more than 500 000 manually annotated interactions from more than 30 model organisms. BioGRID maintains complete curation coverage of the literature for the budding yeast Saccharomyces cerevisiae, the fission yeast Schizosaccharomyces pombe and the model plant Arabidopsis thaliana. A number of themed curation projects in areas of biomedical importance are also supported. BioGRID has established collaborations and/or shares data records for the annotation of interactions and phenotypes with most major model organism databases, including Saccharomyces Genome Database, PomBase, WormBase, FlyBase and The Arabidopsis Information Resource. BioGRID also actively engages with the text-mining community to benchmark and deploy automated tools to expedite curation workflows. BioGRID data are freely accessible through both a user-defined interactive interface and in batch downloads in a wide variety of formats, including PSI-MI2.5 and tab-delimited files. BioGRID records can also be interrogated and analyzed with a series of new bioinformatics tools, which include a post-translational modification viewer, a graphical viewer, a REST service and a Cytoscape plugin.",poster,cp69
Medicine,p1686,d1,d5837fd1cf0d51e547553c3612928de880a08589,c72,Workshop on Research on Enterprise Networking,JASPAR: an open-access database for eukaryotic transcription factor binding profiles,"The analysis of regulatory regions in genome sequences is strongly based on the detection of potential transcription factor binding sites. The preferred models for representation of transcription factor binding specificity have been termed position-specific scoring matrices. JASPAR is an open-access database of annotated, high-quality, matrix-based transcription factor binding site profiles for multicellular eukaryotes. The profiles were derived exclusively from sets of nucleotide sequences experimentally demonstrated to bind transcription factors. The database is complemented by a web interface for browsing, searching and subset selection, an online sequence analysis utility and a suite of programming tools for genome-wide and comparative genomic analysis of regulatory regions. JASPAR is available at http://jaspar. cgb.ki.se.",poster,cp72
Computer Science,p1686,d3,d5837fd1cf0d51e547553c3612928de880a08589,c72,Workshop on Research on Enterprise Networking,JASPAR: an open-access database for eukaryotic transcription factor binding profiles,"The analysis of regulatory regions in genome sequences is strongly based on the detection of potential transcription factor binding sites. The preferred models for representation of transcription factor binding specificity have been termed position-specific scoring matrices. JASPAR is an open-access database of annotated, high-quality, matrix-based transcription factor binding site profiles for multicellular eukaryotes. The profiles were derived exclusively from sets of nucleotide sequences experimentally demonstrated to bind transcription factors. The database is complemented by a web interface for browsing, searching and subset selection, an online sequence analysis utility and a suite of programming tools for genome-wide and comparative genomic analysis of regulatory regions. JASPAR is available at http://jaspar. cgb.ki.se.",poster,cp72
Biology,p1686,d5,d5837fd1cf0d51e547553c3612928de880a08589,c72,Workshop on Research on Enterprise Networking,JASPAR: an open-access database for eukaryotic transcription factor binding profiles,"The analysis of regulatory regions in genome sequences is strongly based on the detection of potential transcription factor binding sites. The preferred models for representation of transcription factor binding specificity have been termed position-specific scoring matrices. JASPAR is an open-access database of annotated, high-quality, matrix-based transcription factor binding site profiles for multicellular eukaryotes. The profiles were derived exclusively from sets of nucleotide sequences experimentally demonstrated to bind transcription factors. The database is complemented by a web interface for browsing, searching and subset selection, an online sequence analysis utility and a suite of programming tools for genome-wide and comparative genomic analysis of regulatory regions. JASPAR is available at http://jaspar. cgb.ki.se.",poster,cp72
Medicine,p1687,d1,1fcf323ff79c46b401e7a3d8510f50da17073cb8,c102,ACM SIGMOD Conference,The UCSC Genome Browser Database: update 2006,"The University of California Santa Cruz Genome Browser Database (GBD) contains sequence and annotation data for the genomes of about a dozen vertebrate species and several major model organisms. Genome annotations typically include assembly data, sequence composition, genes and gene predictions, mRNA and expressed sequence tag evidence, comparative genomics, regulation, expression and variation data. The database is optimized to support fast interactive performance with web tools that provide powerful visualization and querying capabilities for mining the data. The Genome Browser displays a wide variety of annotations at all scales from single nucleotide level up to a full chromosome. The Table Browser provides direct access to the database tables and sequence data, enabling complex queries on genome-wide datasets. The Proteome Browser graphically displays protein properties. The Gene Sorter allows filtering and comparison of genes by several metrics including expression data and several gene properties. BLAT and In Silico PCR search for sequences in entire genomes in seconds. These tools are highly integrated and provide many hyperlinks to other databases and websites. The GBD, browsing tools, downloadable data files and links to documentation and other information can be found at .",poster,cp102
Computer Science,p1687,d3,1fcf323ff79c46b401e7a3d8510f50da17073cb8,c102,ACM SIGMOD Conference,The UCSC Genome Browser Database: update 2006,"The University of California Santa Cruz Genome Browser Database (GBD) contains sequence and annotation data for the genomes of about a dozen vertebrate species and several major model organisms. Genome annotations typically include assembly data, sequence composition, genes and gene predictions, mRNA and expressed sequence tag evidence, comparative genomics, regulation, expression and variation data. The database is optimized to support fast interactive performance with web tools that provide powerful visualization and querying capabilities for mining the data. The Genome Browser displays a wide variety of annotations at all scales from single nucleotide level up to a full chromosome. The Table Browser provides direct access to the database tables and sequence data, enabling complex queries on genome-wide datasets. The Proteome Browser graphically displays protein properties. The Gene Sorter allows filtering and comparison of genes by several metrics including expression data and several gene properties. BLAT and In Silico PCR search for sequences in entire genomes in seconds. These tools are highly integrated and provide many hyperlinks to other databases and websites. The GBD, browsing tools, downloadable data files and links to documentation and other information can be found at .",poster,cp102
Biology,p1687,d5,1fcf323ff79c46b401e7a3d8510f50da17073cb8,c102,ACM SIGMOD Conference,The UCSC Genome Browser Database: update 2006,"The University of California Santa Cruz Genome Browser Database (GBD) contains sequence and annotation data for the genomes of about a dozen vertebrate species and several major model organisms. Genome annotations typically include assembly data, sequence composition, genes and gene predictions, mRNA and expressed sequence tag evidence, comparative genomics, regulation, expression and variation data. The database is optimized to support fast interactive performance with web tools that provide powerful visualization and querying capabilities for mining the data. The Genome Browser displays a wide variety of annotations at all scales from single nucleotide level up to a full chromosome. The Table Browser provides direct access to the database tables and sequence data, enabling complex queries on genome-wide datasets. The Proteome Browser graphically displays protein properties. The Gene Sorter allows filtering and comparison of genes by several metrics including expression data and several gene properties. BLAT and In Silico PCR search for sequences in entire genomes in seconds. These tools are highly integrated and provide many hyperlinks to other databases and websites. The GBD, browsing tools, downloadable data files and links to documentation and other information can be found at .",poster,cp102
Medicine,p1688,d1,f8dc73630864b4903f8f57b5c8d7b098eb300e4f,c106,International Conference on Biometrics,"Reactome: a database of reactions, pathways and biological processes","Reactome (http://www.reactome.org) is a collaboration among groups at the Ontario Institute for Cancer Research, Cold Spring Harbor Laboratory, New York University School of Medicine and The European Bioinformatics Institute, to develop an open source curated bioinformatics database of human pathways and reactions. Recently, we developed a new web site with improved tools for pathway browsing and data analysis. The Pathway Browser is an Systems Biology Graphical Notation (SBGN)-based visualization system that supports zooming, scrolling and event highlighting. It exploits PSIQUIC web services to overlay our curated pathways with molecular interaction data from the Reactome Functional Interaction Network and external interaction databases such as IntAct, BioGRID, ChEMBL, iRefIndex, MINT and STRING. Our Pathway and Expression Analysis tools enable ID mapping, pathway assignment and overrepresentation analysis of user-supplied data sets. To support pathway annotation and analysis in other species, we continue to make orthology-based inferences of pathways in non-human species, applying Ensembl Compara to identify orthologs of curated human proteins in each of 20 other species. The resulting inferred pathway sets can be browsed and analyzed with our Species Comparison tool. Collaborations are also underway to create manually curated data sets on the Reactome framework for chicken, Drosophila and rice.",poster,cp106
Computer Science,p1688,d3,f8dc73630864b4903f8f57b5c8d7b098eb300e4f,c106,International Conference on Biometrics,"Reactome: a database of reactions, pathways and biological processes","Reactome (http://www.reactome.org) is a collaboration among groups at the Ontario Institute for Cancer Research, Cold Spring Harbor Laboratory, New York University School of Medicine and The European Bioinformatics Institute, to develop an open source curated bioinformatics database of human pathways and reactions. Recently, we developed a new web site with improved tools for pathway browsing and data analysis. The Pathway Browser is an Systems Biology Graphical Notation (SBGN)-based visualization system that supports zooming, scrolling and event highlighting. It exploits PSIQUIC web services to overlay our curated pathways with molecular interaction data from the Reactome Functional Interaction Network and external interaction databases such as IntAct, BioGRID, ChEMBL, iRefIndex, MINT and STRING. Our Pathway and Expression Analysis tools enable ID mapping, pathway assignment and overrepresentation analysis of user-supplied data sets. To support pathway annotation and analysis in other species, we continue to make orthology-based inferences of pathways in non-human species, applying Ensembl Compara to identify orthologs of curated human proteins in each of 20 other species. The resulting inferred pathway sets can be browsed and analyzed with our Species Comparison tool. Collaborations are also underway to create manually curated data sets on the Reactome framework for chicken, Drosophila and rice.",poster,cp106
Biology,p1688,d5,f8dc73630864b4903f8f57b5c8d7b098eb300e4f,c106,International Conference on Biometrics,"Reactome: a database of reactions, pathways and biological processes","Reactome (http://www.reactome.org) is a collaboration among groups at the Ontario Institute for Cancer Research, Cold Spring Harbor Laboratory, New York University School of Medicine and The European Bioinformatics Institute, to develop an open source curated bioinformatics database of human pathways and reactions. Recently, we developed a new web site with improved tools for pathway browsing and data analysis. The Pathway Browser is an Systems Biology Graphical Notation (SBGN)-based visualization system that supports zooming, scrolling and event highlighting. It exploits PSIQUIC web services to overlay our curated pathways with molecular interaction data from the Reactome Functional Interaction Network and external interaction databases such as IntAct, BioGRID, ChEMBL, iRefIndex, MINT and STRING. Our Pathway and Expression Analysis tools enable ID mapping, pathway assignment and overrepresentation analysis of user-supplied data sets. To support pathway annotation and analysis in other species, we continue to make orthology-based inferences of pathways in non-human species, applying Ensembl Compara to identify orthologs of curated human proteins in each of 20 other species. The resulting inferred pathway sets can be browsed and analyzed with our Species Comparison tool. Collaborations are also underway to create manually curated data sets on the Reactome framework for chicken, Drosophila and rice.",poster,cp106
Medicine,p1690,d1,f780775b85986a2bf1d5849c56f621cb99efef3c,c109,Computer Vision and Pattern Recognition,The Dfam database of repetitive DNA families,"Repetitive DNA, especially that due to transposable elements (TEs), makes up a large fraction of many genomes. Dfam is an open access database of families of repetitive DNA elements, in which each family is represented by a multiple sequence alignment and a profile hidden Markov model (HMM). The initial release of Dfam, featured in the 2013 NAR Database Issue, contained 1143 families of repetitive elements found in humans, and was used to produce more than 100 Mb of additional annotation of TE-derived regions in the human genome, with improved speed. Here, we describe recent advances, most notably expansion to 4150 total families including a comprehensive set of known repeat families from four new organisms (mouse, zebrafish, fly and nematode). We describe improvements to coverage, and to our methods for identifying and reducing false annotation. We also describe updates to the website interface. The Dfam website has moved to http://dfam.org. Seed alignments, profile HMMs, hit lists and other underlying data are available for download.",poster,cp109
Computer Science,p1690,d3,f780775b85986a2bf1d5849c56f621cb99efef3c,c109,Computer Vision and Pattern Recognition,The Dfam database of repetitive DNA families,"Repetitive DNA, especially that due to transposable elements (TEs), makes up a large fraction of many genomes. Dfam is an open access database of families of repetitive DNA elements, in which each family is represented by a multiple sequence alignment and a profile hidden Markov model (HMM). The initial release of Dfam, featured in the 2013 NAR Database Issue, contained 1143 families of repetitive elements found in humans, and was used to produce more than 100 Mb of additional annotation of TE-derived regions in the human genome, with improved speed. Here, we describe recent advances, most notably expansion to 4150 total families including a comprehensive set of known repeat families from four new organisms (mouse, zebrafish, fly and nematode). We describe improvements to coverage, and to our methods for identifying and reducing false annotation. We also describe updates to the website interface. The Dfam website has moved to http://dfam.org. Seed alignments, profile HMMs, hit lists and other underlying data are available for download.",poster,cp109
Biology,p1690,d5,f780775b85986a2bf1d5849c56f621cb99efef3c,c109,Computer Vision and Pattern Recognition,The Dfam database of repetitive DNA families,"Repetitive DNA, especially that due to transposable elements (TEs), makes up a large fraction of many genomes. Dfam is an open access database of families of repetitive DNA elements, in which each family is represented by a multiple sequence alignment and a profile hidden Markov model (HMM). The initial release of Dfam, featured in the 2013 NAR Database Issue, contained 1143 families of repetitive elements found in humans, and was used to produce more than 100 Mb of additional annotation of TE-derived regions in the human genome, with improved speed. Here, we describe recent advances, most notably expansion to 4150 total families including a comprehensive set of known repeat families from four new organisms (mouse, zebrafish, fly and nematode). We describe improvements to coverage, and to our methods for identifying and reducing false annotation. We also describe updates to the website interface. The Dfam website has moved to http://dfam.org. Seed alignments, profile HMMs, hit lists and other underlying data are available for download.",poster,cp109
Medicine,p1693,d1,677872c4051ee37a13161ecb8efa32fcbadc7a80,c7,International Symposium on Intelligent Data Analysis,MEROPS: the peptidase database,"Peptidases, their substrates and inhibitors are of great relevance to biology, medicine and biotechnology. The MEROPS database (http://merops.sanger.ac.uk) aims to fulfil the need for an integrated source of information about these. The database has a hierarchical classification in which homologous sets of peptidases and protein inhibitors are grouped into protein species, which are grouped into families, which are in turn grouped into clans. The classification framework is used for attaching information at each level. An important focus of the database has become distinguishing one peptidase from another through identifying the specificity of the peptidase in terms of where it will cleave substrates and with which inhibitors it will interact. We have collected over 39 000 known cleavage sites in proteins, peptides and synthetic substrates. These allow us to display peptidase specificity and alignments of protein substrates to give an indication of how well a cleavage site is conserved, and thus its probable physiological relevance. While the number of new peptidase families and clans has only grown slowly the number of complete genomes has greatly increased. This has allowed us to add an analysis tool to the relevant species pages to show significant gains and losses of peptidase genes relative to related species.",poster,cp7
Computer Science,p1693,d3,677872c4051ee37a13161ecb8efa32fcbadc7a80,c7,International Symposium on Intelligent Data Analysis,MEROPS: the peptidase database,"Peptidases, their substrates and inhibitors are of great relevance to biology, medicine and biotechnology. The MEROPS database (http://merops.sanger.ac.uk) aims to fulfil the need for an integrated source of information about these. The database has a hierarchical classification in which homologous sets of peptidases and protein inhibitors are grouped into protein species, which are grouped into families, which are in turn grouped into clans. The classification framework is used for attaching information at each level. An important focus of the database has become distinguishing one peptidase from another through identifying the specificity of the peptidase in terms of where it will cleave substrates and with which inhibitors it will interact. We have collected over 39 000 known cleavage sites in proteins, peptides and synthetic substrates. These allow us to display peptidase specificity and alignments of protein substrates to give an indication of how well a cleavage site is conserved, and thus its probable physiological relevance. While the number of new peptidase families and clans has only grown slowly the number of complete genomes has greatly increased. This has allowed us to add an analysis tool to the relevant species pages to show significant gains and losses of peptidase genes relative to related species.",poster,cp7
Biology,p1693,d5,677872c4051ee37a13161ecb8efa32fcbadc7a80,c7,International Symposium on Intelligent Data Analysis,MEROPS: the peptidase database,"Peptidases, their substrates and inhibitors are of great relevance to biology, medicine and biotechnology. The MEROPS database (http://merops.sanger.ac.uk) aims to fulfil the need for an integrated source of information about these. The database has a hierarchical classification in which homologous sets of peptidases and protein inhibitors are grouped into protein species, which are grouped into families, which are in turn grouped into clans. The classification framework is used for attaching information at each level. An important focus of the database has become distinguishing one peptidase from another through identifying the specificity of the peptidase in terms of where it will cleave substrates and with which inhibitors it will interact. We have collected over 39 000 known cleavage sites in proteins, peptides and synthetic substrates. These allow us to display peptidase specificity and alignments of protein substrates to give an indication of how well a cleavage site is conserved, and thus its probable physiological relevance. While the number of new peptidase families and clans has only grown slowly the number of complete genomes has greatly increased. This has allowed us to add an analysis tool to the relevant species pages to show significant gains and losses of peptidase genes relative to related species.",poster,cp7
Medicine,p1694,d1,a7a581f7f052570fa099dcb887b9934617760791,j343,Proteomics,Comet: An open‐source MS/MS sequence database search tool,"Proteomics research routinely involves identifying peptides and proteins via MS/MS sequence database search. Thus the database search engine is an integral tool in many proteomics research groups. Here, we introduce the Comet search engine to the existing landscape of commercial and open‐source database search tools. Comet is open source, freely available, and based on one of the original sequence database search tools that has been widely used for many years.",fullPaper,jv343
Biology,p1694,d5,a7a581f7f052570fa099dcb887b9934617760791,j343,Proteomics,Comet: An open‐source MS/MS sequence database search tool,"Proteomics research routinely involves identifying peptides and proteins via MS/MS sequence database search. Thus the database search engine is an integral tool in many proteomics research groups. Here, we introduce the Comet search engine to the existing landscape of commercial and open‐source database search tools. Comet is open source, freely available, and based on one of the original sequence database search tools that has been widely used for many years.",fullPaper,jv343
Medicine,p1695,d1,fb1d51c023da037dac928d7991de1b77ade22728,j376,Therapeutic Drug Monitoring,METLIN: A Metabolite Mass Spectral Database,"Endogenous metabolites have gained increasing interest over the past 5 years largely for their implications in diagnostic and pharmaceutical biomarker discovery. METLIN (http://metlin.scripps.edu), a freely accessible web-based data repository, has been developed to assist in a broad array of metabolite research and to facilitate metabolite identification through mass analysis. METLIN includes an annotated list of known metabolite structural information that is easily cross-correlated with its catalogue of high-resolution Fourier transform mass spectrometry (FTMS) spectra, tandem mass spectrometry (MS/MS) spectra, and LC/MS data.",fullPaper,jv376
Chemistry,p1695,d8,fb1d51c023da037dac928d7991de1b77ade22728,j376,Therapeutic Drug Monitoring,METLIN: A Metabolite Mass Spectral Database,"Endogenous metabolites have gained increasing interest over the past 5 years largely for their implications in diagnostic and pharmaceutical biomarker discovery. METLIN (http://metlin.scripps.edu), a freely accessible web-based data repository, has been developed to assist in a broad array of metabolite research and to facilitate metabolite identification through mass analysis. METLIN includes an annotated list of known metabolite structural information that is easily cross-correlated with its catalogue of high-resolution Fourier transform mass spectrometry (FTMS) spectra, tandem mass spectrometry (MS/MS) spectra, and LC/MS data.",fullPaper,jv376
Medicine,p1698,d1,ecf1848db9f0e81a8a0dbffeb8162772cb6be733,c97,International Conference on Computational Logic,The COG database: new developments in phylogenetic classification of proteins from complete genomes,"The database of Clusters of Orthologous Groups of proteins (COGs), which represents an attempt on a phylogenetic classification of the proteins encoded in complete genomes, currently consists of 2791 COGs including 45 350 proteins from 30 genomes of bacteria, archaea and the yeast Saccharomyces cerevisiae (http://www.ncbi.nlm.nih. gov/COG). In addition, a supplement to the COGs is available, in which proteins encoded in the genomes of two multicellular eukaryotes, the nematode Caenorhabditis elegans and the fruit fly Drosophila melanogaster, and shared with bacteria and/or archaea were included. The new features added to the COG database include information pages with structural and functional details on each COG and literature references, improvements of the COGNITOR program that is used to fit new proteins into the COGs, and classification of genomes and COGs constructed by using principal component analysis.",poster,cp97
Computer Science,p1698,d3,ecf1848db9f0e81a8a0dbffeb8162772cb6be733,c97,International Conference on Computational Logic,The COG database: new developments in phylogenetic classification of proteins from complete genomes,"The database of Clusters of Orthologous Groups of proteins (COGs), which represents an attempt on a phylogenetic classification of the proteins encoded in complete genomes, currently consists of 2791 COGs including 45 350 proteins from 30 genomes of bacteria, archaea and the yeast Saccharomyces cerevisiae (http://www.ncbi.nlm.nih. gov/COG). In addition, a supplement to the COGs is available, in which proteins encoded in the genomes of two multicellular eukaryotes, the nematode Caenorhabditis elegans and the fruit fly Drosophila melanogaster, and shared with bacteria and/or archaea were included. The new features added to the COG database include information pages with structural and functional details on each COG and literature references, improvements of the COGNITOR program that is used to fit new proteins into the COGs, and classification of genomes and COGs constructed by using principal component analysis.",poster,cp97
Biology,p1698,d5,ecf1848db9f0e81a8a0dbffeb8162772cb6be733,c97,International Conference on Computational Logic,The COG database: new developments in phylogenetic classification of proteins from complete genomes,"The database of Clusters of Orthologous Groups of proteins (COGs), which represents an attempt on a phylogenetic classification of the proteins encoded in complete genomes, currently consists of 2791 COGs including 45 350 proteins from 30 genomes of bacteria, archaea and the yeast Saccharomyces cerevisiae (http://www.ncbi.nlm.nih. gov/COG). In addition, a supplement to the COGs is available, in which proteins encoded in the genomes of two multicellular eukaryotes, the nematode Caenorhabditis elegans and the fruit fly Drosophila melanogaster, and shared with bacteria and/or archaea were included. The new features added to the COG database include information pages with structural and functional details on each COG and literature references, improvements of the COGNITOR program that is used to fit new proteins into the COGs, and classification of genomes and COGs constructed by using principal component analysis.",poster,cp97
Medicine,p1700,d1,6b092ee6d04b4fa76a0452cf0696086a7c04644d,c39,Online World Conference on Soft Computing in Industrial Applications,"DIP, the Database of Interacting Proteins: a research tool for studying cellular networks of protein interactions","The Database of Interacting Proteins (DIP: http://dip.doe-mbi.ucla.edu) is a database that documents experimentally determined protein-protein interactions. It provides the scientific community with an integrated set of tools for browsing and extracting information about protein interaction networks. As of September 2001, the DIP catalogs approximately 11 000 unique interactions among 5900 proteins from >80 organisms; the vast majority from yeast, Helicobacter pylori and human. Tools have been developed that allow users to analyze, visualize and integrate their own experimental data with the information about protein-protein interactions available in the DIP database.",poster,cp39
Computer Science,p1700,d3,6b092ee6d04b4fa76a0452cf0696086a7c04644d,c39,Online World Conference on Soft Computing in Industrial Applications,"DIP, the Database of Interacting Proteins: a research tool for studying cellular networks of protein interactions","The Database of Interacting Proteins (DIP: http://dip.doe-mbi.ucla.edu) is a database that documents experimentally determined protein-protein interactions. It provides the scientific community with an integrated set of tools for browsing and extracting information about protein interaction networks. As of September 2001, the DIP catalogs approximately 11 000 unique interactions among 5900 proteins from >80 organisms; the vast majority from yeast, Helicobacter pylori and human. Tools have been developed that allow users to analyze, visualize and integrate their own experimental data with the information about protein-protein interactions available in the DIP database.",poster,cp39
Biology,p1700,d5,6b092ee6d04b4fa76a0452cf0696086a7c04644d,c39,Online World Conference on Soft Computing in Industrial Applications,"DIP, the Database of Interacting Proteins: a research tool for studying cellular networks of protein interactions","The Database of Interacting Proteins (DIP: http://dip.doe-mbi.ucla.edu) is a database that documents experimentally determined protein-protein interactions. It provides the scientific community with an integrated set of tools for browsing and extracting information about protein interaction networks. As of September 2001, the DIP catalogs approximately 11 000 unique interactions among 5900 proteins from >80 organisms; the vast majority from yeast, Helicobacter pylori and human. Tools have been developed that allow users to analyze, visualize and integrate their own experimental data with the information about protein-protein interactions available in the DIP database.",poster,cp39
Medicine,p1702,d1,f86808d344d7884296c222f0cd6892abe411c636,j107,Nucleic Acids Research,The Ribosomal Database Project.,"The Ribosomal Database Project (RDP) is a curated database that offers ribosome-related data, analysis services, and associated computer programs. The offerings include phylogenetically ordered alignments of ribosomal RNA (rRNA) sequences, derived phylogenetic trees, rRNA secondary structure diagrams, and various software for handling, analyzing and displaying alignments and trees. The data are available via anonymous ftp (rdp.life.uiuc.edu), electronic mail (server/rdp.life.uiuc.edu) and gopher (rdpgopher.life.uiuc.edu). The electronic mail server also provides ribosomal probe checking, approximate phylogenetic placement of user-submitted sequences, screening for chimeric nature of newly sequenced rRNAs, and automated alignment.",fullPaper,jv107
Computer Science,p1702,d3,f86808d344d7884296c222f0cd6892abe411c636,j107,Nucleic Acids Research,The Ribosomal Database Project.,"The Ribosomal Database Project (RDP) is a curated database that offers ribosome-related data, analysis services, and associated computer programs. The offerings include phylogenetically ordered alignments of ribosomal RNA (rRNA) sequences, derived phylogenetic trees, rRNA secondary structure diagrams, and various software for handling, analyzing and displaying alignments and trees. The data are available via anonymous ftp (rdp.life.uiuc.edu), electronic mail (server/rdp.life.uiuc.edu) and gopher (rdpgopher.life.uiuc.edu). The electronic mail server also provides ribosomal probe checking, approximate phylogenetic placement of user-submitted sequences, screening for chimeric nature of newly sequenced rRNAs, and automated alignment.",fullPaper,jv107
Medicine,p1703,d1,60add4035c2b6d4b9888cb1028c31e6bed793d60,c98,Vision,The UCSC Genome Browser database: 2014 update,"The University of California Santa Cruz (UCSC) Genome Browser (http://genome.ucsc.edu) offers online public access to a growing database of genomic sequence and annotations for a large collection of organisms, primarily vertebrates, with an emphasis on the human and mouse genomes. The Browser’s web-based tools provide an integrated environment for visualizing, comparing, analysing and sharing both publicly available and user-generated genomic data sets. As of September 2013, the database contained genomic sequence and a basic set of annotation ‘tracks’ for ∼90 organisms. Significant new annotations include a 60-species multiple alignment conservation track on the mouse, updated UCSC Genes tracks for human and mouse, and several new sets of variation and ENCODE data. New software tools include a Variant Annotation Integrator that returns predicted functional effects of a set of variants uploaded as a custom track, an extension to UCSC Genes that displays haplotype alleles for protein-coding genes and an expansion of data hubs that includes the capability to display remotely hosted user-provided assembly sequence in addition to annotation data. To improve European access, we have added a Genome Browser mirror (http://genome-euro.ucsc.edu) hosted at Bielefeld University in Germany.",poster,cp98
Computer Science,p1703,d3,60add4035c2b6d4b9888cb1028c31e6bed793d60,c98,Vision,The UCSC Genome Browser database: 2014 update,"The University of California Santa Cruz (UCSC) Genome Browser (http://genome.ucsc.edu) offers online public access to a growing database of genomic sequence and annotations for a large collection of organisms, primarily vertebrates, with an emphasis on the human and mouse genomes. The Browser’s web-based tools provide an integrated environment for visualizing, comparing, analysing and sharing both publicly available and user-generated genomic data sets. As of September 2013, the database contained genomic sequence and a basic set of annotation ‘tracks’ for ∼90 organisms. Significant new annotations include a 60-species multiple alignment conservation track on the mouse, updated UCSC Genes tracks for human and mouse, and several new sets of variation and ENCODE data. New software tools include a Variant Annotation Integrator that returns predicted functional effects of a set of variants uploaded as a custom track, an extension to UCSC Genes that displays haplotype alleles for protein-coding genes and an expansion of data hubs that includes the capability to display remotely hosted user-provided assembly sequence in addition to annotation data. To improve European access, we have added a Genome Browser mirror (http://genome-euro.ucsc.edu) hosted at Bielefeld University in Germany.",poster,cp98
Biology,p1703,d5,60add4035c2b6d4b9888cb1028c31e6bed793d60,c98,Vision,The UCSC Genome Browser database: 2014 update,"The University of California Santa Cruz (UCSC) Genome Browser (http://genome.ucsc.edu) offers online public access to a growing database of genomic sequence and annotations for a large collection of organisms, primarily vertebrates, with an emphasis on the human and mouse genomes. The Browser’s web-based tools provide an integrated environment for visualizing, comparing, analysing and sharing both publicly available and user-generated genomic data sets. As of September 2013, the database contained genomic sequence and a basic set of annotation ‘tracks’ for ∼90 organisms. Significant new annotations include a 60-species multiple alignment conservation track on the mouse, updated UCSC Genes tracks for human and mouse, and several new sets of variation and ENCODE data. New software tools include a Variant Annotation Integrator that returns predicted functional effects of a set of variants uploaded as a custom track, an extension to UCSC Genes that displays haplotype alleles for protein-coding genes and an expansion of data hubs that includes the capability to display remotely hosted user-provided assembly sequence in addition to annotation data. To improve European access, we have added a Genome Browser mirror (http://genome-euro.ucsc.edu) hosted at Bielefeld University in Germany.",poster,cp98
Medicine,p1705,d1,b0738246da135894f966469ed110be183ec4bbff,c3,Knowledge Discovery and Data Mining,CircFunBase: a database for functional circular RNAs,"Abstract Increasing evidence reveals that circular RNAs (circRNAs) are widespread in eukaryotes and play important roles in diverse biological processes. However, a comprehensive functionally annotated circRNA database is still lacking. CircFunBase is a web-accessible database that aims to provide a high-quality functional circRNA resource including experimentally validated and computationally predicted functions. The current version of CircFunBase documents more than 7000 manually curated functional circRNA entries, mainly including Homo sapiens, Mus musculus etc. CircFunBase provides visualized circRNA-miRNA interaction networks. In addition, a genome browser is provided to visualize the genome context of circRNAs. As a biological information platform for circRNAs, CircFunBase will contribute for circRNA studies and bridge the gap between circRNAs and their functions.",poster,cp3
Computer Science,p1705,d3,b0738246da135894f966469ed110be183ec4bbff,c3,Knowledge Discovery and Data Mining,CircFunBase: a database for functional circular RNAs,"Abstract Increasing evidence reveals that circular RNAs (circRNAs) are widespread in eukaryotes and play important roles in diverse biological processes. However, a comprehensive functionally annotated circRNA database is still lacking. CircFunBase is a web-accessible database that aims to provide a high-quality functional circRNA resource including experimentally validated and computationally predicted functions. The current version of CircFunBase documents more than 7000 manually curated functional circRNA entries, mainly including Homo sapiens, Mus musculus etc. CircFunBase provides visualized circRNA-miRNA interaction networks. In addition, a genome browser is provided to visualize the genome context of circRNAs. As a biological information platform for circRNAs, CircFunBase will contribute for circRNA studies and bridge the gap between circRNAs and their functions.",poster,cp3
Biology,p1705,d5,b0738246da135894f966469ed110be183ec4bbff,c3,Knowledge Discovery and Data Mining,CircFunBase: a database for functional circular RNAs,"Abstract Increasing evidence reveals that circular RNAs (circRNAs) are widespread in eukaryotes and play important roles in diverse biological processes. However, a comprehensive functionally annotated circRNA database is still lacking. CircFunBase is a web-accessible database that aims to provide a high-quality functional circRNA resource including experimentally validated and computationally predicted functions. The current version of CircFunBase documents more than 7000 manually curated functional circRNA entries, mainly including Homo sapiens, Mus musculus etc. CircFunBase provides visualized circRNA-miRNA interaction networks. In addition, a genome browser is provided to visualize the genome context of circRNAs. As a biological information platform for circRNAs, CircFunBase will contribute for circRNA studies and bridge the gap between circRNAs and their functions.",poster,cp3
Medicine,p1707,d1,10b40befe5942e997f88ab40bac1acd931147893,c59,Australian Software Engineering Conference,"PATRIC, the bacterial bioinformatics database and analysis resource","The Pathosystems Resource Integration Center (PATRIC) is the all-bacterial Bioinformatics Resource Center (BRC) (http://www.patricbrc.org). A joint effort by two of the original National Institute of Allergy and Infectious Diseases-funded BRCs, PATRIC provides researchers with an online resource that stores and integrates a variety of data types [e.g. genomics, transcriptomics, protein–protein interactions (PPIs), three-dimensional protein structures and sequence typing data] and associated metadata. Datatypes are summarized for individual genomes and across taxonomic levels. All genomes in PATRIC, currently more than 10 000, are consistently annotated using RAST, the Rapid Annotations using Subsystems Technology. Summaries of different data types are also provided for individual genes, where comparisons of different annotations are available, and also include available transcriptomic data. PATRIC provides a variety of ways for researchers to find data of interest and a private workspace where they can store both genomic and gene associations, and their own private data. Both private and public data can be analyzed together using a suite of tools to perform comparative genomic or transcriptomic analysis. PATRIC also includes integrated information related to disease and PPIs. All the data and integrated analysis and visualization tools are freely available. This manuscript describes updates to the PATRIC since its initial report in the 2007 NAR Database Issue.",poster,cp59
Computer Science,p1707,d3,10b40befe5942e997f88ab40bac1acd931147893,c59,Australian Software Engineering Conference,"PATRIC, the bacterial bioinformatics database and analysis resource","The Pathosystems Resource Integration Center (PATRIC) is the all-bacterial Bioinformatics Resource Center (BRC) (http://www.patricbrc.org). A joint effort by two of the original National Institute of Allergy and Infectious Diseases-funded BRCs, PATRIC provides researchers with an online resource that stores and integrates a variety of data types [e.g. genomics, transcriptomics, protein–protein interactions (PPIs), three-dimensional protein structures and sequence typing data] and associated metadata. Datatypes are summarized for individual genomes and across taxonomic levels. All genomes in PATRIC, currently more than 10 000, are consistently annotated using RAST, the Rapid Annotations using Subsystems Technology. Summaries of different data types are also provided for individual genes, where comparisons of different annotations are available, and also include available transcriptomic data. PATRIC provides a variety of ways for researchers to find data of interest and a private workspace where they can store both genomic and gene associations, and their own private data. Both private and public data can be analyzed together using a suite of tools to perform comparative genomic or transcriptomic analysis. PATRIC also includes integrated information related to disease and PPIs. All the data and integrated analysis and visualization tools are freely available. This manuscript describes updates to the PATRIC since its initial report in the 2007 NAR Database Issue.",poster,cp59
Biology,p1707,d5,10b40befe5942e997f88ab40bac1acd931147893,c59,Australian Software Engineering Conference,"PATRIC, the bacterial bioinformatics database and analysis resource","The Pathosystems Resource Integration Center (PATRIC) is the all-bacterial Bioinformatics Resource Center (BRC) (http://www.patricbrc.org). A joint effort by two of the original National Institute of Allergy and Infectious Diseases-funded BRCs, PATRIC provides researchers with an online resource that stores and integrates a variety of data types [e.g. genomics, transcriptomics, protein–protein interactions (PPIs), three-dimensional protein structures and sequence typing data] and associated metadata. Datatypes are summarized for individual genomes and across taxonomic levels. All genomes in PATRIC, currently more than 10 000, are consistently annotated using RAST, the Rapid Annotations using Subsystems Technology. Summaries of different data types are also provided for individual genes, where comparisons of different annotations are available, and also include available transcriptomic data. PATRIC provides a variety of ways for researchers to find data of interest and a private workspace where they can store both genomic and gene associations, and their own private data. Both private and public data can be analyzed together using a suite of tools to perform comparative genomic or transcriptomic analysis. PATRIC also includes integrated information related to disease and PPIs. All the data and integrated analysis and visualization tools are freely available. This manuscript describes updates to the PATRIC since its initial report in the 2007 NAR Database Issue.",poster,cp59
Medicine,p1710,d1,24019050c30b7e5bf1be28e48b8cb5278c4286fd,c103,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,PH2 - A dermoscopic image database for research and benchmarking,"The increasing incidence of melanoma has recently promoted the development of computer-aided diagnosis systems for the classification of dermoscopic images. Unfortunately, the performance of such systems cannot be compared since they are evaluated in different sets of images by their authors and there are no public databases available to perform a fair evaluation of multiple systems. In this paper, a dermoscopic image database, called PH2, is presented. The PH2 database includes the manual segmentation, the clinical diagnosis, and the identification of several dermoscopic structures, performed by expert dermatologists, in a set of 200 dermoscopic images. The PH2 database will be made freely available for research and benchmarking purposes.",fullPaper,cp103
Computer Science,p1710,d3,24019050c30b7e5bf1be28e48b8cb5278c4286fd,c103,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,PH2 - A dermoscopic image database for research and benchmarking,"The increasing incidence of melanoma has recently promoted the development of computer-aided diagnosis systems for the classification of dermoscopic images. Unfortunately, the performance of such systems cannot be compared since they are evaluated in different sets of images by their authors and there are no public databases available to perform a fair evaluation of multiple systems. In this paper, a dermoscopic image database, called PH2, is presented. The PH2 database includes the manual segmentation, the clinical diagnosis, and the identification of several dermoscopic structures, performed by expert dermatologists, in a set of 200 dermoscopic images. The PH2 database will be made freely available for research and benchmarking purposes.",fullPaper,cp103
Medicine,p1712,d1,5d0b1c9e2e24b38489c8021261217b023e42a652,c31,Information Security Solutions Europe,OPM database and PPM web server: resources for positioning of proteins in membranes,"The Orientations of Proteins in Membranes (OPM) database is a curated web resource that provides spatial positions of membrane-bound peptides and proteins of known three-dimensional structure in the lipid bilayer, together with their structural classification, topology and intracellular localization. OPM currently contains more than 1200 transmembrane and peripheral proteins and peptides from approximately 350 organisms that represent approximately 3800 Protein Data Bank entries. Proteins are classified into classes, superfamilies and families and assigned to 21 distinct membrane types. Spatial positions of proteins with respect to the lipid bilayer are optimized by the PPM 2.0 method that accounts for the hydrophobic, hydrogen bonding and electrostatic interactions of the proteins with the anisotropic water-lipid environment described by the dielectric constant and hydrogen-bonding profiles. The OPM database is freely accessible at http://opm.phar.umich.edu. Data can be sorted, searched or retrieved using the hierarchical classification, source organism, localization in different types of membranes. The database offers downloadable coordinates of proteins and peptides with membrane boundaries. A gallery of protein images and several visualization tools are provided. The database is supplemented by the PPM server (http://opm.phar.umich.edu/server.php) which can be used for calculating spatial positions in membranes of newly determined proteins structures or theoretical models.",poster,cp31
Computer Science,p1712,d3,5d0b1c9e2e24b38489c8021261217b023e42a652,c31,Information Security Solutions Europe,OPM database and PPM web server: resources for positioning of proteins in membranes,"The Orientations of Proteins in Membranes (OPM) database is a curated web resource that provides spatial positions of membrane-bound peptides and proteins of known three-dimensional structure in the lipid bilayer, together with their structural classification, topology and intracellular localization. OPM currently contains more than 1200 transmembrane and peripheral proteins and peptides from approximately 350 organisms that represent approximately 3800 Protein Data Bank entries. Proteins are classified into classes, superfamilies and families and assigned to 21 distinct membrane types. Spatial positions of proteins with respect to the lipid bilayer are optimized by the PPM 2.0 method that accounts for the hydrophobic, hydrogen bonding and electrostatic interactions of the proteins with the anisotropic water-lipid environment described by the dielectric constant and hydrogen-bonding profiles. The OPM database is freely accessible at http://opm.phar.umich.edu. Data can be sorted, searched or retrieved using the hierarchical classification, source organism, localization in different types of membranes. The database offers downloadable coordinates of proteins and peptides with membrane boundaries. A gallery of protein images and several visualization tools are provided. The database is supplemented by the PPM server (http://opm.phar.umich.edu/server.php) which can be used for calculating spatial positions in membranes of newly determined proteins structures or theoretical models.",poster,cp31
Biology,p1712,d5,5d0b1c9e2e24b38489c8021261217b023e42a652,c31,Information Security Solutions Europe,OPM database and PPM web server: resources for positioning of proteins in membranes,"The Orientations of Proteins in Membranes (OPM) database is a curated web resource that provides spatial positions of membrane-bound peptides and proteins of known three-dimensional structure in the lipid bilayer, together with their structural classification, topology and intracellular localization. OPM currently contains more than 1200 transmembrane and peripheral proteins and peptides from approximately 350 organisms that represent approximately 3800 Protein Data Bank entries. Proteins are classified into classes, superfamilies and families and assigned to 21 distinct membrane types. Spatial positions of proteins with respect to the lipid bilayer are optimized by the PPM 2.0 method that accounts for the hydrophobic, hydrogen bonding and electrostatic interactions of the proteins with the anisotropic water-lipid environment described by the dielectric constant and hydrogen-bonding profiles. The OPM database is freely accessible at http://opm.phar.umich.edu. Data can be sorted, searched or retrieved using the hierarchical classification, source organism, localization in different types of membranes. The database offers downloadable coordinates of proteins and peptides with membrane boundaries. A gallery of protein images and several visualization tools are provided. The database is supplemented by the PPM server (http://opm.phar.umich.edu/server.php) which can be used for calculating spatial positions in membranes of newly determined proteins structures or theoretical models.",poster,cp31
Medicine,p1716,d1,6787a08978444d97d6860359ba24b5a735d492e2,c18,International Conference on Exploring Services Science,RegNetwork: an integrated database of transcriptional and post-transcriptional regulatory networks in human and mouse,"Transcriptional and post-transcriptional regulation of gene expression is of fundamental importance to numerous biological processes. Nowadays, an increasing amount of gene regulatory relationships have been documented in various databases and literature. However, to more efficiently exploit such knowledge for biomedical research and applications, it is necessary to construct a genome-wide regulatory network database to integrate the information on gene regulatory relationships that are widely scattered in many different places. Therefore, in this work, we build a knowledge-based database, named ‘RegNetwork’, of gene regulatory networks for human and mouse by collecting and integrating the documented regulatory interactions among transcription factors (TFs), microRNAs (miRNAs) and target genes from 25 selected databases. Moreover, we also inferred and incorporated potential regulatory relationships based on transcription factor binding site (TFBS) motifs into RegNetwork. As a result, RegNetwork contains a comprehensive set of experimentally observed or predicted transcriptional and post-transcriptional regulatory relationships, and the database framework is flexibly designed for potential extensions to include gene regulatory networks for other organisms in the future. Based on RegNetwork, we characterized the statistical and topological properties of genome-wide regulatory networks for human and mouse, we also extracted and interpreted simple yet important network motifs that involve the interplays between TF-miRNA and their targets. In summary, RegNetwork provides an integrated resource on the prior information for gene regulatory relationships, and it enables us to further investigate context-specific transcriptional and post-transcriptional regulatory interactions based on domain-specific experimental data. Database URL: http://www.regnetworkweb.org",poster,cp18
Computer Science,p1716,d3,6787a08978444d97d6860359ba24b5a735d492e2,c18,International Conference on Exploring Services Science,RegNetwork: an integrated database of transcriptional and post-transcriptional regulatory networks in human and mouse,"Transcriptional and post-transcriptional regulation of gene expression is of fundamental importance to numerous biological processes. Nowadays, an increasing amount of gene regulatory relationships have been documented in various databases and literature. However, to more efficiently exploit such knowledge for biomedical research and applications, it is necessary to construct a genome-wide regulatory network database to integrate the information on gene regulatory relationships that are widely scattered in many different places. Therefore, in this work, we build a knowledge-based database, named ‘RegNetwork’, of gene regulatory networks for human and mouse by collecting and integrating the documented regulatory interactions among transcription factors (TFs), microRNAs (miRNAs) and target genes from 25 selected databases. Moreover, we also inferred and incorporated potential regulatory relationships based on transcription factor binding site (TFBS) motifs into RegNetwork. As a result, RegNetwork contains a comprehensive set of experimentally observed or predicted transcriptional and post-transcriptional regulatory relationships, and the database framework is flexibly designed for potential extensions to include gene regulatory networks for other organisms in the future. Based on RegNetwork, we characterized the statistical and topological properties of genome-wide regulatory networks for human and mouse, we also extracted and interpreted simple yet important network motifs that involve the interplays between TF-miRNA and their targets. In summary, RegNetwork provides an integrated resource on the prior information for gene regulatory relationships, and it enables us to further investigate context-specific transcriptional and post-transcriptional regulatory interactions based on domain-specific experimental data. Database URL: http://www.regnetworkweb.org",poster,cp18
Biology,p1716,d5,6787a08978444d97d6860359ba24b5a735d492e2,c18,International Conference on Exploring Services Science,RegNetwork: an integrated database of transcriptional and post-transcriptional regulatory networks in human and mouse,"Transcriptional and post-transcriptional regulation of gene expression is of fundamental importance to numerous biological processes. Nowadays, an increasing amount of gene regulatory relationships have been documented in various databases and literature. However, to more efficiently exploit such knowledge for biomedical research and applications, it is necessary to construct a genome-wide regulatory network database to integrate the information on gene regulatory relationships that are widely scattered in many different places. Therefore, in this work, we build a knowledge-based database, named ‘RegNetwork’, of gene regulatory networks for human and mouse by collecting and integrating the documented regulatory interactions among transcription factors (TFs), microRNAs (miRNAs) and target genes from 25 selected databases. Moreover, we also inferred and incorporated potential regulatory relationships based on transcription factor binding site (TFBS) motifs into RegNetwork. As a result, RegNetwork contains a comprehensive set of experimentally observed or predicted transcriptional and post-transcriptional regulatory relationships, and the database framework is flexibly designed for potential extensions to include gene regulatory networks for other organisms in the future. Based on RegNetwork, we characterized the statistical and topological properties of genome-wide regulatory networks for human and mouse, we also extracted and interpreted simple yet important network motifs that involve the interplays between TF-miRNA and their targets. In summary, RegNetwork provides an integrated resource on the prior information for gene regulatory relationships, and it enables us to further investigate context-specific transcriptional and post-transcriptional regulatory interactions based on domain-specific experimental data. Database URL: http://www.regnetworkweb.org",poster,cp18
Medicine,p1717,d1,5637a5e6cf3a858a8adc17efdf714ce21dbe1a2e,c86,International Conference on Big Data and Education,Mouse Genome Database (MGD)-2018: knowledgebase for the laboratory mouse,"Abstract The Mouse Genome Database (MGD; http://www.informatics.jax.org) is the key community mouse database which supports basic, translational and computational research by providing integrated data on the genetics, genomics, and biology of the laboratory mouse. MGD serves as the source for biological reference data sets related to mouse genes, gene functions, phenotypes and disease models with an increasing emphasis on the association of these data to human biology and disease. We report here on recent enhancements to this resource, including improved access to mouse disease model and human phenotype data and enhanced relationships of mouse models to human disease.",poster,cp86
Computer Science,p1717,d3,5637a5e6cf3a858a8adc17efdf714ce21dbe1a2e,c86,International Conference on Big Data and Education,Mouse Genome Database (MGD)-2018: knowledgebase for the laboratory mouse,"Abstract The Mouse Genome Database (MGD; http://www.informatics.jax.org) is the key community mouse database which supports basic, translational and computational research by providing integrated data on the genetics, genomics, and biology of the laboratory mouse. MGD serves as the source for biological reference data sets related to mouse genes, gene functions, phenotypes and disease models with an increasing emphasis on the association of these data to human biology and disease. We report here on recent enhancements to this resource, including improved access to mouse disease model and human phenotype data and enhanced relationships of mouse models to human disease.",poster,cp86
Biology,p1717,d5,5637a5e6cf3a858a8adc17efdf714ce21dbe1a2e,c86,International Conference on Big Data and Education,Mouse Genome Database (MGD)-2018: knowledgebase for the laboratory mouse,"Abstract The Mouse Genome Database (MGD; http://www.informatics.jax.org) is the key community mouse database which supports basic, translational and computational research by providing integrated data on the genetics, genomics, and biology of the laboratory mouse. MGD serves as the source for biological reference data sets related to mouse genes, gene functions, phenotypes and disease models with an increasing emphasis on the association of these data to human biology and disease. We report here on recent enhancements to this resource, including improved access to mouse disease model and human phenotype data and enhanced relationships of mouse models to human disease.",poster,cp86
Medicine,p1718,d1,ad9a8e6346a1259022abf526670eaf592270e59f,c62,International Conference on Advanced Data and Information Engineering,Mouse Genome Database (MGD)-2017: community knowledge resource for the laboratory mouse,"The Mouse Genome Database (MGD: http://www.informatics.jax.org) is the primary community data resource for the laboratory mouse. It provides a highly integrated and highly curated system offering a comprehensive view of current knowledge about mouse genes, genetic markers and genomic features as well as the associations of those features with sequence, phenotypes, functional and comparative information, and their relationships to human diseases. MGD continues to enhance access to these data, to extend the scope of data content and visualizations, and to provide infrastructure and user support that ensures effective and efficient use of MGD in the advancement of scientific knowledge. Here, we report on recent enhancements made to the resource and new features.",poster,cp62
Computer Science,p1718,d3,ad9a8e6346a1259022abf526670eaf592270e59f,c62,International Conference on Advanced Data and Information Engineering,Mouse Genome Database (MGD)-2017: community knowledge resource for the laboratory mouse,"The Mouse Genome Database (MGD: http://www.informatics.jax.org) is the primary community data resource for the laboratory mouse. It provides a highly integrated and highly curated system offering a comprehensive view of current knowledge about mouse genes, genetic markers and genomic features as well as the associations of those features with sequence, phenotypes, functional and comparative information, and their relationships to human diseases. MGD continues to enhance access to these data, to extend the scope of data content and visualizations, and to provide infrastructure and user support that ensures effective and efficient use of MGD in the advancement of scientific knowledge. Here, we report on recent enhancements made to the resource and new features.",poster,cp62
Biology,p1718,d5,ad9a8e6346a1259022abf526670eaf592270e59f,c62,International Conference on Advanced Data and Information Engineering,Mouse Genome Database (MGD)-2017: community knowledge resource for the laboratory mouse,"The Mouse Genome Database (MGD: http://www.informatics.jax.org) is the primary community data resource for the laboratory mouse. It provides a highly integrated and highly curated system offering a comprehensive view of current knowledge about mouse genes, genetic markers and genomic features as well as the associations of those features with sequence, phenotypes, functional and comparative information, and their relationships to human diseases. MGD continues to enhance access to these data, to extend the scope of data content and visualizations, and to provide infrastructure and user support that ensures effective and efficient use of MGD in the advancement of scientific knowledge. Here, we report on recent enhancements made to the resource and new features.",poster,cp62
Medicine,p1719,d1,7d25222fcf62dae782ae6b18fae0b33d9f7923fe,j377,Human Mutation,Human Gene Mutation Database (HGMD®): 2003 update,"The Human Gene Mutation Database (HGMD) constitutes a comprehensive core collection of data on germ‐line mutations in nuclear genes underlying or associated with human inherited disease (www.hgmd.org). Data catalogued includes: single base‐pair substitutions in coding, regulatory and splicing‐relevant regions; micro‐deletions and micro‐insertions; indels; triplet repeat expansions as well as gross deletions; insertions; duplications; and complex rearrangements. Each mutation is entered into HGMD only once in order to avoid confusion between recurrent and identical‐by‐descent lesions. By March 2003, the database contained in excess of 39,415 different lesions detected in 1,516 different nuclear genes, with new entries currently accumulating at a rate exceeding 5,000 per annum. Since its inception, HGMD has been expanded to include cDNA reference sequences for more than 87% of listed genes, splice junction sequences, disease‐associated and functional polymorphisms, as well as links to data present in publicly available online locus‐specific mutation databases. Although HGMD has recently entered into a licensing agreement with Celera Genomics (Rockville, MD), mutation data will continue to be made freely available via the Internet. Hum Mutat 21:577–581, 2003. © 2003 Wiley‐Liss, Inc.",fullPaper,jv377
Biology,p1719,d5,7d25222fcf62dae782ae6b18fae0b33d9f7923fe,j377,Human Mutation,Human Gene Mutation Database (HGMD®): 2003 update,"The Human Gene Mutation Database (HGMD) constitutes a comprehensive core collection of data on germ‐line mutations in nuclear genes underlying or associated with human inherited disease (www.hgmd.org). Data catalogued includes: single base‐pair substitutions in coding, regulatory and splicing‐relevant regions; micro‐deletions and micro‐insertions; indels; triplet repeat expansions as well as gross deletions; insertions; duplications; and complex rearrangements. Each mutation is entered into HGMD only once in order to avoid confusion between recurrent and identical‐by‐descent lesions. By March 2003, the database contained in excess of 39,415 different lesions detected in 1,516 different nuclear genes, with new entries currently accumulating at a rate exceeding 5,000 per annum. Since its inception, HGMD has been expanded to include cDNA reference sequences for more than 87% of listed genes, splice junction sequences, disease‐associated and functional polymorphisms, as well as links to data present in publicly available online locus‐specific mutation databases. Although HGMD has recently entered into a licensing agreement with Celera Genomics (Rockville, MD), mutation data will continue to be made freely available via the Internet. Hum Mutat 21:577–581, 2003. © 2003 Wiley‐Liss, Inc.",fullPaper,jv377
Medicine,p1720,d1,a094ea0a295db1b4d86d504cd885fde5d3396c75,c58,Extreme Science and Engineering Discovery Environment,The RDP-II (Ribosomal Database Project),"The Ribosomal Database Project (RDP-II), previously described by Maidak et al. [Nucleic Acids Res. (2000), 28, 173-174], continued during the past year to add new rRNA sequences to the aligned data and to improve the analysis commands. Release 8.0 (June 1, 2000) consisted of 16 277 aligned prokaryotic small subunit (SSU) rRNA sequences while the number of eukaryotic and mitochondrial SSU rRNA sequences in aligned form remained at 2055 and 1503, respectively. The number of prokaryotic SSU rRNA sequences more than doubled from the previous release 14 months earlier, and approximately 75% are longer than 899 bp. An RDP-II mirror site in Japan is now available (http://wdcm.nig.ac.jp/RDP/html/index.h tml). RDP-II provides aligned and annotated rRNA sequences, derived phylogenetic trees and taxonomic hierarchies, and analysis services through its WWW server (http://rdp.cme.msu.edu/). Analysis services include rRNA probe checking, approximate phylogenetic placement of user sequences, screening user sequences for possible chimeric rRNA sequences, automated alignment, production of similarity matrices and services to plan and analyze terminal restriction fragment polymorphism experiments. The RDP-II email address for questions and comments has been changed from curator@cme.msu.edu to rdpstaff@msu.edu.",poster,cp58
Computer Science,p1720,d3,a094ea0a295db1b4d86d504cd885fde5d3396c75,c58,Extreme Science and Engineering Discovery Environment,The RDP-II (Ribosomal Database Project),"The Ribosomal Database Project (RDP-II), previously described by Maidak et al. [Nucleic Acids Res. (2000), 28, 173-174], continued during the past year to add new rRNA sequences to the aligned data and to improve the analysis commands. Release 8.0 (June 1, 2000) consisted of 16 277 aligned prokaryotic small subunit (SSU) rRNA sequences while the number of eukaryotic and mitochondrial SSU rRNA sequences in aligned form remained at 2055 and 1503, respectively. The number of prokaryotic SSU rRNA sequences more than doubled from the previous release 14 months earlier, and approximately 75% are longer than 899 bp. An RDP-II mirror site in Japan is now available (http://wdcm.nig.ac.jp/RDP/html/index.h tml). RDP-II provides aligned and annotated rRNA sequences, derived phylogenetic trees and taxonomic hierarchies, and analysis services through its WWW server (http://rdp.cme.msu.edu/). Analysis services include rRNA probe checking, approximate phylogenetic placement of user sequences, screening user sequences for possible chimeric rRNA sequences, automated alignment, production of similarity matrices and services to plan and analyze terminal restriction fragment polymorphism experiments. The RDP-II email address for questions and comments has been changed from curator@cme.msu.edu to rdpstaff@msu.edu.",poster,cp58
Biology,p1720,d5,a094ea0a295db1b4d86d504cd885fde5d3396c75,c58,Extreme Science and Engineering Discovery Environment,The RDP-II (Ribosomal Database Project),"The Ribosomal Database Project (RDP-II), previously described by Maidak et al. [Nucleic Acids Res. (2000), 28, 173-174], continued during the past year to add new rRNA sequences to the aligned data and to improve the analysis commands. Release 8.0 (June 1, 2000) consisted of 16 277 aligned prokaryotic small subunit (SSU) rRNA sequences while the number of eukaryotic and mitochondrial SSU rRNA sequences in aligned form remained at 2055 and 1503, respectively. The number of prokaryotic SSU rRNA sequences more than doubled from the previous release 14 months earlier, and approximately 75% are longer than 899 bp. An RDP-II mirror site in Japan is now available (http://wdcm.nig.ac.jp/RDP/html/index.h tml). RDP-II provides aligned and annotated rRNA sequences, derived phylogenetic trees and taxonomic hierarchies, and analysis services through its WWW server (http://rdp.cme.msu.edu/). Analysis services include rRNA probe checking, approximate phylogenetic placement of user sequences, screening user sequences for possible chimeric rRNA sequences, automated alignment, production of similarity matrices and services to plan and analyze terminal restriction fragment polymorphism experiments. The RDP-II email address for questions and comments has been changed from curator@cme.msu.edu to rdpstaff@msu.edu.",poster,cp58
Medicine,p1721,d1,fded6f9cefb06c1da7acddd5b9b89b1eabb08b7d,c98,Vision,The Ensembl genome database project,"The Ensembl (http://www.ensembl.org/) database project provides a bioinformatics framework to organise biology around the sequences of large genomes. It is a comprehensive source of stable automatic annotation of the human genome sequence, with confirmed gene predictions that have been integrated with external data sources, and is available as either an interactive web site or as flat files. It is also an open source software engineering project to develop a portable system able to handle very large genomes and associated requirements from sequence analysis to data storage and visualisation. The Ensembl site is one of the leading sources of human genome sequence annotation and provided much of the analysis for publication by the international human genome project of the draft genome. The Ensembl system is being installed around the world in both companies and academic sites on machines ranging from supercomputers to laptops.",poster,cp98
Computer Science,p1721,d3,fded6f9cefb06c1da7acddd5b9b89b1eabb08b7d,c98,Vision,The Ensembl genome database project,"The Ensembl (http://www.ensembl.org/) database project provides a bioinformatics framework to organise biology around the sequences of large genomes. It is a comprehensive source of stable automatic annotation of the human genome sequence, with confirmed gene predictions that have been integrated with external data sources, and is available as either an interactive web site or as flat files. It is also an open source software engineering project to develop a portable system able to handle very large genomes and associated requirements from sequence analysis to data storage and visualisation. The Ensembl site is one of the leading sources of human genome sequence annotation and provided much of the analysis for publication by the international human genome project of the draft genome. The Ensembl system is being installed around the world in both companies and academic sites on machines ranging from supercomputers to laptops.",poster,cp98
Biology,p1721,d5,fded6f9cefb06c1da7acddd5b9b89b1eabb08b7d,c98,Vision,The Ensembl genome database project,"The Ensembl (http://www.ensembl.org/) database project provides a bioinformatics framework to organise biology around the sequences of large genomes. It is a comprehensive source of stable automatic annotation of the human genome sequence, with confirmed gene predictions that have been integrated with external data sources, and is available as either an interactive web site or as flat files. It is also an open source software engineering project to develop a portable system able to handle very large genomes and associated requirements from sequence analysis to data storage and visualisation. The Ensembl site is one of the leading sources of human genome sequence annotation and provided much of the analysis for publication by the international human genome project of the draft genome. The Ensembl system is being installed around the world in both companies and academic sites on machines ranging from supercomputers to laptops.",poster,cp98
Medicine,p1722,d1,355f5e57eab748d07ec768e4c07b06dfd66a3ba5,c45,IEEE Symposium on Security and Privacy,BindingDB: a web-accessible database of experimentally determined protein–ligand binding affinities,"BindingDB () is a publicly accessible database currently containing ∼20 000 experimentally determined binding affinities of protein–ligand complexes, for 110 protein targets including isoforms and mutational variants, and ∼11 000 small molecule ligands. The data are extracted from the scientific literature, data collection focusing on proteins that are drug-targets or candidate drug-targets and for which structural data are present in the Protein Data Bank. The BindingDB website supports a range of query types, including searches by chemical structure, substructure and similarity; protein sequence; ligand and protein names; affinity ranges and molecular weight. Data sets generated by BindingDB queries can be downloaded in the form of annotated SDfiles for further analysis, or used as the basis for virtual screening of a compound database uploaded by the user. The data in BindingDB are linked both to structural data in the PDB via PDB IDs and chemical and sequence searches, and to the literature in PubMed via PubMed IDs.",poster,cp45
Computer Science,p1722,d3,355f5e57eab748d07ec768e4c07b06dfd66a3ba5,c45,IEEE Symposium on Security and Privacy,BindingDB: a web-accessible database of experimentally determined protein–ligand binding affinities,"BindingDB () is a publicly accessible database currently containing ∼20 000 experimentally determined binding affinities of protein–ligand complexes, for 110 protein targets including isoforms and mutational variants, and ∼11 000 small molecule ligands. The data are extracted from the scientific literature, data collection focusing on proteins that are drug-targets or candidate drug-targets and for which structural data are present in the Protein Data Bank. The BindingDB website supports a range of query types, including searches by chemical structure, substructure and similarity; protein sequence; ligand and protein names; affinity ranges and molecular weight. Data sets generated by BindingDB queries can be downloaded in the form of annotated SDfiles for further analysis, or used as the basis for virtual screening of a compound database uploaded by the user. The data in BindingDB are linked both to structural data in the PDB via PDB IDs and chemical and sequence searches, and to the literature in PubMed via PubMed IDs.",poster,cp45
Biology,p1722,d5,355f5e57eab748d07ec768e4c07b06dfd66a3ba5,c45,IEEE Symposium on Security and Privacy,BindingDB: a web-accessible database of experimentally determined protein–ligand binding affinities,"BindingDB () is a publicly accessible database currently containing ∼20 000 experimentally determined binding affinities of protein–ligand complexes, for 110 protein targets including isoforms and mutational variants, and ∼11 000 small molecule ligands. The data are extracted from the scientific literature, data collection focusing on proteins that are drug-targets or candidate drug-targets and for which structural data are present in the Protein Data Bank. The BindingDB website supports a range of query types, including searches by chemical structure, substructure and similarity; protein sequence; ligand and protein names; affinity ranges and molecular weight. Data sets generated by BindingDB queries can be downloaded in the form of annotated SDfiles for further analysis, or used as the basis for virtual screening of a compound database uploaded by the user. The data in BindingDB are linked both to structural data in the PDB via PDB IDs and chemical and sequence searches, and to the literature in PubMed via PubMed IDs.",poster,cp45
Medicine,p1726,d1,a057d7736e3ae4675054a104a1301dba2ff8dbba,c2,International Conference on Software Engineering,The Pfam protein families database,"Pfam is a large collection of protein families and domains. Over the past 2 years the number of families in Pfam has doubled and now stands at 6190 (version 10.0). Methodology improvements for searching the Pfam collection locally as well as via the web are described. Other recent innovations include modelling of discontinuous domains allowing Pfam domain definitions to be closer to those found in structure databases. Pfam is available on the web in the UK (http://www.sanger.ac.uk/Software/Pfam/), the USA (http://pfam.wustl.edu/), France (http://pfam.jouy.inra.fr/) and Sweden (http://Pfam.cgb.ki.se/).",poster,cp2
Computer Science,p1726,d3,a057d7736e3ae4675054a104a1301dba2ff8dbba,c2,International Conference on Software Engineering,The Pfam protein families database,"Pfam is a large collection of protein families and domains. Over the past 2 years the number of families in Pfam has doubled and now stands at 6190 (version 10.0). Methodology improvements for searching the Pfam collection locally as well as via the web are described. Other recent innovations include modelling of discontinuous domains allowing Pfam domain definitions to be closer to those found in structure databases. Pfam is available on the web in the UK (http://www.sanger.ac.uk/Software/Pfam/), the USA (http://pfam.wustl.edu/), France (http://pfam.jouy.inra.fr/) and Sweden (http://Pfam.cgb.ki.se/).",poster,cp2
Medicine,p1727,d1,788b43b7c62b497cf69b31544c6f81c6f4856d42,c53,International Conference on Learning Representations,Pfam: the protein families database,"Pfam, available via servers in the UK (http://pfam.sanger.ac.uk/) and the USA (http://pfam.janelia.org/), is a widely used database of protein families, containing 14 831 manually curated entries in the current release, version 27.0. Since the last update article 2 years ago, we have generated 1182 new families and maintained sequence coverage of the UniProt Knowledgebase (UniProtKB) at nearly 80%, despite a 50% increase in the size of the underlying sequence database. Since our 2012 article describing Pfam, we have also undertaken a comprehensive review of the features that are provided by Pfam over and above the basic family data. For each feature, we determined the relevance, computational burden, usage statistics and the functionality of the feature in a website context. As a consequence of this review, we have removed some features, enhanced others and developed new ones to meet the changing demands of computational biology. Here, we describe the changes to Pfam content. Notably, we now provide family alignments based on four different representative proteome sequence data sets and a new interactive DNA search interface. We also discuss the mapping between Pfam and known 3D structures.",poster,cp53
Computer Science,p1727,d3,788b43b7c62b497cf69b31544c6f81c6f4856d42,c53,International Conference on Learning Representations,Pfam: the protein families database,"Pfam, available via servers in the UK (http://pfam.sanger.ac.uk/) and the USA (http://pfam.janelia.org/), is a widely used database of protein families, containing 14 831 manually curated entries in the current release, version 27.0. Since the last update article 2 years ago, we have generated 1182 new families and maintained sequence coverage of the UniProt Knowledgebase (UniProtKB) at nearly 80%, despite a 50% increase in the size of the underlying sequence database. Since our 2012 article describing Pfam, we have also undertaken a comprehensive review of the features that are provided by Pfam over and above the basic family data. For each feature, we determined the relevance, computational burden, usage statistics and the functionality of the feature in a website context. As a consequence of this review, we have removed some features, enhanced others and developed new ones to meet the changing demands of computational biology. Here, we describe the changes to Pfam content. Notably, we now provide family alignments based on four different representative proteome sequence data sets and a new interactive DNA search interface. We also discuss the mapping between Pfam and known 3D structures.",poster,cp53
Biology,p1727,d5,788b43b7c62b497cf69b31544c6f81c6f4856d42,c53,International Conference on Learning Representations,Pfam: the protein families database,"Pfam, available via servers in the UK (http://pfam.sanger.ac.uk/) and the USA (http://pfam.janelia.org/), is a widely used database of protein families, containing 14 831 manually curated entries in the current release, version 27.0. Since the last update article 2 years ago, we have generated 1182 new families and maintained sequence coverage of the UniProt Knowledgebase (UniProtKB) at nearly 80%, despite a 50% increase in the size of the underlying sequence database. Since our 2012 article describing Pfam, we have also undertaken a comprehensive review of the features that are provided by Pfam over and above the basic family data. For each feature, we determined the relevance, computational burden, usage statistics and the functionality of the feature in a website context. As a consequence of this review, we have removed some features, enhanced others and developed new ones to meet the changing demands of computational biology. Here, we describe the changes to Pfam content. Notably, we now provide family alignments based on four different representative proteome sequence data sets and a new interactive DNA search interface. We also discuss the mapping between Pfam and known 3D structures.",poster,cp53
Medicine,p1729,d1,dc85ca80fb3d75fe63106f631a2f7cd251d2851e,j193,Nature Genetics,Identification of protein coding regions by database similarity search,Abstract,fullPaper,jv193
Biology,p1729,d5,dc85ca80fb3d75fe63106f631a2f7cd251d2851e,j193,Nature Genetics,Identification of protein coding regions by database similarity search,Abstract,fullPaper,jv193
Medicine,p1730,d1,3962ddcab496d4a2a7196dcef370c87f58a3133d,c80,International Symposium on Software Engineering for Adaptive and Self-Managing Systems,The NCBI Taxonomy database,"The NCBI Taxonomy database (http://www.ncbi.nlm.nih.gov/taxonomy) is the standard nomenclature and classification repository for the International Nucleotide Sequence Database Collaboration (INSDC), comprising the GenBank, ENA (EMBL) and DDBJ databases. It includes organism names and taxonomic lineages for each of the sequences represented in the INSDC’s nucleotide and protein sequence databases. The taxonomy database is manually curated by a small group of scientists at the NCBI who use the current taxonomic literature to maintain a phylogenetic taxonomy for the source organisms represented in the sequence databases. The taxonomy database is a central organizing hub for many of the resources at the NCBI, and provides a means for clustering elements within other domains of NCBI web site, for internal linking between domains of the Entrez system and for linking out to taxon-specific external resources on the web. Our primary purpose is to index the domain of sequences as conveniently as possible for our user community.",poster,cp80
Computer Science,p1730,d3,3962ddcab496d4a2a7196dcef370c87f58a3133d,c80,International Symposium on Software Engineering for Adaptive and Self-Managing Systems,The NCBI Taxonomy database,"The NCBI Taxonomy database (http://www.ncbi.nlm.nih.gov/taxonomy) is the standard nomenclature and classification repository for the International Nucleotide Sequence Database Collaboration (INSDC), comprising the GenBank, ENA (EMBL) and DDBJ databases. It includes organism names and taxonomic lineages for each of the sequences represented in the INSDC’s nucleotide and protein sequence databases. The taxonomy database is manually curated by a small group of scientists at the NCBI who use the current taxonomic literature to maintain a phylogenetic taxonomy for the source organisms represented in the sequence databases. The taxonomy database is a central organizing hub for many of the resources at the NCBI, and provides a means for clustering elements within other domains of NCBI web site, for internal linking between domains of the Entrez system and for linking out to taxon-specific external resources on the web. Our primary purpose is to index the domain of sequences as conveniently as possible for our user community.",poster,cp80
Biology,p1730,d5,3962ddcab496d4a2a7196dcef370c87f58a3133d,c80,International Symposium on Software Engineering for Adaptive and Self-Managing Systems,The NCBI Taxonomy database,"The NCBI Taxonomy database (http://www.ncbi.nlm.nih.gov/taxonomy) is the standard nomenclature and classification repository for the International Nucleotide Sequence Database Collaboration (INSDC), comprising the GenBank, ENA (EMBL) and DDBJ databases. It includes organism names and taxonomic lineages for each of the sequences represented in the INSDC’s nucleotide and protein sequence databases. The taxonomy database is manually curated by a small group of scientists at the NCBI who use the current taxonomic literature to maintain a phylogenetic taxonomy for the source organisms represented in the sequence databases. The taxonomy database is a central organizing hub for many of the resources at the NCBI, and provides a means for clustering elements within other domains of NCBI web site, for internal linking between domains of the Entrez system and for linking out to taxon-specific external resources on the web. Our primary purpose is to index the domain of sequences as conveniently as possible for our user community.",poster,cp80
Medicine,p1731,d1,a0e5802cf66257d0412de878682dc9caccca0719,c87,International Conference on Big Data Research,Rfam: an RNA family database,"Rfam is a collection of multiple sequence alignments and covariance models representing non-coding RNA families. Rfam is available on the web in the UK at http://www.sanger.ac.uk/Software/Rfam/ and in the US at http://rfam.wustl.edu/. These websites allow the user to search a query sequence against a library of covariance models, and view multiple sequence alignments and family annotation. The database can also be downloaded in flatfile form and searched locally using the INFERNAL package (http://infernal.wustl.edu/). The first release of Rfam (1.0) contains 25 families, which annotate over 50 000 non-coding RNA genes in the taxonomic divisions of the EMBL nucleotide database.",poster,cp87
Computer Science,p1731,d3,a0e5802cf66257d0412de878682dc9caccca0719,c87,International Conference on Big Data Research,Rfam: an RNA family database,"Rfam is a collection of multiple sequence alignments and covariance models representing non-coding RNA families. Rfam is available on the web in the UK at http://www.sanger.ac.uk/Software/Rfam/ and in the US at http://rfam.wustl.edu/. These websites allow the user to search a query sequence against a library of covariance models, and view multiple sequence alignments and family annotation. The database can also be downloaded in flatfile form and searched locally using the INFERNAL package (http://infernal.wustl.edu/). The first release of Rfam (1.0) contains 25 families, which annotate over 50 000 non-coding RNA genes in the taxonomic divisions of the EMBL nucleotide database.",poster,cp87
Biology,p1731,d5,a0e5802cf66257d0412de878682dc9caccca0719,c87,International Conference on Big Data Research,Rfam: an RNA family database,"Rfam is a collection of multiple sequence alignments and covariance models representing non-coding RNA families. Rfam is available on the web in the UK at http://www.sanger.ac.uk/Software/Rfam/ and in the US at http://rfam.wustl.edu/. These websites allow the user to search a query sequence against a library of covariance models, and view multiple sequence alignments and family annotation. The database can also be downloaded in flatfile form and searched locally using the INFERNAL package (http://infernal.wustl.edu/). The first release of Rfam (1.0) contains 25 families, which annotate over 50 000 non-coding RNA genes in the taxonomic divisions of the EMBL nucleotide database.",poster,cp87
Medicine,p1733,d1,0c13dc68a9b91e94dc23676ec878fa9f1794b866,j379,Accounts of Chemical Research,"The Reticular Chemistry Structure Resource (RCSR) database of, and symbols for, crystal nets.","During the past decade, interest has grown tremendously in the design and synthesis of crystalline materials constructed from molecular clusters linked by extended groups of atoms. Most notable are metal-organic frameworks (MOFs), in which polyatomic inorganic metal-containing clusters are joined by polytopic linkers. (Although these materials are sometimes referred to as coordination polymers, we prefer to differentiate them, because MOFs are based on strong linkages that yield robust frameworks.) The realization that MOFs could be designed and synthesized in a rational way from molecular building blocks led to the emergence of a discipline that we call reticular chemistry. MOFs can be represented as a special kind of graph called a periodic net. Such descriptions date back to the earliest crystallographic studies but have become much more common recently because thousands of new structures and hundreds of underlying nets have been reported. In the simplest cases (e.g., the structure of diamond), the atoms in the crystal become the vertices of the net, and bonds are the links (edges) that connect them. In the case of MOFs, polyatomic groups act as the vertices and edges of the net. Because of the explosive growth in this area, a need has arisen for a universal system of nomenclature, classification, identification, and retrieval of these topological structures. We have developed a system of symbols for the identification of three periodic nets of interest, and this system is now in wide use. In this Account, we explain the underlying methodology of assigning symbols and describe the Reticular Chemistry Structure Resource (RCSR), in which about 1600 such nets are collected and illustrated in a database that can be searched by symbol, name, keywords, and attributes. The resource also contains searchable data for polyhedra and layers. The database entries come from systematic enumerations or from known chemical compounds or both. In the latter case, references to occurrences are provided. We describe some crystallographic, topological, and other attributes of nets and explain how they are reported in the database. We also describe how the database can be used as a tool for the design and structural analysis of new materials. Associated with each net is a natural tiling, which is a natural partition of space into space-filling tiles. The database allows export of data that can be used to analyze and illustrate such tilings.",fullPaper,jv379
Chemistry,p1733,d8,0c13dc68a9b91e94dc23676ec878fa9f1794b866,j379,Accounts of Chemical Research,"The Reticular Chemistry Structure Resource (RCSR) database of, and symbols for, crystal nets.","During the past decade, interest has grown tremendously in the design and synthesis of crystalline materials constructed from molecular clusters linked by extended groups of atoms. Most notable are metal-organic frameworks (MOFs), in which polyatomic inorganic metal-containing clusters are joined by polytopic linkers. (Although these materials are sometimes referred to as coordination polymers, we prefer to differentiate them, because MOFs are based on strong linkages that yield robust frameworks.) The realization that MOFs could be designed and synthesized in a rational way from molecular building blocks led to the emergence of a discipline that we call reticular chemistry. MOFs can be represented as a special kind of graph called a periodic net. Such descriptions date back to the earliest crystallographic studies but have become much more common recently because thousands of new structures and hundreds of underlying nets have been reported. In the simplest cases (e.g., the structure of diamond), the atoms in the crystal become the vertices of the net, and bonds are the links (edges) that connect them. In the case of MOFs, polyatomic groups act as the vertices and edges of the net. Because of the explosive growth in this area, a need has arisen for a universal system of nomenclature, classification, identification, and retrieval of these topological structures. We have developed a system of symbols for the identification of three periodic nets of interest, and this system is now in wide use. In this Account, we explain the underlying methodology of assigning symbols and describe the Reticular Chemistry Structure Resource (RCSR), in which about 1600 such nets are collected and illustrated in a database that can be searched by symbol, name, keywords, and attributes. The resource also contains searchable data for polyhedra and layers. The database entries come from systematic enumerations or from known chemical compounds or both. In the latter case, references to occurrences are provided. We describe some crystallographic, topological, and other attributes of nets and explain how they are reported in the database. We also describe how the database can be used as a tool for the design and structural analysis of new materials. Associated with each net is a natural tiling, which is a natural partition of space into space-filling tiles. The database allows export of data that can be used to analyze and illustrate such tilings.",fullPaper,jv379
Medicine,p1735,d1,a175453ceff401c67ce503750418e66936331b06,c33,Workshop on Python for High-Performance and Scientific Computing,NCBI GEO: mining tens of millions of expression profiles—database and tools update,"The Gene Expression Omnibus (GEO) repository at the National Center for Biotechnology Information (NCBI) archives and freely disseminates microarray and other forms of high-throughput data generated by the scientific community. The database has a minimum information about a microarray experiment (MIAME)-compliant infrastructure that captures fully annotated raw and processed data. Several data deposit options and formats are supported, including web forms, spreadsheets, XML and Simple Omnibus Format in Text (SOFT). In addition to data storage, a collection of user-friendly web-based interfaces and applications are available to help users effectively explore, visualize and download the thousands of experiments and tens of millions of gene expression patterns stored in GEO. This paper provides a summary of the GEO database structure and user facilities, and describes recent enhancements to database design, performance, submission format options, data query and retrieval utilities. GEO is accessible at",poster,cp33
Computer Science,p1735,d3,a175453ceff401c67ce503750418e66936331b06,c33,Workshop on Python for High-Performance and Scientific Computing,NCBI GEO: mining tens of millions of expression profiles—database and tools update,"The Gene Expression Omnibus (GEO) repository at the National Center for Biotechnology Information (NCBI) archives and freely disseminates microarray and other forms of high-throughput data generated by the scientific community. The database has a minimum information about a microarray experiment (MIAME)-compliant infrastructure that captures fully annotated raw and processed data. Several data deposit options and formats are supported, including web forms, spreadsheets, XML and Simple Omnibus Format in Text (SOFT). In addition to data storage, a collection of user-friendly web-based interfaces and applications are available to help users effectively explore, visualize and download the thousands of experiments and tens of millions of gene expression patterns stored in GEO. This paper provides a summary of the GEO database structure and user facilities, and describes recent enhancements to database design, performance, submission format options, data query and retrieval utilities. GEO is accessible at",poster,cp33
Biology,p1735,d5,a175453ceff401c67ce503750418e66936331b06,c33,Workshop on Python for High-Performance and Scientific Computing,NCBI GEO: mining tens of millions of expression profiles—database and tools update,"The Gene Expression Omnibus (GEO) repository at the National Center for Biotechnology Information (NCBI) archives and freely disseminates microarray and other forms of high-throughput data generated by the scientific community. The database has a minimum information about a microarray experiment (MIAME)-compliant infrastructure that captures fully annotated raw and processed data. Several data deposit options and formats are supported, including web forms, spreadsheets, XML and Simple Omnibus Format in Text (SOFT). In addition to data storage, a collection of user-friendly web-based interfaces and applications are available to help users effectively explore, visualize and download the thousands of experiments and tens of millions of gene expression patterns stored in GEO. This paper provides a summary of the GEO database structure and user facilities, and describes recent enhancements to database design, performance, submission format options, data query and retrieval utilities. GEO is accessible at",poster,cp33
Medicine,p1740,d1,854b86751d0d02bf205c6fb4bf2f2d13804430ee,j380,Journal of Human Genetics,"Human genetic variation database, a reference database of genetic variations in the Japanese population",Abstract,fullPaper,jv380
Biology,p1740,d5,854b86751d0d02bf205c6fb4bf2f2d13804430ee,j380,Journal of Human Genetics,"Human genetic variation database, a reference database of genetic variations in the Japanese population",Abstract,fullPaper,jv380
Medicine,p1742,d1,a35934a0b4c63925ee27d2bd77c75b31cf6d8072,c83,"International Convention on Information and Communication Technology, Electronics and Microelectronics",VFDB: a reference database for bacterial virulence factors,"Bacterial pathogens continue to impose a major threat to public health worldwide in the 21st century. Intensified studies on bacterial pathogenesis have greatly expanded our knowledge about the mechanisms of the disease processes at the molecular level over the last decades. To facilitate future research, it becomes necessary to form a database collectively presenting the virulence factors (VFs) of various medical significant bacterial pathogens. The aim of virulence factor database (VFDB) (http://www.mgc.ac.cn/VFs/) is to provide such a source for scientists to rapidly access to current knowledge about VFs from various bacterial pathogens. VFDB is comprehensive and user-friendly. One can search VFDB by browsing each genus or by typing keywords. Furthermore, a BLAST search tool against all known VF-related genes is also available. VFDB provides a unified gateway to store, search, retrieve and update information about VFs from various bacterial pathogens.",poster,cp83
Computer Science,p1742,d3,a35934a0b4c63925ee27d2bd77c75b31cf6d8072,c83,"International Convention on Information and Communication Technology, Electronics and Microelectronics",VFDB: a reference database for bacterial virulence factors,"Bacterial pathogens continue to impose a major threat to public health worldwide in the 21st century. Intensified studies on bacterial pathogenesis have greatly expanded our knowledge about the mechanisms of the disease processes at the molecular level over the last decades. To facilitate future research, it becomes necessary to form a database collectively presenting the virulence factors (VFs) of various medical significant bacterial pathogens. The aim of virulence factor database (VFDB) (http://www.mgc.ac.cn/VFs/) is to provide such a source for scientists to rapidly access to current knowledge about VFs from various bacterial pathogens. VFDB is comprehensive and user-friendly. One can search VFDB by browsing each genus or by typing keywords. Furthermore, a BLAST search tool against all known VF-related genes is also available. VFDB provides a unified gateway to store, search, retrieve and update information about VFs from various bacterial pathogens.",poster,cp83
Biology,p1742,d5,a35934a0b4c63925ee27d2bd77c75b31cf6d8072,c83,"International Convention on Information and Communication Technology, Electronics and Microelectronics",VFDB: a reference database for bacterial virulence factors,"Bacterial pathogens continue to impose a major threat to public health worldwide in the 21st century. Intensified studies on bacterial pathogenesis have greatly expanded our knowledge about the mechanisms of the disease processes at the molecular level over the last decades. To facilitate future research, it becomes necessary to form a database collectively presenting the virulence factors (VFs) of various medical significant bacterial pathogens. The aim of virulence factor database (VFDB) (http://www.mgc.ac.cn/VFs/) is to provide such a source for scientists to rapidly access to current knowledge about VFs from various bacterial pathogens. VFDB is comprehensive and user-friendly. One can search VFDB by browsing each genus or by typing keywords. Furthermore, a BLAST search tool against all known VF-related genes is also available. VFDB provides a unified gateway to store, search, retrieve and update information about VFs from various bacterial pathogens.",poster,cp83
Medicine,p1743,d1,6827419c860e6b9c825a1138d48633ecef00d4d5,c87,International Conference on Big Data Research,HPIDB 2.0: a curated database for host–pathogen interactions,"Identification and analysis of host–pathogen interactions (HPI) is essential to study infectious diseases. However, HPI data are sparse in existing molecular interaction databases, especially for agricultural host–pathogen systems. Therefore, resources that annotate, predict and display the HPI that underpin infectious diseases are critical for developing novel intervention strategies. HPIDB 2.0 (http://www.agbase.msstate.edu/hpi/main.html) is a resource for HPI data, and contains 45, 238 manually curated entries in the current release. Since the first description of the database in 2010, multiple enhancements to HPIDB data and interface services were made that are described here. Notably, HPIDB 2.0 now provides targeted biocuration of molecular interaction data. As a member of the International Molecular Exchange consortium, annotations provided by HPIDB 2.0 curators meet community standards to provide detailed contextual experimental information and facilitate data sharing. Moreover, HPIDB 2.0 provides access to rapidly available community annotations that capture minimum molecular interaction information to address immediate researcher needs for HPI network analysis. In addition to curation, HPIDB 2.0 integrates HPI from existing external sources and contains tools to infer additional HPI where annotated data are scarce. Compared to other interaction databases, our data collection approach ensures HPIDB 2.0 users access the most comprehensive HPI data from a wide range of pathogens and their hosts (594 pathogen and 70 host species, as of February 2016). Improvements also include enhanced search capacity, addition of Gene Ontology functional information, and implementation of network visualization. The changes made to HPIDB 2.0 content and interface ensure that users, especially agricultural researchers, are able to easily access and analyse high quality, comprehensive HPI data. All HPIDB 2.0 data are updated regularly, are publically available for direct download, and are disseminated to other molecular interaction resources. Database URL: http://www.agbase.msstate.edu/hpi/main.html",poster,cp87
Computer Science,p1743,d3,6827419c860e6b9c825a1138d48633ecef00d4d5,c87,International Conference on Big Data Research,HPIDB 2.0: a curated database for host–pathogen interactions,"Identification and analysis of host–pathogen interactions (HPI) is essential to study infectious diseases. However, HPI data are sparse in existing molecular interaction databases, especially for agricultural host–pathogen systems. Therefore, resources that annotate, predict and display the HPI that underpin infectious diseases are critical for developing novel intervention strategies. HPIDB 2.0 (http://www.agbase.msstate.edu/hpi/main.html) is a resource for HPI data, and contains 45, 238 manually curated entries in the current release. Since the first description of the database in 2010, multiple enhancements to HPIDB data and interface services were made that are described here. Notably, HPIDB 2.0 now provides targeted biocuration of molecular interaction data. As a member of the International Molecular Exchange consortium, annotations provided by HPIDB 2.0 curators meet community standards to provide detailed contextual experimental information and facilitate data sharing. Moreover, HPIDB 2.0 provides access to rapidly available community annotations that capture minimum molecular interaction information to address immediate researcher needs for HPI network analysis. In addition to curation, HPIDB 2.0 integrates HPI from existing external sources and contains tools to infer additional HPI where annotated data are scarce. Compared to other interaction databases, our data collection approach ensures HPIDB 2.0 users access the most comprehensive HPI data from a wide range of pathogens and their hosts (594 pathogen and 70 host species, as of February 2016). Improvements also include enhanced search capacity, addition of Gene Ontology functional information, and implementation of network visualization. The changes made to HPIDB 2.0 content and interface ensure that users, especially agricultural researchers, are able to easily access and analyse high quality, comprehensive HPI data. All HPIDB 2.0 data are updated regularly, are publically available for direct download, and are disseminated to other molecular interaction resources. Database URL: http://www.agbase.msstate.edu/hpi/main.html",poster,cp87
Medicine,p1744,d1,0a4365f6c30d40ca7610e9afac6c339f3c72224c,j381,Pharmacoepidemiology and Drug Safety,Validation of the national health insurance research database with ischemic stroke cases in Taiwan,The National Health Insurance Research Database (NHIRD) is commonly used for pharmacoepidemiological research in Taiwan. This study evaluated the validity of the database for patients with a principal diagnosis of ischemic stroke.,fullPaper,jv381
Medicine,p1746,d1,768b97bd5a8d4944be4a6f71b12ffad372a6ce5e,c114,Chinese Conference on Biometric Recognition,The UCSC Genome Browser database: extensions and updates 2013,"The University of California Santa Cruz (UCSC) Genome Browser (http://genome.ucsc.edu) offers online public access to a growing database of genomic sequence and annotations for a wide variety of organisms. The Browser is an integrated tool set for visualizing, comparing, analysing and sharing both publicly available and user-generated genomic datasets. As of September 2012, genomic sequence and a basic set of annotation ‘tracks’ are provided for 63 organisms, including 26 mammals, 13 non-mammal vertebrates, 3 invertebrate deuterostomes, 13 insects, 6 worms, yeast and sea hare. In the past year 19 new genome assemblies have been added, and we anticipate releasing another 28 in early 2013. Further, a large number of annotation tracks have been either added, updated by contributors or remapped to the latest human reference genome. Among these are an updated UCSC Genes track for human and mouse assemblies. We have also introduced several features to improve usability, including new navigation menus. This article provides an update to the UCSC Genome Browser database, which has been previously featured in the Database issue of this journal.",poster,cp114
Computer Science,p1746,d3,768b97bd5a8d4944be4a6f71b12ffad372a6ce5e,c114,Chinese Conference on Biometric Recognition,The UCSC Genome Browser database: extensions and updates 2013,"The University of California Santa Cruz (UCSC) Genome Browser (http://genome.ucsc.edu) offers online public access to a growing database of genomic sequence and annotations for a wide variety of organisms. The Browser is an integrated tool set for visualizing, comparing, analysing and sharing both publicly available and user-generated genomic datasets. As of September 2012, genomic sequence and a basic set of annotation ‘tracks’ are provided for 63 organisms, including 26 mammals, 13 non-mammal vertebrates, 3 invertebrate deuterostomes, 13 insects, 6 worms, yeast and sea hare. In the past year 19 new genome assemblies have been added, and we anticipate releasing another 28 in early 2013. Further, a large number of annotation tracks have been either added, updated by contributors or remapped to the latest human reference genome. Among these are an updated UCSC Genes track for human and mouse assemblies. We have also introduced several features to improve usability, including new navigation menus. This article provides an update to the UCSC Genome Browser database, which has been previously featured in the Database issue of this journal.",poster,cp114
Biology,p1746,d5,768b97bd5a8d4944be4a6f71b12ffad372a6ce5e,c114,Chinese Conference on Biometric Recognition,The UCSC Genome Browser database: extensions and updates 2013,"The University of California Santa Cruz (UCSC) Genome Browser (http://genome.ucsc.edu) offers online public access to a growing database of genomic sequence and annotations for a wide variety of organisms. The Browser is an integrated tool set for visualizing, comparing, analysing and sharing both publicly available and user-generated genomic datasets. As of September 2012, genomic sequence and a basic set of annotation ‘tracks’ are provided for 63 organisms, including 26 mammals, 13 non-mammal vertebrates, 3 invertebrate deuterostomes, 13 insects, 6 worms, yeast and sea hare. In the past year 19 new genome assemblies have been added, and we anticipate releasing another 28 in early 2013. Further, a large number of annotation tracks have been either added, updated by contributors or remapped to the latest human reference genome. Among these are an updated UCSC Genes track for human and mouse assemblies. We have also introduced several features to improve usability, including new navigation menus. This article provides an update to the UCSC Genome Browser database, which has been previously featured in the Database issue of this journal.",poster,cp114
Medicine,p1747,d1,bcc73dd05b7b7a6616c41df428e3624375c95e56,c30,PS,ATtRACT—a database of RNA-binding proteins and associated motifs,"RNA-binding proteins (RBPs) play a crucial role in key cellular processes, including RNA transport, splicing, polyadenylation and stability. Understanding the interaction between RBPs and RNA is key to improve our knowledge of RNA processing, localization and regulation in a global manner. Despite advances in recent years, a unified non-redundant resource that includes information on experimentally validated motifs, RBPs and integrated tools to exploit this information is lacking. Here, we developed a database named ATtRACT (available at http://attract.cnic.es) that compiles information on 370 RBPs and 1583 RBP consensus binding motifs, 192 of which are not present in any other database. To populate ATtRACT we (i) extracted and hand-curated experimentally validated data from CISBP-RNA, SpliceAid–F, RBPDB databases, (ii) integrated and updated the unavailable ASD database and (iii) extracted information from Protein-RNA complexes present in Protein Data Bank database through computational analyses. ATtRACT provides also efficient algorithms to search a specific motif and scan one or more RNA sequences at a time. It also allows discovering de novo motifs enriched in a set of related sequences and compare them with the motifs included in the database. Database URL: http:// attract. cnic. es",poster,cp30
Computer Science,p1747,d3,bcc73dd05b7b7a6616c41df428e3624375c95e56,c30,PS,ATtRACT—a database of RNA-binding proteins and associated motifs,"RNA-binding proteins (RBPs) play a crucial role in key cellular processes, including RNA transport, splicing, polyadenylation and stability. Understanding the interaction between RBPs and RNA is key to improve our knowledge of RNA processing, localization and regulation in a global manner. Despite advances in recent years, a unified non-redundant resource that includes information on experimentally validated motifs, RBPs and integrated tools to exploit this information is lacking. Here, we developed a database named ATtRACT (available at http://attract.cnic.es) that compiles information on 370 RBPs and 1583 RBP consensus binding motifs, 192 of which are not present in any other database. To populate ATtRACT we (i) extracted and hand-curated experimentally validated data from CISBP-RNA, SpliceAid–F, RBPDB databases, (ii) integrated and updated the unavailable ASD database and (iii) extracted information from Protein-RNA complexes present in Protein Data Bank database through computational analyses. ATtRACT provides also efficient algorithms to search a specific motif and scan one or more RNA sequences at a time. It also allows discovering de novo motifs enriched in a set of related sequences and compare them with the motifs included in the database. Database URL: http:// attract. cnic. es",poster,cp30
Biology,p1747,d5,bcc73dd05b7b7a6616c41df428e3624375c95e56,c30,PS,ATtRACT—a database of RNA-binding proteins and associated motifs,"RNA-binding proteins (RBPs) play a crucial role in key cellular processes, including RNA transport, splicing, polyadenylation and stability. Understanding the interaction between RBPs and RNA is key to improve our knowledge of RNA processing, localization and regulation in a global manner. Despite advances in recent years, a unified non-redundant resource that includes information on experimentally validated motifs, RBPs and integrated tools to exploit this information is lacking. Here, we developed a database named ATtRACT (available at http://attract.cnic.es) that compiles information on 370 RBPs and 1583 RBP consensus binding motifs, 192 of which are not present in any other database. To populate ATtRACT we (i) extracted and hand-curated experimentally validated data from CISBP-RNA, SpliceAid–F, RBPDB databases, (ii) integrated and updated the unavailable ASD database and (iii) extracted information from Protein-RNA complexes present in Protein Data Bank database through computational analyses. ATtRACT provides also efficient algorithms to search a specific motif and scan one or more RNA sequences at a time. It also allows discovering de novo motifs enriched in a set of related sequences and compare them with the motifs included in the database. Database URL: http:// attract. cnic. es",poster,cp30
Medicine,p1752,d1,49b60b92201710d095684b6df1b1d93f92122dd0,c60,Network and Distributed System Security Symposium,MINT: the Molecular INTeraction database,"The Molecular INTeraction database (MINT, ) aims at storing, in a structured format, information about molecular interactions (MIs) by extracting experimental details from work published in peer-reviewed journals. At present the MINT team focuses the curation work on physical interactions between proteins. Genetic or computationally inferred interactions are not included in the database. Over the past four years MINT has undergone extensive revision. The new version of MINT is based on a completely remodeled database structure, which offers more efficient data exploration and analysis, and is characterized by entries with a richer annotation. Over the past few years the number of curated physical interactions has soared to over 95 000. The whole dataset can be freely accessed online in both interactive and batch modes through web-based interfaces and an FTP server. MINT now includes, as an integrated addition, HomoMINT, a database of interactions between human proteins inferred from experiments with ortholog proteins in model organisms ().",poster,cp60
Computer Science,p1752,d3,49b60b92201710d095684b6df1b1d93f92122dd0,c60,Network and Distributed System Security Symposium,MINT: the Molecular INTeraction database,"The Molecular INTeraction database (MINT, ) aims at storing, in a structured format, information about molecular interactions (MIs) by extracting experimental details from work published in peer-reviewed journals. At present the MINT team focuses the curation work on physical interactions between proteins. Genetic or computationally inferred interactions are not included in the database. Over the past four years MINT has undergone extensive revision. The new version of MINT is based on a completely remodeled database structure, which offers more efficient data exploration and analysis, and is characterized by entries with a richer annotation. Over the past few years the number of curated physical interactions has soared to over 95 000. The whole dataset can be freely accessed online in both interactive and batch modes through web-based interfaces and an FTP server. MINT now includes, as an integrated addition, HomoMINT, a database of interactions between human proteins inferred from experiments with ortholog proteins in model organisms ().",poster,cp60
Biology,p1752,d5,49b60b92201710d095684b6df1b1d93f92122dd0,c60,Network and Distributed System Security Symposium,MINT: the Molecular INTeraction database,"The Molecular INTeraction database (MINT, ) aims at storing, in a structured format, information about molecular interactions (MIs) by extracting experimental details from work published in peer-reviewed journals. At present the MINT team focuses the curation work on physical interactions between proteins. Genetic or computationally inferred interactions are not included in the database. Over the past four years MINT has undergone extensive revision. The new version of MINT is based on a completely remodeled database structure, which offers more efficient data exploration and analysis, and is characterized by entries with a richer annotation. Over the past few years the number of curated physical interactions has soared to over 95 000. The whole dataset can be freely accessed online in both interactive and batch modes through web-based interfaces and an FTP server. MINT now includes, as an integrated addition, HomoMINT, a database of interactions between human proteins inferred from experiments with ortholog proteins in model organisms ().",poster,cp60
Medicine,p1753,d1,c6804d16e1f4e3bde4d535384a699e65479634b6,c99,Symposium on the Theory of Computing,MODOMICS: a database of RNA modification pathways—2013 update,"MODOMICS is a database of RNA modifications that provides comprehensive information concerning the chemical structures of modified ribonucleosides, their biosynthetic pathways, RNA-modifying enzymes and location of modified residues in RNA sequences. In the current database version, accessible at http://modomics.genesilico.pl, we included new features: a census of human and yeast snoRNAs involved in RNA-guided RNA modification, a new section covering the 5′-end capping process, and a catalogue of ‘building blocks’ for chemical synthesis of a large variety of modified nucleosides. The MODOMICS collections of RNA modifications, RNA-modifying enzymes and modified RNAs have been also updated. A number of newly identified modified ribonucleosides and more than one hundred functionally and structurally characterized proteins from various organisms have been added. In the RNA sequences section, snRNAs and snoRNAs with experimentally mapped modified nucleosides have been added and the current collection of rRNA and tRNA sequences has been substantially enlarged. To facilitate literature searches, each record in MODOMICS has been cross-referenced to other databases and to selected key publications. New options for database searching and querying have been implemented, including a BLAST search of protein sequences and a PARALIGN search of the collected nucleic acid sequences.",poster,cp99
Computer Science,p1753,d3,c6804d16e1f4e3bde4d535384a699e65479634b6,c99,Symposium on the Theory of Computing,MODOMICS: a database of RNA modification pathways—2013 update,"MODOMICS is a database of RNA modifications that provides comprehensive information concerning the chemical structures of modified ribonucleosides, their biosynthetic pathways, RNA-modifying enzymes and location of modified residues in RNA sequences. In the current database version, accessible at http://modomics.genesilico.pl, we included new features: a census of human and yeast snoRNAs involved in RNA-guided RNA modification, a new section covering the 5′-end capping process, and a catalogue of ‘building blocks’ for chemical synthesis of a large variety of modified nucleosides. The MODOMICS collections of RNA modifications, RNA-modifying enzymes and modified RNAs have been also updated. A number of newly identified modified ribonucleosides and more than one hundred functionally and structurally characterized proteins from various organisms have been added. In the RNA sequences section, snRNAs and snoRNAs with experimentally mapped modified nucleosides have been added and the current collection of rRNA and tRNA sequences has been substantially enlarged. To facilitate literature searches, each record in MODOMICS has been cross-referenced to other databases and to selected key publications. New options for database searching and querying have been implemented, including a BLAST search of protein sequences and a PARALIGN search of the collected nucleic acid sequences.",poster,cp99
Biology,p1753,d5,c6804d16e1f4e3bde4d535384a699e65479634b6,c99,Symposium on the Theory of Computing,MODOMICS: a database of RNA modification pathways—2013 update,"MODOMICS is a database of RNA modifications that provides comprehensive information concerning the chemical structures of modified ribonucleosides, their biosynthetic pathways, RNA-modifying enzymes and location of modified residues in RNA sequences. In the current database version, accessible at http://modomics.genesilico.pl, we included new features: a census of human and yeast snoRNAs involved in RNA-guided RNA modification, a new section covering the 5′-end capping process, and a catalogue of ‘building blocks’ for chemical synthesis of a large variety of modified nucleosides. The MODOMICS collections of RNA modifications, RNA-modifying enzymes and modified RNAs have been also updated. A number of newly identified modified ribonucleosides and more than one hundred functionally and structurally characterized proteins from various organisms have been added. In the RNA sequences section, snRNAs and snoRNAs with experimentally mapped modified nucleosides have been added and the current collection of rRNA and tRNA sequences has been substantially enlarged. To facilitate literature searches, each record in MODOMICS has been cross-referenced to other databases and to selected key publications. New options for database searching and querying have been implemented, including a BLAST search of protein sequences and a PARALIGN search of the collected nucleic acid sequences.",poster,cp99
Medicine,p1754,d1,ff9ef51fbc40937d38dcea754b43a3e06ca39a01,c98,Vision,EpiFactors: a comprehensive database of human epigenetic factors and complexes,"Epigenetics refers to stable and long-term alterations of cellular traits that are not caused by changes in the DNA sequence per se. Rather, covalent modifications of DNA and histones affect gene expression and genome stability via proteins that recognize and act upon such modifications. Many enzymes that catalyse epigenetic modifications or are critical for enzymatic complexes have been discovered, and this is encouraging investigators to study the role of these proteins in diverse normal and pathological processes. Rapidly growing knowledge in the area has resulted in the need for a resource that compiles, organizes and presents curated information to the researchers in an easily accessible and user-friendly form. Here we present EpiFactors, a manually curated database providing information about epigenetic regulators, their complexes, targets and products. EpiFactors contains information on 815 proteins, including 95 histones and protamines. For 789 of these genes, we include expressions values across several samples, in particular a collection of 458 human primary cell samples (for approximately 200 cell types, in many cases from three individual donors), covering most mammalian cell steady states, 255 different cancer cell lines (representing approximately 150 cancer subtypes) and 134 human postmortem tissues. Expression values were obtained by the FANTOM5 consortium using Cap Analysis of Gene Expression technique. EpiFactors also contains information on 69 protein complexes that are involved in epigenetic regulation. The resource is practical for a wide range of users, including biologists, pharmacologists and clinicians. Database URL: http://epifactors.autosome.ru",poster,cp98
Computer Science,p1754,d3,ff9ef51fbc40937d38dcea754b43a3e06ca39a01,c98,Vision,EpiFactors: a comprehensive database of human epigenetic factors and complexes,"Epigenetics refers to stable and long-term alterations of cellular traits that are not caused by changes in the DNA sequence per se. Rather, covalent modifications of DNA and histones affect gene expression and genome stability via proteins that recognize and act upon such modifications. Many enzymes that catalyse epigenetic modifications or are critical for enzymatic complexes have been discovered, and this is encouraging investigators to study the role of these proteins in diverse normal and pathological processes. Rapidly growing knowledge in the area has resulted in the need for a resource that compiles, organizes and presents curated information to the researchers in an easily accessible and user-friendly form. Here we present EpiFactors, a manually curated database providing information about epigenetic regulators, their complexes, targets and products. EpiFactors contains information on 815 proteins, including 95 histones and protamines. For 789 of these genes, we include expressions values across several samples, in particular a collection of 458 human primary cell samples (for approximately 200 cell types, in many cases from three individual donors), covering most mammalian cell steady states, 255 different cancer cell lines (representing approximately 150 cancer subtypes) and 134 human postmortem tissues. Expression values were obtained by the FANTOM5 consortium using Cap Analysis of Gene Expression technique. EpiFactors also contains information on 69 protein complexes that are involved in epigenetic regulation. The resource is practical for a wide range of users, including biologists, pharmacologists and clinicians. Database URL: http://epifactors.autosome.ru",poster,cp98
Biology,p1754,d5,ff9ef51fbc40937d38dcea754b43a3e06ca39a01,c98,Vision,EpiFactors: a comprehensive database of human epigenetic factors and complexes,"Epigenetics refers to stable and long-term alterations of cellular traits that are not caused by changes in the DNA sequence per se. Rather, covalent modifications of DNA and histones affect gene expression and genome stability via proteins that recognize and act upon such modifications. Many enzymes that catalyse epigenetic modifications or are critical for enzymatic complexes have been discovered, and this is encouraging investigators to study the role of these proteins in diverse normal and pathological processes. Rapidly growing knowledge in the area has resulted in the need for a resource that compiles, organizes and presents curated information to the researchers in an easily accessible and user-friendly form. Here we present EpiFactors, a manually curated database providing information about epigenetic regulators, their complexes, targets and products. EpiFactors contains information on 815 proteins, including 95 histones and protamines. For 789 of these genes, we include expressions values across several samples, in particular a collection of 458 human primary cell samples (for approximately 200 cell types, in many cases from three individual donors), covering most mammalian cell steady states, 255 different cancer cell lines (representing approximately 150 cancer subtypes) and 134 human postmortem tissues. Expression values were obtained by the FANTOM5 consortium using Cap Analysis of Gene Expression technique. EpiFactors also contains information on 69 protein complexes that are involved in epigenetic regulation. The resource is practical for a wide range of users, including biologists, pharmacologists and clinicians. Database URL: http://epifactors.autosome.ru",poster,cp98
Medicine,p1755,d1,529124bc3ff7fa655729c49bf10aedaddd49787e,c37,International Workshop on the Semantic Web,Phenol-Explorer 3.0: a major update of the Phenol-Explorer database to incorporate data on the effects of food processing on polyphenol content,"Polyphenols are a major class of bioactive phytochemicals whose consumption may play a role in the prevention of a number of chronic diseases such as cardiovascular diseases, type II diabetes and cancers. Phenol-Explorer, launched in 2009, is the only freely available web-based database on the content of polyphenols in food and their in vivo metabolism and pharmacokinetics. Here we report the third release of the database (Phenol-Explorer 3.0), which adds data on the effects of food processing on polyphenol contents in foods. Data on >100 foods, covering 161 polyphenols or groups of polyphenols before and after processing, were collected from 129 peer-reviewed publications and entered into new tables linked to the existing relational design. The effect of processing on polyphenol content is expressed in the form of retention factor coefficients, or the proportion of a given polyphenol retained after processing, adjusted for change in water content. The result is the first database on the effects of food processing on polyphenol content and, following the model initially defined for Phenol-Explorer, all data may be traced back to original sources. The new update will allow polyphenol scientists to more accurately estimate polyphenol exposure from dietary surveys. Database URL: http://www.phenol-explorer.eu",poster,cp37
Computer Science,p1755,d3,529124bc3ff7fa655729c49bf10aedaddd49787e,c37,International Workshop on the Semantic Web,Phenol-Explorer 3.0: a major update of the Phenol-Explorer database to incorporate data on the effects of food processing on polyphenol content,"Polyphenols are a major class of bioactive phytochemicals whose consumption may play a role in the prevention of a number of chronic diseases such as cardiovascular diseases, type II diabetes and cancers. Phenol-Explorer, launched in 2009, is the only freely available web-based database on the content of polyphenols in food and their in vivo metabolism and pharmacokinetics. Here we report the third release of the database (Phenol-Explorer 3.0), which adds data on the effects of food processing on polyphenol contents in foods. Data on >100 foods, covering 161 polyphenols or groups of polyphenols before and after processing, were collected from 129 peer-reviewed publications and entered into new tables linked to the existing relational design. The effect of processing on polyphenol content is expressed in the form of retention factor coefficients, or the proportion of a given polyphenol retained after processing, adjusted for change in water content. The result is the first database on the effects of food processing on polyphenol content and, following the model initially defined for Phenol-Explorer, all data may be traced back to original sources. The new update will allow polyphenol scientists to more accurately estimate polyphenol exposure from dietary surveys. Database URL: http://www.phenol-explorer.eu",poster,cp37
Medicine,p1760,d1,00899cac0e2e770fe6b28deac002eceb8c3c4bea,c54,ACM SIGCOMM Symposium on Software Defined Networking Research,Phenol-Explorer: an online comprehensive database on polyphenol contents in foods,"A number of databases on the plant metabolome describe the chemistry and biosynthesis of plant chemicals. However, no such database is specifically focused on foods and more precisely on polyphenols, one of the major classes of phytochemicals. As antoxidants, polyphenols influence human health and may play a role in the prevention of a number of chronic diseases such as cardiovascular diseases, some cancers or type 2 diabetes. To determine polyphenol intake in populations and study their association with health, it is essential to have detailed information on their content in foods. However this information is not easily collected due to the variety of their chemical structures and the variability of their content in a given food. Phenol-Explorer is the first comprehensive web-based database on polyphenol content in foods. It contains more than 37 000 original data points collected from 638 scientific articles published in peer-reviewed journals. The quality of these data has been evaluated before they were aggregated to produce final representative mean content values for 502 polyphenols in 452 foods. The web interface allows making various queries on the aggregated data to identify foods containing a given polyphenol or polyphenols present in a given food. For each mean content value, it is possible to trace all original content values and their literature sources. Phenol-Explorer is a major step forward in the development of databases on food constituents and the food metabolome. It should help researchers to better understand the role of phytochemicals in the technical and nutritional quality of food, and food manufacturers to develop tailor-made healthy foods. Database URL: http://www.phenol-explorer.eu",poster,cp54
Computer Science,p1760,d3,00899cac0e2e770fe6b28deac002eceb8c3c4bea,c54,ACM SIGCOMM Symposium on Software Defined Networking Research,Phenol-Explorer: an online comprehensive database on polyphenol contents in foods,"A number of databases on the plant metabolome describe the chemistry and biosynthesis of plant chemicals. However, no such database is specifically focused on foods and more precisely on polyphenols, one of the major classes of phytochemicals. As antoxidants, polyphenols influence human health and may play a role in the prevention of a number of chronic diseases such as cardiovascular diseases, some cancers or type 2 diabetes. To determine polyphenol intake in populations and study their association with health, it is essential to have detailed information on their content in foods. However this information is not easily collected due to the variety of their chemical structures and the variability of their content in a given food. Phenol-Explorer is the first comprehensive web-based database on polyphenol content in foods. It contains more than 37 000 original data points collected from 638 scientific articles published in peer-reviewed journals. The quality of these data has been evaluated before they were aggregated to produce final representative mean content values for 502 polyphenols in 452 foods. The web interface allows making various queries on the aggregated data to identify foods containing a given polyphenol or polyphenols present in a given food. For each mean content value, it is possible to trace all original content values and their literature sources. Phenol-Explorer is a major step forward in the development of databases on food constituents and the food metabolome. It should help researchers to better understand the role of phytochemicals in the technical and nutritional quality of food, and food manufacturers to develop tailor-made healthy foods. Database URL: http://www.phenol-explorer.eu",poster,cp54
Medicine,p1762,d1,0d2072d81d03247949126e87d6201788cb646526,c99,Symposium on the Theory of Computing,"The PROSITE database, its status in 2002","PROSITE [Bairoch and Bucher (1994) Nucleic Acids Res., 22, 3583-3589; Hofmann et al. (1999) Nucleic Acids Res., 27, 215-219] is a method of identifying the functions of uncharacterized proteins translated from genomic or cDNA sequences. The PROSITE database (http://www.expasy.org/prosite/) consists of biologically significant patterns and profiles designed in such a way that with appropriate computational tools it can rapidly and reliably help to determine to which known family of proteins (if any) a new sequence belongs, or which known domain(s) it contains.",poster,cp99
Computer Science,p1762,d3,0d2072d81d03247949126e87d6201788cb646526,c99,Symposium on the Theory of Computing,"The PROSITE database, its status in 2002","PROSITE [Bairoch and Bucher (1994) Nucleic Acids Res., 22, 3583-3589; Hofmann et al. (1999) Nucleic Acids Res., 27, 215-219] is a method of identifying the functions of uncharacterized proteins translated from genomic or cDNA sequences. The PROSITE database (http://www.expasy.org/prosite/) consists of biologically significant patterns and profiles designed in such a way that with appropriate computational tools it can rapidly and reliably help to determine to which known family of proteins (if any) a new sequence belongs, or which known domain(s) it contains.",poster,cp99
Biology,p1762,d5,0d2072d81d03247949126e87d6201788cb646526,c99,Symposium on the Theory of Computing,"The PROSITE database, its status in 2002","PROSITE [Bairoch and Bucher (1994) Nucleic Acids Res., 22, 3583-3589; Hofmann et al. (1999) Nucleic Acids Res., 27, 215-219] is a method of identifying the functions of uncharacterized proteins translated from genomic or cDNA sequences. The PROSITE database (http://www.expasy.org/prosite/) consists of biologically significant patterns and profiles designed in such a way that with appropriate computational tools it can rapidly and reliably help to determine to which known family of proteins (if any) a new sequence belongs, or which known domain(s) it contains.",poster,cp99
Medicine,p1763,d1,756686211e7d253ac7abb8cb9e290141880f0889,j383,Plant and Cell Physiology,Rice Annotation Project Database (RAP-DB): An Integrative and Interactive Database for Rice Genomics,"The Rice Annotation Project Database (RAP-DB, http://rapdb.dna.affrc.go.jp/) has been providing a comprehensive set of gene annotations for the genome sequence of rice, Oryza sativa (japonica group) cv. Nipponbare. Since the first release in 2005, RAP-DB has been updated several times along with the genome assembly updates. Here, we present our newest RAP-DB based on the latest genome assembly, Os-Nipponbare-Reference-IRGSP-1.0 (IRGSP-1.0), which was released in 2011. We detected 37,869 loci by mapping transcript and protein sequences of 150 monocot species. To provide plant researchers with highly reliable and up to date rice gene annotations, we have been incorporating literature-based manually curated data, and 1,626 loci currently incorporate literature-based annotation data, including commonly used gene names or gene symbols. Transcriptional activities are shown at the nucleotide level by mapping RNA-Seq reads derived from 27 samples. We also mapped the Illumina reads of a Japanese leading japonica cultivar, Koshihikari, and a Chinese indica cultivar, Guangluai-4, to the genome and show alignments together with the single nucleotide polymorphisms (SNPs) and gene functional annotations through a newly developed browser, Short-Read Assembly Browser (S-RAB). We have developed two satellite databases, Plant Gene Family Database (PGFD) and Integrative Database of Cereal Gene Phylogeny (IDCGP), which display gene family and homologous gene relationships among diverse plant species. RAP-DB and the satellite databases offer simple and user-friendly web interfaces, enabling plant and genome researchers to access the data easily and facilitating a broad range of plant research topics.",fullPaper,jv383
Biology,p1763,d5,756686211e7d253ac7abb8cb9e290141880f0889,j383,Plant and Cell Physiology,Rice Annotation Project Database (RAP-DB): An Integrative and Interactive Database for Rice Genomics,"The Rice Annotation Project Database (RAP-DB, http://rapdb.dna.affrc.go.jp/) has been providing a comprehensive set of gene annotations for the genome sequence of rice, Oryza sativa (japonica group) cv. Nipponbare. Since the first release in 2005, RAP-DB has been updated several times along with the genome assembly updates. Here, we present our newest RAP-DB based on the latest genome assembly, Os-Nipponbare-Reference-IRGSP-1.0 (IRGSP-1.0), which was released in 2011. We detected 37,869 loci by mapping transcript and protein sequences of 150 monocot species. To provide plant researchers with highly reliable and up to date rice gene annotations, we have been incorporating literature-based manually curated data, and 1,626 loci currently incorporate literature-based annotation data, including commonly used gene names or gene symbols. Transcriptional activities are shown at the nucleotide level by mapping RNA-Seq reads derived from 27 samples. We also mapped the Illumina reads of a Japanese leading japonica cultivar, Koshihikari, and a Chinese indica cultivar, Guangluai-4, to the genome and show alignments together with the single nucleotide polymorphisms (SNPs) and gene functional annotations through a newly developed browser, Short-Read Assembly Browser (S-RAB). We have developed two satellite databases, Plant Gene Family Database (PGFD) and Integrative Database of Cereal Gene Phylogeny (IDCGP), which display gene family and homologous gene relationships among diverse plant species. RAP-DB and the satellite databases offer simple and user-friendly web interfaces, enabling plant and genome researchers to access the data easily and facilitating a broad range of plant research topics.",fullPaper,jv383
Medicine,p1764,d1,e8ddd8c820dba886f618f0e84ce38ecd0b967b39,c110,Biometrics and Identity Management,Human Protein Reference Database—2009 update,"Human Protein Reference Database (HPRD—http://www.hprd.org/), initially described in 2003, is a database of curated proteomic information pertaining to human proteins. We have recently added a number of new features in HPRD. These include PhosphoMotif Finder, which allows users to find the presence of over 320 experimentally verified phosphorylation motifs in proteins of interest. Another new feature is a protein distributed annotation system—Human Proteinpedia (http://www.humanproteinpedia.org/)—through which laboratories can submit their data, which is mapped onto protein entries in HPRD. Over 75 laboratories involved in proteomics research have already participated in this effort by submitting data for over 15 000 human proteins. The submitted data includes mass spectrometry and protein microarray-derived data, among other data types. Finally, HPRD is also linked to a compendium of human signaling pathways developed by our group, NetPath (http://www.netpath.org/), which currently contains annotations for several cancer and immune signaling pathways. Since the last update, more than 5500 new protein sequences have been added, making HPRD a comprehensive resource for studying the human proteome.",poster,cp110
Computer Science,p1764,d3,e8ddd8c820dba886f618f0e84ce38ecd0b967b39,c110,Biometrics and Identity Management,Human Protein Reference Database—2009 update,"Human Protein Reference Database (HPRD—http://www.hprd.org/), initially described in 2003, is a database of curated proteomic information pertaining to human proteins. We have recently added a number of new features in HPRD. These include PhosphoMotif Finder, which allows users to find the presence of over 320 experimentally verified phosphorylation motifs in proteins of interest. Another new feature is a protein distributed annotation system—Human Proteinpedia (http://www.humanproteinpedia.org/)—through which laboratories can submit their data, which is mapped onto protein entries in HPRD. Over 75 laboratories involved in proteomics research have already participated in this effort by submitting data for over 15 000 human proteins. The submitted data includes mass spectrometry and protein microarray-derived data, among other data types. Finally, HPRD is also linked to a compendium of human signaling pathways developed by our group, NetPath (http://www.netpath.org/), which currently contains annotations for several cancer and immune signaling pathways. Since the last update, more than 5500 new protein sequences have been added, making HPRD a comprehensive resource for studying the human proteome.",poster,cp110
Biology,p1764,d5,e8ddd8c820dba886f618f0e84ce38ecd0b967b39,c110,Biometrics and Identity Management,Human Protein Reference Database—2009 update,"Human Protein Reference Database (HPRD—http://www.hprd.org/), initially described in 2003, is a database of curated proteomic information pertaining to human proteins. We have recently added a number of new features in HPRD. These include PhosphoMotif Finder, which allows users to find the presence of over 320 experimentally verified phosphorylation motifs in proteins of interest. Another new feature is a protein distributed annotation system—Human Proteinpedia (http://www.humanproteinpedia.org/)—through which laboratories can submit their data, which is mapped onto protein entries in HPRD. Over 75 laboratories involved in proteomics research have already participated in this effort by submitting data for over 15 000 human proteins. The submitted data includes mass spectrometry and protein microarray-derived data, among other data types. Finally, HPRD is also linked to a compendium of human signaling pathways developed by our group, NetPath (http://www.netpath.org/), which currently contains annotations for several cancer and immune signaling pathways. Since the last update, more than 5500 new protein sequences have been added, making HPRD a comprehensive resource for studying the human proteome.",poster,cp110
Medicine,p1765,d1,607ca163b2635f9992e773d1b1d07d39d5d33f4e,j6,Genome Biology,"DAVID: Database for Annotation, Visualization, and Integrated Discovery",Abstract,fullPaper,jv6
Medicine,p1768,d1,f707571329e1aa7180a1a289b1aa250eabdc8618,j107,Nucleic Acids Research,InterPro in 2011: new developments in the family and domain prediction database,"InterPro (http://www.ebi.ac.uk/interpro/) is a database that integrates diverse information about protein families, domains and functional sites, and makes it freely available to the public via Web-based interfaces and services. Central to the database are diagnostic models, known as signatures, against which protein sequences can be searched to determine their potential function. InterPro has utility in the large-scale analysis of whole genomes and meta-genomes, as well as in characterizing individual protein sequences. Herein we give an overview of new developments in the database and its associated software since 2009, including updates to database content, curation processes and Web and programmatic interfaces.",fullPaper,jv107
Biology,p1768,d5,f707571329e1aa7180a1a289b1aa250eabdc8618,j107,Nucleic Acids Research,InterPro in 2011: new developments in the family and domain prediction database,"InterPro (http://www.ebi.ac.uk/interpro/) is a database that integrates diverse information about protein families, domains and functional sites, and makes it freely available to the public via Web-based interfaces and services. Central to the database are diagnostic models, known as signatures, against which protein sequences can be searched to determine their potential function. InterPro has utility in the large-scale analysis of whole genomes and meta-genomes, as well as in characterizing individual protein sequences. Herein we give an overview of new developments in the database and its associated software since 2009, including updates to database content, curation processes and Web and programmatic interfaces.",fullPaper,jv107
Medicine,p1769,d1,93c5bb23406778eb421a3dfa5978a231e022792a,j384,Academic Radiology,INbreast: toward a full-field digital mammographic database.,Abstract,fullPaper,jv384
Computer Science,p1769,d3,93c5bb23406778eb421a3dfa5978a231e022792a,j384,Academic Radiology,INbreast: toward a full-field digital mammographic database.,Abstract,fullPaper,jv384
Medicine,p1770,d1,334511feb95634c91d06359fd497d01fc60767f7,c76,Group,DIP: The Database of Interacting Proteins: 2001 update,"The Database of Interacting Proteins (DIP; http://dip.doe-mbi.ucla. edu) is a database that documents experimentally determined protein-protein interactions. Since January 2000 the number of protein-protein interactions in DIP has nearly tripled to 3472 and the number of proteins to 2659. New interactive tools have been developed to aid in the visualization, navigation and study of networks of protein interactions.",poster,cp76
Computer Science,p1770,d3,334511feb95634c91d06359fd497d01fc60767f7,c76,Group,DIP: The Database of Interacting Proteins: 2001 update,"The Database of Interacting Proteins (DIP; http://dip.doe-mbi.ucla. edu) is a database that documents experimentally determined protein-protein interactions. Since January 2000 the number of protein-protein interactions in DIP has nearly tripled to 3472 and the number of proteins to 2659. New interactive tools have been developed to aid in the visualization, navigation and study of networks of protein interactions.",poster,cp76
Biology,p1770,d5,334511feb95634c91d06359fd497d01fc60767f7,c76,Group,DIP: The Database of Interacting Proteins: 2001 update,"The Database of Interacting Proteins (DIP; http://dip.doe-mbi.ucla. edu) is a database that documents experimentally determined protein-protein interactions. Since January 2000 the number of protein-protein interactions in DIP has nearly tripled to 3472 and the number of proteins to 2659. New interactive tools have been developed to aid in the visualization, navigation and study of networks of protein interactions.",poster,cp76
Medicine,p1771,d1,2129930aa4803610ea25860e770cbc73440acbf4,j385,"Proteins: Structure, Function, and Bioinformatics",Database of homology‐derived protein structures and the structural meaning of sequence alignment,"The database of known protein three‐dimensional structures can be significantly increased by the use of sequence homology, based on the following observations. (1) The database of known sequences, currently at more than 12,000 proteins, is two orders of magnitude larger than the database of known structures. (2) The currently most powerful method of predicting protein structures is model building by homology. (3) Structural homology can be inferred from the level of sequence similarity. (4) The threshold of sequence similarity sufficient for structural homology depends strongly on the length of the alignment. Here, we first quantify the relation between sequence similarity, structure similarity, and alignment length by an exhaustive survey of alignments between proteins of known structure and report a homology threshold curve as a function of alignment length. We then produce a database of homology‐derived secondary structure of proteins (HSSP) by aligning to each protein of known structure all sequences deemed homologous on the basis of the threshold curve. For each known protein structure, the derived database contains the aligned sequences, secondary structure, sequence variability, and sequence profile. Tertiary structures of the aligned sequences are implied, but not modeled explicity. The database effectively increases the number of known protein structures by a factor of five to more than 1800. The results may be useful in assessing the structural significance of matches in sequence database searches, in deriving preferences and patterns for structure prediction, in elucidating the structural role of conserved residues, and in modeling three‐dimensional detail by homology.",fullPaper,jv385
Biology,p1771,d5,2129930aa4803610ea25860e770cbc73440acbf4,j385,"Proteins: Structure, Function, and Bioinformatics",Database of homology‐derived protein structures and the structural meaning of sequence alignment,"The database of known protein three‐dimensional structures can be significantly increased by the use of sequence homology, based on the following observations. (1) The database of known sequences, currently at more than 12,000 proteins, is two orders of magnitude larger than the database of known structures. (2) The currently most powerful method of predicting protein structures is model building by homology. (3) Structural homology can be inferred from the level of sequence similarity. (4) The threshold of sequence similarity sufficient for structural homology depends strongly on the length of the alignment. Here, we first quantify the relation between sequence similarity, structure similarity, and alignment length by an exhaustive survey of alignments between proteins of known structure and report a homology threshold curve as a function of alignment length. We then produce a database of homology‐derived secondary structure of proteins (HSSP) by aligning to each protein of known structure all sequences deemed homologous on the basis of the threshold curve. For each known protein structure, the derived database contains the aligned sequences, secondary structure, sequence variability, and sequence profile. Tertiary structures of the aligned sequences are implied, but not modeled explicity. The database effectively increases the number of known protein structures by a factor of five to more than 1800. The results may be useful in assessing the structural significance of matches in sequence database searches, in deriving preferences and patterns for structure prediction, in elucidating the structural role of conserved residues, and in modeling three‐dimensional detail by homology.",fullPaper,jv385
Medicine,p1772,d1,acd36b17c486957cebacc3ad68bd83ed417bf9cc,j377,Human Mutation,The IARC TP53 database: New online mutation analysis and recommendations to users,"Mutations in the tumor suppressor gene TP53 are frequent in most human cancers. Comparison of the mutation patterns in different cancers may reveal clues on the natural history of the disease. Over the past 10 years, several databases of TP53 mutations have been developed. The most extensive of these databases is maintained and developed at the International Agency for Research on Cancer. The database compiles all mutations (somatic and inherited), as well as polymorphisms, that have been reported in the published literature since 1989. The IARC TP53 mutation dataset is the largest dataset available on the variations of any human gene. The database is available at www.iarc.fr/P53/. In this paper, we describe recent developments of the database. These developments include restructuring of the database, which is now patient‐centered, with more detailed annotations on the patient (carcinogen exposure, virus infection, genetic background). In addition, a new on‐line application to retrieve somatic mutation data and analyze mutation patterns is now available. We also discuss limitations on the use of the database and provide recommendations to users. Hum Mutat 19:607–614, 2002. © 2002 Wiley‐Liss, Inc.",fullPaper,jv377
Biology,p1772,d5,acd36b17c486957cebacc3ad68bd83ed417bf9cc,j377,Human Mutation,The IARC TP53 database: New online mutation analysis and recommendations to users,"Mutations in the tumor suppressor gene TP53 are frequent in most human cancers. Comparison of the mutation patterns in different cancers may reveal clues on the natural history of the disease. Over the past 10 years, several databases of TP53 mutations have been developed. The most extensive of these databases is maintained and developed at the International Agency for Research on Cancer. The database compiles all mutations (somatic and inherited), as well as polymorphisms, that have been reported in the published literature since 1989. The IARC TP53 mutation dataset is the largest dataset available on the variations of any human gene. The database is available at www.iarc.fr/P53/. In this paper, we describe recent developments of the database. These developments include restructuring of the database, which is now patient‐centered, with more detailed annotations on the patient (carcinogen exposure, virus infection, genetic background). In addition, a new on‐line application to retrieve somatic mutation data and analyze mutation patterns is now available. We also discuss limitations on the use of the database and provide recommendations to users. Hum Mutat 19:607–614, 2002. © 2002 Wiley‐Liss, Inc.",fullPaper,jv377
Medicine,p1773,d1,e03a1c0f0a8d95aa84bb21f2aa95e5053e4cb655,c104,North American Chapter of the Association for Computational Linguistics,miR2Disease: a manually curated database for microRNA deregulation in human disease,"‘miR2Disease’, a manually curated database, aims at providing a comprehensive resource of microRNA deregulation in various human diseases. The current version of miR2Disease documents 1939 curated relationships between 299 human microRNAs and 94 human diseases by reviewing more than 600 published papers. Around one-seventh of the microRNA–disease relationships represent the pathogenic roles of deregulated microRNA in human disease. Each entry in the miR2Disease contains detailed information on a microRNA–disease relationship, including a microRNA ID, the disease name, a brief description of the microRNA–disease relationship, an expression pattern of the microRNA, the detection method for microRNA expression, experimentally verified target gene(s) of the microRNA and a literature reference. miR2Disease provides a user-friendly interface for a convenient retrieval of each entry by microRNA ID, disease name, or target gene. In addition, miR2Disease offers a submission page that allows researchers to submit established microRNA–disease relationships that are not documented. Once approved by the submission review committee, the submitted records will be included in the database. miR2Disease is freely available at http://www.miR2Disease.org.",poster,cp104
Computer Science,p1773,d3,e03a1c0f0a8d95aa84bb21f2aa95e5053e4cb655,c104,North American Chapter of the Association for Computational Linguistics,miR2Disease: a manually curated database for microRNA deregulation in human disease,"‘miR2Disease’, a manually curated database, aims at providing a comprehensive resource of microRNA deregulation in various human diseases. The current version of miR2Disease documents 1939 curated relationships between 299 human microRNAs and 94 human diseases by reviewing more than 600 published papers. Around one-seventh of the microRNA–disease relationships represent the pathogenic roles of deregulated microRNA in human disease. Each entry in the miR2Disease contains detailed information on a microRNA–disease relationship, including a microRNA ID, the disease name, a brief description of the microRNA–disease relationship, an expression pattern of the microRNA, the detection method for microRNA expression, experimentally verified target gene(s) of the microRNA and a literature reference. miR2Disease provides a user-friendly interface for a convenient retrieval of each entry by microRNA ID, disease name, or target gene. In addition, miR2Disease offers a submission page that allows researchers to submit established microRNA–disease relationships that are not documented. Once approved by the submission review committee, the submitted records will be included in the database. miR2Disease is freely available at http://www.miR2Disease.org.",poster,cp104
Biology,p1773,d5,e03a1c0f0a8d95aa84bb21f2aa95e5053e4cb655,c104,North American Chapter of the Association for Computational Linguistics,miR2Disease: a manually curated database for microRNA deregulation in human disease,"‘miR2Disease’, a manually curated database, aims at providing a comprehensive resource of microRNA deregulation in various human diseases. The current version of miR2Disease documents 1939 curated relationships between 299 human microRNAs and 94 human diseases by reviewing more than 600 published papers. Around one-seventh of the microRNA–disease relationships represent the pathogenic roles of deregulated microRNA in human disease. Each entry in the miR2Disease contains detailed information on a microRNA–disease relationship, including a microRNA ID, the disease name, a brief description of the microRNA–disease relationship, an expression pattern of the microRNA, the detection method for microRNA expression, experimentally verified target gene(s) of the microRNA and a literature reference. miR2Disease provides a user-friendly interface for a convenient retrieval of each entry by microRNA ID, disease name, or target gene. In addition, miR2Disease offers a submission page that allows researchers to submit established microRNA–disease relationships that are not documented. Once approved by the submission review committee, the submitted records will be included in the database. miR2Disease is freely available at http://www.miR2Disease.org.",poster,cp104
Medicine,p1774,d1,feae36b19d8f8566aefbb8b30e9fb55c1592f0a7,j193,Nature Genetics,A gene expression database for the molecular pharmacology of cancer,Abstract,fullPaper,jv193
Biology,p1774,d5,feae36b19d8f8566aefbb8b30e9fb55c1592f0a7,j193,Nature Genetics,A gene expression database for the molecular pharmacology of cancer,Abstract,fullPaper,jv193
Medicine,p1775,d1,aec58dd1363d679e9e1d7917d74c0bcde504b65f,j386,Novartis Foundation symposium,The KEGG database.,"KEGG (http://www.genome.ad.jp/kegg/) is a suite of databases and associated software for understanding and simulating higher-order functional behaviours of the cell or the organism from its genome information. First, KEGG computerizes data and knowledge on protein interaction networks (PATHWAY database) and chemical reactions (LIGAND database) that are responsible for various cellular processes. Second, KEGG attempts to reconstruct protein interaction networks for all organisms whose genomes are completely sequenced (GENES and SSDB databases). Third, KEGG can be utilized as reference knowledge for functional genomics (EXPRESSION database) and proteomics (BRITE database) experiments. I will review the current status of KEGG and report on new developments in graph representation and graph computations.",fullPaper,jv386
Computer Science,p1775,d3,aec58dd1363d679e9e1d7917d74c0bcde504b65f,j386,Novartis Foundation symposium,The KEGG database.,"KEGG (http://www.genome.ad.jp/kegg/) is a suite of databases and associated software for understanding and simulating higher-order functional behaviours of the cell or the organism from its genome information. First, KEGG computerizes data and knowledge on protein interaction networks (PATHWAY database) and chemical reactions (LIGAND database) that are responsible for various cellular processes. Second, KEGG attempts to reconstruct protein interaction networks for all organisms whose genomes are completely sequenced (GENES and SSDB databases). Third, KEGG can be utilized as reference knowledge for functional genomics (EXPRESSION database) and proteomics (BRITE database) experiments. I will review the current status of KEGG and report on new developments in graph representation and graph computations.",fullPaper,jv386
Medicine,p1776,d1,3e83d54c5e8dfba82638b4f75ace31505ea60ff0,c37,International Workshop on the Semantic Web,The IntAct molecular interaction database in 2010,"IntAct is an open-source, open data molecular interaction database and toolkit. Data is abstracted from the literature or from direct data depositions by expert curators following a deep annotation model providing a high level of detail. As of September 2009, IntAct contains over 200.000 curated binary interaction evidences. In response to the growing data volume and user requests, IntAct now provides a two-tiered view of the interaction data. The search interface allows the user to iteratively develop complex queries, exploiting the detailed annotation with hierarchical controlled vocabularies. Results are provided at any stage in a simplified, tabular view. Specialized views then allows ‘zooming in’ on the full annotation of interactions, interactors and their properties. IntAct source code and data are freely available at http://www.ebi.ac.uk/intact.",poster,cp37
Computer Science,p1776,d3,3e83d54c5e8dfba82638b4f75ace31505ea60ff0,c37,International Workshop on the Semantic Web,The IntAct molecular interaction database in 2010,"IntAct is an open-source, open data molecular interaction database and toolkit. Data is abstracted from the literature or from direct data depositions by expert curators following a deep annotation model providing a high level of detail. As of September 2009, IntAct contains over 200.000 curated binary interaction evidences. In response to the growing data volume and user requests, IntAct now provides a two-tiered view of the interaction data. The search interface allows the user to iteratively develop complex queries, exploiting the detailed annotation with hierarchical controlled vocabularies. Results are provided at any stage in a simplified, tabular view. Specialized views then allows ‘zooming in’ on the full annotation of interactions, interactors and their properties. IntAct source code and data are freely available at http://www.ebi.ac.uk/intact.",poster,cp37
Biology,p1776,d5,3e83d54c5e8dfba82638b4f75ace31505ea60ff0,c37,International Workshop on the Semantic Web,The IntAct molecular interaction database in 2010,"IntAct is an open-source, open data molecular interaction database and toolkit. Data is abstracted from the literature or from direct data depositions by expert curators following a deep annotation model providing a high level of detail. As of September 2009, IntAct contains over 200.000 curated binary interaction evidences. In response to the growing data volume and user requests, IntAct now provides a two-tiered view of the interaction data. The search interface allows the user to iteratively develop complex queries, exploiting the detailed annotation with hierarchical controlled vocabularies. Results are provided at any stage in a simplified, tabular view. Specialized views then allows ‘zooming in’ on the full annotation of interactions, interactors and their properties. IntAct source code and data are freely available at http://www.ebi.ac.uk/intact.",poster,cp37
Medicine,p1777,d1,b7340682cd94e4a40df2823cee68ff166be93f86,c54,ACM SIGCOMM Symposium on Software Defined Networking Research,The ConsensusPathDB interaction database: 2013 update,"Knowledge of the various interactions between molecules in the cell is crucial for understanding cellular processes in health and disease. Currently available interaction databases, being largely complementary to each other, must be integrated to obtain a comprehensive global map of the different types of interactions. We have previously reported the development of an integrative interaction database called ConsensusPathDB (http://ConsensusPathDB.org) that aims to fulfill this task. In this update article, we report its significant progress in terms of interaction content and web interface tools. ConsensusPathDB has grown mainly due to the integration of 12 further databases; it now contains 215 541 unique interactions and 4601 pathways from overall 30 databases. Binary protein interactions are scored with our confidence assessment tool, IntScore. The ConsensusPathDB web interface allows users to take advantage of these integrated interaction and pathway data in different contexts. Recent developments include pathway analysis of metabolite lists, visualization of functional gene/metabolite sets as overlap graphs, gene set analysis based on protein complexes and induced network modules analysis that connects a list of genes through various interaction types. To facilitate the interactive, visual interpretation of interaction and pathway data, we have re-implemented the graph visualization feature of ConsensusPathDB using the Cytoscape.js library.",poster,cp54
Computer Science,p1777,d3,b7340682cd94e4a40df2823cee68ff166be93f86,c54,ACM SIGCOMM Symposium on Software Defined Networking Research,The ConsensusPathDB interaction database: 2013 update,"Knowledge of the various interactions between molecules in the cell is crucial for understanding cellular processes in health and disease. Currently available interaction databases, being largely complementary to each other, must be integrated to obtain a comprehensive global map of the different types of interactions. We have previously reported the development of an integrative interaction database called ConsensusPathDB (http://ConsensusPathDB.org) that aims to fulfill this task. In this update article, we report its significant progress in terms of interaction content and web interface tools. ConsensusPathDB has grown mainly due to the integration of 12 further databases; it now contains 215 541 unique interactions and 4601 pathways from overall 30 databases. Binary protein interactions are scored with our confidence assessment tool, IntScore. The ConsensusPathDB web interface allows users to take advantage of these integrated interaction and pathway data in different contexts. Recent developments include pathway analysis of metabolite lists, visualization of functional gene/metabolite sets as overlap graphs, gene set analysis based on protein complexes and induced network modules analysis that connects a list of genes through various interaction types. To facilitate the interactive, visual interpretation of interaction and pathway data, we have re-implemented the graph visualization feature of ConsensusPathDB using the Cytoscape.js library.",poster,cp54
Biology,p1777,d5,b7340682cd94e4a40df2823cee68ff166be93f86,c54,ACM SIGCOMM Symposium on Software Defined Networking Research,The ConsensusPathDB interaction database: 2013 update,"Knowledge of the various interactions between molecules in the cell is crucial for understanding cellular processes in health and disease. Currently available interaction databases, being largely complementary to each other, must be integrated to obtain a comprehensive global map of the different types of interactions. We have previously reported the development of an integrative interaction database called ConsensusPathDB (http://ConsensusPathDB.org) that aims to fulfill this task. In this update article, we report its significant progress in terms of interaction content and web interface tools. ConsensusPathDB has grown mainly due to the integration of 12 further databases; it now contains 215 541 unique interactions and 4601 pathways from overall 30 databases. Binary protein interactions are scored with our confidence assessment tool, IntScore. The ConsensusPathDB web interface allows users to take advantage of these integrated interaction and pathway data in different contexts. Recent developments include pathway analysis of metabolite lists, visualization of functional gene/metabolite sets as overlap graphs, gene set analysis based on protein complexes and induced network modules analysis that connects a list of genes through various interaction types. To facilitate the interactive, visual interpretation of interaction and pathway data, we have re-implemented the graph visualization feature of ConsensusPathDB using the Cytoscape.js library.",poster,cp54
Medicine,p1780,d1,30ae1edd4e6f13a28f87ec150c407e6820c7da60,j387,BMC Cancer,PROGgeneV2: enhancements on the existing database,Abstract,fullPaper,jv387
Computer Science,p1780,d3,30ae1edd4e6f13a28f87ec150c407e6820c7da60,j387,BMC Cancer,PROGgeneV2: enhancements on the existing database,Abstract,fullPaper,jv387
Medicine,p1783,d1,cd4ef5b8da1a543871357f4bca7483e89ff9e3b5,c92,International Symposium on Computer Architecture,CDD: a Conserved Domain Database for protein classification,"The Conserved Domain Database (CDD) is the protein classification component of NCBI's Entrez query and retrieval system. CDD is linked to other Entrez databases such as Proteins, Taxonomy and PubMed®, and can be accessed at http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=cdd. CD-Search, which is available at http://www.ncbi.nlm.nih.gov/Structure/cdd/wrpsb.cgi, is a fast, interactive tool to identify conserved domains in new protein sequences. CD-Search results for protein sequences in Entrez are pre-computed to provide links between proteins and domain models, and computational annotation visible upon request. Protein–protein queries submitted to NCBI's BLAST search service at http://www.ncbi.nlm.nih.gov/BLAST are scanned for the presence of conserved domains by default. While CDD started out as essentially a mirror of publicly available domain alignment collections, such as SMART, Pfam and COG, we have continued an effort to update, and in some cases replace these models with domain hierarchies curated at the NCBI. Here, we report on the progress of the curation effort and associated improvements in the functionality of the CDD information retrieval system.",poster,cp92
Computer Science,p1783,d3,cd4ef5b8da1a543871357f4bca7483e89ff9e3b5,c92,International Symposium on Computer Architecture,CDD: a Conserved Domain Database for protein classification,"The Conserved Domain Database (CDD) is the protein classification component of NCBI's Entrez query and retrieval system. CDD is linked to other Entrez databases such as Proteins, Taxonomy and PubMed®, and can be accessed at http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=cdd. CD-Search, which is available at http://www.ncbi.nlm.nih.gov/Structure/cdd/wrpsb.cgi, is a fast, interactive tool to identify conserved domains in new protein sequences. CD-Search results for protein sequences in Entrez are pre-computed to provide links between proteins and domain models, and computational annotation visible upon request. Protein–protein queries submitted to NCBI's BLAST search service at http://www.ncbi.nlm.nih.gov/BLAST are scanned for the presence of conserved domains by default. While CDD started out as essentially a mirror of publicly available domain alignment collections, such as SMART, Pfam and COG, we have continued an effort to update, and in some cases replace these models with domain hierarchies curated at the NCBI. Here, we report on the progress of the curation effort and associated improvements in the functionality of the CDD information retrieval system.",poster,cp92
Biology,p1783,d5,cd4ef5b8da1a543871357f4bca7483e89ff9e3b5,c92,International Symposium on Computer Architecture,CDD: a Conserved Domain Database for protein classification,"The Conserved Domain Database (CDD) is the protein classification component of NCBI's Entrez query and retrieval system. CDD is linked to other Entrez databases such as Proteins, Taxonomy and PubMed®, and can be accessed at http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=cdd. CD-Search, which is available at http://www.ncbi.nlm.nih.gov/Structure/cdd/wrpsb.cgi, is a fast, interactive tool to identify conserved domains in new protein sequences. CD-Search results for protein sequences in Entrez are pre-computed to provide links between proteins and domain models, and computational annotation visible upon request. Protein–protein queries submitted to NCBI's BLAST search service at http://www.ncbi.nlm.nih.gov/BLAST are scanned for the presence of conserved domains by default. While CDD started out as essentially a mirror of publicly available domain alignment collections, such as SMART, Pfam and COG, we have continued an effort to update, and in some cases replace these models with domain hierarchies curated at the NCBI. Here, we report on the progress of the curation effort and associated improvements in the functionality of the CDD information retrieval system.",poster,cp92
Medicine,p1784,d1,ab99f2afd0f09ee707540022f0895a09fa1107f1,c91,International Symposium on High-Performance Computer Architecture,Database resources of the National Center for Biotechnology,"In addition to maintaining the GenBank(R) nucleic acid sequence database, the National Center for Biotechnology Information (NCBI) provides data analysis and retrieval resources for the data in GenBank and other biological data made available through NCBI's Web site. NCBI resources include Entrez, PubMed, PubMed Central (PMC), LocusLink, the NCBITaxonomy Browser, BLAST, BLAST Link (BLink), Electronic PCR (e-PCR), Open Reading Frame (ORF) Finder, References Sequence (RefSeq), UniGene, HomoloGene, ProtEST, Database of Single Nucleotide Polymorphisms (dbSNP), Human/Mouse Homology Map, Cancer Chromosome Aberration Project (CCAP), Entrez Genomes and related tools, the Map Viewer, Model Maker (MM), Evidence Viewer (EV), Clusters of Orthologous Groups (COGs) database, Retroviral Genotyping Tools, SAGEmap, Gene Expression Omnibus (GEO), Online Mendelian Inheritance in Man (OMIM), the Molecular Modeling Database (MMDB), the Conserved Domain Database (CDD), and the Conserved Domain Architecture Retrieval Tool (CDART). Augmenting many of the Web applications are custom implementations of the BLAST program optimized to search specialized data sets. All of the resources can be accessed through the NCBI home page at: http://www.ncbi.nlm.nih.gov.",poster,cp91
Computer Science,p1784,d3,ab99f2afd0f09ee707540022f0895a09fa1107f1,c91,International Symposium on High-Performance Computer Architecture,Database resources of the National Center for Biotechnology,"In addition to maintaining the GenBank(R) nucleic acid sequence database, the National Center for Biotechnology Information (NCBI) provides data analysis and retrieval resources for the data in GenBank and other biological data made available through NCBI's Web site. NCBI resources include Entrez, PubMed, PubMed Central (PMC), LocusLink, the NCBITaxonomy Browser, BLAST, BLAST Link (BLink), Electronic PCR (e-PCR), Open Reading Frame (ORF) Finder, References Sequence (RefSeq), UniGene, HomoloGene, ProtEST, Database of Single Nucleotide Polymorphisms (dbSNP), Human/Mouse Homology Map, Cancer Chromosome Aberration Project (CCAP), Entrez Genomes and related tools, the Map Viewer, Model Maker (MM), Evidence Viewer (EV), Clusters of Orthologous Groups (COGs) database, Retroviral Genotyping Tools, SAGEmap, Gene Expression Omnibus (GEO), Online Mendelian Inheritance in Man (OMIM), the Molecular Modeling Database (MMDB), the Conserved Domain Database (CDD), and the Conserved Domain Architecture Retrieval Tool (CDART). Augmenting many of the Web applications are custom implementations of the BLAST program optimized to search specialized data sets. All of the resources can be accessed through the NCBI home page at: http://www.ncbi.nlm.nih.gov.",poster,cp91
Biology,p1784,d5,ab99f2afd0f09ee707540022f0895a09fa1107f1,c91,International Symposium on High-Performance Computer Architecture,Database resources of the National Center for Biotechnology,"In addition to maintaining the GenBank(R) nucleic acid sequence database, the National Center for Biotechnology Information (NCBI) provides data analysis and retrieval resources for the data in GenBank and other biological data made available through NCBI's Web site. NCBI resources include Entrez, PubMed, PubMed Central (PMC), LocusLink, the NCBITaxonomy Browser, BLAST, BLAST Link (BLink), Electronic PCR (e-PCR), Open Reading Frame (ORF) Finder, References Sequence (RefSeq), UniGene, HomoloGene, ProtEST, Database of Single Nucleotide Polymorphisms (dbSNP), Human/Mouse Homology Map, Cancer Chromosome Aberration Project (CCAP), Entrez Genomes and related tools, the Map Viewer, Model Maker (MM), Evidence Viewer (EV), Clusters of Orthologous Groups (COGs) database, Retroviral Genotyping Tools, SAGEmap, Gene Expression Omnibus (GEO), Online Mendelian Inheritance in Man (OMIM), the Molecular Modeling Database (MMDB), the Conserved Domain Database (CDD), and the Conserved Domain Architecture Retrieval Tool (CDART). Augmenting many of the Web applications are custom implementations of the BLAST program optimized to search specialized data sets. All of the resources can be accessed through the NCBI home page at: http://www.ncbi.nlm.nih.gov.",poster,cp91
Medicine,p1785,d1,42a77b1006675dc954495459ba3bf9c194258c04,c46,Ideal,BIND: the Biomolecular Interaction Network Database,"The Biomolecular Interaction Network Database (BIND: http://bind.ca) archives biomolecular interaction, complex and pathway information. A web-based system is available to query, view and submit records. BIND continues to grow with the addition of individual submissions as well as interaction data from the PDB and a number of large-scale interaction and complex mapping experiments using yeast two hybrid, mass spectrometry, genetic interactions and phage display. We have developed a new graphical analysis tool that provides users with a view of the domain composition of proteins in interaction and complex records to help relate functional domains to protein interactions. An interaction network clustering tool has also been developed to help focus on regions of interest. Continued input from users has helped further mature the BIND data specification, which now includes the ability to store detailed information about genetic interactions. The BIND data specification is available as ASN.1 and XML DTD.",poster,cp46
Computer Science,p1785,d3,42a77b1006675dc954495459ba3bf9c194258c04,c46,Ideal,BIND: the Biomolecular Interaction Network Database,"The Biomolecular Interaction Network Database (BIND: http://bind.ca) archives biomolecular interaction, complex and pathway information. A web-based system is available to query, view and submit records. BIND continues to grow with the addition of individual submissions as well as interaction data from the PDB and a number of large-scale interaction and complex mapping experiments using yeast two hybrid, mass spectrometry, genetic interactions and phage display. We have developed a new graphical analysis tool that provides users with a view of the domain composition of proteins in interaction and complex records to help relate functional domains to protein interactions. An interaction network clustering tool has also been developed to help focus on regions of interest. Continued input from users has helped further mature the BIND data specification, which now includes the ability to store detailed information about genetic interactions. The BIND data specification is available as ASN.1 and XML DTD.",poster,cp46
Biology,p1785,d5,42a77b1006675dc954495459ba3bf9c194258c04,c46,Ideal,BIND: the Biomolecular Interaction Network Database,"The Biomolecular Interaction Network Database (BIND: http://bind.ca) archives biomolecular interaction, complex and pathway information. A web-based system is available to query, view and submit records. BIND continues to grow with the addition of individual submissions as well as interaction data from the PDB and a number of large-scale interaction and complex mapping experiments using yeast two hybrid, mass spectrometry, genetic interactions and phage display. We have developed a new graphical analysis tool that provides users with a view of the domain composition of proteins in interaction and complex records to help relate functional domains to protein interactions. An interaction network clustering tool has also been developed to help focus on regions of interest. Continued input from users has helped further mature the BIND data specification, which now includes the ability to store detailed information about genetic interactions. The BIND data specification is available as ASN.1 and XML DTD.",poster,cp46
Medicine,p1789,d1,c64e51ab702ecdfc65281eb06fae8722809a0756,j388,"Behavoir research methods, instruments & computers",A lifespan database of adult facial stimuli,Abstract,fullPaper,jv388
Psychology,p1789,d10,c64e51ab702ecdfc65281eb06fae8722809a0756,j388,"Behavoir research methods, instruments & computers",A lifespan database of adult facial stimuli,Abstract,fullPaper,jv388
Medicine,p1791,d1,1bdc29257650e2bed7d11a9a4afb4eeea0bb1296,c45,IEEE Symposium on Security and Privacy,"The PROSITE database, its status in 1997","The PROSITE database consists of biologically significant patterns and profiles formulated in such a way that with appropriate computational tools it can help to determine to which known family of protein (if any) a new sequence belongs, or which known domain(s) it contains.",poster,cp45
Computer Science,p1791,d3,1bdc29257650e2bed7d11a9a4afb4eeea0bb1296,c45,IEEE Symposium on Security and Privacy,"The PROSITE database, its status in 1997","The PROSITE database consists of biologically significant patterns and profiles formulated in such a way that with appropriate computational tools it can help to determine to which known family of protein (if any) a new sequence belongs, or which known domain(s) it contains.",poster,cp45
Biology,p1791,d5,1bdc29257650e2bed7d11a9a4afb4eeea0bb1296,c45,IEEE Symposium on Security and Privacy,"The PROSITE database, its status in 1997","The PROSITE database consists of biologically significant patterns and profiles formulated in such a way that with appropriate computational tools it can help to determine to which known family of protein (if any) a new sequence belongs, or which known domain(s) it contains.",poster,cp45
Medicine,p1793,d1,20bdfd777432f7eab7ab2cc146297df1db654090,c20,ACM Conference on Economics and Computation,"MINT, the molecular interaction database: 2012 update","The Molecular INTeraction Database (MINT, http://mint.bio.uniroma2.it/mint/) is a public repository for protein–protein interactions (PPI) reported in peer-reviewed journals. The database grows steadily over the years and at September 2011 contains approximately 235 000 binary interactions captured from over 4750 publications. The web interface allows the users to search, visualize and download interactions data. MINT is one of the members of the International Molecular Exchange consortium (IMEx) and adopts the Molecular Interaction Ontology of the Proteomics Standard Initiative (PSI-MI) standards for curation and data exchange. MINT data are freely accessible and downloadable at http://mint.bio.uniroma2.it/mint/download.do. We report here the growth of the database, the major changes in curation policy and a new algorithm to assign a confidence to each interaction.",poster,cp20
Computer Science,p1793,d3,20bdfd777432f7eab7ab2cc146297df1db654090,c20,ACM Conference on Economics and Computation,"MINT, the molecular interaction database: 2012 update","The Molecular INTeraction Database (MINT, http://mint.bio.uniroma2.it/mint/) is a public repository for protein–protein interactions (PPI) reported in peer-reviewed journals. The database grows steadily over the years and at September 2011 contains approximately 235 000 binary interactions captured from over 4750 publications. The web interface allows the users to search, visualize and download interactions data. MINT is one of the members of the International Molecular Exchange consortium (IMEx) and adopts the Molecular Interaction Ontology of the Proteomics Standard Initiative (PSI-MI) standards for curation and data exchange. MINT data are freely accessible and downloadable at http://mint.bio.uniroma2.it/mint/download.do. We report here the growth of the database, the major changes in curation policy and a new algorithm to assign a confidence to each interaction.",poster,cp20
Biology,p1793,d5,20bdfd777432f7eab7ab2cc146297df1db654090,c20,ACM Conference on Economics and Computation,"MINT, the molecular interaction database: 2012 update","The Molecular INTeraction Database (MINT, http://mint.bio.uniroma2.it/mint/) is a public repository for protein–protein interactions (PPI) reported in peer-reviewed journals. The database grows steadily over the years and at September 2011 contains approximately 235 000 binary interactions captured from over 4750 publications. The web interface allows the users to search, visualize and download interactions data. MINT is one of the members of the International Molecular Exchange consortium (IMEx) and adopts the Molecular Interaction Ontology of the Proteomics Standard Initiative (PSI-MI) standards for curation and data exchange. MINT data are freely accessible and downloadable at http://mint.bio.uniroma2.it/mint/download.do. We report here the growth of the database, the major changes in curation policy and a new algorithm to assign a confidence to each interaction.",poster,cp20
Medicine,p1797,d1,107e98602c1be84b1654d6a1b241b7c97a94c71f,c24,International Conference on Data Technologies and Applications,"ExoCarta 2012: database of exosomal proteins, RNA and lipids","Exosomes are membraneous nanovesicles of endocytic origin released by most cell types from diverse organisms; they play a critical role in cell–cell communication. ExoCarta (http://www.exocarta.org) is a manually curated database of exosomal proteins, RNA and lipids. The database catalogs information from both published and unpublished exosomal studies. The mode of exosomal purification and characterization, the biophysical and molecular properties are listed in the database aiding biomedical scientists in assessing the quality of the exosomal preparation and the corresponding data obtained. Currently, ExoCarta (Version 3.1) contains information on 11 261 protein entries, 2375 mRNA entries and 764 miRNA entries that were obtained from 134 exosomal studies. In addition to the data update, as a new feature, lipids identified in exosomes are added to ExoCarta. We believe that this free web-based community resource will aid researchers in identifying molecular signatures (proteins/RNA/lipids) that are specific to certain tissue/cell type derived exosomes and trigger new exosomal studies.",poster,cp24
Computer Science,p1797,d3,107e98602c1be84b1654d6a1b241b7c97a94c71f,c24,International Conference on Data Technologies and Applications,"ExoCarta 2012: database of exosomal proteins, RNA and lipids","Exosomes are membraneous nanovesicles of endocytic origin released by most cell types from diverse organisms; they play a critical role in cell–cell communication. ExoCarta (http://www.exocarta.org) is a manually curated database of exosomal proteins, RNA and lipids. The database catalogs information from both published and unpublished exosomal studies. The mode of exosomal purification and characterization, the biophysical and molecular properties are listed in the database aiding biomedical scientists in assessing the quality of the exosomal preparation and the corresponding data obtained. Currently, ExoCarta (Version 3.1) contains information on 11 261 protein entries, 2375 mRNA entries and 764 miRNA entries that were obtained from 134 exosomal studies. In addition to the data update, as a new feature, lipids identified in exosomes are added to ExoCarta. We believe that this free web-based community resource will aid researchers in identifying molecular signatures (proteins/RNA/lipids) that are specific to certain tissue/cell type derived exosomes and trigger new exosomal studies.",poster,cp24
Biology,p1797,d5,107e98602c1be84b1654d6a1b241b7c97a94c71f,c24,International Conference on Data Technologies and Applications,"ExoCarta 2012: database of exosomal proteins, RNA and lipids","Exosomes are membraneous nanovesicles of endocytic origin released by most cell types from diverse organisms; they play a critical role in cell–cell communication. ExoCarta (http://www.exocarta.org) is a manually curated database of exosomal proteins, RNA and lipids. The database catalogs information from both published and unpublished exosomal studies. The mode of exosomal purification and characterization, the biophysical and molecular properties are listed in the database aiding biomedical scientists in assessing the quality of the exosomal preparation and the corresponding data obtained. Currently, ExoCarta (Version 3.1) contains information on 11 261 protein entries, 2375 mRNA entries and 764 miRNA entries that were obtained from 134 exosomal studies. In addition to the data update, as a new feature, lipids identified in exosomes are added to ExoCarta. We believe that this free web-based community resource will aid researchers in identifying molecular signatures (proteins/RNA/lipids) that are specific to certain tissue/cell type derived exosomes and trigger new exosomal studies.",poster,cp24
Medicine,p1798,d1,f5f4a0933cf097fb14ab84ca295b1bdfe01f97e0,c70,Annual Meeting of the Association for Computational Linguistics,PlasmoDB: a functional genomic database for malaria parasites,"PlasmoDB (http://PlasmoDB.org) is a functional genomic database for Plasmodium spp. that provides a resource for data analysis and visualization in a gene-by-gene or genome-wide scale. PlasmoDB belongs to a family of genomic resources that are housed under the EuPathDB (http://EuPathDB.org) Bioinformatics Resource Center (BRC) umbrella. The latest release, PlasmoDB 5.5, contains numerous new data types from several broad categories—annotated genomes, evidence of transcription, proteomics evidence, protein function evidence, population biology and evolution. Data in PlasmoDB can be queried by selecting the data of interest from a query grid or drop down menus. Various results can then be combined with each other on the query history page. Search results can be downloaded with associated functional data and registered users can store their query history for future retrieval or analysis.",poster,cp70
Computer Science,p1798,d3,f5f4a0933cf097fb14ab84ca295b1bdfe01f97e0,c70,Annual Meeting of the Association for Computational Linguistics,PlasmoDB: a functional genomic database for malaria parasites,"PlasmoDB (http://PlasmoDB.org) is a functional genomic database for Plasmodium spp. that provides a resource for data analysis and visualization in a gene-by-gene or genome-wide scale. PlasmoDB belongs to a family of genomic resources that are housed under the EuPathDB (http://EuPathDB.org) Bioinformatics Resource Center (BRC) umbrella. The latest release, PlasmoDB 5.5, contains numerous new data types from several broad categories—annotated genomes, evidence of transcription, proteomics evidence, protein function evidence, population biology and evolution. Data in PlasmoDB can be queried by selecting the data of interest from a query grid or drop down menus. Various results can then be combined with each other on the query history page. Search results can be downloaded with associated functional data and registered users can store their query history for future retrieval or analysis.",poster,cp70
Biology,p1798,d5,f5f4a0933cf097fb14ab84ca295b1bdfe01f97e0,c70,Annual Meeting of the Association for Computational Linguistics,PlasmoDB: a functional genomic database for malaria parasites,"PlasmoDB (http://PlasmoDB.org) is a functional genomic database for Plasmodium spp. that provides a resource for data analysis and visualization in a gene-by-gene or genome-wide scale. PlasmoDB belongs to a family of genomic resources that are housed under the EuPathDB (http://EuPathDB.org) Bioinformatics Resource Center (BRC) umbrella. The latest release, PlasmoDB 5.5, contains numerous new data types from several broad categories—annotated genomes, evidence of transcription, proteomics evidence, protein function evidence, population biology and evolution. Data in PlasmoDB can be queried by selecting the data of interest from a query grid or drop down menus. Various results can then be combined with each other on the query history page. Search results can be downloaded with associated functional data and registered users can store their query history for future retrieval or analysis.",poster,cp70
Medicine,p1799,d1,fd8ba9431c2a1780e27b6f5e1e4947db9c85801c,c96,Human Language Technology - The Baltic Perspectiv,Crystallography Open Database (COD): an open-access collection of crystal structures and platform for world-wide collaboration,"Using an open-access distribution model, the Crystallography Open Database (COD, http://www.crystallography.net) collects all known ‘small molecule / small to medium sized unit cell’ crystal structures and makes them available freely on the Internet. As of today, the COD has aggregated ∼150 000 structures, offering basic search capabilities and the possibility to download the whole database, or parts thereof using a variety of standard open communication protocols. A newly developed website provides capabilities for all registered users to deposit published and so far unpublished structures as personal communications or pre-publication depositions. Such a setup enables extension of the COD database by many users simultaneously. This increases the possibilities for growth of the COD database, and is the first step towards establishing a world wide Internet-based collaborative platform dedicated to the collection and curation of structural knowledge.",poster,cp96
Computer Science,p1799,d3,fd8ba9431c2a1780e27b6f5e1e4947db9c85801c,c96,Human Language Technology - The Baltic Perspectiv,Crystallography Open Database (COD): an open-access collection of crystal structures and platform for world-wide collaboration,"Using an open-access distribution model, the Crystallography Open Database (COD, http://www.crystallography.net) collects all known ‘small molecule / small to medium sized unit cell’ crystal structures and makes them available freely on the Internet. As of today, the COD has aggregated ∼150 000 structures, offering basic search capabilities and the possibility to download the whole database, or parts thereof using a variety of standard open communication protocols. A newly developed website provides capabilities for all registered users to deposit published and so far unpublished structures as personal communications or pre-publication depositions. Such a setup enables extension of the COD database by many users simultaneously. This increases the possibilities for growth of the COD database, and is the first step towards establishing a world wide Internet-based collaborative platform dedicated to the collection and curation of structural knowledge.",poster,cp96
Biology,p1799,d5,fd8ba9431c2a1780e27b6f5e1e4947db9c85801c,c96,Human Language Technology - The Baltic Perspectiv,Crystallography Open Database (COD): an open-access collection of crystal structures and platform for world-wide collaboration,"Using an open-access distribution model, the Crystallography Open Database (COD, http://www.crystallography.net) collects all known ‘small molecule / small to medium sized unit cell’ crystal structures and makes them available freely on the Internet. As of today, the COD has aggregated ∼150 000 structures, offering basic search capabilities and the possibility to download the whole database, or parts thereof using a variety of standard open communication protocols. A newly developed website provides capabilities for all registered users to deposit published and so far unpublished structures as personal communications or pre-publication depositions. Such a setup enables extension of the COD database by many users simultaneously. This increases the possibilities for growth of the COD database, and is the first step towards establishing a world wide Internet-based collaborative platform dedicated to the collection and curation of structural knowledge.",poster,cp96
Medicine,p1801,d1,5745c2aff2a61d6f4a5b1083663fe7ed54d22672,c36,International Conference on Information Technology Based Higher Education and Training,MODOMICS: a database of RNA modification pathways,"MODOMICS is the first comprehensive database resource for systems biology of RNA modification. It integrates information about the chemical structure of modified nucleosides, their localization in RNA sequences, pathways of their biosynthesis and enzymes that carry out the respective reactions. MODOMICS also provides literature information, and links to other databases, including the available protein sequence and structure data. The current list of modifications and pathways is comprehensive, while the dataset of enzymes is limited to Escherichia coli and Saccharomyces cerevisiae and sequence alignments are presented only for tRNAs from these organisms. RNAs and enzymes from other organisms will be included in the near future. MODOMICS can be queried by the type of nucleoside (e.g. A, G, C, U, I, m1A, nm5s2U, etc.), type of RNA, position of a particular nucleoside, type of reaction (e.g. methylation, thiolation, deamination, etc.) and name or sequence of an enzyme of interest. Options for data presentation include graphs of pathways involving the query nucleoside, multiple sequence alignments of RNA sequences and tabular forms with enzyme and literature data. The contents of MODOMICS can be accessed through the World Wide Web at .",poster,cp36
Computer Science,p1801,d3,5745c2aff2a61d6f4a5b1083663fe7ed54d22672,c36,International Conference on Information Technology Based Higher Education and Training,MODOMICS: a database of RNA modification pathways,"MODOMICS is the first comprehensive database resource for systems biology of RNA modification. It integrates information about the chemical structure of modified nucleosides, their localization in RNA sequences, pathways of their biosynthesis and enzymes that carry out the respective reactions. MODOMICS also provides literature information, and links to other databases, including the available protein sequence and structure data. The current list of modifications and pathways is comprehensive, while the dataset of enzymes is limited to Escherichia coli and Saccharomyces cerevisiae and sequence alignments are presented only for tRNAs from these organisms. RNAs and enzymes from other organisms will be included in the near future. MODOMICS can be queried by the type of nucleoside (e.g. A, G, C, U, I, m1A, nm5s2U, etc.), type of RNA, position of a particular nucleoside, type of reaction (e.g. methylation, thiolation, deamination, etc.) and name or sequence of an enzyme of interest. Options for data presentation include graphs of pathways involving the query nucleoside, multiple sequence alignments of RNA sequences and tabular forms with enzyme and literature data. The contents of MODOMICS can be accessed through the World Wide Web at .",poster,cp36
Biology,p1801,d5,5745c2aff2a61d6f4a5b1083663fe7ed54d22672,c36,International Conference on Information Technology Based Higher Education and Training,MODOMICS: a database of RNA modification pathways,"MODOMICS is the first comprehensive database resource for systems biology of RNA modification. It integrates information about the chemical structure of modified nucleosides, their localization in RNA sequences, pathways of their biosynthesis and enzymes that carry out the respective reactions. MODOMICS also provides literature information, and links to other databases, including the available protein sequence and structure data. The current list of modifications and pathways is comprehensive, while the dataset of enzymes is limited to Escherichia coli and Saccharomyces cerevisiae and sequence alignments are presented only for tRNAs from these organisms. RNAs and enzymes from other organisms will be included in the near future. MODOMICS can be queried by the type of nucleoside (e.g. A, G, C, U, I, m1A, nm5s2U, etc.), type of RNA, position of a particular nucleoside, type of reaction (e.g. methylation, thiolation, deamination, etc.) and name or sequence of an enzyme of interest. Options for data presentation include graphs of pathways involving the query nucleoside, multiple sequence alignments of RNA sequences and tabular forms with enzyme and literature data. The contents of MODOMICS can be accessed through the World Wide Web at .",poster,cp36
Medicine,p1805,d1,fbbe83989ce11b91ab0dfb0d5e822b08cd885f48,c93,ASE BigData & SocialInformatics,The UCSC Genome Browser database: update 2010,"The University of California, Santa Cruz (UCSC) Genome Browser website (http://genome.ucsc.edu/) provides a large database of publicly available sequence and annotation data along with an integrated tool set for examining and comparing the genomes of organisms, aligning sequence to genomes, and displaying and sharing users’ own annotation data. As of September 2009, genomic sequence and a basic set of annotation ‘tracks’ are provided for 47 organisms, including 14 mammals, 10 non-mammal vertebrates, 3 invertebrate deuterostomes, 13 insects, 6 worms and a yeast. New data highlights this year include an updated human genome browser, a 44-species multiple sequence alignment track, improved variation and phenotype tracks and 16 new genome-wide ENCODE tracks. New features include drag-and-zoom navigation, a Wiki track for user-added annotations, new custom track formats for large datasets (bigBed and bigWig), a new multiple alignment output tool, links to variation and protein structure tools, in silico PCR utility enhancements, and improved track configuration tools.",poster,cp93
Computer Science,p1805,d3,fbbe83989ce11b91ab0dfb0d5e822b08cd885f48,c93,ASE BigData & SocialInformatics,The UCSC Genome Browser database: update 2010,"The University of California, Santa Cruz (UCSC) Genome Browser website (http://genome.ucsc.edu/) provides a large database of publicly available sequence and annotation data along with an integrated tool set for examining and comparing the genomes of organisms, aligning sequence to genomes, and displaying and sharing users’ own annotation data. As of September 2009, genomic sequence and a basic set of annotation ‘tracks’ are provided for 47 organisms, including 14 mammals, 10 non-mammal vertebrates, 3 invertebrate deuterostomes, 13 insects, 6 worms and a yeast. New data highlights this year include an updated human genome browser, a 44-species multiple sequence alignment track, improved variation and phenotype tracks and 16 new genome-wide ENCODE tracks. New features include drag-and-zoom navigation, a Wiki track for user-added annotations, new custom track formats for large datasets (bigBed and bigWig), a new multiple alignment output tool, links to variation and protein structure tools, in silico PCR utility enhancements, and improved track configuration tools.",poster,cp93
Biology,p1805,d5,fbbe83989ce11b91ab0dfb0d5e822b08cd885f48,c93,ASE BigData & SocialInformatics,The UCSC Genome Browser database: update 2010,"The University of California, Santa Cruz (UCSC) Genome Browser website (http://genome.ucsc.edu/) provides a large database of publicly available sequence and annotation data along with an integrated tool set for examining and comparing the genomes of organisms, aligning sequence to genomes, and displaying and sharing users’ own annotation data. As of September 2009, genomic sequence and a basic set of annotation ‘tracks’ are provided for 47 organisms, including 14 mammals, 10 non-mammal vertebrates, 3 invertebrate deuterostomes, 13 insects, 6 worms and a yeast. New data highlights this year include an updated human genome browser, a 44-species multiple sequence alignment track, improved variation and phenotype tracks and 16 new genome-wide ENCODE tracks. New features include drag-and-zoom navigation, a Wiki track for user-added annotations, new custom track formats for large datasets (bigBed and bigWig), a new multiple alignment output tool, links to variation and protein structure tools, in silico PCR utility enhancements, and improved track configuration tools.",poster,cp93
Medicine,p1807,d1,b7e3bec7efda0946a73d8cfa06550ef2b1a2b2bd,j390,British Journal of Cancer,The COSMIC (Catalogue of Somatic Mutations in Cancer) database and website,Abstract,fullPaper,jv390
Biology,p1807,d5,b7e3bec7efda0946a73d8cfa06550ef2b1a2b2bd,j390,British Journal of Cancer,The COSMIC (Catalogue of Somatic Mutations in Cancer) database and website,Abstract,fullPaper,jv390
Medicine,p1808,d1,baf03356bf3403f0fc111e1b348a77a01ef48899,c54,ACM SIGCOMM Symposium on Software Defined Networking Research,The MetaCyc database of metabolic pathways and enzymes and the BioCyc collection of Pathway/Genome Databases,"The MetaCyc database (MetaCyc.org) is a comprehensive and freely accessible database describing metabolic pathways and enzymes from all domains of life. MetaCyc pathways are experimentally determined, mostly small-molecule metabolic pathways and are curated from the primary scientific literature. MetaCyc contains >2100 pathways derived from >37 000 publications, and is the largest curated collection of metabolic pathways currently available. BioCyc (BioCyc.org) is a collection of >3000 organism-specific Pathway/Genome Databases (PGDBs), each containing the full genome and predicted metabolic network of one organism, including metabolites, enzymes, reactions, metabolic pathways, predicted operons, transport systems and pathway-hole fillers. Additions to BioCyc over the past 2 years include YeastCyc, a PGDB for Saccharomyces cerevisiae, and 891 new genomes from the Human Microbiome Project. The BioCyc Web site offers a variety of tools for querying and analysis of PGDBs, including Omics Viewers and tools for comparative analysis. New developments include atom mappings in reactions, a new representation of glycan degradation pathways, improved compound structure display, better coverage of enzyme kinetic data, enhancements of the Web Groups functionality, improvements to the Omics viewers, a new representation of the Enzyme Commission system and, for the desktop version of the software, the ability to save display states.",poster,cp54
Computer Science,p1808,d3,baf03356bf3403f0fc111e1b348a77a01ef48899,c54,ACM SIGCOMM Symposium on Software Defined Networking Research,The MetaCyc database of metabolic pathways and enzymes and the BioCyc collection of Pathway/Genome Databases,"The MetaCyc database (MetaCyc.org) is a comprehensive and freely accessible database describing metabolic pathways and enzymes from all domains of life. MetaCyc pathways are experimentally determined, mostly small-molecule metabolic pathways and are curated from the primary scientific literature. MetaCyc contains >2100 pathways derived from >37 000 publications, and is the largest curated collection of metabolic pathways currently available. BioCyc (BioCyc.org) is a collection of >3000 organism-specific Pathway/Genome Databases (PGDBs), each containing the full genome and predicted metabolic network of one organism, including metabolites, enzymes, reactions, metabolic pathways, predicted operons, transport systems and pathway-hole fillers. Additions to BioCyc over the past 2 years include YeastCyc, a PGDB for Saccharomyces cerevisiae, and 891 new genomes from the Human Microbiome Project. The BioCyc Web site offers a variety of tools for querying and analysis of PGDBs, including Omics Viewers and tools for comparative analysis. New developments include atom mappings in reactions, a new representation of glycan degradation pathways, improved compound structure display, better coverage of enzyme kinetic data, enhancements of the Web Groups functionality, improvements to the Omics viewers, a new representation of the Enzyme Commission system and, for the desktop version of the software, the ability to save display states.",poster,cp54
Medicine,p1809,d1,5c86baa92ece7425ceb09232dddd9538c224ce5e,c92,International Symposium on Computer Architecture,The Human Oral Microbiome Database: a web accessible resource for investigating oral microbe taxonomic and genomic information,"The human oral microbiome is the most studied human microflora, but 53% of the species have not yet been validly named and 35% remain uncultivated. The uncultivated taxa are known primarily from 16S rRNA sequence information. Sequence information tied solely to obscure isolate or clone numbers, and usually lacking accurate phylogenetic placement, is a major impediment to working with human oral microbiome data. The goal of creating the Human Oral Microbiome Database (HOMD) is to provide the scientific community with a body site-specific comprehensive database for the more than 600 prokaryote species that are present in the human oral cavity based on a curated 16S rRNA gene-based provisional naming scheme. Currently, two primary types of information are provided in HOMD—taxonomic and genomic. Named oral species and taxa identified from 16S rRNA gene sequence analysis of oral isolates and cloning studies were placed into defined 16S rRNA phylotypes and each given unique Human Oral Taxon (HOT) number. The HOT interlinks phenotypic, phylogenetic, genomic, clinical and bibliographic information for each taxon. A BLAST search tool is provided to match user 16S rRNA gene sequences to a curated, full length, 16S rRNA gene reference data set. For genomic analysis, HOMD provides comprehensive set of analysis tools and maintains frequently updated annotations for all the human oral microbial genomes that have been sequenced and publicly released. Oral bacterial genome sequences, determined as part of the Human Microbiome Project, are being added to the HOMD as they become available. We provide HOMD as a conceptual model for the presentation of microbiome data for other human body sites. Database URL: http://www.homd.org",poster,cp92
Computer Science,p1809,d3,5c86baa92ece7425ceb09232dddd9538c224ce5e,c92,International Symposium on Computer Architecture,The Human Oral Microbiome Database: a web accessible resource for investigating oral microbe taxonomic and genomic information,"The human oral microbiome is the most studied human microflora, but 53% of the species have not yet been validly named and 35% remain uncultivated. The uncultivated taxa are known primarily from 16S rRNA sequence information. Sequence information tied solely to obscure isolate or clone numbers, and usually lacking accurate phylogenetic placement, is a major impediment to working with human oral microbiome data. The goal of creating the Human Oral Microbiome Database (HOMD) is to provide the scientific community with a body site-specific comprehensive database for the more than 600 prokaryote species that are present in the human oral cavity based on a curated 16S rRNA gene-based provisional naming scheme. Currently, two primary types of information are provided in HOMD—taxonomic and genomic. Named oral species and taxa identified from 16S rRNA gene sequence analysis of oral isolates and cloning studies were placed into defined 16S rRNA phylotypes and each given unique Human Oral Taxon (HOT) number. The HOT interlinks phenotypic, phylogenetic, genomic, clinical and bibliographic information for each taxon. A BLAST search tool is provided to match user 16S rRNA gene sequences to a curated, full length, 16S rRNA gene reference data set. For genomic analysis, HOMD provides comprehensive set of analysis tools and maintains frequently updated annotations for all the human oral microbial genomes that have been sequenced and publicly released. Oral bacterial genome sequences, determined as part of the Human Microbiome Project, are being added to the HOMD as they become available. We provide HOMD as a conceptual model for the presentation of microbiome data for other human body sites. Database URL: http://www.homd.org",poster,cp92
Biology,p1809,d5,5c86baa92ece7425ceb09232dddd9538c224ce5e,c92,International Symposium on Computer Architecture,The Human Oral Microbiome Database: a web accessible resource for investigating oral microbe taxonomic and genomic information,"The human oral microbiome is the most studied human microflora, but 53% of the species have not yet been validly named and 35% remain uncultivated. The uncultivated taxa are known primarily from 16S rRNA sequence information. Sequence information tied solely to obscure isolate or clone numbers, and usually lacking accurate phylogenetic placement, is a major impediment to working with human oral microbiome data. The goal of creating the Human Oral Microbiome Database (HOMD) is to provide the scientific community with a body site-specific comprehensive database for the more than 600 prokaryote species that are present in the human oral cavity based on a curated 16S rRNA gene-based provisional naming scheme. Currently, two primary types of information are provided in HOMD—taxonomic and genomic. Named oral species and taxa identified from 16S rRNA gene sequence analysis of oral isolates and cloning studies were placed into defined 16S rRNA phylotypes and each given unique Human Oral Taxon (HOT) number. The HOT interlinks phenotypic, phylogenetic, genomic, clinical and bibliographic information for each taxon. A BLAST search tool is provided to match user 16S rRNA gene sequences to a curated, full length, 16S rRNA gene reference data set. For genomic analysis, HOMD provides comprehensive set of analysis tools and maintains frequently updated annotations for all the human oral microbial genomes that have been sequenced and publicly released. Oral bacterial genome sequences, determined as part of the Human Microbiome Project, are being added to the HOMD as they become available. We provide HOMD as a conceptual model for the presentation of microbiome data for other human body sites. Database URL: http://www.homd.org",poster,cp92
Medicine,p1810,d1,9c1411c9ebc6260edc5798c9339e189e759b2168,c98,Vision,GMD@CSB.DB: the Golm Metabolome Database,"UNLABELLED
Metabolomics, in particular gas chromatography-mass spectrometry (GC-MS) based metabolite profiling of biological extracts, is rapidly becoming one of the cornerstones of functional genomics and systems biology. Metabolite profiling has profound applications in discovering the mode of action of drugs or herbicides, and in unravelling the effect of altered gene expression on metabolism and organism performance in biotechnological applications. As such the technology needs to be available to many laboratories. For this, an open exchange of information is required, like that already achieved for transcript and protein data. One of the key-steps in metabolite profiling is the unambiguous identification of metabolites in highly complex metabolite preparations from biological samples. Collections of mass spectra, which comprise frequently observed metabolites of either known or unknown exact chemical structure, represent the most effective means to pool the identification efforts currently performed in many laboratories around the world. Here we present GMD, The Golm Metabolome Database, an open access metabolome database, which should enable these processes. GMD provides public access to custom mass spectral libraries, metabolite profiling experiments as well as additional information and tools, e.g. with regard to methods, spectral information or compounds. The main goal will be the representation of an exchange platform for experimental research activities and bioinformatics to develop and improve metabolomics by multidisciplinary cooperation.


AVAILABILITY
http://csbdb.mpimp-golm.mpg.de/gmd.html


CONTACT
Steinhauser@mpimp-golm.mpg.de


SUPPLEMENTARY INFORMATION
http://csbdb.mpimp-golm.mpg.de/",poster,cp98
Computer Science,p1810,d3,9c1411c9ebc6260edc5798c9339e189e759b2168,c98,Vision,GMD@CSB.DB: the Golm Metabolome Database,"UNLABELLED
Metabolomics, in particular gas chromatography-mass spectrometry (GC-MS) based metabolite profiling of biological extracts, is rapidly becoming one of the cornerstones of functional genomics and systems biology. Metabolite profiling has profound applications in discovering the mode of action of drugs or herbicides, and in unravelling the effect of altered gene expression on metabolism and organism performance in biotechnological applications. As such the technology needs to be available to many laboratories. For this, an open exchange of information is required, like that already achieved for transcript and protein data. One of the key-steps in metabolite profiling is the unambiguous identification of metabolites in highly complex metabolite preparations from biological samples. Collections of mass spectra, which comprise frequently observed metabolites of either known or unknown exact chemical structure, represent the most effective means to pool the identification efforts currently performed in many laboratories around the world. Here we present GMD, The Golm Metabolome Database, an open access metabolome database, which should enable these processes. GMD provides public access to custom mass spectral libraries, metabolite profiling experiments as well as additional information and tools, e.g. with regard to methods, spectral information or compounds. The main goal will be the representation of an exchange platform for experimental research activities and bioinformatics to develop and improve metabolomics by multidisciplinary cooperation.


AVAILABILITY
http://csbdb.mpimp-golm.mpg.de/gmd.html


CONTACT
Steinhauser@mpimp-golm.mpg.de


SUPPLEMENTARY INFORMATION
http://csbdb.mpimp-golm.mpg.de/",poster,cp98
Biology,p1810,d5,9c1411c9ebc6260edc5798c9339e189e759b2168,c98,Vision,GMD@CSB.DB: the Golm Metabolome Database,"UNLABELLED
Metabolomics, in particular gas chromatography-mass spectrometry (GC-MS) based metabolite profiling of biological extracts, is rapidly becoming one of the cornerstones of functional genomics and systems biology. Metabolite profiling has profound applications in discovering the mode of action of drugs or herbicides, and in unravelling the effect of altered gene expression on metabolism and organism performance in biotechnological applications. As such the technology needs to be available to many laboratories. For this, an open exchange of information is required, like that already achieved for transcript and protein data. One of the key-steps in metabolite profiling is the unambiguous identification of metabolites in highly complex metabolite preparations from biological samples. Collections of mass spectra, which comprise frequently observed metabolites of either known or unknown exact chemical structure, represent the most effective means to pool the identification efforts currently performed in many laboratories around the world. Here we present GMD, The Golm Metabolome Database, an open access metabolome database, which should enable these processes. GMD provides public access to custom mass spectral libraries, metabolite profiling experiments as well as additional information and tools, e.g. with regard to methods, spectral information or compounds. The main goal will be the representation of an exchange platform for experimental research activities and bioinformatics to develop and improve metabolomics by multidisciplinary cooperation.


AVAILABILITY
http://csbdb.mpimp-golm.mpg.de/gmd.html


CONTACT
Steinhauser@mpimp-golm.mpg.de


SUPPLEMENTARY INFORMATION
http://csbdb.mpimp-golm.mpg.de/",poster,cp98
Medicine,p1813,d1,85331cdfdae93103b0c86bcb12bb1b47158ced08,c93,ASE BigData & SocialInformatics,The Transporter Classification Database,"The Transporter Classification Database (TCDB; http://www.tcdb.org) serves as a common reference point for transport protein research. The database contains more than 10 000 non-redundant proteins that represent all currently recognized families of transmembrane molecular transport systems. Proteins in TCDB are organized in a five level hierarchical system, where the first two levels are the class and subclass, the second two are the family and subfamily, and the last one is the transport system. Superfamilies that contain multiple families are included as hyperlinks to the five tier TC hierarchy. TCDB includes proteins from all types of living organisms and is the only transporter classification system that is both universal and recognized by the International Union of Biochemistry and Molecular Biology. It has been expanded by manual curation, contains extensive text descriptions providing structural, functional, mechanistic and evolutionary information, is supported by unique software and is interconnected to many other relevant databases. TCDB is of increasing usefulness to the international scientific community and can serve as a model for the expansion of database technologies. This manuscript describes an update of the database descriptions previously featured in NAR database issues.",poster,cp93
Computer Science,p1813,d3,85331cdfdae93103b0c86bcb12bb1b47158ced08,c93,ASE BigData & SocialInformatics,The Transporter Classification Database,"The Transporter Classification Database (TCDB; http://www.tcdb.org) serves as a common reference point for transport protein research. The database contains more than 10 000 non-redundant proteins that represent all currently recognized families of transmembrane molecular transport systems. Proteins in TCDB are organized in a five level hierarchical system, where the first two levels are the class and subclass, the second two are the family and subfamily, and the last one is the transport system. Superfamilies that contain multiple families are included as hyperlinks to the five tier TC hierarchy. TCDB includes proteins from all types of living organisms and is the only transporter classification system that is both universal and recognized by the International Union of Biochemistry and Molecular Biology. It has been expanded by manual curation, contains extensive text descriptions providing structural, functional, mechanistic and evolutionary information, is supported by unique software and is interconnected to many other relevant databases. TCDB is of increasing usefulness to the international scientific community and can serve as a model for the expansion of database technologies. This manuscript describes an update of the database descriptions previously featured in NAR database issues.",poster,cp93
Biology,p1813,d5,85331cdfdae93103b0c86bcb12bb1b47158ced08,c93,ASE BigData & SocialInformatics,The Transporter Classification Database,"The Transporter Classification Database (TCDB; http://www.tcdb.org) serves as a common reference point for transport protein research. The database contains more than 10 000 non-redundant proteins that represent all currently recognized families of transmembrane molecular transport systems. Proteins in TCDB are organized in a five level hierarchical system, where the first two levels are the class and subclass, the second two are the family and subfamily, and the last one is the transport system. Superfamilies that contain multiple families are included as hyperlinks to the five tier TC hierarchy. TCDB includes proteins from all types of living organisms and is the only transporter classification system that is both universal and recognized by the International Union of Biochemistry and Molecular Biology. It has been expanded by manual curation, contains extensive text descriptions providing structural, functional, mechanistic and evolutionary information, is supported by unique software and is interconnected to many other relevant databases. TCDB is of increasing usefulness to the international scientific community and can serve as a model for the expansion of database technologies. This manuscript describes an update of the database descriptions previously featured in NAR database issues.",poster,cp93
Medicine,p1815,d1,20f5782a5fed99979ae406849d7d11bd59314996,j193,Nature Genetics,dbEST — database for “expressed sequence tags”,Abstract,fullPaper,jv193
Biology,p1815,d5,20f5782a5fed99979ae406849d7d11bd59314996,j193,Nature Genetics,dbEST — database for “expressed sequence tags”,Abstract,fullPaper,jv193
Medicine,p1819,d1,d769ca2ac7b7057df74fdf9d4d0de91f1b07917d,c101,Interspeech,ARDB—Antibiotic Resistance Genes Database,"The treatment of infections is increasingly compromised by the ability of bacteria to develop resistance to antibiotics through mutations or through the acquisition of resistance genes. Antibiotic resistance genes also have the potential to be used for bio-terror purposes through genetically modified organisms. In order to facilitate the identification and characterization of these genes, we have created a manually curated database—the Antibiotic Resistance Genes Database (ARDB)—unifying most of the publicly available information on antibiotic resistance. Each gene and resistance type is annotated with rich information, including resistance profile, mechanism of action, ontology, COG and CDD annotations, as well as external links to sequence and protein databases. Our database also supports sequence similarity searches and implements an initial version of a tool for characterizing common mutations that confer antibiotic resistance. The information we provide can be used as compendium of antibiotic resistance factors as well as to identify the resistance genes of newly sequenced genes, genomes, or metagenomes. Currently, ARDB contains resistance information for 13 293 genes, 377 types, 257 antibiotics, 632 genomes, 933 species and 124 genera. ARDB is available at http://ardb.cbcb.umd.edu/.",poster,cp101
Computer Science,p1819,d3,d769ca2ac7b7057df74fdf9d4d0de91f1b07917d,c101,Interspeech,ARDB—Antibiotic Resistance Genes Database,"The treatment of infections is increasingly compromised by the ability of bacteria to develop resistance to antibiotics through mutations or through the acquisition of resistance genes. Antibiotic resistance genes also have the potential to be used for bio-terror purposes through genetically modified organisms. In order to facilitate the identification and characterization of these genes, we have created a manually curated database—the Antibiotic Resistance Genes Database (ARDB)—unifying most of the publicly available information on antibiotic resistance. Each gene and resistance type is annotated with rich information, including resistance profile, mechanism of action, ontology, COG and CDD annotations, as well as external links to sequence and protein databases. Our database also supports sequence similarity searches and implements an initial version of a tool for characterizing common mutations that confer antibiotic resistance. The information we provide can be used as compendium of antibiotic resistance factors as well as to identify the resistance genes of newly sequenced genes, genomes, or metagenomes. Currently, ARDB contains resistance information for 13 293 genes, 377 types, 257 antibiotics, 632 genomes, 933 species and 124 genera. ARDB is available at http://ardb.cbcb.umd.edu/.",poster,cp101
Biology,p1819,d5,d769ca2ac7b7057df74fdf9d4d0de91f1b07917d,c101,Interspeech,ARDB—Antibiotic Resistance Genes Database,"The treatment of infections is increasingly compromised by the ability of bacteria to develop resistance to antibiotics through mutations or through the acquisition of resistance genes. Antibiotic resistance genes also have the potential to be used for bio-terror purposes through genetically modified organisms. In order to facilitate the identification and characterization of these genes, we have created a manually curated database—the Antibiotic Resistance Genes Database (ARDB)—unifying most of the publicly available information on antibiotic resistance. Each gene and resistance type is annotated with rich information, including resistance profile, mechanism of action, ontology, COG and CDD annotations, as well as external links to sequence and protein databases. Our database also supports sequence similarity searches and implements an initial version of a tool for characterizing common mutations that confer antibiotic resistance. The information we provide can be used as compendium of antibiotic resistance factors as well as to identify the resistance genes of newly sequenced genes, genomes, or metagenomes. Currently, ARDB contains resistance information for 13 293 genes, 377 types, 257 antibiotics, 632 genomes, 933 species and 124 genera. ARDB is available at http://ardb.cbcb.umd.edu/.",poster,cp101
Medicine,p1820,d1,151ec57b35f6a7431fdce934a57ae15451079d85,c61,"International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems",NCBI’s Database of Genotypes and Phenotypes: dbGaP,"The Database of Genotypes and Phenotypes (dbGap, http://www.ncbi.nlm.nih.gov/gap) is a National Institutes of Health-sponsored repository charged to archive, curate and distribute information produced by studies investigating the interaction of genotype and phenotype. Information in dbGaP is organized as a hierarchical structure and includes the accessioned objects, phenotypes (as variables and datasets), various molecular assay data (SNP and Expression Array data, Sequence and Epigenomic marks), analyses and documents. Publicly accessible metadata about submitted studies, summary level data, and documents related to studies can be accessed freely on the dbGaP website. Individual-level data are accessible via Controlled Access application to scientists across the globe.",poster,cp61
Computer Science,p1820,d3,151ec57b35f6a7431fdce934a57ae15451079d85,c61,"International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems",NCBI’s Database of Genotypes and Phenotypes: dbGaP,"The Database of Genotypes and Phenotypes (dbGap, http://www.ncbi.nlm.nih.gov/gap) is a National Institutes of Health-sponsored repository charged to archive, curate and distribute information produced by studies investigating the interaction of genotype and phenotype. Information in dbGaP is organized as a hierarchical structure and includes the accessioned objects, phenotypes (as variables and datasets), various molecular assay data (SNP and Expression Array data, Sequence and Epigenomic marks), analyses and documents. Publicly accessible metadata about submitted studies, summary level data, and documents related to studies can be accessed freely on the dbGaP website. Individual-level data are accessible via Controlled Access application to scientists across the globe.",poster,cp61
Biology,p1820,d5,151ec57b35f6a7431fdce934a57ae15451079d85,c61,"International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems",NCBI’s Database of Genotypes and Phenotypes: dbGaP,"The Database of Genotypes and Phenotypes (dbGap, http://www.ncbi.nlm.nih.gov/gap) is a National Institutes of Health-sponsored repository charged to archive, curate and distribute information produced by studies investigating the interaction of genotype and phenotype. Information in dbGaP is organized as a hierarchical structure and includes the accessioned objects, phenotypes (as variables and datasets), various molecular assay data (SNP and Expression Array data, Sequence and Epigenomic marks), analyses and documents. Publicly accessible metadata about submitted studies, summary level data, and documents related to studies can be accessed freely on the dbGaP website. Individual-level data are accessible via Controlled Access application to scientists across the globe.",poster,cp61
Medicine,p1821,d1,11fdda41735869a5962b698e9d4fc6524ee96d4c,j391,Journal of Applied Crystallography,Crystallography Open Database – an open-access collection of crystal structures,"The Crystallography Open Database (COD) is an ongoing initiative by crystallographers to gather all published inorganic, metal–organic and small organic molecule structures in one database, providing a straightforward search and retrieval interface. The COD adopts an open-access model for its >80 000 structure files.",fullPaper,jv391
Computer Science,p1821,d3,11fdda41735869a5962b698e9d4fc6524ee96d4c,j391,Journal of Applied Crystallography,Crystallography Open Database – an open-access collection of crystal structures,"The Crystallography Open Database (COD) is an ongoing initiative by crystallographers to gather all published inorganic, metal–organic and small organic molecule structures in one database, providing a straightforward search and retrieval interface. The COD adopts an open-access model for its >80 000 structure files.",fullPaper,jv391
Medicine,p1823,d1,9ebe338e49e63ff97348aca0db521ac3ff01bcef,c50,Conference on Emerging Network Experiment and Technology,CDD: specific functional annotation with the Conserved Domain Database,"NCBI's Conserved Domain Database (CDD) is a collection of multiple sequence alignments and derived database search models, which represent protein domains conserved in molecular evolution. The collection can be accessed at http://www.ncbi.nlm.nih.gov/Structure/cdd/cdd.shtml, and is also part of NCBI's Entrez query and retrieval system, cross-linked to numerous other resources. CDD provides annotation of domain footprints and conserved functional sites on protein sequences. Precalculated domain annotation can be retrieved for protein sequences tracked in NCBI's Entrez system, and CDD's collection of models can be queried with novel protein sequences via the CD-Search service at http://www.ncbi.nlm.nih.gov/Structure/cdd/wrpsb.cgi. Starting with the latest version of CDD, v2.14, information from redundant and homologous domain models is summarized at a superfamily level, and domain annotation on proteins is flagged as either ‘specific’ (identifying molecular function with high confidence) or as ‘non-specific’ (identifying superfamily membership only).",poster,cp50
Computer Science,p1823,d3,9ebe338e49e63ff97348aca0db521ac3ff01bcef,c50,Conference on Emerging Network Experiment and Technology,CDD: specific functional annotation with the Conserved Domain Database,"NCBI's Conserved Domain Database (CDD) is a collection of multiple sequence alignments and derived database search models, which represent protein domains conserved in molecular evolution. The collection can be accessed at http://www.ncbi.nlm.nih.gov/Structure/cdd/cdd.shtml, and is also part of NCBI's Entrez query and retrieval system, cross-linked to numerous other resources. CDD provides annotation of domain footprints and conserved functional sites on protein sequences. Precalculated domain annotation can be retrieved for protein sequences tracked in NCBI's Entrez system, and CDD's collection of models can be queried with novel protein sequences via the CD-Search service at http://www.ncbi.nlm.nih.gov/Structure/cdd/wrpsb.cgi. Starting with the latest version of CDD, v2.14, information from redundant and homologous domain models is summarized at a superfamily level, and domain annotation on proteins is flagged as either ‘specific’ (identifying molecular function with high confidence) or as ‘non-specific’ (identifying superfamily membership only).",poster,cp50
Biology,p1823,d5,9ebe338e49e63ff97348aca0db521ac3ff01bcef,c50,Conference on Emerging Network Experiment and Technology,CDD: specific functional annotation with the Conserved Domain Database,"NCBI's Conserved Domain Database (CDD) is a collection of multiple sequence alignments and derived database search models, which represent protein domains conserved in molecular evolution. The collection can be accessed at http://www.ncbi.nlm.nih.gov/Structure/cdd/cdd.shtml, and is also part of NCBI's Entrez query and retrieval system, cross-linked to numerous other resources. CDD provides annotation of domain footprints and conserved functional sites on protein sequences. Precalculated domain annotation can be retrieved for protein sequences tracked in NCBI's Entrez system, and CDD's collection of models can be queried with novel protein sequences via the CD-Search service at http://www.ncbi.nlm.nih.gov/Structure/cdd/wrpsb.cgi. Starting with the latest version of CDD, v2.14, information from redundant and homologous domain models is summarized at a superfamily level, and domain annotation on proteins is flagged as either ‘specific’ (identifying molecular function with high confidence) or as ‘non-specific’ (identifying superfamily membership only).",poster,cp50
Medicine,p1824,d1,6348c3490aa17188229decfd09b67e5169ce1cfc,c15,Pacific Symposium on Biocomputing,Database indexing for production MegaBLAST searches,"Motivation: The BLAST software package for sequence comparison speeds up homology search by preprocessing a query sequence into a lookup table. Numerous research studies have suggested that preprocessing the database instead would give better performance. However, production usage of sequence comparison methods that preprocess the database has been limited to programs such as BLAT and SSAHA that are designed to find matches when query and database subsequences are highly similar. Results: We developed a new version of the MegaBLAST module of BLAST that does the initial phase of finding short seeds for matches by searching a database index. We also developed a program makembindex that preprocesses the database into a data structure for rapid seed searching. We show that the new ‘indexed MegaBLAST’ is faster than the ‘non-indexed’ version for most practical uses. We show that indexed MegaBLAST is faster than miBLAST, another implementation of BLAST nucleotide searching with a preprocessed database, for most of the 200 queries we tested. To deploy indexed MegaBLAST as part of NCBI'sWeb BLAST service, the storage of databases and the queueing mechanism were modified, so that some machines are now dedicated to serving queries for a specific database. The response time for such Web queries is now faster than it was when each computer handled queries for multiple databases. Availability: The code for indexed MegaBLAST is part of the blastn program in the NCBI C++ toolkit. The preprocessor program makembindex is also in the toolkit. Indexed MegaBLAST has been used in production on NCBI's Web BLAST service to search one version of the human and mouse genomes since October 2007. The Linux command-line executables for blastn and makembindex, documentation, and some query sets used to carry out the tests described below are available in the directory: ftp://ftp.ncbi.nlm.nih.gov/pub/agarwala/indexed_megablast Contact: schaffer@helix.nih.gov Supplementary information: Supplementary data are available at Bioinformatics online.",poster,cp15
Computer Science,p1824,d3,6348c3490aa17188229decfd09b67e5169ce1cfc,c15,Pacific Symposium on Biocomputing,Database indexing for production MegaBLAST searches,"Motivation: The BLAST software package for sequence comparison speeds up homology search by preprocessing a query sequence into a lookup table. Numerous research studies have suggested that preprocessing the database instead would give better performance. However, production usage of sequence comparison methods that preprocess the database has been limited to programs such as BLAT and SSAHA that are designed to find matches when query and database subsequences are highly similar. Results: We developed a new version of the MegaBLAST module of BLAST that does the initial phase of finding short seeds for matches by searching a database index. We also developed a program makembindex that preprocesses the database into a data structure for rapid seed searching. We show that the new ‘indexed MegaBLAST’ is faster than the ‘non-indexed’ version for most practical uses. We show that indexed MegaBLAST is faster than miBLAST, another implementation of BLAST nucleotide searching with a preprocessed database, for most of the 200 queries we tested. To deploy indexed MegaBLAST as part of NCBI'sWeb BLAST service, the storage of databases and the queueing mechanism were modified, so that some machines are now dedicated to serving queries for a specific database. The response time for such Web queries is now faster than it was when each computer handled queries for multiple databases. Availability: The code for indexed MegaBLAST is part of the blastn program in the NCBI C++ toolkit. The preprocessor program makembindex is also in the toolkit. Indexed MegaBLAST has been used in production on NCBI's Web BLAST service to search one version of the human and mouse genomes since October 2007. The Linux command-line executables for blastn and makembindex, documentation, and some query sets used to carry out the tests described below are available in the directory: ftp://ftp.ncbi.nlm.nih.gov/pub/agarwala/indexed_megablast Contact: schaffer@helix.nih.gov Supplementary information: Supplementary data are available at Bioinformatics online.",poster,cp15
Medicine,p1825,d1,69114cb58049c3b06ce012cf41d493d2e947b6c8,c49,ACM/SIGCOMM Internet Measurement Conference,DBatVir: the database of bat-associated viruses,"Emerging infectious diseases remain a significant threat to public health. Most emerging infectious disease agents in humans are of zoonotic origin. Bats are important reservoir hosts of many highly lethal zoonotic viruses and have been implicated in numerous emerging infectious disease events in recent years. It is essential to enhance our knowledge and understanding of the genetic diversity of the bat-associated viruses to prevent future outbreaks. To facilitate further research, we constructed the database of bat-associated viruses (DBatVir). Known viral sequences detected in bat samples were manually collected and curated, along with the related metadata, such as the sampling time, location, bat species and specimen type. Additional information concerning the bats, including common names, diet type, geographic distribution and phylogeny were integrated into the database to bridge the gap between virologists and zoologists. The database currently covers >4100 bat-associated animal viruses of 23 viral families detected from 196 bat species in 69 countries worldwide. It provides an overview and snapshot of the current research regarding bat-associated viruses, which is essential now that the field is rapidly expanding. With a user-friendly interface and integrated online bioinformatics tools, DBatVir provides a convenient and powerful platform for virologists and zoologists to analyze the virome diversity of bats, as well as for epidemiologists and public health researchers to monitor and track current and future bat-related infectious diseases. Database URL: http://www.mgc.ac.cn/DBatVir/",poster,cp49
Computer Science,p1825,d3,69114cb58049c3b06ce012cf41d493d2e947b6c8,c49,ACM/SIGCOMM Internet Measurement Conference,DBatVir: the database of bat-associated viruses,"Emerging infectious diseases remain a significant threat to public health. Most emerging infectious disease agents in humans are of zoonotic origin. Bats are important reservoir hosts of many highly lethal zoonotic viruses and have been implicated in numerous emerging infectious disease events in recent years. It is essential to enhance our knowledge and understanding of the genetic diversity of the bat-associated viruses to prevent future outbreaks. To facilitate further research, we constructed the database of bat-associated viruses (DBatVir). Known viral sequences detected in bat samples were manually collected and curated, along with the related metadata, such as the sampling time, location, bat species and specimen type. Additional information concerning the bats, including common names, diet type, geographic distribution and phylogeny were integrated into the database to bridge the gap between virologists and zoologists. The database currently covers >4100 bat-associated animal viruses of 23 viral families detected from 196 bat species in 69 countries worldwide. It provides an overview and snapshot of the current research regarding bat-associated viruses, which is essential now that the field is rapidly expanding. With a user-friendly interface and integrated online bioinformatics tools, DBatVir provides a convenient and powerful platform for virologists and zoologists to analyze the virome diversity of bats, as well as for epidemiologists and public health researchers to monitor and track current and future bat-related infectious diseases. Database URL: http://www.mgc.ac.cn/DBatVir/",poster,cp49
Biology,p1825,d5,69114cb58049c3b06ce012cf41d493d2e947b6c8,c49,ACM/SIGCOMM Internet Measurement Conference,DBatVir: the database of bat-associated viruses,"Emerging infectious diseases remain a significant threat to public health. Most emerging infectious disease agents in humans are of zoonotic origin. Bats are important reservoir hosts of many highly lethal zoonotic viruses and have been implicated in numerous emerging infectious disease events in recent years. It is essential to enhance our knowledge and understanding of the genetic diversity of the bat-associated viruses to prevent future outbreaks. To facilitate further research, we constructed the database of bat-associated viruses (DBatVir). Known viral sequences detected in bat samples were manually collected and curated, along with the related metadata, such as the sampling time, location, bat species and specimen type. Additional information concerning the bats, including common names, diet type, geographic distribution and phylogeny were integrated into the database to bridge the gap between virologists and zoologists. The database currently covers >4100 bat-associated animal viruses of 23 viral families detected from 196 bat species in 69 countries worldwide. It provides an overview and snapshot of the current research regarding bat-associated viruses, which is essential now that the field is rapidly expanding. With a user-friendly interface and integrated online bioinformatics tools, DBatVir provides a convenient and powerful platform for virologists and zoologists to analyze the virome diversity of bats, as well as for epidemiologists and public health researchers to monitor and track current and future bat-related infectious diseases. Database URL: http://www.mgc.ac.cn/DBatVir/",poster,cp49
Medicine,p1827,d1,aef87d005e8e3d58f0a0577a7be4d55a10c2d5b3,j107,Nucleic Acids Research,MODOMICS: a database of RNA modification pathways. 2008 update,"MODOMICS, a database devoted to the systems biology of RNA modification, has been subjected to substantial improvements. It provides comprehensive information on the chemical structure of modified nucleosides, pathways of their biosynthesis, sequences of RNAs containing these modifications and RNA-modifying enzymes. MODOMICS also provides cross-references to other databases and to literature. In addition to the previously available manually curated tRNA sequences from a few model organisms, we have now included additional tRNAs and rRNAs, and all RNAs with 3D structures in the Nucleic Acid Database, in which modified nucleosides are present. In total, 3460 modified bases in RNA sequences of different organisms have been annotated. New RNA-modifying enzymes have been also added. The current collection of enzymes includes mainly proteins for the model organisms Escherichia coli and Saccharomyces cerevisiae, and is currently being expanded to include proteins from other organisms, in particular Archaea and Homo sapiens. For enzymes with known structures, links are provided to the corresponding Protein Data Bank entries, while for many others homology models have been created. Many new options for database searching and querying have been included. MODOMICS can be accessed at http://genesilico.pl/modomics.",fullPaper,jv107
Biology,p1827,d5,aef87d005e8e3d58f0a0577a7be4d55a10c2d5b3,j107,Nucleic Acids Research,MODOMICS: a database of RNA modification pathways. 2008 update,"MODOMICS, a database devoted to the systems biology of RNA modification, has been subjected to substantial improvements. It provides comprehensive information on the chemical structure of modified nucleosides, pathways of their biosynthesis, sequences of RNAs containing these modifications and RNA-modifying enzymes. MODOMICS also provides cross-references to other databases and to literature. In addition to the previously available manually curated tRNA sequences from a few model organisms, we have now included additional tRNAs and rRNAs, and all RNAs with 3D structures in the Nucleic Acid Database, in which modified nucleosides are present. In total, 3460 modified bases in RNA sequences of different organisms have been annotated. New RNA-modifying enzymes have been also added. The current collection of enzymes includes mainly proteins for the model organisms Escherichia coli and Saccharomyces cerevisiae, and is currently being expanded to include proteins from other organisms, in particular Archaea and Homo sapiens. For enzymes with known structures, links are provided to the corresponding Protein Data Bank entries, while for many others homology models have been created. Many new options for database searching and querying have been included. MODOMICS can be accessed at http://genesilico.pl/modomics.",fullPaper,jv107
Medicine,p1828,d1,ff89306dcc77b387f01718b497df0116c87c260d,c101,Interspeech,The IMGT/HLA database,"It is 10 years since the IMGT/HLA database was released, providing the HLA community with a searchable repository of highly curated HLA sequences. The HLA complex is located within the 6p21.3 region of human chromosome 6 and contains more than 220 genes of diverse function. Many of the genes encode proteins of the immune system and are highly polymorphic. The naming of these HLA genes and alleles, and their quality control is the responsibility of the WHO Nomenclature Committee for Factors of the HLA System. Through the work of the HLA Informatics Group and in collaboration with the European Bioinformatics Institute, we are able to provide public access to this data through the website http://www.ebi.ac.uk/imgt/hla/. The first release contained 964 sequences, the most recent release 3300 sequences, with around 450 new sequences been added each year. The tools provided on the website have been updated to allow more complex alignments, which include genomic sequence data, as well as the development of tools for probe and primer design and the inclusion of data from the HLA Dictionary. Regular updates to the website ensure that new and confirmatory sequences are dispersed to the HLA community, and the wider research and clinical communities.",poster,cp101
Computer Science,p1828,d3,ff89306dcc77b387f01718b497df0116c87c260d,c101,Interspeech,The IMGT/HLA database,"It is 10 years since the IMGT/HLA database was released, providing the HLA community with a searchable repository of highly curated HLA sequences. The HLA complex is located within the 6p21.3 region of human chromosome 6 and contains more than 220 genes of diverse function. Many of the genes encode proteins of the immune system and are highly polymorphic. The naming of these HLA genes and alleles, and their quality control is the responsibility of the WHO Nomenclature Committee for Factors of the HLA System. Through the work of the HLA Informatics Group and in collaboration with the European Bioinformatics Institute, we are able to provide public access to this data through the website http://www.ebi.ac.uk/imgt/hla/. The first release contained 964 sequences, the most recent release 3300 sequences, with around 450 new sequences been added each year. The tools provided on the website have been updated to allow more complex alignments, which include genomic sequence data, as well as the development of tools for probe and primer design and the inclusion of data from the HLA Dictionary. Regular updates to the website ensure that new and confirmatory sequences are dispersed to the HLA community, and the wider research and clinical communities.",poster,cp101
Biology,p1828,d5,ff89306dcc77b387f01718b497df0116c87c260d,c101,Interspeech,The IMGT/HLA database,"It is 10 years since the IMGT/HLA database was released, providing the HLA community with a searchable repository of highly curated HLA sequences. The HLA complex is located within the 6p21.3 region of human chromosome 6 and contains more than 220 genes of diverse function. Many of the genes encode proteins of the immune system and are highly polymorphic. The naming of these HLA genes and alleles, and their quality control is the responsibility of the WHO Nomenclature Committee for Factors of the HLA System. Through the work of the HLA Informatics Group and in collaboration with the European Bioinformatics Institute, we are able to provide public access to this data through the website http://www.ebi.ac.uk/imgt/hla/. The first release contained 964 sequences, the most recent release 3300 sequences, with around 450 new sequences been added each year. The tools provided on the website have been updated to allow more complex alignments, which include genomic sequence data, as well as the development of tools for probe and primer design and the inclusion of data from the HLA Dictionary. Regular updates to the website ensure that new and confirmatory sequences are dispersed to the HLA community, and the wider research and clinical communities.",poster,cp101
Medicine,p1829,d1,fefea2b9ed93a0c3163432c52a67cf34efa868f7,c25,IEEE International Parallel and Distributed Processing Symposium,SAbDab: the structural antibody database,"Structural antibody database (SAbDab; http://opig.stats.ox.ac.uk/webapps/sabdab) is an online resource containing all the publicly available antibody structures annotated and presented in a consistent fashion. The data are annotated with several properties including experimental information, gene details, correct heavy and light chain pairings, antigen details and, where available, antibody–antigen binding affinity. The user can select structures, according to these attributes as well as structural properties such as complementarity determining region loop conformation and variable domain orientation. Individual structures, datasets and the complete database can be downloaded.",poster,cp25
Computer Science,p1829,d3,fefea2b9ed93a0c3163432c52a67cf34efa868f7,c25,IEEE International Parallel and Distributed Processing Symposium,SAbDab: the structural antibody database,"Structural antibody database (SAbDab; http://opig.stats.ox.ac.uk/webapps/sabdab) is an online resource containing all the publicly available antibody structures annotated and presented in a consistent fashion. The data are annotated with several properties including experimental information, gene details, correct heavy and light chain pairings, antigen details and, where available, antibody–antigen binding affinity. The user can select structures, according to these attributes as well as structural properties such as complementarity determining region loop conformation and variable domain orientation. Individual structures, datasets and the complete database can be downloaded.",poster,cp25
Biology,p1829,d5,fefea2b9ed93a0c3163432c52a67cf34efa868f7,c25,IEEE International Parallel and Distributed Processing Symposium,SAbDab: the structural antibody database,"Structural antibody database (SAbDab; http://opig.stats.ox.ac.uk/webapps/sabdab) is an online resource containing all the publicly available antibody structures annotated and presented in a consistent fashion. The data are annotated with several properties including experimental information, gene details, correct heavy and light chain pairings, antigen details and, where available, antibody–antigen binding affinity. The user can select structures, according to these attributes as well as structural properties such as complementarity determining region loop conformation and variable domain orientation. Individual structures, datasets and the complete database can be downloaded.",poster,cp25
Medicine,p1830,d1,0bd83f3dc2bf04bc592ae05112adf30882db1196,j392,Genome Medicine,The Human Gene Mutation Database: 2008 update,Abstract,fullPaper,jv392
Biology,p1830,d5,0bd83f3dc2bf04bc592ae05112adf30882db1196,j392,Genome Medicine,The Human Gene Mutation Database: 2008 update,Abstract,fullPaper,jv392
Medicine,p1832,d1,462098bcc81f5d8b8a067aa0b8988adba0eef91f,c120,SIGSAND-Europe Symposium,BioNumbers—the database of key numbers in molecular and cell biology,"BioNumbers (http://www.bionumbers.hms.harvard.edu) is a database of key numbers in molecular and cell biology—the quantitative properties of biological systems of interest to computational, systems and molecular cell biologists. Contents of the database range from cell sizes to metabolite concentrations, from reaction rates to generation times, from genome sizes to the number of mitochondria in a cell. While always of importance to biologists, having numbers in hand is becoming increasingly critical for experimenting, modeling, and analyzing biological systems. BioNumbers was motivated by an appreciation of how long it can take to find even the simplest number in the vast biological literature. All numbers are taken directly from a literature source and that reference is provided with the number. BioNumbers is designed to be highly searchable and queries can be performed by keywords or browsed by menus. BioNumbers is a collaborative community platform where registered users can add content and make comments on existing data. All new entries and commentary are curated to maintain high quality. Here we describe the database characteristics and implementation, demonstrate its use, and discuss future directions for its development.",poster,cp120
Computer Science,p1832,d3,462098bcc81f5d8b8a067aa0b8988adba0eef91f,c120,SIGSAND-Europe Symposium,BioNumbers—the database of key numbers in molecular and cell biology,"BioNumbers (http://www.bionumbers.hms.harvard.edu) is a database of key numbers in molecular and cell biology—the quantitative properties of biological systems of interest to computational, systems and molecular cell biologists. Contents of the database range from cell sizes to metabolite concentrations, from reaction rates to generation times, from genome sizes to the number of mitochondria in a cell. While always of importance to biologists, having numbers in hand is becoming increasingly critical for experimenting, modeling, and analyzing biological systems. BioNumbers was motivated by an appreciation of how long it can take to find even the simplest number in the vast biological literature. All numbers are taken directly from a literature source and that reference is provided with the number. BioNumbers is designed to be highly searchable and queries can be performed by keywords or browsed by menus. BioNumbers is a collaborative community platform where registered users can add content and make comments on existing data. All new entries and commentary are curated to maintain high quality. Here we describe the database characteristics and implementation, demonstrate its use, and discuss future directions for its development.",poster,cp120
Biology,p1832,d5,462098bcc81f5d8b8a067aa0b8988adba0eef91f,c120,SIGSAND-Europe Symposium,BioNumbers—the database of key numbers in molecular and cell biology,"BioNumbers (http://www.bionumbers.hms.harvard.edu) is a database of key numbers in molecular and cell biology—the quantitative properties of biological systems of interest to computational, systems and molecular cell biologists. Contents of the database range from cell sizes to metabolite concentrations, from reaction rates to generation times, from genome sizes to the number of mitochondria in a cell. While always of importance to biologists, having numbers in hand is becoming increasingly critical for experimenting, modeling, and analyzing biological systems. BioNumbers was motivated by an appreciation of how long it can take to find even the simplest number in the vast biological literature. All numbers are taken directly from a literature source and that reference is provided with the number. BioNumbers is designed to be highly searchable and queries can be performed by keywords or browsed by menus. BioNumbers is a collaborative community platform where registered users can add content and make comments on existing data. All new entries and commentary are curated to maintain high quality. Here we describe the database characteristics and implementation, demonstrate its use, and discuss future directions for its development.",poster,cp120
Medicine,p1833,d1,e3f9ed5a6c1c2045adfb5f66845338d9352149d3,c73,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,OPM: Orientations of Proteins in Membranes database,"SUMMARY
The Orientations of Proteins in Membranes (OPM) database provides a collection of transmembrane, monotopic and peripheral proteins from the Protein Data Bank whose spatial arrangements in the lipid bilayer have been calculated theoretically and compared with experimental data. The database allows analysis, sorting and searching of membrane proteins based on their structural classification, species, destination membrane, numbers of transmembrane segments and subunits, numbers of secondary structures and the calculated hydrophobic thickness or tilt angle with respect to the bilayer normal. All coordinate files with the calculated membrane boundaries are available for downloading.


AVAILABILITY
http://opm.phar.umich.edu.",poster,cp73
Computer Science,p1833,d3,e3f9ed5a6c1c2045adfb5f66845338d9352149d3,c73,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,OPM: Orientations of Proteins in Membranes database,"SUMMARY
The Orientations of Proteins in Membranes (OPM) database provides a collection of transmembrane, monotopic and peripheral proteins from the Protein Data Bank whose spatial arrangements in the lipid bilayer have been calculated theoretically and compared with experimental data. The database allows analysis, sorting and searching of membrane proteins based on their structural classification, species, destination membrane, numbers of transmembrane segments and subunits, numbers of secondary structures and the calculated hydrophobic thickness or tilt angle with respect to the bilayer normal. All coordinate files with the calculated membrane boundaries are available for downloading.


AVAILABILITY
http://opm.phar.umich.edu.",poster,cp73
Biology,p1833,d5,e3f9ed5a6c1c2045adfb5f66845338d9352149d3,c73,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,OPM: Orientations of Proteins in Membranes database,"SUMMARY
The Orientations of Proteins in Membranes (OPM) database provides a collection of transmembrane, monotopic and peripheral proteins from the Protein Data Bank whose spatial arrangements in the lipid bilayer have been calculated theoretically and compared with experimental data. The database allows analysis, sorting and searching of membrane proteins based on their structural classification, species, destination membrane, numbers of transmembrane segments and subunits, numbers of secondary structures and the calculated hydrophobic thickness or tilt angle with respect to the bilayer normal. All coordinate files with the calculated membrane boundaries are available for downloading.


AVAILABILITY
http://opm.phar.umich.edu.",poster,cp73
Medicine,p1835,d1,fd05e825c5d076b17626996a78bdff5e7752549d,c9,Big Data,ChEBI: a database and ontology for chemical entities of biological interest,"Chemical Entities of Biological Interest (ChEBI) is a freely available dictionary of molecular entities focused on ‘small’ chemical compounds. The molecular entities in question are either natural products or synthetic products used to intervene in the processes of living organisms. Genome-encoded macromolecules (nucleic acids, proteins and peptides derived from proteins by cleavage) are not as a rule included in ChEBI. In addition to molecular entities, ChEBI contains groups (parts of molecular entities) and classes of entities. ChEBI includes an ontological classification, whereby the relationships between molecular entities or classes of entities and their parents and/or children are specified. ChEBI is available online at http://www.ebi.ac.uk/chebi/",poster,cp9
Computer Science,p1835,d3,fd05e825c5d076b17626996a78bdff5e7752549d,c9,Big Data,ChEBI: a database and ontology for chemical entities of biological interest,"Chemical Entities of Biological Interest (ChEBI) is a freely available dictionary of molecular entities focused on ‘small’ chemical compounds. The molecular entities in question are either natural products or synthetic products used to intervene in the processes of living organisms. Genome-encoded macromolecules (nucleic acids, proteins and peptides derived from proteins by cleavage) are not as a rule included in ChEBI. In addition to molecular entities, ChEBI contains groups (parts of molecular entities) and classes of entities. ChEBI includes an ontological classification, whereby the relationships between molecular entities or classes of entities and their parents and/or children are specified. ChEBI is available online at http://www.ebi.ac.uk/chebi/",poster,cp9
Biology,p1835,d5,fd05e825c5d076b17626996a78bdff5e7752549d,c9,Big Data,ChEBI: a database and ontology for chemical entities of biological interest,"Chemical Entities of Biological Interest (ChEBI) is a freely available dictionary of molecular entities focused on ‘small’ chemical compounds. The molecular entities in question are either natural products or synthetic products used to intervene in the processes of living organisms. Genome-encoded macromolecules (nucleic acids, proteins and peptides derived from proteins by cleavage) are not as a rule included in ChEBI. In addition to molecular entities, ChEBI contains groups (parts of molecular entities) and classes of entities. ChEBI includes an ontological classification, whereby the relationships between molecular entities or classes of entities and their parents and/or children are specified. ChEBI is available online at http://www.ebi.ac.uk/chebi/",poster,cp9
Medicine,p1836,d1,d03cab6781985ad3f62aec52048a7e15ee2dee61,c62,International Conference on Advanced Data and Information Engineering,LMSD: LIPID MAPS structure database,"The LIPID MAPS Structure Database (LMSD) is a relational database encompassing structures and annotations of biologically relevant lipids. Structures of lipids in the database come from four sources: (i) LIPID MAPS Consortium's core laboratories and partners; (ii) lipids identified by LIPID MAPS experiments; (iii) computationally generated structures for appropriate lipid classes; (iv) biologically relevant lipids manually curated from LIPID BANK, LIPIDAT and other public sources. All the lipid structures in LMSD are drawn in a consistent fashion. In addition to a classification-based retrieval of lipids, users can search LMSD using either text-based or structure-based search options. The text-based search implementation supports data retrieval by any combination of these data fields: LIPID MAPS ID, systematic or common name, mass, formula, category, main class, and subclass data fields. The structure-based search, in conjunction with optional data fields, provides the capability to perform a substructure search or exact match for the structure drawn by the user. Search results, in addition to structure and annotations, also include relevant links to external databases. The LMSD is publicly available at",poster,cp62
Computer Science,p1836,d3,d03cab6781985ad3f62aec52048a7e15ee2dee61,c62,International Conference on Advanced Data and Information Engineering,LMSD: LIPID MAPS structure database,"The LIPID MAPS Structure Database (LMSD) is a relational database encompassing structures and annotations of biologically relevant lipids. Structures of lipids in the database come from four sources: (i) LIPID MAPS Consortium's core laboratories and partners; (ii) lipids identified by LIPID MAPS experiments; (iii) computationally generated structures for appropriate lipid classes; (iv) biologically relevant lipids manually curated from LIPID BANK, LIPIDAT and other public sources. All the lipid structures in LMSD are drawn in a consistent fashion. In addition to a classification-based retrieval of lipids, users can search LMSD using either text-based or structure-based search options. The text-based search implementation supports data retrieval by any combination of these data fields: LIPID MAPS ID, systematic or common name, mass, formula, category, main class, and subclass data fields. The structure-based search, in conjunction with optional data fields, provides the capability to perform a substructure search or exact match for the structure drawn by the user. Search results, in addition to structure and annotations, also include relevant links to external databases. The LMSD is publicly available at",poster,cp62
Biology,p1836,d5,d03cab6781985ad3f62aec52048a7e15ee2dee61,c62,International Conference on Advanced Data and Information Engineering,LMSD: LIPID MAPS structure database,"The LIPID MAPS Structure Database (LMSD) is a relational database encompassing structures and annotations of biologically relevant lipids. Structures of lipids in the database come from four sources: (i) LIPID MAPS Consortium's core laboratories and partners; (ii) lipids identified by LIPID MAPS experiments; (iii) computationally generated structures for appropriate lipid classes; (iv) biologically relevant lipids manually curated from LIPID BANK, LIPIDAT and other public sources. All the lipid structures in LMSD are drawn in a consistent fashion. In addition to a classification-based retrieval of lipids, users can search LMSD using either text-based or structure-based search options. The text-based search implementation supports data retrieval by any combination of these data fields: LIPID MAPS ID, systematic or common name, mass, formula, category, main class, and subclass data fields. The structure-based search, in conjunction with optional data fields, provides the capability to perform a substructure search or exact match for the structure drawn by the user. Search results, in addition to structure and annotations, also include relevant links to external databases. The LMSD is publicly available at",poster,cp62
Medicine,p1839,d1,100f4767f087e858976013b7117f38d26bb32a66,j351,Analytical Chemistry,Method to correlate tandem mass spectra of modified peptides to amino acid sequences in the protein database.,"A method to correlate uninterpreted tandem mass spectra of modified peptides, produced under low-energy (10-50 eV) collision conditions, with amino acid sequences in a protein database has been developed. The fragmentation patterns observed in the tandem mass spectra of peptides containing covalent modifications is used to directly search and fit linear amino acid sequences in the database. Specific information relevant to sites of modification is not contained in the character-based sequence information of the databases. The search method considers each putative modification site as both modified and unmodified in one pass through the database and simultaneously considers up to three different sites of modification. The search method will identify the correct sequence if the tandem mass spectrum did not represent a modified peptide. This approach is demonstrated with peptides containing modifications such as S-carboxymethylated cysteine, oxidized methionine, phosphoserine, phosphothreonine, or phosphotyrosine. In addition, a scanning approach is used in which neutral loss scans are used to initiate the acquisition of product ion MS/MS spectra of doubly charged phosphorylated peptides during a single chromatographic run for data analysis with the database-searching algorithm. The approach described in this paper provides a convenient method to match the nascent tandem mass spectra of modified peptides to sequences in a protein database and thereby identify previously unknown sites of modification.",fullPaper,jv351
Chemistry,p1839,d8,100f4767f087e858976013b7117f38d26bb32a66,j351,Analytical Chemistry,Method to correlate tandem mass spectra of modified peptides to amino acid sequences in the protein database.,"A method to correlate uninterpreted tandem mass spectra of modified peptides, produced under low-energy (10-50 eV) collision conditions, with amino acid sequences in a protein database has been developed. The fragmentation patterns observed in the tandem mass spectra of peptides containing covalent modifications is used to directly search and fit linear amino acid sequences in the database. Specific information relevant to sites of modification is not contained in the character-based sequence information of the databases. The search method considers each putative modification site as both modified and unmodified in one pass through the database and simultaneously considers up to three different sites of modification. The search method will identify the correct sequence if the tandem mass spectrum did not represent a modified peptide. This approach is demonstrated with peptides containing modifications such as S-carboxymethylated cysteine, oxidized methionine, phosphoserine, phosphothreonine, or phosphotyrosine. In addition, a scanning approach is used in which neutral loss scans are used to initiate the acquisition of product ion MS/MS spectra of doubly charged phosphorylated peptides during a single chromatographic run for data analysis with the database-searching algorithm. The approach described in this paper provides a convenient method to match the nascent tandem mass spectra of modified peptides to sequences in a protein database and thereby identify previously unknown sites of modification.",fullPaper,jv351
Medicine,p1840,d1,7b472238215ac399e119ef152c0ff93f2df1c8e6,c6,Annual Conference on Genetic and Evolutionary Computation,Rfam: updates to the RNA families database,"Rfam is a collection of RNA sequence families, represented by multiple sequence alignments and covariance models (CMs). The primary aim of Rfam is to annotate new members of known RNA families on nucleotide sequences, particularly complete genomes, using sensitive BLAST filters in combination with CMs. A minority of families with a very broad taxonomic range (e.g. tRNA and rRNA) provide the majority of the sequence annotations, whilst the majority of Rfam families (e.g. snoRNAs and miRNAs) have a limited taxonomic range and provide a limited number of annotations. Recent improvements to the website, methodologies and data used by Rfam are discussed. Rfam is freely available on the Web at http://rfam.sanger.ac.uk/and http://rfam.janelia.org/.",poster,cp6
Computer Science,p1840,d3,7b472238215ac399e119ef152c0ff93f2df1c8e6,c6,Annual Conference on Genetic and Evolutionary Computation,Rfam: updates to the RNA families database,"Rfam is a collection of RNA sequence families, represented by multiple sequence alignments and covariance models (CMs). The primary aim of Rfam is to annotate new members of known RNA families on nucleotide sequences, particularly complete genomes, using sensitive BLAST filters in combination with CMs. A minority of families with a very broad taxonomic range (e.g. tRNA and rRNA) provide the majority of the sequence annotations, whilst the majority of Rfam families (e.g. snoRNAs and miRNAs) have a limited taxonomic range and provide a limited number of annotations. Recent improvements to the website, methodologies and data used by Rfam are discussed. Rfam is freely available on the Web at http://rfam.sanger.ac.uk/and http://rfam.janelia.org/.",poster,cp6
Biology,p1840,d5,7b472238215ac399e119ef152c0ff93f2df1c8e6,c6,Annual Conference on Genetic and Evolutionary Computation,Rfam: updates to the RNA families database,"Rfam is a collection of RNA sequence families, represented by multiple sequence alignments and covariance models (CMs). The primary aim of Rfam is to annotate new members of known RNA families on nucleotide sequences, particularly complete genomes, using sensitive BLAST filters in combination with CMs. A minority of families with a very broad taxonomic range (e.g. tRNA and rRNA) provide the majority of the sequence annotations, whilst the majority of Rfam families (e.g. snoRNAs and miRNAs) have a limited taxonomic range and provide a limited number of annotations. Recent improvements to the website, methodologies and data used by Rfam are discussed. Rfam is freely available on the Web at http://rfam.sanger.ac.uk/and http://rfam.janelia.org/.",poster,cp6
Medicine,p1841,d1,240faf3bbdea0673f5bd6e3668e4de1de905ceee,c70,Annual Meeting of the Association for Computational Linguistics,"PROSITE, a protein domain database for functional characterization and annotation","PROSITE consists of documentation entries describing protein domains, families and functional sites, as well as associated patterns and profiles to identify them. It is complemented by ProRule, a collection of rules based on profiles and patterns, which increases the discriminatory power of these profiles and patterns by providing additional information about functionally and/or structurally critical amino acids. PROSITE is largely used for the annotation of domain features of UniProtKB/Swiss-Prot entries. Among the 983 (DNA-binding) domains, repeats and zinc fingers present in Swiss-Prot (release 57.8 of 22 September 2009), 696 (∼70%) are annotated with PROSITE descriptors using information from ProRule. In order to allow better functional characterization of domains, PROSITE developments focus on subfamily specific profiles and a new profile building method giving more weight to functionally important residues. Here, we describe AMSA, an annotated multiple sequence alignment format used to build a new generation of generalized profiles, the migration of ScanProsite to Vital-IT, a cluster of 633 CPUs, and the adoption of the Distributed Annotation System (DAS) to facilitate PROSITE data integration and interchange with other sources. The latest version of PROSITE (release 20.54, of 22 September 2009) contains 1308 patterns, 863 profiles and 869 ProRules. PROSITE is accessible at: http://www.expasy.org/prosite/.",poster,cp70
Computer Science,p1841,d3,240faf3bbdea0673f5bd6e3668e4de1de905ceee,c70,Annual Meeting of the Association for Computational Linguistics,"PROSITE, a protein domain database for functional characterization and annotation","PROSITE consists of documentation entries describing protein domains, families and functional sites, as well as associated patterns and profiles to identify them. It is complemented by ProRule, a collection of rules based on profiles and patterns, which increases the discriminatory power of these profiles and patterns by providing additional information about functionally and/or structurally critical amino acids. PROSITE is largely used for the annotation of domain features of UniProtKB/Swiss-Prot entries. Among the 983 (DNA-binding) domains, repeats and zinc fingers present in Swiss-Prot (release 57.8 of 22 September 2009), 696 (∼70%) are annotated with PROSITE descriptors using information from ProRule. In order to allow better functional characterization of domains, PROSITE developments focus on subfamily specific profiles and a new profile building method giving more weight to functionally important residues. Here, we describe AMSA, an annotated multiple sequence alignment format used to build a new generation of generalized profiles, the migration of ScanProsite to Vital-IT, a cluster of 633 CPUs, and the adoption of the Distributed Annotation System (DAS) to facilitate PROSITE data integration and interchange with other sources. The latest version of PROSITE (release 20.54, of 22 September 2009) contains 1308 patterns, 863 profiles and 869 ProRules. PROSITE is accessible at: http://www.expasy.org/prosite/.",poster,cp70
Biology,p1841,d5,240faf3bbdea0673f5bd6e3668e4de1de905ceee,c70,Annual Meeting of the Association for Computational Linguistics,"PROSITE, a protein domain database for functional characterization and annotation","PROSITE consists of documentation entries describing protein domains, families and functional sites, as well as associated patterns and profiles to identify them. It is complemented by ProRule, a collection of rules based on profiles and patterns, which increases the discriminatory power of these profiles and patterns by providing additional information about functionally and/or structurally critical amino acids. PROSITE is largely used for the annotation of domain features of UniProtKB/Swiss-Prot entries. Among the 983 (DNA-binding) domains, repeats and zinc fingers present in Swiss-Prot (release 57.8 of 22 September 2009), 696 (∼70%) are annotated with PROSITE descriptors using information from ProRule. In order to allow better functional characterization of domains, PROSITE developments focus on subfamily specific profiles and a new profile building method giving more weight to functionally important residues. Here, we describe AMSA, an annotated multiple sequence alignment format used to build a new generation of generalized profiles, the migration of ScanProsite to Vital-IT, a cluster of 633 CPUs, and the adoption of the Distributed Annotation System (DAS) to facilitate PROSITE data integration and interchange with other sources. The latest version of PROSITE (release 20.54, of 22 September 2009) contains 1308 patterns, 863 profiles and 869 ProRules. PROSITE is accessible at: http://www.expasy.org/prosite/.",poster,cp70
Medicine,p1845,d1,269a6271fa98bbdc2d456bae7fb419a77c88dc70,c62,International Conference on Advanced Data and Information Engineering,D2P2: database of disordered protein predictions,"We present the Database of Disordered Protein Prediction (D2P2), available at http://d2p2.pro (including website source code). A battery of disorder predictors and their variants, VL-XT, VSL2b, PrDOS, PV2, Espritz and IUPred, were run on all protein sequences from 1765 complete proteomes (to be updated as more genomes are completed). Integrated with these results are all of the predicted (mostly structured) SCOP domains using the SUPERFAMILY predictor. These disorder/structure annotations together enable comparison of the disorder predictors with each other and examination of the overlap between disordered predictions and SCOP domains on a large scale. D2P2 will increase our understanding of the interplay between disorder and structure, the genomic distribution of disorder, and its evolutionary history. The parsed data are made available in a unified format for download as flat files or SQL tables either by genome, by predictor, or for the complete set. An interactive website provides a graphical view of each protein annotated with the SCOP domains and disordered regions from all predictors overlaid (or shown as a consensus). There are statistics and tools for browsing and comparing genomes and their disorder within the context of their position on the tree of life.",poster,cp62
Computer Science,p1845,d3,269a6271fa98bbdc2d456bae7fb419a77c88dc70,c62,International Conference on Advanced Data and Information Engineering,D2P2: database of disordered protein predictions,"We present the Database of Disordered Protein Prediction (D2P2), available at http://d2p2.pro (including website source code). A battery of disorder predictors and their variants, VL-XT, VSL2b, PrDOS, PV2, Espritz and IUPred, were run on all protein sequences from 1765 complete proteomes (to be updated as more genomes are completed). Integrated with these results are all of the predicted (mostly structured) SCOP domains using the SUPERFAMILY predictor. These disorder/structure annotations together enable comparison of the disorder predictors with each other and examination of the overlap between disordered predictions and SCOP domains on a large scale. D2P2 will increase our understanding of the interplay between disorder and structure, the genomic distribution of disorder, and its evolutionary history. The parsed data are made available in a unified format for download as flat files or SQL tables either by genome, by predictor, or for the complete set. An interactive website provides a graphical view of each protein annotated with the SCOP domains and disordered regions from all predictors overlaid (or shown as a consensus). There are statistics and tools for browsing and comparing genomes and their disorder within the context of their position on the tree of life.",poster,cp62
Biology,p1845,d5,269a6271fa98bbdc2d456bae7fb419a77c88dc70,c62,International Conference on Advanced Data and Information Engineering,D2P2: database of disordered protein predictions,"We present the Database of Disordered Protein Prediction (D2P2), available at http://d2p2.pro (including website source code). A battery of disorder predictors and their variants, VL-XT, VSL2b, PrDOS, PV2, Espritz and IUPred, were run on all protein sequences from 1765 complete proteomes (to be updated as more genomes are completed). Integrated with these results are all of the predicted (mostly structured) SCOP domains using the SUPERFAMILY predictor. These disorder/structure annotations together enable comparison of the disorder predictors with each other and examination of the overlap between disordered predictions and SCOP domains on a large scale. D2P2 will increase our understanding of the interplay between disorder and structure, the genomic distribution of disorder, and its evolutionary history. The parsed data are made available in a unified format for download as flat files or SQL tables either by genome, by predictor, or for the complete set. An interactive website provides a graphical view of each protein annotated with the SCOP domains and disordered regions from all predictors overlaid (or shown as a consensus). There are statistics and tools for browsing and comparing genomes and their disorder within the context of their position on the tree of life.",poster,cp62
Medicine,p1846,d1,a03d8f591bc3c2dbdecbd9d515e0469953a3f7ef,j193,Nature Genetics,The NCBI dbGaP database of genotypes and phenotypes,Abstract,fullPaper,jv193
Biology,p1846,d5,a03d8f591bc3c2dbdecbd9d515e0469953a3f7ef,j193,Nature Genetics,The NCBI dbGaP database of genotypes and phenotypes,Abstract,fullPaper,jv193
Medicine,p1850,d1,977fe5853db16e320917a43fb00f334456625a1e,c95,Cyber ..,DIP: the Database of Interacting Proteins,"The Database of Interacting Proteins (DIP; http://dip.doe-mbi.ucla.edu) is a database that documents experimentally determined protein-protein interactions. This database is intended to provide the scientific community with a comprehensive and integrated tool for browsing and efficiently extracting information about protein interactions and interaction networks in biological processes. Beyond cataloging details of protein-protein interactions, the DIP is useful for understanding protein function and protein-protein relationships, studying the properties of networks of interacting proteins, benchmarking predictions of protein-protein interactions, and studying the evolution of protein-protein interactions.",poster,cp95
Computer Science,p1850,d3,977fe5853db16e320917a43fb00f334456625a1e,c95,Cyber ..,DIP: the Database of Interacting Proteins,"The Database of Interacting Proteins (DIP; http://dip.doe-mbi.ucla.edu) is a database that documents experimentally determined protein-protein interactions. This database is intended to provide the scientific community with a comprehensive and integrated tool for browsing and efficiently extracting information about protein interactions and interaction networks in biological processes. Beyond cataloging details of protein-protein interactions, the DIP is useful for understanding protein function and protein-protein relationships, studying the properties of networks of interacting proteins, benchmarking predictions of protein-protein interactions, and studying the evolution of protein-protein interactions.",poster,cp95
Biology,p1850,d5,977fe5853db16e320917a43fb00f334456625a1e,c95,Cyber ..,DIP: the Database of Interacting Proteins,"The Database of Interacting Proteins (DIP; http://dip.doe-mbi.ucla.edu) is a database that documents experimentally determined protein-protein interactions. This database is intended to provide the scientific community with a comprehensive and integrated tool for browsing and efficiently extracting information about protein interactions and interaction networks in biological processes. Beyond cataloging details of protein-protein interactions, the DIP is useful for understanding protein function and protein-protein relationships, studying the properties of networks of interacting proteins, benchmarking predictions of protein-protein interactions, and studying the evolution of protein-protein interactions.",poster,cp95
Medicine,p1852,d1,a0883d134b5abb7928483eb0859832a66a51fbf9,c84,EUROCON Conference,The PROSITE database,"The PROSITE database consists of a large collection of biologically meaningful signatures that are described as patterns or profiles. Each signature is linked to a documentation that provides useful biological information on the protein family, domain or functional site identified by the signature. The PROSITE database is now complemented by a series of rules that can give more precise information about specific residues. During the last 2 years, the documentation and the ScanProsite web pages were redesigned to add more functionalities. The latest version of PROSITE (release 19.11 of September 27, 2005) contains 1329 patterns and 552 profile entries. Over the past 2 years more than 200 domains have been added, and now 52% of UniProtKB/Swiss-Prot entries (release 48.1 of September 27, 2005) have a cross-reference to a PROSITE entry. The database is accessible at .",poster,cp84
Computer Science,p1852,d3,a0883d134b5abb7928483eb0859832a66a51fbf9,c84,EUROCON Conference,The PROSITE database,"The PROSITE database consists of a large collection of biologically meaningful signatures that are described as patterns or profiles. Each signature is linked to a documentation that provides useful biological information on the protein family, domain or functional site identified by the signature. The PROSITE database is now complemented by a series of rules that can give more precise information about specific residues. During the last 2 years, the documentation and the ScanProsite web pages were redesigned to add more functionalities. The latest version of PROSITE (release 19.11 of September 27, 2005) contains 1329 patterns and 552 profile entries. Over the past 2 years more than 200 domains have been added, and now 52% of UniProtKB/Swiss-Prot entries (release 48.1 of September 27, 2005) have a cross-reference to a PROSITE entry. The database is accessible at .",poster,cp84
Biology,p1852,d5,a0883d134b5abb7928483eb0859832a66a51fbf9,c84,EUROCON Conference,The PROSITE database,"The PROSITE database consists of a large collection of biologically meaningful signatures that are described as patterns or profiles. Each signature is linked to a documentation that provides useful biological information on the protein family, domain or functional site identified by the signature. The PROSITE database is now complemented by a series of rules that can give more precise information about specific residues. During the last 2 years, the documentation and the ScanProsite web pages were redesigned to add more functionalities. The latest version of PROSITE (release 19.11 of September 27, 2005) contains 1329 patterns and 552 profile entries. Over the past 2 years more than 200 domains have been added, and now 52% of UniProtKB/Swiss-Prot entries (release 48.1 of September 27, 2005) have a cross-reference to a PROSITE entry. The database is accessible at .",poster,cp84
Medicine,p1857,d1,db45667093e4fa4f95bc402c10b460052119717f,c31,Information Security Solutions Europe,mVOC: a database of microbial volatiles,"Scents are well known to be emitted from flowers and animals. In nature, these volatiles are responsible for inter- and intra-organismic communication, e.g. attraction and defence. Consequently, they influence and improve the establishment of organisms and populations in ecological niches by acting as single compounds or in mixtures. Despite the known wealth of volatile organic compounds (VOCs) from species of the plant and animal kingdom, in the past, less attention has been focused on volatiles of microorganisms. Although fast and affordable sequencing methods facilitate the detection of microbial diseases, however, the analysis of signature or fingerprint volatiles will be faster and easier. Microbial VOCs (mVOCs) are presently used as marker to detect human diseases, food spoilage or moulds in houses. Furthermore, mVOCs exhibited antagonistic potential against pathogens in vitro, but their biological roles in the ecosystems remain to be investigated. Information on volatile emission from bacteria and fungi is presently scattered in the literature, and no public and up-to-date collection on mVOCs is available. To address this need, we have developed mVOC, a database available online at http://bioinformatics.charite.de/mvoc.",poster,cp31
Computer Science,p1857,d3,db45667093e4fa4f95bc402c10b460052119717f,c31,Information Security Solutions Europe,mVOC: a database of microbial volatiles,"Scents are well known to be emitted from flowers and animals. In nature, these volatiles are responsible for inter- and intra-organismic communication, e.g. attraction and defence. Consequently, they influence and improve the establishment of organisms and populations in ecological niches by acting as single compounds or in mixtures. Despite the known wealth of volatile organic compounds (VOCs) from species of the plant and animal kingdom, in the past, less attention has been focused on volatiles of microorganisms. Although fast and affordable sequencing methods facilitate the detection of microbial diseases, however, the analysis of signature or fingerprint volatiles will be faster and easier. Microbial VOCs (mVOCs) are presently used as marker to detect human diseases, food spoilage or moulds in houses. Furthermore, mVOCs exhibited antagonistic potential against pathogens in vitro, but their biological roles in the ecosystems remain to be investigated. Information on volatile emission from bacteria and fungi is presently scattered in the literature, and no public and up-to-date collection on mVOCs is available. To address this need, we have developed mVOC, a database available online at http://bioinformatics.charite.de/mvoc.",poster,cp31
Biology,p1857,d5,db45667093e4fa4f95bc402c10b460052119717f,c31,Information Security Solutions Europe,mVOC: a database of microbial volatiles,"Scents are well known to be emitted from flowers and animals. In nature, these volatiles are responsible for inter- and intra-organismic communication, e.g. attraction and defence. Consequently, they influence and improve the establishment of organisms and populations in ecological niches by acting as single compounds or in mixtures. Despite the known wealth of volatile organic compounds (VOCs) from species of the plant and animal kingdom, in the past, less attention has been focused on volatiles of microorganisms. Although fast and affordable sequencing methods facilitate the detection of microbial diseases, however, the analysis of signature or fingerprint volatiles will be faster and easier. Microbial VOCs (mVOCs) are presently used as marker to detect human diseases, food spoilage or moulds in houses. Furthermore, mVOCs exhibited antagonistic potential against pathogens in vitro, but their biological roles in the ecosystems remain to be investigated. Information on volatile emission from bacteria and fungi is presently scattered in the literature, and no public and up-to-date collection on mVOCs is available. To address this need, we have developed mVOC, a database available online at http://bioinformatics.charite.de/mvoc.",poster,cp31
Medicine,p1858,d1,02e72b05d309cbc7652666c72a2dbb0bc68cd9ea,c110,Biometrics and Identity Management,Immune epitope database analysis resource,"The immune epitope database analysis resource (IEDB-AR: http://tools.iedb.org) is a collection of tools for prediction and analysis of molecular targets of T- and B-cell immune responses (i.e. epitopes). Since its last publication in the NAR webserver issue in 2008, a new generation of peptide:MHC binding and T-cell epitope predictive tools have been added. As validated by different labs and in the first international competition for predicting peptide:MHC-I binding, their predictive performances have improved considerably. In addition, a new B-cell epitope prediction tool was added, and the homology mapping tool was updated to enable mapping of discontinuous epitopes onto 3D structures. Furthermore, to serve a wider range of users, the number of ways in which IEDB-AR can be accessed has been expanded. Specifically, the predictive tools can be programmatically accessed using a web interface and can also be downloaded as software packages.",poster,cp110
Computer Science,p1858,d3,02e72b05d309cbc7652666c72a2dbb0bc68cd9ea,c110,Biometrics and Identity Management,Immune epitope database analysis resource,"The immune epitope database analysis resource (IEDB-AR: http://tools.iedb.org) is a collection of tools for prediction and analysis of molecular targets of T- and B-cell immune responses (i.e. epitopes). Since its last publication in the NAR webserver issue in 2008, a new generation of peptide:MHC binding and T-cell epitope predictive tools have been added. As validated by different labs and in the first international competition for predicting peptide:MHC-I binding, their predictive performances have improved considerably. In addition, a new B-cell epitope prediction tool was added, and the homology mapping tool was updated to enable mapping of discontinuous epitopes onto 3D structures. Furthermore, to serve a wider range of users, the number of ways in which IEDB-AR can be accessed has been expanded. Specifically, the predictive tools can be programmatically accessed using a web interface and can also be downloaded as software packages.",poster,cp110
Biology,p1858,d5,02e72b05d309cbc7652666c72a2dbb0bc68cd9ea,c110,Biometrics and Identity Management,Immune epitope database analysis resource,"The immune epitope database analysis resource (IEDB-AR: http://tools.iedb.org) is a collection of tools for prediction and analysis of molecular targets of T- and B-cell immune responses (i.e. epitopes). Since its last publication in the NAR webserver issue in 2008, a new generation of peptide:MHC binding and T-cell epitope predictive tools have been added. As validated by different labs and in the first international competition for predicting peptide:MHC-I binding, their predictive performances have improved considerably. In addition, a new B-cell epitope prediction tool was added, and the homology mapping tool was updated to enable mapping of discontinuous epitopes onto 3D structures. Furthermore, to serve a wider range of users, the number of ways in which IEDB-AR can be accessed has been expanded. Specifically, the predictive tools can be programmatically accessed using a web interface and can also be downloaded as software packages.",poster,cp110
Medicine,p1859,d1,0758a501039f9e2dfb7607507f9734155c52c7fc,c82,Symposium on Networked Systems Design and Implementation,The Comparative Toxicogenomics Database: update 2013,"The Comparative Toxicogenomics Database (CTD; http://ctdbase.org/) provides information about interactions between environmental chemicals and gene products and their relationships to diseases. Chemical–gene, chemical–disease and gene–disease interactions manually curated from the literature are integrated to generate expanded networks and predict many novel associations between different data types. CTD now contains over 15 million toxicogenomic relationships. To navigate this sea of data, we added several new features, including DiseaseComps (which finds comparable diseases that share toxicogenomic profiles), statistical scoring for inferred gene–disease and pathway–chemical relationships, filtering options for several tools to refine user analysis and our new Gene Set Enricher (which provides biological annotations that are enriched for gene sets). To improve data visualization, we added a Cytoscape Web view to our ChemComps feature, included color-coded interactions and created a ‘slim list’ for our MEDIC disease vocabulary (allowing diseases to be grouped for meta-analysis, visualization and better data management). CTD continues to promote interoperability with external databases by providing content and cross-links to their sites. Together, this wealth of expanded chemical–gene–disease data, combined with novel ways to analyze and view content, continues to help users generate testable hypotheses about the molecular mechanisms of environmental diseases.",poster,cp82
Computer Science,p1859,d3,0758a501039f9e2dfb7607507f9734155c52c7fc,c82,Symposium on Networked Systems Design and Implementation,The Comparative Toxicogenomics Database: update 2013,"The Comparative Toxicogenomics Database (CTD; http://ctdbase.org/) provides information about interactions between environmental chemicals and gene products and their relationships to diseases. Chemical–gene, chemical–disease and gene–disease interactions manually curated from the literature are integrated to generate expanded networks and predict many novel associations between different data types. CTD now contains over 15 million toxicogenomic relationships. To navigate this sea of data, we added several new features, including DiseaseComps (which finds comparable diseases that share toxicogenomic profiles), statistical scoring for inferred gene–disease and pathway–chemical relationships, filtering options for several tools to refine user analysis and our new Gene Set Enricher (which provides biological annotations that are enriched for gene sets). To improve data visualization, we added a Cytoscape Web view to our ChemComps feature, included color-coded interactions and created a ‘slim list’ for our MEDIC disease vocabulary (allowing diseases to be grouped for meta-analysis, visualization and better data management). CTD continues to promote interoperability with external databases by providing content and cross-links to their sites. Together, this wealth of expanded chemical–gene–disease data, combined with novel ways to analyze and view content, continues to help users generate testable hypotheses about the molecular mechanisms of environmental diseases.",poster,cp82
Biology,p1859,d5,0758a501039f9e2dfb7607507f9734155c52c7fc,c82,Symposium on Networked Systems Design and Implementation,The Comparative Toxicogenomics Database: update 2013,"The Comparative Toxicogenomics Database (CTD; http://ctdbase.org/) provides information about interactions between environmental chemicals and gene products and their relationships to diseases. Chemical–gene, chemical–disease and gene–disease interactions manually curated from the literature are integrated to generate expanded networks and predict many novel associations between different data types. CTD now contains over 15 million toxicogenomic relationships. To navigate this sea of data, we added several new features, including DiseaseComps (which finds comparable diseases that share toxicogenomic profiles), statistical scoring for inferred gene–disease and pathway–chemical relationships, filtering options for several tools to refine user analysis and our new Gene Set Enricher (which provides biological annotations that are enriched for gene sets). To improve data visualization, we added a Cytoscape Web view to our ChemComps feature, included color-coded interactions and created a ‘slim list’ for our MEDIC disease vocabulary (allowing diseases to be grouped for meta-analysis, visualization and better data management). CTD continues to promote interoperability with external databases by providing content and cross-links to their sites. Together, this wealth of expanded chemical–gene–disease data, combined with novel ways to analyze and view content, continues to help users generate testable hypotheses about the molecular mechanisms of environmental diseases.",poster,cp82
Medicine,p1861,d1,47e7dc1724b5a3c12154c134f898c58ca4e9c49c,c21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,SGD: Saccharomyces Genome Database,"The Saccharomyces Genome Database (SGD) provides Internet access to the complete Saccharomyces cerevisiae genomic sequence, its genes and their products, the phenotypes of its mutants, and the literature supporting these data. The amount of information and the number of features provided by SGD have increased greatly following the release of the S.cerevisiae genomic sequence, which is currently the only complete sequence of a eukaryotic genome. SGD aids researchers by providing not only basic information, but also tools such as sequence similarity searching that lead to detailed information about features of the genome and relationships between genes. SGD presents information using a variety of user-friendly, dynamically created graphical displays illustrating physical, genetic and sequence feature maps. SGD can be accessed via the World Wide Web at http://genome-www.stanford.edu/Saccharomyces/",poster,cp21
Computer Science,p1861,d3,47e7dc1724b5a3c12154c134f898c58ca4e9c49c,c21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,SGD: Saccharomyces Genome Database,"The Saccharomyces Genome Database (SGD) provides Internet access to the complete Saccharomyces cerevisiae genomic sequence, its genes and their products, the phenotypes of its mutants, and the literature supporting these data. The amount of information and the number of features provided by SGD have increased greatly following the release of the S.cerevisiae genomic sequence, which is currently the only complete sequence of a eukaryotic genome. SGD aids researchers by providing not only basic information, but also tools such as sequence similarity searching that lead to detailed information about features of the genome and relationships between genes. SGD presents information using a variety of user-friendly, dynamically created graphical displays illustrating physical, genetic and sequence feature maps. SGD can be accessed via the World Wide Web at http://genome-www.stanford.edu/Saccharomyces/",poster,cp21
Biology,p1861,d5,47e7dc1724b5a3c12154c134f898c58ca4e9c49c,c21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,SGD: Saccharomyces Genome Database,"The Saccharomyces Genome Database (SGD) provides Internet access to the complete Saccharomyces cerevisiae genomic sequence, its genes and their products, the phenotypes of its mutants, and the literature supporting these data. The amount of information and the number of features provided by SGD have increased greatly following the release of the S.cerevisiae genomic sequence, which is currently the only complete sequence of a eukaryotic genome. SGD aids researchers by providing not only basic information, but also tools such as sequence similarity searching that lead to detailed information about features of the genome and relationships between genes. SGD presents information using a variety of user-friendly, dynamically created graphical displays illustrating physical, genetic and sequence feature maps. SGD can be accessed via the World Wide Web at http://genome-www.stanford.edu/Saccharomyces/",poster,cp21
Medicine,p1862,d1,8a61d0e598d1a1d47fa4f744081cd39255a3f508,j135,PLoS ONE,TCM Database@Taiwan: The World's Largest Traditional Chinese Medicine Database for Drug Screening In Silico,"Rapid advancing computational technologies have greatly speeded up the development of computer-aided drug design (CADD). Recently, pharmaceutical companies have increasingly shifted their attentions toward traditional Chinese medicine (TCM) for novel lead compounds. Despite the growing number of studies on TCM, there is no free 3D small molecular structure database of TCM available for virtual screening or molecular simulation. To address this shortcoming, we have constructed TCM Database@Taiwan (http://tcm.cmu.edu.tw/) based on information collected from Chinese medical texts and scientific publications. TCM Database@Taiwan is currently the world's largest non-commercial TCM database. This web-based database contains more than 20,000 pure compounds isolated from 453 TCM ingredients. Both cdx (2D) and Tripos mol2 (3D) formats of each pure compound in the database are available for download and virtual screening. The TCM database includes both simple and advanced web-based query options that can specify search clauses, such as molecular properties, substructures, TCM ingredients, and TCM classification, based on intended drug actions. The TCM database can be easily accessed by all researchers conducting CADD. Over the last eight years, numerous volunteers have devoted their time to analyze TCM ingredients from Chinese medical texts as well as to construct structure files for each isolated compound. We believe that TCM Database@Taiwan will be a milestone on the path towards modernizing traditional Chinese medicine.",fullPaper,jv135
Medicine,p1863,d1,755ef09cc0f7593b792482edc5bf799138243acf,j113,Behavior Research Methods,The Geneva affective picture database (GAPED): a new 730-picture database focusing on valence and normative significance,Abstract,fullPaper,jv113
Psychology,p1863,d10,755ef09cc0f7593b792482edc5bf799138243acf,j113,Behavior Research Methods,The Geneva affective picture database (GAPED): a new 730-picture database focusing on valence and normative significance,Abstract,fullPaper,jv113
Medicine,p1864,d1,ff9186e43abd68e55fbcb9ba992944c7497bacab,j394,Trends in Genetics,Repbase update: a database and an electronic journal of repetitive elements.,Abstract,fullPaper,jv394
Biology,p1864,d5,ff9186e43abd68e55fbcb9ba992944c7497bacab,j394,Trends in Genetics,Repbase update: a database and an electronic journal of repetitive elements.,Abstract,fullPaper,jv394
Medicine,p1866,d1,999db6b11c1fe6377118081c84f79f6ae6b4262d,c57,IEEE International Geoscience and Remote Sensing Symposium,"MEROPS: the database of proteolytic enzymes, their substrates and inhibitors","Peptidases, their substrates and inhibitors are of great relevance to biology, medicine and biotechnology. The MEROPS database (http://merops.sanger.ac.uk) aims to fulfil the need for an integrated source of information about these. The database has hierarchical classifications in which homologous sets of peptidases and protein inhibitors are grouped into protein species, which are grouped into families, which are in turn grouped into clans. The database has been expanded to include proteolytic enzymes other than peptidases. Special identifiers for peptidases from a variety of model organisms have been established so that orthologues can be detected in other species. A table of predicted active-site residue and metal ligand positions and the residue ranges of the peptidase domains in orthologues has been added to each peptidase summary. New displays of tertiary structures, which can be rotated or have the surfaces displayed, have been added to the structure pages. New indexes for gene names and peptidase substrates have been made available. Among the enhancements to existing features are the inclusion of small-molecule inhibitors in the tables of peptidase–inhibitor interactions, a table of known cleavage sites for each protein substrate, and tables showing the substrate-binding preferences of peptidases derived from combinatorial peptide substrate libraries.",poster,cp57
Computer Science,p1866,d3,999db6b11c1fe6377118081c84f79f6ae6b4262d,c57,IEEE International Geoscience and Remote Sensing Symposium,"MEROPS: the database of proteolytic enzymes, their substrates and inhibitors","Peptidases, their substrates and inhibitors are of great relevance to biology, medicine and biotechnology. The MEROPS database (http://merops.sanger.ac.uk) aims to fulfil the need for an integrated source of information about these. The database has hierarchical classifications in which homologous sets of peptidases and protein inhibitors are grouped into protein species, which are grouped into families, which are in turn grouped into clans. The database has been expanded to include proteolytic enzymes other than peptidases. Special identifiers for peptidases from a variety of model organisms have been established so that orthologues can be detected in other species. A table of predicted active-site residue and metal ligand positions and the residue ranges of the peptidase domains in orthologues has been added to each peptidase summary. New displays of tertiary structures, which can be rotated or have the surfaces displayed, have been added to the structure pages. New indexes for gene names and peptidase substrates have been made available. Among the enhancements to existing features are the inclusion of small-molecule inhibitors in the tables of peptidase–inhibitor interactions, a table of known cleavage sites for each protein substrate, and tables showing the substrate-binding preferences of peptidases derived from combinatorial peptide substrate libraries.",poster,cp57
Biology,p1866,d5,999db6b11c1fe6377118081c84f79f6ae6b4262d,c57,IEEE International Geoscience and Remote Sensing Symposium,"MEROPS: the database of proteolytic enzymes, their substrates and inhibitors","Peptidases, their substrates and inhibitors are of great relevance to biology, medicine and biotechnology. The MEROPS database (http://merops.sanger.ac.uk) aims to fulfil the need for an integrated source of information about these. The database has hierarchical classifications in which homologous sets of peptidases and protein inhibitors are grouped into protein species, which are grouped into families, which are in turn grouped into clans. The database has been expanded to include proteolytic enzymes other than peptidases. Special identifiers for peptidases from a variety of model organisms have been established so that orthologues can be detected in other species. A table of predicted active-site residue and metal ligand positions and the residue ranges of the peptidase domains in orthologues has been added to each peptidase summary. New displays of tertiary structures, which can be rotated or have the surfaces displayed, have been added to the structure pages. New indexes for gene names and peptidase substrates have been made available. Among the enhancements to existing features are the inclusion of small-molecule inhibitors in the tables of peptidase–inhibitor interactions, a table of known cleavage sites for each protein substrate, and tables showing the substrate-binding preferences of peptidases derived from combinatorial peptide substrate libraries.",poster,cp57
Medicine,p1867,d1,638f10c6cc396907b98424621f6420a4287d342f,c51,International Conference on Engineering Education,rrndb: the Ribosomal RNA Operon Copy Number Database,"The Ribosomal RNA Operon Copy Number Database (rrndb) is an Internet-accessible database containing annotated information on rRNA operon copy number among prokaryotes. Gene redundancy is uncommon in prokaryotic genomes, yet the rRNA genes can vary from one to as many as 15 copies. Despite the widespread use of 16S rRNA gene sequences for identification of prokaryotes, information on the number and sequence of individual rRNA genes in a genome is not readily accessible. In an attempt to understand the evolutionary implications of rRNA operon redundancy, we have created a phylogenetically arranged report on rRNA gene copy number for a diverse collection of prokaryotic microorganisms. Each entry (organism) in the rrndb contains detailed information linked directly to external websites including the Ribosomal Database Project, GenBank, PubMed and several culture collections. Data contained in the rrndb will be valuable to researchers investigating microbial ecology and evolution using 16S rRNA gene sequences. The rrndb web site is directly accessible on the WWW at http://rrndb.cme. msu.edu.",poster,cp51
Computer Science,p1867,d3,638f10c6cc396907b98424621f6420a4287d342f,c51,International Conference on Engineering Education,rrndb: the Ribosomal RNA Operon Copy Number Database,"The Ribosomal RNA Operon Copy Number Database (rrndb) is an Internet-accessible database containing annotated information on rRNA operon copy number among prokaryotes. Gene redundancy is uncommon in prokaryotic genomes, yet the rRNA genes can vary from one to as many as 15 copies. Despite the widespread use of 16S rRNA gene sequences for identification of prokaryotes, information on the number and sequence of individual rRNA genes in a genome is not readily accessible. In an attempt to understand the evolutionary implications of rRNA operon redundancy, we have created a phylogenetically arranged report on rRNA gene copy number for a diverse collection of prokaryotic microorganisms. Each entry (organism) in the rrndb contains detailed information linked directly to external websites including the Ribosomal Database Project, GenBank, PubMed and several culture collections. Data contained in the rrndb will be valuable to researchers investigating microbial ecology and evolution using 16S rRNA gene sequences. The rrndb web site is directly accessible on the WWW at http://rrndb.cme. msu.edu.",poster,cp51
Biology,p1867,d5,638f10c6cc396907b98424621f6420a4287d342f,c51,International Conference on Engineering Education,rrndb: the Ribosomal RNA Operon Copy Number Database,"The Ribosomal RNA Operon Copy Number Database (rrndb) is an Internet-accessible database containing annotated information on rRNA operon copy number among prokaryotes. Gene redundancy is uncommon in prokaryotic genomes, yet the rRNA genes can vary from one to as many as 15 copies. Despite the widespread use of 16S rRNA gene sequences for identification of prokaryotes, information on the number and sequence of individual rRNA genes in a genome is not readily accessible. In an attempt to understand the evolutionary implications of rRNA operon redundancy, we have created a phylogenetically arranged report on rRNA gene copy number for a diverse collection of prokaryotic microorganisms. Each entry (organism) in the rrndb contains detailed information linked directly to external websites including the Ribosomal Database Project, GenBank, PubMed and several culture collections. Data contained in the rrndb will be valuable to researchers investigating microbial ecology and evolution using 16S rRNA gene sequences. The rrndb web site is directly accessible on the WWW at http://rrndb.cme. msu.edu.",poster,cp51
Medicine,p1868,d1,f3cdea5fe196a7558afd9fcb8f3dacb69fdbe3d3,c114,Chinese Conference on Biometric Recognition,"The InterPro database, an integrated documentation resource for protein families, domains and functional sites","Signature databases are vital tools for identifying distant relationships in novel sequences and hence for inferring protein function. InterPro is an integrated documentation resource for protein families, domains and functional sites, which amalgamates the efforts of the PROSITE, PRINTS, Pfam and ProDom database projects. Each InterPro entry includes a functional description, annotation, literature references and links back to the relevant member database(s). Release 2.0 of InterPro (October 2000) contains over 3000 entries, representing families, domains, repeats and sites of post-translational modification encoded by a total of 6804 different regular expressions, profiles, fingerprints and Hidden Markov Models. Each InterPro entry lists all the matches against SWISS-PROT and TrEMBL (more than 1,000,000 hits from 462,500 proteins in SWISS-PROT and TrEMBL). The database is accessible for text- and sequence-based searches at http://www.ebi.ac.uk/interpro/. Questions can be emailed to interhelp@ebi.ac.uk.",poster,cp114
Computer Science,p1868,d3,f3cdea5fe196a7558afd9fcb8f3dacb69fdbe3d3,c114,Chinese Conference on Biometric Recognition,"The InterPro database, an integrated documentation resource for protein families, domains and functional sites","Signature databases are vital tools for identifying distant relationships in novel sequences and hence for inferring protein function. InterPro is an integrated documentation resource for protein families, domains and functional sites, which amalgamates the efforts of the PROSITE, PRINTS, Pfam and ProDom database projects. Each InterPro entry includes a functional description, annotation, literature references and links back to the relevant member database(s). Release 2.0 of InterPro (October 2000) contains over 3000 entries, representing families, domains, repeats and sites of post-translational modification encoded by a total of 6804 different regular expressions, profiles, fingerprints and Hidden Markov Models. Each InterPro entry lists all the matches against SWISS-PROT and TrEMBL (more than 1,000,000 hits from 462,500 proteins in SWISS-PROT and TrEMBL). The database is accessible for text- and sequence-based searches at http://www.ebi.ac.uk/interpro/. Questions can be emailed to interhelp@ebi.ac.uk.",poster,cp114
Biology,p1868,d5,f3cdea5fe196a7558afd9fcb8f3dacb69fdbe3d3,c114,Chinese Conference on Biometric Recognition,"The InterPro database, an integrated documentation resource for protein families, domains and functional sites","Signature databases are vital tools for identifying distant relationships in novel sequences and hence for inferring protein function. InterPro is an integrated documentation resource for protein families, domains and functional sites, which amalgamates the efforts of the PROSITE, PRINTS, Pfam and ProDom database projects. Each InterPro entry includes a functional description, annotation, literature references and links back to the relevant member database(s). Release 2.0 of InterPro (October 2000) contains over 3000 entries, representing families, domains, repeats and sites of post-translational modification encoded by a total of 6804 different regular expressions, profiles, fingerprints and Hidden Markov Models. Each InterPro entry lists all the matches against SWISS-PROT and TrEMBL (more than 1,000,000 hits from 462,500 proteins in SWISS-PROT and TrEMBL). The database is accessible for text- and sequence-based searches at http://www.ebi.ac.uk/interpro/. Questions can be emailed to interhelp@ebi.ac.uk.",poster,cp114
Medicine,p1870,d1,6c26791be6a51844f2784cd402876b18f110c5e4,c90,International Conference on Collaboration Technologies and Systems,IntAct: an open source molecular interaction database,"IntAct provides an open source database and toolkit for the storage, presentation and analysis of protein interactions. The web interface provides both textual and graphical representations of protein interactions, and allows exploring interaction networks in the context of the GO annotations of the interacting proteins. A web service allows direct computational access to retrieve interaction networks in XML format. IntAct currently contains approximately 2200 binary and complex interactions imported from the literature and curated in collaboration with the Swiss-Prot team, making intensive use of controlled vocabularies to ensure data consistency. All IntAct software, data and controlled vocabularies are available at http://www.ebi.ac.uk/intact.",poster,cp90
Computer Science,p1870,d3,6c26791be6a51844f2784cd402876b18f110c5e4,c90,International Conference on Collaboration Technologies and Systems,IntAct: an open source molecular interaction database,"IntAct provides an open source database and toolkit for the storage, presentation and analysis of protein interactions. The web interface provides both textual and graphical representations of protein interactions, and allows exploring interaction networks in the context of the GO annotations of the interacting proteins. A web service allows direct computational access to retrieve interaction networks in XML format. IntAct currently contains approximately 2200 binary and complex interactions imported from the literature and curated in collaboration with the Swiss-Prot team, making intensive use of controlled vocabularies to ensure data consistency. All IntAct software, data and controlled vocabularies are available at http://www.ebi.ac.uk/intact.",poster,cp90
Biology,p1870,d5,6c26791be6a51844f2784cd402876b18f110c5e4,c90,International Conference on Collaboration Technologies and Systems,IntAct: an open source molecular interaction database,"IntAct provides an open source database and toolkit for the storage, presentation and analysis of protein interactions. The web interface provides both textual and graphical representations of protein interactions, and allows exploring interaction networks in the context of the GO annotations of the interacting proteins. A web service allows direct computational access to retrieve interaction networks in XML format. IntAct currently contains approximately 2200 binary and complex interactions imported from the literature and curated in collaboration with the Swiss-Prot team, making intensive use of controlled vocabularies to ensure data consistency. All IntAct software, data and controlled vocabularies are available at http://www.ebi.ac.uk/intact.",poster,cp90
Medicine,p1871,d1,633888a9e6ac257c3e1e3d480525231c1627dc8d,c71,International Joint Conference on Artificial Intelligence,"The RNA modification database, RNAMDB: 2011 update","Since its inception in 1994, The RNA Modification Database (RNAMDB, http://rna-mdb.cas.albany.edu/RNAmods/) has served as a focal point for information pertaining to naturally occurring RNA modifications. In its current state, the database employs an easy-to-use, searchable interface for obtaining detailed data on the 109 currently known RNA modifications. Each entry provides the chemical structure, common name and symbol, elemental composition and mass, CA registry numbers and index name, phylogenetic source, type of RNA species in which it is found, and references to the first reported structure determination and synthesis. Though newly transferred in its entirety to The RNA Institute, the RNAMDB continues to grow with two notable additions, agmatidine and 8-methyladenosine, appended in the last year. The RNA Modification Database is staying up-to-date with significant improvements being prepared for inclusion within the next year and the following year. The expanded future role of The RNA Modification Database will be to serve as a primary information portal for researchers across the entire spectrum of RNA-related research.",poster,cp71
Computer Science,p1871,d3,633888a9e6ac257c3e1e3d480525231c1627dc8d,c71,International Joint Conference on Artificial Intelligence,"The RNA modification database, RNAMDB: 2011 update","Since its inception in 1994, The RNA Modification Database (RNAMDB, http://rna-mdb.cas.albany.edu/RNAmods/) has served as a focal point for information pertaining to naturally occurring RNA modifications. In its current state, the database employs an easy-to-use, searchable interface for obtaining detailed data on the 109 currently known RNA modifications. Each entry provides the chemical structure, common name and symbol, elemental composition and mass, CA registry numbers and index name, phylogenetic source, type of RNA species in which it is found, and references to the first reported structure determination and synthesis. Though newly transferred in its entirety to The RNA Institute, the RNAMDB continues to grow with two notable additions, agmatidine and 8-methyladenosine, appended in the last year. The RNA Modification Database is staying up-to-date with significant improvements being prepared for inclusion within the next year and the following year. The expanded future role of The RNA Modification Database will be to serve as a primary information portal for researchers across the entire spectrum of RNA-related research.",poster,cp71
Biology,p1871,d5,633888a9e6ac257c3e1e3d480525231c1627dc8d,c71,International Joint Conference on Artificial Intelligence,"The RNA modification database, RNAMDB: 2011 update","Since its inception in 1994, The RNA Modification Database (RNAMDB, http://rna-mdb.cas.albany.edu/RNAmods/) has served as a focal point for information pertaining to naturally occurring RNA modifications. In its current state, the database employs an easy-to-use, searchable interface for obtaining detailed data on the 109 currently known RNA modifications. Each entry provides the chemical structure, common name and symbol, elemental composition and mass, CA registry numbers and index name, phylogenetic source, type of RNA species in which it is found, and references to the first reported structure determination and synthesis. Though newly transferred in its entirety to The RNA Institute, the RNAMDB continues to grow with two notable additions, agmatidine and 8-methyladenosine, appended in the last year. The RNA Modification Database is staying up-to-date with significant improvements being prepared for inclusion within the next year and the following year. The expanded future role of The RNA Modification Database will be to serve as a primary information portal for researchers across the entire spectrum of RNA-related research.",poster,cp71
Medicine,p1873,d1,61076194ec631a89daa30edbcc90bc7be37804cc,c87,International Conference on Big Data Research,The NCBI BioSystems database,"The NCBI BioSystems database, found at http://www.ncbi.nlm.nih.gov/biosystems/, centralizes and cross-links existing biological systems databases, increasing their utility and target audience by integrating their pathways and systems into NCBI resources. This integration allows users of NCBI’s Entrez databases to quickly categorize proteins, genes and small molecules by metabolic pathway, disease state or other BioSystem type, without requiring time-consuming inference of biological relationships from the literature or multiple experimental datasets.",poster,cp87
Computer Science,p1873,d3,61076194ec631a89daa30edbcc90bc7be37804cc,c87,International Conference on Big Data Research,The NCBI BioSystems database,"The NCBI BioSystems database, found at http://www.ncbi.nlm.nih.gov/biosystems/, centralizes and cross-links existing biological systems databases, increasing their utility and target audience by integrating their pathways and systems into NCBI resources. This integration allows users of NCBI’s Entrez databases to quickly categorize proteins, genes and small molecules by metabolic pathway, disease state or other BioSystem type, without requiring time-consuming inference of biological relationships from the literature or multiple experimental datasets.",poster,cp87
Biology,p1873,d5,61076194ec631a89daa30edbcc90bc7be37804cc,c87,International Conference on Big Data Research,The NCBI BioSystems database,"The NCBI BioSystems database, found at http://www.ncbi.nlm.nih.gov/biosystems/, centralizes and cross-links existing biological systems databases, increasing their utility and target audience by integrating their pathways and systems into NCBI resources. This integration allows users of NCBI’s Entrez databases to quickly categorize proteins, genes and small molecules by metabolic pathway, disease state or other BioSystem type, without requiring time-consuming inference of biological relationships from the literature or multiple experimental datasets.",poster,cp87
Medicine,p1874,d1,fa3c3fb3db6d54105c7990b6fd3ef41f3aff439d,c54,ACM SIGCOMM Symposium on Software Defined Networking Research,Human immunodeficiency virus reverse transcriptase and protease sequence database,"The HIV reverse transcriptase and protease sequence database is an on-line relational database that catalogues evolutionary and drug-related sequence variation in the human immunodeficiency virus (HIV) reverse transcriptase (RT) and protease enzymes, the molecular targets of antiretroviral therapy (http://hivdb.stanford.edu). The database contains a compilation of nearly all published HIV RT and protease sequences, including submissions to GenBank, sequences published in journal articles and sequences of HIV isolates from persons participating in clinical trials. Sequences are linked to data about the source of the sequence, the antiretroviral drug treatment history of the person from whom the sequence was obtained and the results of in vitro drug susceptibility testing. Sequence data on two new molecular targets of HIV drug therapy--gp41 (cell fusion) and integrase--will be added to the database in 2003.",poster,cp54
Computer Science,p1874,d3,fa3c3fb3db6d54105c7990b6fd3ef41f3aff439d,c54,ACM SIGCOMM Symposium on Software Defined Networking Research,Human immunodeficiency virus reverse transcriptase and protease sequence database,"The HIV reverse transcriptase and protease sequence database is an on-line relational database that catalogues evolutionary and drug-related sequence variation in the human immunodeficiency virus (HIV) reverse transcriptase (RT) and protease enzymes, the molecular targets of antiretroviral therapy (http://hivdb.stanford.edu). The database contains a compilation of nearly all published HIV RT and protease sequences, including submissions to GenBank, sequences published in journal articles and sequences of HIV isolates from persons participating in clinical trials. Sequences are linked to data about the source of the sequence, the antiretroviral drug treatment history of the person from whom the sequence was obtained and the results of in vitro drug susceptibility testing. Sequence data on two new molecular targets of HIV drug therapy--gp41 (cell fusion) and integrase--will be added to the database in 2003.",poster,cp54
Medicine,p1875,d1,df0708235e6c40899f7c9c14dff25ea8b86fdd19,j270,New England Journal of Medicine,The ClinicalTrials.gov results database--update and key issues.,"BACKGROUND
The ClinicalTrials.gov trial registry was expanded in 2008 to include a database for reporting summary results. We summarize the structure and contents of the results database, provide an update of relevant policies, and show how the data can be used to gain insight into the state of clinical research.


METHODS
We analyzed ClinicalTrials.gov data that were publicly available between September 2009 and September 2010.


RESULTS
As of September 27, 2010, ClinicalTrials.gov received approximately 330 new and 2000 revised registrations each week, along with 30 new and 80 revised results submissions. We characterized the 79,413 registry and 2178 results of trial records available as of September 2010. From a sample cohort of results records, 78 of 150 (52%) had associated publications within 2 years after posting. Of results records available publicly, 20% reported more than two primary outcome measures and 5% reported more than five. Of a sample of 100 registry record outcome measures, 61% lacked specificity in describing the metric used in the planned analysis. In a sample of 700 results records, the mean number of different analysis populations per study group was 2.5 (median, 1; range, 1 to 25). Of these trials, 24% reported results for 90% or less of their participants.


CONCLUSIONS
ClinicalTrials.gov provides access to study results not otherwise available to the public. Although the database allows examination of various aspects of ongoing and completed clinical trials, its ultimate usefulness depends on the research community to submit accurate, informative data.",fullPaper,jv270
Medicine,p1876,d1,782d8e30d599e1499555268b5b97c4a86b6bc25b,c89,Conference on Uncertainty in Artificial Intelligence,The EMBL Nucleotide Sequence Database,"The EMBL Nucleotide Sequence Database (http://www.ebi.ac.uk/embl), maintained at the European Bioinformatics Institute (EBI) near Cambridge, UK, is a comprehensive collection of nucleotide sequences and annotation from available public sources. The database is part of an international collaboration with DDBJ (Japan) and GenBank (USA). Data are exchanged daily between the collaborating institutes to achieve swift synchrony. Webin is the preferred tool for individual submissions of nucleotide sequences, including Third Party Annotation (TPA) and alignments. Automated procedures are provided for submissions from large-scale sequencing projects and data from the European Patent Office. New and updated data records are distributed daily and the whole EMBL Nucleotide Sequence Database is released four times a year. Access to the sequence data is provided via ftp and several WWW interfaces. With the web-based Sequence Retrieval System (SRS) it is also possible to link nucleotide data to other specialist molecular biology databases maintained at the EBI. Other tools are available for sequence similarity searching (e.g. FASTA and BLAST). Changes over the past year include the removal of the sequence length limit, the launch of the EMBLCDSs dataset, extension of the Sequence Version Archive functionality and the revision of quality rules for TPA data.",poster,cp89
Computer Science,p1876,d3,782d8e30d599e1499555268b5b97c4a86b6bc25b,c89,Conference on Uncertainty in Artificial Intelligence,The EMBL Nucleotide Sequence Database,"The EMBL Nucleotide Sequence Database (http://www.ebi.ac.uk/embl), maintained at the European Bioinformatics Institute (EBI) near Cambridge, UK, is a comprehensive collection of nucleotide sequences and annotation from available public sources. The database is part of an international collaboration with DDBJ (Japan) and GenBank (USA). Data are exchanged daily between the collaborating institutes to achieve swift synchrony. Webin is the preferred tool for individual submissions of nucleotide sequences, including Third Party Annotation (TPA) and alignments. Automated procedures are provided for submissions from large-scale sequencing projects and data from the European Patent Office. New and updated data records are distributed daily and the whole EMBL Nucleotide Sequence Database is released four times a year. Access to the sequence data is provided via ftp and several WWW interfaces. With the web-based Sequence Retrieval System (SRS) it is also possible to link nucleotide data to other specialist molecular biology databases maintained at the EBI. Other tools are available for sequence similarity searching (e.g. FASTA and BLAST). Changes over the past year include the removal of the sequence length limit, the launch of the EMBLCDSs dataset, extension of the Sequence Version Archive functionality and the revision of quality rules for TPA data.",poster,cp89
Medicine,p1877,d1,cd7763d7c118bc875ea34b30b52d0d95257b1418,c29,ACM-SIAM Symposium on Discrete Algorithms,TRANSFAC: a database on transcription factors and their DNA binding sites,TRANSFAC is a database about eukaryotic transcription regulating DNA sequence elements and the transcription factors binding to and acting through them. This report summarizes the present status of this database and accompanying retrieval tools.,poster,cp29
Computer Science,p1877,d3,cd7763d7c118bc875ea34b30b52d0d95257b1418,c29,ACM-SIAM Symposium on Discrete Algorithms,TRANSFAC: a database on transcription factors and their DNA binding sites,TRANSFAC is a database about eukaryotic transcription regulating DNA sequence elements and the transcription factors binding to and acting through them. This report summarizes the present status of this database and accompanying retrieval tools.,poster,cp29
Biology,p1877,d5,cd7763d7c118bc875ea34b30b52d0d95257b1418,c29,ACM-SIAM Symposium on Discrete Algorithms,TRANSFAC: a database on transcription factors and their DNA binding sites,TRANSFAC is a database about eukaryotic transcription regulating DNA sequence elements and the transcription factors binding to and acting through them. This report summarizes the present status of this database and accompanying retrieval tools.,poster,cp29
Medicine,p1879,d1,9a0723e76b4fce1cdfd407ed31a2b45130b4b423,c52,Workshop on Applied Computational Geometry,"The PROSITE database, its status in 1999","The PROSITE database (http://www.expasy.ch/sprot/prosite.htm l) consists of biologically significant patterns and profiles formulated in such a way that with appropriate computational tools it can help to determine to which known family of protein (if any) a new sequence belongs, or which known domain(s) it contains.",poster,cp52
Computer Science,p1879,d3,9a0723e76b4fce1cdfd407ed31a2b45130b4b423,c52,Workshop on Applied Computational Geometry,"The PROSITE database, its status in 1999","The PROSITE database (http://www.expasy.ch/sprot/prosite.htm l) consists of biologically significant patterns and profiles formulated in such a way that with appropriate computational tools it can help to determine to which known family of protein (if any) a new sequence belongs, or which known domain(s) it contains.",poster,cp52
Biology,p1879,d5,9a0723e76b4fce1cdfd407ed31a2b45130b4b423,c52,Workshop on Applied Computational Geometry,"The PROSITE database, its status in 1999","The PROSITE database (http://www.expasy.ch/sprot/prosite.htm l) consists of biologically significant patterns and profiles formulated in such a way that with appropriate computational tools it can help to determine to which known family of protein (if any) a new sequence belongs, or which known domain(s) it contains.",poster,cp52
Medicine,p1885,d1,fe134631e96a8937b1cc93952e895d882c536655,c109,Computer Vision and Pattern Recognition,The International Nucleotide Sequence Database Collaboration,"Under the International Nucleotide Sequence Database Collaboration (INSDC; http://www.insdc.org), globally comprehensive public domain nucleotide sequence is captured, preserved and presented. The partners of this long-standing collaboration work closely together to provide data formats and conventions that enable consistent data submission to their databases and support regular data exchange around the globe. Clearly defined policy and governance in relation to free access to data and relationships with journal publishers have positioned INSDC databases as a key provider of the scientific record and a core foundation for the global bioinformatics data infrastructure. While growth in sequence data volumes comes no longer as a surprise to INSDC partners, the uptake of next-generation sequencing technology by mainstream science that we have witnessed in recent years brings a step-change to growth, necessarily making a clear mark on INSDC strategy. In this article, we introduce the INSDC, outline data growth patterns and comment on the challenges of increased growth.",poster,cp109
Computer Science,p1885,d3,fe134631e96a8937b1cc93952e895d882c536655,c109,Computer Vision and Pattern Recognition,The International Nucleotide Sequence Database Collaboration,"Under the International Nucleotide Sequence Database Collaboration (INSDC; http://www.insdc.org), globally comprehensive public domain nucleotide sequence is captured, preserved and presented. The partners of this long-standing collaboration work closely together to provide data formats and conventions that enable consistent data submission to their databases and support regular data exchange around the globe. Clearly defined policy and governance in relation to free access to data and relationships with journal publishers have positioned INSDC databases as a key provider of the scientific record and a core foundation for the global bioinformatics data infrastructure. While growth in sequence data volumes comes no longer as a surprise to INSDC partners, the uptake of next-generation sequencing technology by mainstream science that we have witnessed in recent years brings a step-change to growth, necessarily making a clear mark on INSDC strategy. In this article, we introduce the INSDC, outline data growth patterns and comment on the challenges of increased growth.",poster,cp109
Medicine,p1889,d1,062cea54e5d58ee41aea607cbf2ba0cf457aa4e7,c112,British Machine Vision Conference,The DIARETDB1 Diabetic Retinopathy Database and Evaluation Protocol,"Automatic diagnosis of diabetic retinopathy from digital fundus images has been an active research topic in the medical image processing community. The research interest is justified by the excellent potential for new products in the medical industry and significant reductions in health care costs. However, the maturity of proposed algorithms cannot be judged due to the lack of commonly accepted and representative image database with a verified ground truth and strict evaluation protocol. In this study, an evaluation methodology is proposed and an image database with ground truth is described. The database is publicly available for benchmarking diagnosis algorithms. With the proposed database and protocol, it is possible to compare different algorithms, and correspondingly, analyse their maturity for technology transfer from the research laboratories to the medical practice.",fullPaper,cp112
Computer Science,p1889,d3,062cea54e5d58ee41aea607cbf2ba0cf457aa4e7,c112,British Machine Vision Conference,The DIARETDB1 Diabetic Retinopathy Database and Evaluation Protocol,"Automatic diagnosis of diabetic retinopathy from digital fundus images has been an active research topic in the medical image processing community. The research interest is justified by the excellent potential for new products in the medical industry and significant reductions in health care costs. However, the maturity of proposed algorithms cannot be judged due to the lack of commonly accepted and representative image database with a verified ground truth and strict evaluation protocol. In this study, an evaluation methodology is proposed and an image database with ground truth is described. The database is publicly available for benchmarking diagnosis algorithms. With the proposed database and protocol, it is possible to compare different algorithms, and correspondingly, analyse their maturity for technology transfer from the research laboratories to the medical practice.",fullPaper,cp112
Medicine,p1891,d1,251c272eef27fed72dd4e4e07c202f20e6dbd55a,c91,International Symposium on High-Performance Computer Architecture,A new version of the RDP (Ribosomal Database Project),"The Ribosomal Database Project (RDP-II), previously described by Maidak et al. [ Nucleic Acids Res. (1997), 25, 109-111], is now hosted by the Center for Microbial Ecology at Michigan State University. RDP-II is a curated database that offers ribosomal RNA (rRNA) nucleotide sequence data in aligned and unaligned forms, analysis services, and associated computer programs. During the past two years, data alignments have been updated and now include >9700 small subunit rRNA sequences. The recent development of an ObjectStore database will provide more rapid updating of data, better data accuracy and increased user access. RDP-II includes phylogenetically ordered alignments of rRNA sequences, derived phylogenetic trees, rRNA secondary structure diagrams, and various software programs for handling, analyzing and displaying alignments and trees. The data are available via anonymous ftp (ftp.cme.msu. edu) and WWW (http://www.cme.msu.edu/RDP). The WWW server provides ribosomal probe checking, approximate phylogenetic placement of user-submitted sequences, screening for possible chimeric rRNA sequences, automated alignment, and a suggested placement of an unknown sequence on an existing phylogenetic tree. Additional utilities also exist at RDP-II, including distance matrix, T-RFLP, and a Java-based viewer of the phylogenetic trees that can be used to create subtrees.",poster,cp91
Computer Science,p1891,d3,251c272eef27fed72dd4e4e07c202f20e6dbd55a,c91,International Symposium on High-Performance Computer Architecture,A new version of the RDP (Ribosomal Database Project),"The Ribosomal Database Project (RDP-II), previously described by Maidak et al. [ Nucleic Acids Res. (1997), 25, 109-111], is now hosted by the Center for Microbial Ecology at Michigan State University. RDP-II is a curated database that offers ribosomal RNA (rRNA) nucleotide sequence data in aligned and unaligned forms, analysis services, and associated computer programs. During the past two years, data alignments have been updated and now include >9700 small subunit rRNA sequences. The recent development of an ObjectStore database will provide more rapid updating of data, better data accuracy and increased user access. RDP-II includes phylogenetically ordered alignments of rRNA sequences, derived phylogenetic trees, rRNA secondary structure diagrams, and various software programs for handling, analyzing and displaying alignments and trees. The data are available via anonymous ftp (ftp.cme.msu. edu) and WWW (http://www.cme.msu.edu/RDP). The WWW server provides ribosomal probe checking, approximate phylogenetic placement of user-submitted sequences, screening for possible chimeric rRNA sequences, automated alignment, and a suggested placement of an unknown sequence on an existing phylogenetic tree. Additional utilities also exist at RDP-II, including distance matrix, T-RFLP, and a Java-based viewer of the phylogenetic trees that can be used to create subtrees.",poster,cp91
Biology,p1891,d5,251c272eef27fed72dd4e4e07c202f20e6dbd55a,c91,International Symposium on High-Performance Computer Architecture,A new version of the RDP (Ribosomal Database Project),"The Ribosomal Database Project (RDP-II), previously described by Maidak et al. [ Nucleic Acids Res. (1997), 25, 109-111], is now hosted by the Center for Microbial Ecology at Michigan State University. RDP-II is a curated database that offers ribosomal RNA (rRNA) nucleotide sequence data in aligned and unaligned forms, analysis services, and associated computer programs. During the past two years, data alignments have been updated and now include >9700 small subunit rRNA sequences. The recent development of an ObjectStore database will provide more rapid updating of data, better data accuracy and increased user access. RDP-II includes phylogenetically ordered alignments of rRNA sequences, derived phylogenetic trees, rRNA secondary structure diagrams, and various software programs for handling, analyzing and displaying alignments and trees. The data are available via anonymous ftp (ftp.cme.msu. edu) and WWW (http://www.cme.msu.edu/RDP). The WWW server provides ribosomal probe checking, approximate phylogenetic placement of user-submitted sequences, screening for possible chimeric rRNA sequences, automated alignment, and a suggested placement of an unknown sequence on an existing phylogenetic tree. Additional utilities also exist at RDP-II, including distance matrix, T-RFLP, and a Java-based viewer of the phylogenetic trees that can be used to create subtrees.",poster,cp91
Medicine,p1893,d1,73f072aac4f44b860ab8eefd428573dfad3d44fc,j107,Nucleic Acids Research,MIPS: a database for genomes and protein sequences.,"The Munich Information Center for Protein Sequences (MIPS-GSF, Neuherberg, Germany) continues to provide genome-related information in a systematic way. MIPS supports both national and European sequencing and functional analysis projects, develops and maintains automatically generated and manually annotated genome-specific databases, develops systematic classification schemes for the functional annotation of protein sequences, and provides tools for the comprehensive analysis of protein sequences. This report updates the information on the yeast genome (CYGD), the Neurospora crassa genome (MNCDB), the databases for the comprehensive set of genomes (PEDANT genomes), the database of annotated human EST clusters (HIB), the database of complete cDNAs from the DHGP (German Human Genome Project), as well as the project specific databases for the GABI (Genome Analysis in Plants) and HNB (Helmholtz-Netzwerk Bioinformatik) networks. The Arabidospsis thaliana database (MATDB), the database of mitochondrial proteins (MITOP) and our contribution to the PIR International Protein Sequence Database have been described elsewhere [Schoof et al. (2002) Nucleic Acids Res., 30, 91-93; Scharfe et al. (2000) Nucleic Acids Res., 28, 155-158; Barker et al. (2001) Nucleic Acids Res., 29, 29-32]. All databases described, the protein analysis tools provided and the detailed descriptions of our projects can be accessed through the MIPS World Wide Web server (http://mips.gsf.de).",fullPaper,jv107
Medicine,p1895,d1,816b0957fa05347951a1b37c29e21b67d9257c91,c96,Human Language Technology - The Baltic Perspectiv,The ENZYME database in 2000,The ENZYME database is a repository of information related to the nomenclature of enzymes. In recent years it has became an indispensable resource for the development of metabolic databases. The current version contains information on 3705 enzymes. It is available through the ExPASy WWW server (http://www.expasy.ch/enzyme/ ).,poster,cp96
Computer Science,p1895,d3,816b0957fa05347951a1b37c29e21b67d9257c91,c96,Human Language Technology - The Baltic Perspectiv,The ENZYME database in 2000,The ENZYME database is a repository of information related to the nomenclature of enzymes. In recent years it has became an indispensable resource for the development of metabolic databases. The current version contains information on 3705 enzymes. It is available through the ExPASy WWW server (http://www.expasy.ch/enzyme/ ).,poster,cp96
Biology,p1895,d5,816b0957fa05347951a1b37c29e21b67d9257c91,c96,Human Language Technology - The Baltic Perspectiv,The ENZYME database in 2000,The ENZYME database is a repository of information related to the nomenclature of enzymes. In recent years it has became an indispensable resource for the development of metabolic databases. The current version contains information on 3705 enzymes. It is available through the ExPASy WWW server (http://www.expasy.ch/enzyme/ ).,poster,cp96
Medicine,p1896,d1,0d6b4182d465f70e359e30372e550121a0fc94b0,c118,International Conference on Image Analysis and Processing,SCOP: a structural classification of proteins database,"The Structural Classification of Proteins (SCOP) database provides a detailed and comprehensive description of the relationships of known protein structures. The classification is on hierarchical levels: the first two levels, family and superfamily, describe near and distant evolutionary relationships; the third, fold, describes geometrical relationships. The distinction between evolutionary relationships and those that arise from the physics and chemistry of proteins is a feature that is unique to this database so far. The sequences of proteins in SCOP provide the basis of the ASTRAL sequence libraries that can be used as a source of data to calibrate sequence search algorithms and for the generation of statistics on, or selections of, protein structures. Links can be made from SCOP to PDB-ISL: a library containing sequences homologous to proteins of known structure. Sequences of proteins of unknown structure can be matched to distantly related proteins of known structure by using pairwise sequence comparison methods to find homologues in PDB-ISL. The database and its associated files are freely accessible from a number of WWW sites mirrored from URL http://scop.mrc-lmb.cam.ac.uk/scop/",poster,cp118
Computer Science,p1896,d3,0d6b4182d465f70e359e30372e550121a0fc94b0,c118,International Conference on Image Analysis and Processing,SCOP: a structural classification of proteins database,"The Structural Classification of Proteins (SCOP) database provides a detailed and comprehensive description of the relationships of known protein structures. The classification is on hierarchical levels: the first two levels, family and superfamily, describe near and distant evolutionary relationships; the third, fold, describes geometrical relationships. The distinction between evolutionary relationships and those that arise from the physics and chemistry of proteins is a feature that is unique to this database so far. The sequences of proteins in SCOP provide the basis of the ASTRAL sequence libraries that can be used as a source of data to calibrate sequence search algorithms and for the generation of statistics on, or selections of, protein structures. Links can be made from SCOP to PDB-ISL: a library containing sequences homologous to proteins of known structure. Sequences of proteins of unknown structure can be matched to distantly related proteins of known structure by using pairwise sequence comparison methods to find homologues in PDB-ISL. The database and its associated files are freely accessible from a number of WWW sites mirrored from URL http://scop.mrc-lmb.cam.ac.uk/scop/",poster,cp118
Biology,p1896,d5,0d6b4182d465f70e359e30372e550121a0fc94b0,c118,International Conference on Image Analysis and Processing,SCOP: a structural classification of proteins database,"The Structural Classification of Proteins (SCOP) database provides a detailed and comprehensive description of the relationships of known protein structures. The classification is on hierarchical levels: the first two levels, family and superfamily, describe near and distant evolutionary relationships; the third, fold, describes geometrical relationships. The distinction between evolutionary relationships and those that arise from the physics and chemistry of proteins is a feature that is unique to this database so far. The sequences of proteins in SCOP provide the basis of the ASTRAL sequence libraries that can be used as a source of data to calibrate sequence search algorithms and for the generation of statistics on, or selections of, protein structures. Links can be made from SCOP to PDB-ISL: a library containing sequences homologous to proteins of known structure. Sequences of proteins of unknown structure can be matched to distantly related proteins of known structure by using pairwise sequence comparison methods to find homologues in PDB-ISL. The database and its associated files are freely accessible from a number of WWW sites mirrored from URL http://scop.mrc-lmb.cam.ac.uk/scop/",poster,cp118
Medicine,p1897,d1,c4907ef7d044ad71cc8b292c8b1e146987422ec7,c103,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,IPD—the Immuno Polymorphism Database,"The Immuno Polymorphism Database (IPD), http://www.ebi.ac.uk/ipd/ is a set of specialist databases related to the study of polymorphic genes in the immune system. The IPD project works with specialist groups or nomenclature committees who provide and curate individual sections before they are submitted to IPD for online publication. The IPD project stores all the data in a set of related databases. IPD currently consists of four databases: IPD-KIR, contains the allelic sequences of killer-cell immunoglobulin-like receptors, IPD-MHC, a database of sequences of the major histocompatibility complex of different species; IPD-HPA, alloantigens expressed only on platelets; and IPD-ESTDAB, which provides access to the European Searchable Tumour Cell-Line Database, a cell bank of immunologically characterized melanoma cell lines. The data is currently available online from the website and FTP directory. This article describes the latest updates and additional tools added to the IPD project.",poster,cp103
Computer Science,p1897,d3,c4907ef7d044ad71cc8b292c8b1e146987422ec7,c103,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,IPD—the Immuno Polymorphism Database,"The Immuno Polymorphism Database (IPD), http://www.ebi.ac.uk/ipd/ is a set of specialist databases related to the study of polymorphic genes in the immune system. The IPD project works with specialist groups or nomenclature committees who provide and curate individual sections before they are submitted to IPD for online publication. The IPD project stores all the data in a set of related databases. IPD currently consists of four databases: IPD-KIR, contains the allelic sequences of killer-cell immunoglobulin-like receptors, IPD-MHC, a database of sequences of the major histocompatibility complex of different species; IPD-HPA, alloantigens expressed only on platelets; and IPD-ESTDAB, which provides access to the European Searchable Tumour Cell-Line Database, a cell bank of immunologically characterized melanoma cell lines. The data is currently available online from the website and FTP directory. This article describes the latest updates and additional tools added to the IPD project.",poster,cp103
Biology,p1897,d5,c4907ef7d044ad71cc8b292c8b1e146987422ec7,c103,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,IPD—the Immuno Polymorphism Database,"The Immuno Polymorphism Database (IPD), http://www.ebi.ac.uk/ipd/ is a set of specialist databases related to the study of polymorphic genes in the immune system. The IPD project works with specialist groups or nomenclature committees who provide and curate individual sections before they are submitted to IPD for online publication. The IPD project stores all the data in a set of related databases. IPD currently consists of four databases: IPD-KIR, contains the allelic sequences of killer-cell immunoglobulin-like receptors, IPD-MHC, a database of sequences of the major histocompatibility complex of different species; IPD-HPA, alloantigens expressed only on platelets; and IPD-ESTDAB, which provides access to the European Searchable Tumour Cell-Line Database, a cell bank of immunologically characterized melanoma cell lines. The data is currently available online from the website and FTP directory. This article describes the latest updates and additional tools added to the IPD project.",poster,cp103
Medicine,p1898,d1,2c9b060388b88841cf8095cd9efcbfeb805357f6,c25,IEEE International Parallel and Distributed Processing Symposium,PubChem's BioAssay Database,"PubChem (http://pubchem.ncbi.nlm.nih.gov) is a public repository for biological activity data of small molecules and RNAi reagents. The mission of PubChem is to deliver free and easy access to all deposited data, and to provide intuitive data analysis tools. The PubChem BioAssay database currently contains 500 000 descriptions of assay protocols, covering 5000 protein targets, 30 000 gene targets and providing over 130 million bioactivity outcomes. PubChem's bioassay data are integrated into the NCBI Entrez information retrieval system, thus making PubChem data searchable and accessible by Entrez queries. Also, as a repository, PubChem constantly optimizes and develops its deposition system answering many demands of both high- and low-volume depositors. The PubChem information platform allows users to search, review and download bioassay description and data. The PubChem platform also enables researchers to collect, compare and analyze biological test results through web-based and programmatic tools. In this work, we provide an update for the PubChem BioAssay resource, including information content growth, data model extension and new developments of data submission, retrieval, analysis and download tools.",poster,cp25
Computer Science,p1898,d3,2c9b060388b88841cf8095cd9efcbfeb805357f6,c25,IEEE International Parallel and Distributed Processing Symposium,PubChem's BioAssay Database,"PubChem (http://pubchem.ncbi.nlm.nih.gov) is a public repository for biological activity data of small molecules and RNAi reagents. The mission of PubChem is to deliver free and easy access to all deposited data, and to provide intuitive data analysis tools. The PubChem BioAssay database currently contains 500 000 descriptions of assay protocols, covering 5000 protein targets, 30 000 gene targets and providing over 130 million bioactivity outcomes. PubChem's bioassay data are integrated into the NCBI Entrez information retrieval system, thus making PubChem data searchable and accessible by Entrez queries. Also, as a repository, PubChem constantly optimizes and develops its deposition system answering many demands of both high- and low-volume depositors. The PubChem information platform allows users to search, review and download bioassay description and data. The PubChem platform also enables researchers to collect, compare and analyze biological test results through web-based and programmatic tools. In this work, we provide an update for the PubChem BioAssay resource, including information content growth, data model extension and new developments of data submission, retrieval, analysis and download tools.",poster,cp25
Biology,p1898,d5,2c9b060388b88841cf8095cd9efcbfeb805357f6,c25,IEEE International Parallel and Distributed Processing Symposium,PubChem's BioAssay Database,"PubChem (http://pubchem.ncbi.nlm.nih.gov) is a public repository for biological activity data of small molecules and RNAi reagents. The mission of PubChem is to deliver free and easy access to all deposited data, and to provide intuitive data analysis tools. The PubChem BioAssay database currently contains 500 000 descriptions of assay protocols, covering 5000 protein targets, 30 000 gene targets and providing over 130 million bioactivity outcomes. PubChem's bioassay data are integrated into the NCBI Entrez information retrieval system, thus making PubChem data searchable and accessible by Entrez queries. Also, as a repository, PubChem constantly optimizes and develops its deposition system answering many demands of both high- and low-volume depositors. The PubChem information platform allows users to search, review and download bioassay description and data. The PubChem platform also enables researchers to collect, compare and analyze biological test results through web-based and programmatic tools. In this work, we provide an update for the PubChem BioAssay resource, including information content growth, data model extension and new developments of data submission, retrieval, analysis and download tools.",poster,cp25
Medicine,p1901,d1,ad21c3cd8871347e3bdb7cb2800049f7e8a97aca,c84,EUROCON Conference,The IntAct molecular interaction database in 2012,"IntAct is an open-source, open data molecular interaction database populated by data either curated from the literature or from direct data depositions. Two levels of curation are now available within the database, with both IMEx-level annotation and less detailed MIMIx-compatible entries currently supported. As from September 2011, IntAct contains approximately 275 000 curated binary interaction evidences from over 5000 publications. The IntAct website has been improved to enhance the search process and in particular the graphical display of the results. New data download formats are also available, which will facilitate the inclusion of IntAct's data in the Semantic Web. IntAct is an active contributor to the IMEx consortium (http://www.imexconsortium.org). IntAct source code and data are freely available at http://www.ebi.ac.uk/intact.",poster,cp84
Computer Science,p1901,d3,ad21c3cd8871347e3bdb7cb2800049f7e8a97aca,c84,EUROCON Conference,The IntAct molecular interaction database in 2012,"IntAct is an open-source, open data molecular interaction database populated by data either curated from the literature or from direct data depositions. Two levels of curation are now available within the database, with both IMEx-level annotation and less detailed MIMIx-compatible entries currently supported. As from September 2011, IntAct contains approximately 275 000 curated binary interaction evidences from over 5000 publications. The IntAct website has been improved to enhance the search process and in particular the graphical display of the results. New data download formats are also available, which will facilitate the inclusion of IntAct's data in the Semantic Web. IntAct is an active contributor to the IMEx consortium (http://www.imexconsortium.org). IntAct source code and data are freely available at http://www.ebi.ac.uk/intact.",poster,cp84
Biology,p1901,d5,ad21c3cd8871347e3bdb7cb2800049f7e8a97aca,c84,EUROCON Conference,The IntAct molecular interaction database in 2012,"IntAct is an open-source, open data molecular interaction database populated by data either curated from the literature or from direct data depositions. Two levels of curation are now available within the database, with both IMEx-level annotation and less detailed MIMIx-compatible entries currently supported. As from September 2011, IntAct contains approximately 275 000 curated binary interaction evidences from over 5000 publications. The IntAct website has been improved to enhance the search process and in particular the graphical display of the results. New data download formats are also available, which will facilitate the inclusion of IntAct's data in the Semantic Web. IntAct is an active contributor to the IMEx consortium (http://www.imexconsortium.org). IntAct source code and data are freely available at http://www.ebi.ac.uk/intact.",poster,cp84
Medicine,p1902,d1,17891bdbfec1950f7c361db96dca043cfbf54769,c108,IEEE International Conference on Multimedia and Expo,The TIGRFAMs database of protein families,"TIGRFAMs is a collection of manually curated protein families consisting of hidden Markov models (HMMs), multiple sequence alignments, commentary, Gene Ontology (GO) assignments, literature references and pointers to related TIGRFAMs, Pfam and InterPro models. These models are designed to support both automated and manually curated annotation of genomes. TIGRFAMs contains models of full-length proteins and shorter regions at the levels of superfamilies, subfamilies and equivalogs, where equivalogs are sets of homologous proteins conserved with respect to function since their last common ancestor. The scope of each model is set by raising or lowering cutoff scores and choosing members of the seed alignment to group proteins sharing specific function (equivalog) or more general properties. The overall goal is to provide information with maximum utility for the annotation process. TIGRFAMs is thus complementary to Pfam, whose models typically achieve broad coverage across distant homologs but end at the boundaries of conserved structural domains. The database currently contains over 1600 protein families. TIGRFAMs is available for searching or downloading at www.tigr.org/TIGRFAMs.",poster,cp108
Computer Science,p1902,d3,17891bdbfec1950f7c361db96dca043cfbf54769,c108,IEEE International Conference on Multimedia and Expo,The TIGRFAMs database of protein families,"TIGRFAMs is a collection of manually curated protein families consisting of hidden Markov models (HMMs), multiple sequence alignments, commentary, Gene Ontology (GO) assignments, literature references and pointers to related TIGRFAMs, Pfam and InterPro models. These models are designed to support both automated and manually curated annotation of genomes. TIGRFAMs contains models of full-length proteins and shorter regions at the levels of superfamilies, subfamilies and equivalogs, where equivalogs are sets of homologous proteins conserved with respect to function since their last common ancestor. The scope of each model is set by raising or lowering cutoff scores and choosing members of the seed alignment to group proteins sharing specific function (equivalog) or more general properties. The overall goal is to provide information with maximum utility for the annotation process. TIGRFAMs is thus complementary to Pfam, whose models typically achieve broad coverage across distant homologs but end at the boundaries of conserved structural domains. The database currently contains over 1600 protein families. TIGRFAMs is available for searching or downloading at www.tigr.org/TIGRFAMs.",poster,cp108
Biology,p1902,d5,17891bdbfec1950f7c361db96dca043cfbf54769,c108,IEEE International Conference on Multimedia and Expo,The TIGRFAMs database of protein families,"TIGRFAMs is a collection of manually curated protein families consisting of hidden Markov models (HMMs), multiple sequence alignments, commentary, Gene Ontology (GO) assignments, literature references and pointers to related TIGRFAMs, Pfam and InterPro models. These models are designed to support both automated and manually curated annotation of genomes. TIGRFAMs contains models of full-length proteins and shorter regions at the levels of superfamilies, subfamilies and equivalogs, where equivalogs are sets of homologous proteins conserved with respect to function since their last common ancestor. The scope of each model is set by raising or lowering cutoff scores and choosing members of the seed alignment to group proteins sharing specific function (equivalog) or more general properties. The overall goal is to provide information with maximum utility for the annotation process. TIGRFAMs is thus complementary to Pfam, whose models typically achieve broad coverage across distant homologs but end at the boundaries of conserved structural domains. The database currently contains over 1600 protein families. TIGRFAMs is available for searching or downloading at www.tigr.org/TIGRFAMs.",poster,cp108
Medicine,p1903,d1,b507931840ac06717d273b267254a92c8bb7e918,c94,International Conferences on Contemporary Computing and Informatics,Drugs and Lactation Database: LactMed,The National Library of Medicine's Drugs and Lactation Database is an essential resource for any health care professional treating or answering the questions of breastfeeding women. It is available to anyone free of charge online through the TOXNET platform.,poster,cp94
Medicine,p1904,d1,5c06b60a6940df55271fe5917848abf7ab3ca706,c6,Annual Conference on Genetic and Evolutionary Computation,The RDP (Ribosomal Database Project),"The Ribosomal Database Project (RDP) is a curated database that offers ribosome-related data, analysis services and associated computer programs. The offerings include phylogenetically ordered alignments of ribosomal RNA (rRNA) sequences, derived phylogenetic trees, rRNA secondary structure diagrams, and various software for handling, analyzing and displaying alignments and trees. The data are available via anonymous FTP (rdp.life.uiuc.edu), electronic mail (server@rdp.life.uiuc.edu), gopher (rdpgopher.life.uiuc.edu) and WWW (http://rdpwww.life.uiuc.edu/ ). The electronic mail and WWW servers provide ribosomal probe checking, approximate phylogenetic placement of user-submitted sequences, screening for possible chimeric rRNA sequences, automated alignment, and a suggested placement of an unknown sequence on an existing phylogenetic tree.",poster,cp6
Computer Science,p1904,d3,5c06b60a6940df55271fe5917848abf7ab3ca706,c6,Annual Conference on Genetic and Evolutionary Computation,The RDP (Ribosomal Database Project),"The Ribosomal Database Project (RDP) is a curated database that offers ribosome-related data, analysis services and associated computer programs. The offerings include phylogenetically ordered alignments of ribosomal RNA (rRNA) sequences, derived phylogenetic trees, rRNA secondary structure diagrams, and various software for handling, analyzing and displaying alignments and trees. The data are available via anonymous FTP (rdp.life.uiuc.edu), electronic mail (server@rdp.life.uiuc.edu), gopher (rdpgopher.life.uiuc.edu) and WWW (http://rdpwww.life.uiuc.edu/ ). The electronic mail and WWW servers provide ribosomal probe checking, approximate phylogenetic placement of user-submitted sequences, screening for possible chimeric rRNA sequences, automated alignment, and a suggested placement of an unknown sequence on an existing phylogenetic tree.",poster,cp6
Medicine,p1909,d1,49578a040f3346f81759ac40cc174cd12cb40045,c89,Conference on Uncertainty in Artificial Intelligence,"JASPAR, the open access database of transcription factor-binding profiles: new content and tools in the 2008 update","JASPAR is a popular open-access database for matrix models describing DNA-binding preferences for transcription factors and other DNA patterns. With its third major release, JASPAR has been expanded and equipped with additional functions aimed at both casual and power users. The heart of the JASPAR database—the JASPAR CORE sub-database—has increased by 12% in size, and three new specialized sub-databases have been added. New functions include clustering of matrix models by similarity, generation of random matrices by sampling from selected sets of existing models and a language-independent Web Service applications programming interface for matrix retrieval. JASPAR is available at http://jaspar.genereg.net.",poster,cp89
Computer Science,p1909,d3,49578a040f3346f81759ac40cc174cd12cb40045,c89,Conference on Uncertainty in Artificial Intelligence,"JASPAR, the open access database of transcription factor-binding profiles: new content and tools in the 2008 update","JASPAR is a popular open-access database for matrix models describing DNA-binding preferences for transcription factors and other DNA patterns. With its third major release, JASPAR has been expanded and equipped with additional functions aimed at both casual and power users. The heart of the JASPAR database—the JASPAR CORE sub-database—has increased by 12% in size, and three new specialized sub-databases have been added. New functions include clustering of matrix models by similarity, generation of random matrices by sampling from selected sets of existing models and a language-independent Web Service applications programming interface for matrix retrieval. JASPAR is available at http://jaspar.genereg.net.",poster,cp89
Biology,p1909,d5,49578a040f3346f81759ac40cc174cd12cb40045,c89,Conference on Uncertainty in Artificial Intelligence,"JASPAR, the open access database of transcription factor-binding profiles: new content and tools in the 2008 update","JASPAR is a popular open-access database for matrix models describing DNA-binding preferences for transcription factors and other DNA patterns. With its third major release, JASPAR has been expanded and equipped with additional functions aimed at both casual and power users. The heart of the JASPAR database—the JASPAR CORE sub-database—has increased by 12% in size, and three new specialized sub-databases have been added. New functions include clustering of matrix models by similarity, generation of random matrices by sampling from selected sets of existing models and a language-independent Web Service applications programming interface for matrix retrieval. JASPAR is available at http://jaspar.genereg.net.",poster,cp89
Medicine,p1910,d1,440819897d051bdb57182fd2a61777c7a8b710b7,c101,Interspeech,The BioGRID Interaction Database: 2008 update,"The Biological General Repository for Interaction Datasets (BioGRID) database (http://www.thebiogrid.org) was developed to house and distribute collections of protein and genetic interactions from major model organism species. BioGRID currently contains over 198 000 interactions from six different species, as derived from both high-throughput studies and conventional focused studies. Through comprehensive curation efforts, BioGRID now includes a virtually complete set of interactions reported to date in the primary literature for both the budding yeast Saccharomyces cerevisiae and the fission yeast Schizosaccharomyces pombe. A number of new features have been added to the BioGRID including an improved user interface to display interactions based on different attributes, a mirror site and a dedicated interaction management system to coordinate curation across different locations. The BioGRID provides interaction data with monthly updates to Saccharomyces Genome Database, Flybase and Entrez Gene. Source code for the BioGRID and the linked Osprey network visualization system is now freely available without restriction.",poster,cp101
Computer Science,p1910,d3,440819897d051bdb57182fd2a61777c7a8b710b7,c101,Interspeech,The BioGRID Interaction Database: 2008 update,"The Biological General Repository for Interaction Datasets (BioGRID) database (http://www.thebiogrid.org) was developed to house and distribute collections of protein and genetic interactions from major model organism species. BioGRID currently contains over 198 000 interactions from six different species, as derived from both high-throughput studies and conventional focused studies. Through comprehensive curation efforts, BioGRID now includes a virtually complete set of interactions reported to date in the primary literature for both the budding yeast Saccharomyces cerevisiae and the fission yeast Schizosaccharomyces pombe. A number of new features have been added to the BioGRID including an improved user interface to display interactions based on different attributes, a mirror site and a dedicated interaction management system to coordinate curation across different locations. The BioGRID provides interaction data with monthly updates to Saccharomyces Genome Database, Flybase and Entrez Gene. Source code for the BioGRID and the linked Osprey network visualization system is now freely available without restriction.",poster,cp101
Biology,p1910,d5,440819897d051bdb57182fd2a61777c7a8b710b7,c101,Interspeech,The BioGRID Interaction Database: 2008 update,"The Biological General Repository for Interaction Datasets (BioGRID) database (http://www.thebiogrid.org) was developed to house and distribute collections of protein and genetic interactions from major model organism species. BioGRID currently contains over 198 000 interactions from six different species, as derived from both high-throughput studies and conventional focused studies. Through comprehensive curation efforts, BioGRID now includes a virtually complete set of interactions reported to date in the primary literature for both the budding yeast Saccharomyces cerevisiae and the fission yeast Schizosaccharomyces pombe. A number of new features have been added to the BioGRID including an improved user interface to display interactions based on different attributes, a mirror site and a dedicated interaction management system to coordinate curation across different locations. The BioGRID provides interaction data with monthly updates to Saccharomyces Genome Database, Flybase and Entrez Gene. Source code for the BioGRID and the linked Osprey network visualization system is now freely available without restriction.",poster,cp101
Medicine,p1914,d1,8b683b12f9efc1d8bdf330182a0afd7c14369ce1,c92,International Symposium on Computer Architecture,The Comparative Toxicogenomics Database: update 2011,"The Comparative Toxicogenomics Database (CTD) is a public resource that promotes understanding about the interaction of environmental chemicals with gene products, and their effects on human health. Biocurators at CTD manually curate a triad of chemical–gene, chemical–disease and gene–disease relationships from the literature. These core data are then integrated to construct chemical–gene–disease networks and to predict many novel relationships using different types of associated data. Since 2009, we dramatically increased the content of CTD to 1.4 million chemical–gene–disease data points and added many features, statistical analyses and analytical tools, including GeneComps and ChemComps (to find comparable genes and chemicals that share toxicogenomic profiles), enriched Gene Ontology terms associated with chemicals, statistically ranked chemical–disease inferences, Venn diagram tools to discover overlapping and unique attributes of any set of chemicals, genes or disease, and enhanced gene pathway data content, among other features. Together, this wealth of expanded chemical–gene–disease data continues to help users generate testable hypotheses about the molecular mechanisms of environmental diseases. CTD is freely available at http://ctd.mdibl.org.",poster,cp92
Computer Science,p1914,d3,8b683b12f9efc1d8bdf330182a0afd7c14369ce1,c92,International Symposium on Computer Architecture,The Comparative Toxicogenomics Database: update 2011,"The Comparative Toxicogenomics Database (CTD) is a public resource that promotes understanding about the interaction of environmental chemicals with gene products, and their effects on human health. Biocurators at CTD manually curate a triad of chemical–gene, chemical–disease and gene–disease relationships from the literature. These core data are then integrated to construct chemical–gene–disease networks and to predict many novel relationships using different types of associated data. Since 2009, we dramatically increased the content of CTD to 1.4 million chemical–gene–disease data points and added many features, statistical analyses and analytical tools, including GeneComps and ChemComps (to find comparable genes and chemicals that share toxicogenomic profiles), enriched Gene Ontology terms associated with chemicals, statistically ranked chemical–disease inferences, Venn diagram tools to discover overlapping and unique attributes of any set of chemicals, genes or disease, and enhanced gene pathway data content, among other features. Together, this wealth of expanded chemical–gene–disease data continues to help users generate testable hypotheses about the molecular mechanisms of environmental diseases. CTD is freely available at http://ctd.mdibl.org.",poster,cp92
Biology,p1914,d5,8b683b12f9efc1d8bdf330182a0afd7c14369ce1,c92,International Symposium on Computer Architecture,The Comparative Toxicogenomics Database: update 2011,"The Comparative Toxicogenomics Database (CTD) is a public resource that promotes understanding about the interaction of environmental chemicals with gene products, and their effects on human health. Biocurators at CTD manually curate a triad of chemical–gene, chemical–disease and gene–disease relationships from the literature. These core data are then integrated to construct chemical–gene–disease networks and to predict many novel relationships using different types of associated data. Since 2009, we dramatically increased the content of CTD to 1.4 million chemical–gene–disease data points and added many features, statistical analyses and analytical tools, including GeneComps and ChemComps (to find comparable genes and chemicals that share toxicogenomic profiles), enriched Gene Ontology terms associated with chemicals, statistically ranked chemical–disease inferences, Venn diagram tools to discover overlapping and unique attributes of any set of chemicals, genes or disease, and enhanced gene pathway data content, among other features. Together, this wealth of expanded chemical–gene–disease data continues to help users generate testable hypotheses about the molecular mechanisms of environmental diseases. CTD is freely available at http://ctd.mdibl.org.",poster,cp92
Medicine,p1916,d1,9d79185d82c03c30778f3635bfbdcf605330f41b,c50,Conference on Emerging Network Experiment and Technology,The Immune Epitope Database 2.0,"The Immune Epitope Database (IEDB, www.iedb.org) provides a catalog of experimentally characterized B and T cell epitopes, as well as data on Major Histocompatibility Complex (MHC) binding and MHC ligand elution experiments. The database represents the molecular structures recognized by adaptive immune receptors and the experimental contexts in which these molecules were determined to be immune epitopes. Epitopes recognized in humans, nonhuman primates, rodents, pigs, cats and all other tested species are included. Both positive and negative experimental results are captured. Over the course of 4 years, the data from 180 978 experiments were curated manually from the literature, which covers ∼99% of all publicly available information on peptide epitopes mapped in infectious agents (excluding HIV) and 93% of those mapped in allergens. In addition, data that would otherwise be unavailable to the public from 129 186 experiments were submitted directly by investigators. The curation of epitopes related to autoimmunity is expected to be completed by the end of 2010. The database can be queried by epitope structure, source organism, MHC restriction, assay type or host organism, among other criteria. The database structure, as well as its querying, browsing and reporting interfaces, was completely redesigned for the IEDB 2.0 release, which became publicly available in early 2009.",poster,cp50
Computer Science,p1916,d3,9d79185d82c03c30778f3635bfbdcf605330f41b,c50,Conference on Emerging Network Experiment and Technology,The Immune Epitope Database 2.0,"The Immune Epitope Database (IEDB, www.iedb.org) provides a catalog of experimentally characterized B and T cell epitopes, as well as data on Major Histocompatibility Complex (MHC) binding and MHC ligand elution experiments. The database represents the molecular structures recognized by adaptive immune receptors and the experimental contexts in which these molecules were determined to be immune epitopes. Epitopes recognized in humans, nonhuman primates, rodents, pigs, cats and all other tested species are included. Both positive and negative experimental results are captured. Over the course of 4 years, the data from 180 978 experiments were curated manually from the literature, which covers ∼99% of all publicly available information on peptide epitopes mapped in infectious agents (excluding HIV) and 93% of those mapped in allergens. In addition, data that would otherwise be unavailable to the public from 129 186 experiments were submitted directly by investigators. The curation of epitopes related to autoimmunity is expected to be completed by the end of 2010. The database can be queried by epitope structure, source organism, MHC restriction, assay type or host organism, among other criteria. The database structure, as well as its querying, browsing and reporting interfaces, was completely redesigned for the IEDB 2.0 release, which became publicly available in early 2009.",poster,cp50
Biology,p1916,d5,9d79185d82c03c30778f3635bfbdcf605330f41b,c50,Conference on Emerging Network Experiment and Technology,The Immune Epitope Database 2.0,"The Immune Epitope Database (IEDB, www.iedb.org) provides a catalog of experimentally characterized B and T cell epitopes, as well as data on Major Histocompatibility Complex (MHC) binding and MHC ligand elution experiments. The database represents the molecular structures recognized by adaptive immune receptors and the experimental contexts in which these molecules were determined to be immune epitopes. Epitopes recognized in humans, nonhuman primates, rodents, pigs, cats and all other tested species are included. Both positive and negative experimental results are captured. Over the course of 4 years, the data from 180 978 experiments were curated manually from the literature, which covers ∼99% of all publicly available information on peptide epitopes mapped in infectious agents (excluding HIV) and 93% of those mapped in allergens. In addition, data that would otherwise be unavailable to the public from 129 186 experiments were submitted directly by investigators. The curation of epitopes related to autoimmunity is expected to be completed by the end of 2010. The database can be queried by epitope structure, source organism, MHC restriction, assay type or host organism, among other criteria. The database structure, as well as its querying, browsing and reporting interfaces, was completely redesigned for the IEDB 2.0 release, which became publicly available in early 2009.",poster,cp50
Medicine,p1917,d1,7463a0b934ac40a353773840485bb56d35fbbb66,c81,ACM Symposium on Applied Computing,Database on medicinal plants used in Ayurveda,Abstract,poster,cp81
Medicine,p1919,d1,2033531aeaf7d0da158cdaacae9b208407bd4a1c,c69,Neural Information Processing Systems,AAindex: Amino Acid Index Database,"AAindex is a database of numerical indices representing various physicochemical and biochemical properties of amino acids and pairs of amino acids. It consists of two sections: AAindex1 for the amino acid index of 20 numerical values and AAindex2 for the amino acid mutation matrix of 210 numerical values. Each entry of either AAindex1 or AAindex2 consists of the definition, the reference information, a list of related entries in terms of the correlation coefficient, and the actual data. The database may be accessed through the DBGET/LinkDB system at GenomeNet (http://www.genome.ad. jp/dbget/) or may be downloaded by anonymous FTP (ftp://ftp.genome. ad.jp/db/genomenet/aaindex/).",poster,cp69
Computer Science,p1919,d3,2033531aeaf7d0da158cdaacae9b208407bd4a1c,c69,Neural Information Processing Systems,AAindex: Amino Acid Index Database,"AAindex is a database of numerical indices representing various physicochemical and biochemical properties of amino acids and pairs of amino acids. It consists of two sections: AAindex1 for the amino acid index of 20 numerical values and AAindex2 for the amino acid mutation matrix of 210 numerical values. Each entry of either AAindex1 or AAindex2 consists of the definition, the reference information, a list of related entries in terms of the correlation coefficient, and the actual data. The database may be accessed through the DBGET/LinkDB system at GenomeNet (http://www.genome.ad. jp/dbget/) or may be downloaded by anonymous FTP (ftp://ftp.genome. ad.jp/db/genomenet/aaindex/).",poster,cp69
Biology,p1919,d5,2033531aeaf7d0da158cdaacae9b208407bd4a1c,c69,Neural Information Processing Systems,AAindex: Amino Acid Index Database,"AAindex is a database of numerical indices representing various physicochemical and biochemical properties of amino acids and pairs of amino acids. It consists of two sections: AAindex1 for the amino acid index of 20 numerical values and AAindex2 for the amino acid mutation matrix of 210 numerical values. Each entry of either AAindex1 or AAindex2 consists of the definition, the reference information, a list of related entries in terms of the correlation coefficient, and the actual data. The database may be accessed through the DBGET/LinkDB system at GenomeNet (http://www.genome.ad. jp/dbget/) or may be downloaded by anonymous FTP (ftp://ftp.genome. ad.jp/db/genomenet/aaindex/).",poster,cp69
Medicine,p1921,d1,a047888622c576ccf06a7708ae18a5d9ec5f09fd,j388,"Behavoir research methods, instruments & computers",Lexique 2 : A new French lexical database,Abstract,fullPaper,jv388
Computer Science,p1921,d3,a047888622c576ccf06a7708ae18a5d9ec5f09fd,j388,"Behavoir research methods, instruments & computers",Lexique 2 : A new French lexical database,Abstract,fullPaper,jv388
Medicine,p1924,d1,cc42c8dac7c3fb8cd522136f1c7c31ae45a3121f,j107,Nucleic Acids Research,The MDM2 gene amplification database.,"The p53 tumor suppressor gene is inactivated in human tumors by several distinct mechanisms. The best characterized inactivation mechanisms are: (i) gene mutation; (ii) p53 protein association with viral proteins; (iii) p53 protein association with the MDM2 cellular oncoprotein. The MDM2 gene has been shown to be abnormally up-regulated in human tumors and tumor cell lines by gene amplification, increased transcript levels and enhanced translation. This communication presents a brief review of the spectrum of MDM2 abnormalities in human tumors and compares the tissue distribution of MDM2 amplification and p53 mutation frequencies. In this study, 3889 samples from tumors or xenografts from 28 tumor types were examined for MDM2 amplification from previously published sources. The overall frequency of MDM2 amplification in these human tumors was 7%. Gene amplification was observed in 19 tumor types, with the highest frequency observed in soft tissue tumors (20%), osteosarcomas (16%) and esophageal carcinomas (13%). Tumors which showed a higher incidence of MDM2 amplification than p53 mutation were soft tissue tumors, testicular germ cell cancers and neuro-blastomas. Data from studies where both MDM2 amplification and p53 mutations were analyzed within the same samples showed that mutations in these two genes do not generally occur within the same tumor. In these studies, 29 out of a total of 33 MDM2 amplification-positive tumors had wild-type p53. We hypothesize that heretofore uncharacterized carcinogens favor MDM2 amplification over p53 mutations in certain tumor types. A database listing the MDM2 gene amplifications is available on the World Wide Web at http://www. infosci.coh.org/mdm2 . Charts of MDM2 amplification frequencies and comparisons with p53 genetic alterations are also available at this Web site.",fullPaper,jv107
Biology,p1924,d5,cc42c8dac7c3fb8cd522136f1c7c31ae45a3121f,j107,Nucleic Acids Research,The MDM2 gene amplification database.,"The p53 tumor suppressor gene is inactivated in human tumors by several distinct mechanisms. The best characterized inactivation mechanisms are: (i) gene mutation; (ii) p53 protein association with viral proteins; (iii) p53 protein association with the MDM2 cellular oncoprotein. The MDM2 gene has been shown to be abnormally up-regulated in human tumors and tumor cell lines by gene amplification, increased transcript levels and enhanced translation. This communication presents a brief review of the spectrum of MDM2 abnormalities in human tumors and compares the tissue distribution of MDM2 amplification and p53 mutation frequencies. In this study, 3889 samples from tumors or xenografts from 28 tumor types were examined for MDM2 amplification from previously published sources. The overall frequency of MDM2 amplification in these human tumors was 7%. Gene amplification was observed in 19 tumor types, with the highest frequency observed in soft tissue tumors (20%), osteosarcomas (16%) and esophageal carcinomas (13%). Tumors which showed a higher incidence of MDM2 amplification than p53 mutation were soft tissue tumors, testicular germ cell cancers and neuro-blastomas. Data from studies where both MDM2 amplification and p53 mutations were analyzed within the same samples showed that mutations in these two genes do not generally occur within the same tumor. In these studies, 29 out of a total of 33 MDM2 amplification-positive tumors had wild-type p53. We hypothesize that heretofore uncharacterized carcinogens favor MDM2 amplification over p53 mutations in certain tumor types. A database listing the MDM2 gene amplifications is available on the World Wide Web at http://www. infosci.coh.org/mdm2 . Charts of MDM2 amplification frequencies and comparisons with p53 genetic alterations are also available at this Web site.",fullPaper,jv107
Medicine,p1926,d1,a7ece6b4ad1f1688ba8afe3720e7f6942ec68f58,c117,Very Large Data Bases Conference,The MetaCyc Database of metabolic pathways and enzymes and the BioCyc collection of Pathway/Genome Databases,"MetaCyc (MetaCyc.org) is a universal database of metabolic pathways and enzymes from all domains of life. The pathways in MetaCyc are curated from the primary scientific literature, and are experimentally determined small-molecule metabolic pathways. Each reaction in a MetaCyc pathway is annotated with one or more well-characterized enzymes. Because MetaCyc contains only experimentally elucidated knowledge, it provides a uniquely high-quality resource for metabolic pathways and enzymes. BioCyc (BioCyc.org) is a collection of more than 350 organism-specific Pathway/Genome Databases (PGDBs). Each BioCyc PGDB contains the predicted metabolic network of one organism, including metabolic pathways, enzymes, metabolites and reactions predicted by the Pathway Tools software using MetaCyc as a reference database. BioCyc PGDBs also contain predicted operons and predicted pathway hole fillers—predictions of which enzymes may catalyze pathway reactions that have not been assigned to an enzyme. The BioCyc website offers many tools for computational analysis of PGDBs, including comparative analysis and analysis of omics data in a pathway context. The BioCyc PGDBs generated by SRI are offered for adoption by any interested party for the ongoing integration of metabolic and genome-related information about an organism.",poster,cp117
Computer Science,p1926,d3,a7ece6b4ad1f1688ba8afe3720e7f6942ec68f58,c117,Very Large Data Bases Conference,The MetaCyc Database of metabolic pathways and enzymes and the BioCyc collection of Pathway/Genome Databases,"MetaCyc (MetaCyc.org) is a universal database of metabolic pathways and enzymes from all domains of life. The pathways in MetaCyc are curated from the primary scientific literature, and are experimentally determined small-molecule metabolic pathways. Each reaction in a MetaCyc pathway is annotated with one or more well-characterized enzymes. Because MetaCyc contains only experimentally elucidated knowledge, it provides a uniquely high-quality resource for metabolic pathways and enzymes. BioCyc (BioCyc.org) is a collection of more than 350 organism-specific Pathway/Genome Databases (PGDBs). Each BioCyc PGDB contains the predicted metabolic network of one organism, including metabolic pathways, enzymes, metabolites and reactions predicted by the Pathway Tools software using MetaCyc as a reference database. BioCyc PGDBs also contain predicted operons and predicted pathway hole fillers—predictions of which enzymes may catalyze pathway reactions that have not been assigned to an enzyme. The BioCyc website offers many tools for computational analysis of PGDBs, including comparative analysis and analysis of omics data in a pathway context. The BioCyc PGDBs generated by SRI are offered for adoption by any interested party for the ongoing integration of metabolic and genome-related information about an organism.",poster,cp117
Biology,p1926,d5,a7ece6b4ad1f1688ba8afe3720e7f6942ec68f58,c117,Very Large Data Bases Conference,The MetaCyc Database of metabolic pathways and enzymes and the BioCyc collection of Pathway/Genome Databases,"MetaCyc (MetaCyc.org) is a universal database of metabolic pathways and enzymes from all domains of life. The pathways in MetaCyc are curated from the primary scientific literature, and are experimentally determined small-molecule metabolic pathways. Each reaction in a MetaCyc pathway is annotated with one or more well-characterized enzymes. Because MetaCyc contains only experimentally elucidated knowledge, it provides a uniquely high-quality resource for metabolic pathways and enzymes. BioCyc (BioCyc.org) is a collection of more than 350 organism-specific Pathway/Genome Databases (PGDBs). Each BioCyc PGDB contains the predicted metabolic network of one organism, including metabolic pathways, enzymes, metabolites and reactions predicted by the Pathway Tools software using MetaCyc as a reference database. BioCyc PGDBs also contain predicted operons and predicted pathway hole fillers—predictions of which enzymes may catalyze pathway reactions that have not been assigned to an enzyme. The BioCyc website offers many tools for computational analysis of PGDBs, including comparative analysis and analysis of omics data in a pathway context. The BioCyc PGDBs generated by SRI are offered for adoption by any interested party for the ongoing integration of metabolic and genome-related information about an organism.",poster,cp117
Medicine,p1932,d1,b4ea6e57966ffdab58ec410e085acc1232064303,c91,International Symposium on High-Performance Computer Architecture,"The PANTHER database of protein families, subfamilies, functions and pathways","PANTHER is a large collection of protein families that have been subdivided into functionally related subfamilies, using human expertise. These subfamilies model the divergence of specific functions within protein families, allowing more accurate association with function (ontology terms and pathways), as well as inference of amino acids important for functional specificity. Hidden Markov models (HMMs) are built for each family and subfamily for classifying additional protein sequences. The latest version, 5.0, contains 6683 protein families, divided into 31 705 subfamilies, covering ∼90% of mammalian protein-coding genes. PANTHER 5.0 includes a number of significant improvements over previous versions, most notably (i) representation of pathways (primarily signaling pathways) and association with subfamilies and individual protein sequences; (ii) an improved methodology for defining the PANTHER families and subfamilies, and for building the HMMs; (iii) resources for scoring sequences against PANTHER HMMs both over the web and locally; and (iv) a number of new web resources to facilitate analysis of large gene lists, including data generated from high-throughput expression experiments. Efforts are underway to add PANTHER to the InterPro suite of databases, and to make PANTHER consistent with the PIRSF database. PANTHER is now publicly available without restriction at http://panther.appliedbiosystems.com.",poster,cp91
Computer Science,p1932,d3,b4ea6e57966ffdab58ec410e085acc1232064303,c91,International Symposium on High-Performance Computer Architecture,"The PANTHER database of protein families, subfamilies, functions and pathways","PANTHER is a large collection of protein families that have been subdivided into functionally related subfamilies, using human expertise. These subfamilies model the divergence of specific functions within protein families, allowing more accurate association with function (ontology terms and pathways), as well as inference of amino acids important for functional specificity. Hidden Markov models (HMMs) are built for each family and subfamily for classifying additional protein sequences. The latest version, 5.0, contains 6683 protein families, divided into 31 705 subfamilies, covering ∼90% of mammalian protein-coding genes. PANTHER 5.0 includes a number of significant improvements over previous versions, most notably (i) representation of pathways (primarily signaling pathways) and association with subfamilies and individual protein sequences; (ii) an improved methodology for defining the PANTHER families and subfamilies, and for building the HMMs; (iii) resources for scoring sequences against PANTHER HMMs both over the web and locally; and (iv) a number of new web resources to facilitate analysis of large gene lists, including data generated from high-throughput expression experiments. Efforts are underway to add PANTHER to the InterPro suite of databases, and to make PANTHER consistent with the PIRSF database. PANTHER is now publicly available without restriction at http://panther.appliedbiosystems.com.",poster,cp91
Biology,p1932,d5,b4ea6e57966ffdab58ec410e085acc1232064303,c91,International Symposium on High-Performance Computer Architecture,"The PANTHER database of protein families, subfamilies, functions and pathways","PANTHER is a large collection of protein families that have been subdivided into functionally related subfamilies, using human expertise. These subfamilies model the divergence of specific functions within protein families, allowing more accurate association with function (ontology terms and pathways), as well as inference of amino acids important for functional specificity. Hidden Markov models (HMMs) are built for each family and subfamily for classifying additional protein sequences. The latest version, 5.0, contains 6683 protein families, divided into 31 705 subfamilies, covering ∼90% of mammalian protein-coding genes. PANTHER 5.0 includes a number of significant improvements over previous versions, most notably (i) representation of pathways (primarily signaling pathways) and association with subfamilies and individual protein sequences; (ii) an improved methodology for defining the PANTHER families and subfamilies, and for building the HMMs; (iii) resources for scoring sequences against PANTHER HMMs both over the web and locally; and (iv) a number of new web resources to facilitate analysis of large gene lists, including data generated from high-throughput expression experiments. Efforts are underway to add PANTHER to the InterPro suite of databases, and to make PANTHER consistent with the PIRSF database. PANTHER is now publicly available without restriction at http://panther.appliedbiosystems.com.",poster,cp91
Medicine,p1933,d1,794c048acd3d2d35e3248161729fcf142b4966c6,c115,International Conference on Information Integration and Web-based Applications & Services,Phospho.ELM: a database of phosphorylation sites—update 2008,"Phospho.ELM is a manually curated database of eukaryotic phosphorylation sites. The resource includes data collected from published literature as well as high-throughput data sets. The current release of Phospho.ELM (version 7.0, July 2007) contains 4078 phospho-protein sequences covering 12 025 phospho-serine, 2362 phospho-threonine and 2083 phospho-tyrosine sites. The entries provide information about the phosphorylated proteins and the exact position of known phosphorylated instances, the kinases responsible for the modification (where known) and links to bibliographic references. The database entries have hyperlinks to easily access further information from UniProt, PubMed, SMART, ELM, MSD as well as links to the protein interaction databases MINT and STRING. A new BLAST search tool, complementary to retrieval by keyword and UniProt accession number, allows users to submit a protein query (by sequence or UniProt accession) to search against the curated data set of phosphorylated peptides. Phospho.ELM is available on line at: http://phospho.elm.eu.org",poster,cp115
Computer Science,p1933,d3,794c048acd3d2d35e3248161729fcf142b4966c6,c115,International Conference on Information Integration and Web-based Applications & Services,Phospho.ELM: a database of phosphorylation sites—update 2008,"Phospho.ELM is a manually curated database of eukaryotic phosphorylation sites. The resource includes data collected from published literature as well as high-throughput data sets. The current release of Phospho.ELM (version 7.0, July 2007) contains 4078 phospho-protein sequences covering 12 025 phospho-serine, 2362 phospho-threonine and 2083 phospho-tyrosine sites. The entries provide information about the phosphorylated proteins and the exact position of known phosphorylated instances, the kinases responsible for the modification (where known) and links to bibliographic references. The database entries have hyperlinks to easily access further information from UniProt, PubMed, SMART, ELM, MSD as well as links to the protein interaction databases MINT and STRING. A new BLAST search tool, complementary to retrieval by keyword and UniProt accession number, allows users to submit a protein query (by sequence or UniProt accession) to search against the curated data set of phosphorylated peptides. Phospho.ELM is available on line at: http://phospho.elm.eu.org",poster,cp115
Biology,p1933,d5,794c048acd3d2d35e3248161729fcf142b4966c6,c115,International Conference on Information Integration and Web-based Applications & Services,Phospho.ELM: a database of phosphorylation sites—update 2008,"Phospho.ELM is a manually curated database of eukaryotic phosphorylation sites. The resource includes data collected from published literature as well as high-throughput data sets. The current release of Phospho.ELM (version 7.0, July 2007) contains 4078 phospho-protein sequences covering 12 025 phospho-serine, 2362 phospho-threonine and 2083 phospho-tyrosine sites. The entries provide information about the phosphorylated proteins and the exact position of known phosphorylated instances, the kinases responsible for the modification (where known) and links to bibliographic references. The database entries have hyperlinks to easily access further information from UniProt, PubMed, SMART, ELM, MSD as well as links to the protein interaction databases MINT and STRING. A new BLAST search tool, complementary to retrieval by keyword and UniProt accession number, allows users to submit a protein query (by sequence or UniProt accession) to search against the curated data set of phosphorylated peptides. Phospho.ELM is available on line at: http://phospho.elm.eu.org",poster,cp115
Medicine,p1934,d1,54f4ef17f6b7a315260e001af85fc8b6fc1d2a7b,c72,Workshop on Research on Enterprise Networking,The UCSC Genome Browser Database: update 2009,"The UCSC Genome Browser Database (GBD, http://genome.ucsc.edu) is a publicly available collection of genome assembly sequence data and integrated annotations for a large number of organisms, including extensive comparative-genomic resources. In the past year, 13 new genome assemblies have been added, including two important primate species, orangutan and marmoset, bringing the total to 46 assemblies for 24 different vertebrates and 39 assemblies for 22 different invertebrate animals. The GBD datasets may be viewed graphically with the UCSC Genome Browser, which uses a coordinate-based display system allowing users to juxtapose a wide variety of data. These data include all mRNAs from GenBank mapped to all organisms, RefSeq alignments, gene predictions, regulatory elements, gene expression data, repeats, SNPs and other variation data, as well as pairwise and multiple-genome alignments. A variety of other bioinformatics tools are also provided, including BLAT, the Table Browser, the Gene Sorter, the Proteome Browser, VisiGene and Genome Graphs.",poster,cp72
Computer Science,p1934,d3,54f4ef17f6b7a315260e001af85fc8b6fc1d2a7b,c72,Workshop on Research on Enterprise Networking,The UCSC Genome Browser Database: update 2009,"The UCSC Genome Browser Database (GBD, http://genome.ucsc.edu) is a publicly available collection of genome assembly sequence data and integrated annotations for a large number of organisms, including extensive comparative-genomic resources. In the past year, 13 new genome assemblies have been added, including two important primate species, orangutan and marmoset, bringing the total to 46 assemblies for 24 different vertebrates and 39 assemblies for 22 different invertebrate animals. The GBD datasets may be viewed graphically with the UCSC Genome Browser, which uses a coordinate-based display system allowing users to juxtapose a wide variety of data. These data include all mRNAs from GenBank mapped to all organisms, RefSeq alignments, gene predictions, regulatory elements, gene expression data, repeats, SNPs and other variation data, as well as pairwise and multiple-genome alignments. A variety of other bioinformatics tools are also provided, including BLAT, the Table Browser, the Gene Sorter, the Proteome Browser, VisiGene and Genome Graphs.",poster,cp72
Biology,p1934,d5,54f4ef17f6b7a315260e001af85fc8b6fc1d2a7b,c72,Workshop on Research on Enterprise Networking,The UCSC Genome Browser Database: update 2009,"The UCSC Genome Browser Database (GBD, http://genome.ucsc.edu) is a publicly available collection of genome assembly sequence data and integrated annotations for a large number of organisms, including extensive comparative-genomic resources. In the past year, 13 new genome assemblies have been added, including two important primate species, orangutan and marmoset, bringing the total to 46 assemblies for 24 different vertebrates and 39 assemblies for 22 different invertebrate animals. The GBD datasets may be viewed graphically with the UCSC Genome Browser, which uses a coordinate-based display system allowing users to juxtapose a wide variety of data. These data include all mRNAs from GenBank mapped to all organisms, RefSeq alignments, gene predictions, regulatory elements, gene expression data, repeats, SNPs and other variation data, as well as pairwise and multiple-genome alignments. A variety of other bioinformatics tools are also provided, including BLAT, the Table Browser, the Gene Sorter, the Proteome Browser, VisiGene and Genome Graphs.",poster,cp72
Medicine,p1936,d1,aa163167f51e580c3dcb1aaef00f60b08b6f64a8,c118,International Conference on Image Analysis and Processing,EcoCyc: a comprehensive database resource for Escherichia coli,"The EcoCyc database (http://EcoCyc.org/) is a comprehensive source of information on the biology of the prototypical model organism Escherichia coli K12. The mission for EcoCyc is to contain both computable descriptions of, and detailed comments describing, all genes, proteins, pathways and molecular interactions in E.coli. Through ongoing manual curation, extensive information such as summary comments, regulatory information, literature citations and evidence types has been extracted from 8862 publications and added to Version 8.5 of the EcoCyc database. The EcoCyc database can be accessed through a World Wide Web interface, while the downloadable Pathway Tools software and data files enable computational exploration of the data and provide enhanced querying capabilities that web interfaces cannot support. For example, EcoCyc contains carefully curated information that can be used as training sets for bioinformatics prediction of entities such as promoters, operons, genetic networks, transcription factor binding sites, metabolic pathways, functionally related genes, protein complexes and protein–ligand interactions.",poster,cp118
Computer Science,p1936,d3,aa163167f51e580c3dcb1aaef00f60b08b6f64a8,c118,International Conference on Image Analysis and Processing,EcoCyc: a comprehensive database resource for Escherichia coli,"The EcoCyc database (http://EcoCyc.org/) is a comprehensive source of information on the biology of the prototypical model organism Escherichia coli K12. The mission for EcoCyc is to contain both computable descriptions of, and detailed comments describing, all genes, proteins, pathways and molecular interactions in E.coli. Through ongoing manual curation, extensive information such as summary comments, regulatory information, literature citations and evidence types has been extracted from 8862 publications and added to Version 8.5 of the EcoCyc database. The EcoCyc database can be accessed through a World Wide Web interface, while the downloadable Pathway Tools software and data files enable computational exploration of the data and provide enhanced querying capabilities that web interfaces cannot support. For example, EcoCyc contains carefully curated information that can be used as training sets for bioinformatics prediction of entities such as promoters, operons, genetic networks, transcription factor binding sites, metabolic pathways, functionally related genes, protein complexes and protein–ligand interactions.",poster,cp118
Biology,p1936,d5,aa163167f51e580c3dcb1aaef00f60b08b6f64a8,c118,International Conference on Image Analysis and Processing,EcoCyc: a comprehensive database resource for Escherichia coli,"The EcoCyc database (http://EcoCyc.org/) is a comprehensive source of information on the biology of the prototypical model organism Escherichia coli K12. The mission for EcoCyc is to contain both computable descriptions of, and detailed comments describing, all genes, proteins, pathways and molecular interactions in E.coli. Through ongoing manual curation, extensive information such as summary comments, regulatory information, literature citations and evidence types has been extracted from 8862 publications and added to Version 8.5 of the EcoCyc database. The EcoCyc database can be accessed through a World Wide Web interface, while the downloadable Pathway Tools software and data files enable computational exploration of the data and provide enhanced querying capabilities that web interfaces cannot support. For example, EcoCyc contains carefully curated information that can be used as training sets for bioinformatics prediction of entities such as promoters, operons, genetic networks, transcription factor binding sites, metabolic pathways, functionally related genes, protein complexes and protein–ligand interactions.",poster,cp118
Medicine,p1938,d1,78597d4989a7f7f892067637dd3271e60569b087,c41,IEEE International Conference on Data Engineering,DisProt: the Database of Disordered Proteins,"The Database of Protein Disorder (DisProt) links structure and function information for intrinsically disordered proteins (IDPs). Intrinsically disordered proteins do not form a fixed three-dimensional structure under physiological conditions, either in their entireties or in segments or regions. We define IDP as a protein that contains at least one experimentally determined disordered region. Although lacking fixed structure, IDPs and regions carry out important biological functions, being typically involved in regulation, signaling and control. Such functions can involve high-specificity low-affinity interactions, the multiple binding of one protein to many partners and the multiple binding of many proteins to one partner. These three features are all enabled and enhanced by protein intrinsic disorder. One of the major hindrances in the study of IDPs has been the lack of organized information. DisProt was developed to enable IDP research by collecting and organizing knowledge regarding the experimental characterization and the functional associations of IDPs. In addition to being a unique source of biological information, DisProt opens doors for a plethora of bioinformatics studies. DisProt is openly available at .",poster,cp41
Computer Science,p1938,d3,78597d4989a7f7f892067637dd3271e60569b087,c41,IEEE International Conference on Data Engineering,DisProt: the Database of Disordered Proteins,"The Database of Protein Disorder (DisProt) links structure and function information for intrinsically disordered proteins (IDPs). Intrinsically disordered proteins do not form a fixed three-dimensional structure under physiological conditions, either in their entireties or in segments or regions. We define IDP as a protein that contains at least one experimentally determined disordered region. Although lacking fixed structure, IDPs and regions carry out important biological functions, being typically involved in regulation, signaling and control. Such functions can involve high-specificity low-affinity interactions, the multiple binding of one protein to many partners and the multiple binding of many proteins to one partner. These three features are all enabled and enhanced by protein intrinsic disorder. One of the major hindrances in the study of IDPs has been the lack of organized information. DisProt was developed to enable IDP research by collecting and organizing knowledge regarding the experimental characterization and the functional associations of IDPs. In addition to being a unique source of biological information, DisProt opens doors for a plethora of bioinformatics studies. DisProt is openly available at .",poster,cp41
Biology,p1938,d5,78597d4989a7f7f892067637dd3271e60569b087,c41,IEEE International Conference on Data Engineering,DisProt: the Database of Disordered Proteins,"The Database of Protein Disorder (DisProt) links structure and function information for intrinsically disordered proteins (IDPs). Intrinsically disordered proteins do not form a fixed three-dimensional structure under physiological conditions, either in their entireties or in segments or regions. We define IDP as a protein that contains at least one experimentally determined disordered region. Although lacking fixed structure, IDPs and regions carry out important biological functions, being typically involved in regulation, signaling and control. Such functions can involve high-specificity low-affinity interactions, the multiple binding of one protein to many partners and the multiple binding of many proteins to one partner. These three features are all enabled and enhanced by protein intrinsic disorder. One of the major hindrances in the study of IDPs has been the lack of organized information. DisProt was developed to enable IDP research by collecting and organizing knowledge regarding the experimental characterization and the functional associations of IDPs. In addition to being a unique source of biological information, DisProt opens doors for a plethora of bioinformatics studies. DisProt is openly available at .",poster,cp41
Medicine,p1939,d1,9aa8a679e3401f1bbf805b738095bf2ed52e08e7,j396,PLoS Medicine,Tuberculosis Drug Resistance Mutation Database,Andreas Sandgren and colleagues describe a new comprehensive resource on drug resistance mutations inM. tuberculosis.,fullPaper,jv396
Biology,p1939,d5,9aa8a679e3401f1bbf805b738095bf2ed52e08e7,j396,PLoS Medicine,Tuberculosis Drug Resistance Mutation Database,Andreas Sandgren and colleagues describe a new comprehensive resource on drug resistance mutations inM. tuberculosis.,fullPaper,jv396
Medicine,p1940,d1,ed431581f9537896d26b7c8d9935dce9ee73871d,c40,European Conference on Computer Vision,The International Nucleotide Sequence Database Collaboration,"The members of the International Nucleotide Sequence Database Collaboration (INSDC; http://www.insdc.org) set out to capture, preserve and present globally comprehensive public domain nucleotide sequence information. The work of the long-standing collaboration includes the provision of data formats, annotation conventions and routine global data exchange. Among the many developments to INSDC resources in 2011 are the newly launched BioProject database and improved handling of assembly information. In this article, we outline INSDC services and update the reader on developments in 2011.",poster,cp40
Computer Science,p1940,d3,ed431581f9537896d26b7c8d9935dce9ee73871d,c40,European Conference on Computer Vision,The International Nucleotide Sequence Database Collaboration,"The members of the International Nucleotide Sequence Database Collaboration (INSDC; http://www.insdc.org) set out to capture, preserve and present globally comprehensive public domain nucleotide sequence information. The work of the long-standing collaboration includes the provision of data formats, annotation conventions and routine global data exchange. Among the many developments to INSDC resources in 2011 are the newly launched BioProject database and improved handling of assembly information. In this article, we outline INSDC services and update the reader on developments in 2011.",poster,cp40
Biology,p1940,d5,ed431581f9537896d26b7c8d9935dce9ee73871d,c40,European Conference on Computer Vision,The International Nucleotide Sequence Database Collaboration,"The members of the International Nucleotide Sequence Database Collaboration (INSDC; http://www.insdc.org) set out to capture, preserve and present globally comprehensive public domain nucleotide sequence information. The work of the long-standing collaboration includes the provision of data formats, annotation conventions and routine global data exchange. Among the many developments to INSDC resources in 2011 are the newly launched BioProject database and improved handling of assembly information. In this article, we outline INSDC services and update the reader on developments in 2011.",poster,cp40
Medicine,p1942,d1,48fa4530c0eddf525b273e222753c978606243f7,c62,International Conference on Advanced Data and Information Engineering,APD: the Antimicrobial Peptide Database,"An antimicrobial peptide database (APD) has been established based on an extensive literature search. It contains detailed information for 525 peptides (498 antibacterial, 155 antifungal, 28 antiviral and 18 antitumor). APD provides interactive interfaces for peptide query, prediction and design. It also provides statistical data for a select group of or all the peptides in the database. Peptide information can be searched using keywords such as peptide name, ID, length, net charge, hydrophobic percentage, key residue, unique sequence motif, structure and activity. APD is a useful tool for studying the structure-function relationship of antimicrobial peptides. The database can be accessed via a web-based browser at the URL: http://aps.unmc.edu/AP/main.html.",poster,cp62
Computer Science,p1942,d3,48fa4530c0eddf525b273e222753c978606243f7,c62,International Conference on Advanced Data and Information Engineering,APD: the Antimicrobial Peptide Database,"An antimicrobial peptide database (APD) has been established based on an extensive literature search. It contains detailed information for 525 peptides (498 antibacterial, 155 antifungal, 28 antiviral and 18 antitumor). APD provides interactive interfaces for peptide query, prediction and design. It also provides statistical data for a select group of or all the peptides in the database. Peptide information can be searched using keywords such as peptide name, ID, length, net charge, hydrophobic percentage, key residue, unique sequence motif, structure and activity. APD is a useful tool for studying the structure-function relationship of antimicrobial peptides. The database can be accessed via a web-based browser at the URL: http://aps.unmc.edu/AP/main.html.",poster,cp62
Biology,p1942,d5,48fa4530c0eddf525b273e222753c978606243f7,c62,International Conference on Advanced Data and Information Engineering,APD: the Antimicrobial Peptide Database,"An antimicrobial peptide database (APD) has been established based on an extensive literature search. It contains detailed information for 525 peptides (498 antibacterial, 155 antifungal, 28 antiviral and 18 antitumor). APD provides interactive interfaces for peptide query, prediction and design. It also provides statistical data for a select group of or all the peptides in the database. Peptide information can be searched using keywords such as peptide name, ID, length, net charge, hydrophobic percentage, key residue, unique sequence motif, structure and activity. APD is a useful tool for studying the structure-function relationship of antimicrobial peptides. The database can be accessed via a web-based browser at the URL: http://aps.unmc.edu/AP/main.html.",poster,cp62
Medicine,p1943,d1,978a4276e0874a590d34aef84e6238ce21f0539e,c113,International Conference on Mobile Data Management,Inparanoid: a comprehensive database of eukaryotic orthologs,"The Inparanoid eukaryotic ortholog database (http://inparanoid.cgb.ki.se/) is a collection of pairwise ortholog groups between 17 whole genomes; Anopheles gambiae, Caenorhabditis briggsae, Caenorhabditis elegans, Drosophila melanogaster, Danio rerio, Takifugu rubripes, Gallus gallus, Homo sapiens, Mus musculus, Pan troglodytes, Rattus norvegicus, Oryza sativa, Plasmodium falciparum, Arabidopsis thaliana, Escherichia coli, Saccharomyces cerevisiae and Schizosaccharomyces pombe. Complete proteomes for these genomes were derived from Ensembl and UniProt and compared pairwise using Blast, followed by a clustering step using the Inparanoid program. An Inparanoid cluster is seeded by a reciprocally best-matching ortholog pair, around which inparalogs (should they exist) are gathered independently, while outparalogs are excluded. The ortholog clusters can be searched on the website using Ensembl gene/protein or UniProt identifiers, annotation text or by Blast alignment against our protein datasets. The entire dataset can be downloaded, as can the Inparanoid program itself.",poster,cp113
Computer Science,p1943,d3,978a4276e0874a590d34aef84e6238ce21f0539e,c113,International Conference on Mobile Data Management,Inparanoid: a comprehensive database of eukaryotic orthologs,"The Inparanoid eukaryotic ortholog database (http://inparanoid.cgb.ki.se/) is a collection of pairwise ortholog groups between 17 whole genomes; Anopheles gambiae, Caenorhabditis briggsae, Caenorhabditis elegans, Drosophila melanogaster, Danio rerio, Takifugu rubripes, Gallus gallus, Homo sapiens, Mus musculus, Pan troglodytes, Rattus norvegicus, Oryza sativa, Plasmodium falciparum, Arabidopsis thaliana, Escherichia coli, Saccharomyces cerevisiae and Schizosaccharomyces pombe. Complete proteomes for these genomes were derived from Ensembl and UniProt and compared pairwise using Blast, followed by a clustering step using the Inparanoid program. An Inparanoid cluster is seeded by a reciprocally best-matching ortholog pair, around which inparalogs (should they exist) are gathered independently, while outparalogs are excluded. The ortholog clusters can be searched on the website using Ensembl gene/protein or UniProt identifiers, annotation text or by Blast alignment against our protein datasets. The entire dataset can be downloaded, as can the Inparanoid program itself.",poster,cp113
Biology,p1943,d5,978a4276e0874a590d34aef84e6238ce21f0539e,c113,International Conference on Mobile Data Management,Inparanoid: a comprehensive database of eukaryotic orthologs,"The Inparanoid eukaryotic ortholog database (http://inparanoid.cgb.ki.se/) is a collection of pairwise ortholog groups between 17 whole genomes; Anopheles gambiae, Caenorhabditis briggsae, Caenorhabditis elegans, Drosophila melanogaster, Danio rerio, Takifugu rubripes, Gallus gallus, Homo sapiens, Mus musculus, Pan troglodytes, Rattus norvegicus, Oryza sativa, Plasmodium falciparum, Arabidopsis thaliana, Escherichia coli, Saccharomyces cerevisiae and Schizosaccharomyces pombe. Complete proteomes for these genomes were derived from Ensembl and UniProt and compared pairwise using Blast, followed by a clustering step using the Inparanoid program. An Inparanoid cluster is seeded by a reciprocally best-matching ortholog pair, around which inparalogs (should they exist) are gathered independently, while outparalogs are excluded. The ortholog clusters can be searched on the website using Ensembl gene/protein or UniProt identifiers, annotation text or by Blast alignment against our protein datasets. The entire dataset can be downloaded, as can the Inparanoid program itself.",poster,cp113
Medicine,p1944,d1,a5375b684c8e6640246df2eaec5f59b2ef94242b,c117,Very Large Data Bases Conference,CDD: a curated Entrez database of conserved domain alignments,"The Conserved Domain Database (CDD) is now indexed as a separate database within the Entrez system and linked to other Entrez databases such as MEDLINE(R). This allows users to search for domain types by name, for example, or to view the domain architecture of any protein in Entrez's sequence database. CDD can be accessed on the WorldWideWeb at http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=cdd. Users may also employ the CD-Search service to identify conserved domains in new sequences, at http://www.ncbi.nlm.nih.gov/Structure/cdd/wrpsb.cgi. CD-Search results, and pre-computed links from Entrez's protein database, are calculated using the RPS-BLAST algorithm and Position Specific Score Matrices (PSSMs) derived from CDD alignments. CD-Searches are also run by default for protein-protein queries submitted to BLAST(R) at http://www.ncbi.nlm.nih.gov/BLAST. CDD mirrors the publicly available domain alignment collections SMART and PFAM, and now also contains alignment models curated at NCBI. Structure information is used to identify the core substructure likely to be present in all family members, and to produce sequence alignments consistent with structure conservation. This alignment model allows NCBI curators to annotate 'columns' corresponding to functional sites conserved among family members.",poster,cp117
Computer Science,p1944,d3,a5375b684c8e6640246df2eaec5f59b2ef94242b,c117,Very Large Data Bases Conference,CDD: a curated Entrez database of conserved domain alignments,"The Conserved Domain Database (CDD) is now indexed as a separate database within the Entrez system and linked to other Entrez databases such as MEDLINE(R). This allows users to search for domain types by name, for example, or to view the domain architecture of any protein in Entrez's sequence database. CDD can be accessed on the WorldWideWeb at http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=cdd. Users may also employ the CD-Search service to identify conserved domains in new sequences, at http://www.ncbi.nlm.nih.gov/Structure/cdd/wrpsb.cgi. CD-Search results, and pre-computed links from Entrez's protein database, are calculated using the RPS-BLAST algorithm and Position Specific Score Matrices (PSSMs) derived from CDD alignments. CD-Searches are also run by default for protein-protein queries submitted to BLAST(R) at http://www.ncbi.nlm.nih.gov/BLAST. CDD mirrors the publicly available domain alignment collections SMART and PFAM, and now also contains alignment models curated at NCBI. Structure information is used to identify the core substructure likely to be present in all family members, and to produce sequence alignments consistent with structure conservation. This alignment model allows NCBI curators to annotate 'columns' corresponding to functional sites conserved among family members.",poster,cp117
Biology,p1944,d5,a5375b684c8e6640246df2eaec5f59b2ef94242b,c117,Very Large Data Bases Conference,CDD: a curated Entrez database of conserved domain alignments,"The Conserved Domain Database (CDD) is now indexed as a separate database within the Entrez system and linked to other Entrez databases such as MEDLINE(R). This allows users to search for domain types by name, for example, or to view the domain architecture of any protein in Entrez's sequence database. CDD can be accessed on the WorldWideWeb at http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=cdd. Users may also employ the CD-Search service to identify conserved domains in new sequences, at http://www.ncbi.nlm.nih.gov/Structure/cdd/wrpsb.cgi. CD-Search results, and pre-computed links from Entrez's protein database, are calculated using the RPS-BLAST algorithm and Position Specific Score Matrices (PSSMs) derived from CDD alignments. CD-Searches are also run by default for protein-protein queries submitted to BLAST(R) at http://www.ncbi.nlm.nih.gov/BLAST. CDD mirrors the publicly available domain alignment collections SMART and PFAM, and now also contains alignment models curated at NCBI. Structure information is used to identify the core substructure likely to be present in all family members, and to produce sequence alignments consistent with structure conservation. This alignment model allows NCBI curators to annotate 'columns' corresponding to functional sites conserved among family members.",poster,cp117
Medicine,p1945,d1,e13a71f639bed2c1c739b206a6a053e8f18651a6,c74,International Conference on Computational Linguistics,"BioModels Database: a free, centralized database of curated, published, quantitative kinetic models of biochemical and cellular systems","BioModels Database (), part of the international initiative BioModels.net, provides access to published, peer-reviewed, quantitative models of biochemical and cellular systems. Each model is carefully curated to verify that it corresponds to the reference publication and gives the proper numerical results. Curators also annotate the components of the models with terms from controlled vocabularies and links to other relevant data resources. This allows the users to search accurately for the models they need. The models can currently be retrieved in the SBML format, and import/export facilities are being developed to extend the spectrum of formats supported by the resource.",poster,cp74
Computer Science,p1945,d3,e13a71f639bed2c1c739b206a6a053e8f18651a6,c74,International Conference on Computational Linguistics,"BioModels Database: a free, centralized database of curated, published, quantitative kinetic models of biochemical and cellular systems","BioModels Database (), part of the international initiative BioModels.net, provides access to published, peer-reviewed, quantitative models of biochemical and cellular systems. Each model is carefully curated to verify that it corresponds to the reference publication and gives the proper numerical results. Curators also annotate the components of the models with terms from controlled vocabularies and links to other relevant data resources. This allows the users to search accurately for the models they need. The models can currently be retrieved in the SBML format, and import/export facilities are being developed to extend the spectrum of formats supported by the resource.",poster,cp74
Biology,p1945,d5,e13a71f639bed2c1c739b206a6a053e8f18651a6,c74,International Conference on Computational Linguistics,"BioModels Database: a free, centralized database of curated, published, quantitative kinetic models of biochemical and cellular systems","BioModels Database (), part of the international initiative BioModels.net, provides access to published, peer-reviewed, quantitative models of biochemical and cellular systems. Each model is carefully curated to verify that it corresponds to the reference publication and gives the proper numerical results. Curators also annotate the components of the models with terms from controlled vocabularies and links to other relevant data resources. This allows the users to search accurately for the models they need. The models can currently be retrieved in the SBML format, and import/export facilities are being developed to extend the spectrum of formats supported by the resource.",poster,cp74
Medicine,p1950,d1,03094c68e9333dbfb17426d22d4e61748d92e414,c102,ACM SIGMOD Conference,The BioGRID Interaction Database: 2011 update,"The Biological General Repository for Interaction Datasets (BioGRID) is a public database that archives and disseminates genetic and protein interaction data from model organisms and humans (http://www.thebiogrid.org). BioGRID currently holds 347 966 interactions (170 162 genetic, 177 804 protein) curated from both high-throughput data sets and individual focused studies, as derived from over 23 000 publications in the primary literature. Complete coverage of the entire literature is maintained for budding yeast (Saccharomyces cerevisiae), fission yeast (Schizosaccharomyces pombe) and thale cress (Arabidopsis thaliana), and efforts to expand curation across multiple metazoan species are underway. The BioGRID houses 48 831 human protein interactions that have been curated from 10 247 publications. Current curation drives are focused on particular areas of biology to enable insights into conserved networks and pathways that are relevant to human health. The BioGRID 3.0 web interface contains new search and display features that enable rapid queries across multiple data types and sources. An automated Interaction Management System (IMS) is used to prioritize, coordinate and track curation across international sites and projects. BioGRID provides interaction data to several model organism databases, resources such as Entrez-Gene and other interaction meta-databases. The entire BioGRID 3.0 data collection may be downloaded in multiple file formats, including PSI MI XML. Source code for BioGRID 3.0 is freely available without any restrictions.",poster,cp102
Computer Science,p1950,d3,03094c68e9333dbfb17426d22d4e61748d92e414,c102,ACM SIGMOD Conference,The BioGRID Interaction Database: 2011 update,"The Biological General Repository for Interaction Datasets (BioGRID) is a public database that archives and disseminates genetic and protein interaction data from model organisms and humans (http://www.thebiogrid.org). BioGRID currently holds 347 966 interactions (170 162 genetic, 177 804 protein) curated from both high-throughput data sets and individual focused studies, as derived from over 23 000 publications in the primary literature. Complete coverage of the entire literature is maintained for budding yeast (Saccharomyces cerevisiae), fission yeast (Schizosaccharomyces pombe) and thale cress (Arabidopsis thaliana), and efforts to expand curation across multiple metazoan species are underway. The BioGRID houses 48 831 human protein interactions that have been curated from 10 247 publications. Current curation drives are focused on particular areas of biology to enable insights into conserved networks and pathways that are relevant to human health. The BioGRID 3.0 web interface contains new search and display features that enable rapid queries across multiple data types and sources. An automated Interaction Management System (IMS) is used to prioritize, coordinate and track curation across international sites and projects. BioGRID provides interaction data to several model organism databases, resources such as Entrez-Gene and other interaction meta-databases. The entire BioGRID 3.0 data collection may be downloaded in multiple file formats, including PSI MI XML. Source code for BioGRID 3.0 is freely available without any restrictions.",poster,cp102
Biology,p1950,d5,03094c68e9333dbfb17426d22d4e61748d92e414,c102,ACM SIGMOD Conference,The BioGRID Interaction Database: 2011 update,"The Biological General Repository for Interaction Datasets (BioGRID) is a public database that archives and disseminates genetic and protein interaction data from model organisms and humans (http://www.thebiogrid.org). BioGRID currently holds 347 966 interactions (170 162 genetic, 177 804 protein) curated from both high-throughput data sets and individual focused studies, as derived from over 23 000 publications in the primary literature. Complete coverage of the entire literature is maintained for budding yeast (Saccharomyces cerevisiae), fission yeast (Schizosaccharomyces pombe) and thale cress (Arabidopsis thaliana), and efforts to expand curation across multiple metazoan species are underway. The BioGRID houses 48 831 human protein interactions that have been curated from 10 247 publications. Current curation drives are focused on particular areas of biology to enable insights into conserved networks and pathways that are relevant to human health. The BioGRID 3.0 web interface contains new search and display features that enable rapid queries across multiple data types and sources. An automated Interaction Management System (IMS) is used to prioritize, coordinate and track curation across international sites and projects. BioGRID provides interaction data to several model organism databases, resources such as Entrez-Gene and other interaction meta-databases. The entire BioGRID 3.0 data collection may be downloaded in multiple file formats, including PSI MI XML. Source code for BioGRID 3.0 is freely available without any restrictions.",poster,cp102
Medicine,p1954,d1,51f3fbc8b948dd93ed6d1e27e320141d0507603d,c31,Information Security Solutions Europe,CPPsite: a curated database of cell penetrating peptides,"Delivering drug molecules into the cell is one of the major challenges in the process of drug development. In past, cell penetrating peptides have been successfully used for delivering a wide variety of therapeutic molecules into various types of cells for the treatment of multiple diseases. These peptides have unique ability to gain access to the interior of almost any type of cell. Due to the huge therapeutic applications of CPPs, we have built a comprehensive database ‘CPPsite’, of cell penetrating peptides, where information is compiled from the literature and patents. CPPsite is a manually curated database of experimentally validated 843 CPPs. Each entry provides information of a peptide that includes ID, PubMed ID, peptide name, peptide sequence, chirality, origin, nature of peptide, sub-cellular localization, uptake efficiency, uptake mechanism, hydrophobicity, amino acid frequency and composition, etc. A wide range of user-friendly tools have been incorporated in this database like searching, browsing, analyzing, mapping tools. In addition, we have derived various types of information from these peptide sequences that include secondary/tertiary structure, amino acid composition and physicochemical properties of peptides. This database will be very useful for developing models for predicting effective cell penetrating peptides. Database URL: http://crdd.osdd.net/raghava/cppsite/.",poster,cp31
Computer Science,p1954,d3,51f3fbc8b948dd93ed6d1e27e320141d0507603d,c31,Information Security Solutions Europe,CPPsite: a curated database of cell penetrating peptides,"Delivering drug molecules into the cell is one of the major challenges in the process of drug development. In past, cell penetrating peptides have been successfully used for delivering a wide variety of therapeutic molecules into various types of cells for the treatment of multiple diseases. These peptides have unique ability to gain access to the interior of almost any type of cell. Due to the huge therapeutic applications of CPPs, we have built a comprehensive database ‘CPPsite’, of cell penetrating peptides, where information is compiled from the literature and patents. CPPsite is a manually curated database of experimentally validated 843 CPPs. Each entry provides information of a peptide that includes ID, PubMed ID, peptide name, peptide sequence, chirality, origin, nature of peptide, sub-cellular localization, uptake efficiency, uptake mechanism, hydrophobicity, amino acid frequency and composition, etc. A wide range of user-friendly tools have been incorporated in this database like searching, browsing, analyzing, mapping tools. In addition, we have derived various types of information from these peptide sequences that include secondary/tertiary structure, amino acid composition and physicochemical properties of peptides. This database will be very useful for developing models for predicting effective cell penetrating peptides. Database URL: http://crdd.osdd.net/raghava/cppsite/.",poster,cp31
Chemistry,p1954,d8,51f3fbc8b948dd93ed6d1e27e320141d0507603d,c31,Information Security Solutions Europe,CPPsite: a curated database of cell penetrating peptides,"Delivering drug molecules into the cell is one of the major challenges in the process of drug development. In past, cell penetrating peptides have been successfully used for delivering a wide variety of therapeutic molecules into various types of cells for the treatment of multiple diseases. These peptides have unique ability to gain access to the interior of almost any type of cell. Due to the huge therapeutic applications of CPPs, we have built a comprehensive database ‘CPPsite’, of cell penetrating peptides, where information is compiled from the literature and patents. CPPsite is a manually curated database of experimentally validated 843 CPPs. Each entry provides information of a peptide that includes ID, PubMed ID, peptide name, peptide sequence, chirality, origin, nature of peptide, sub-cellular localization, uptake efficiency, uptake mechanism, hydrophobicity, amino acid frequency and composition, etc. A wide range of user-friendly tools have been incorporated in this database like searching, browsing, analyzing, mapping tools. In addition, we have derived various types of information from these peptide sequences that include secondary/tertiary structure, amino acid composition and physicochemical properties of peptides. This database will be very useful for developing models for predicting effective cell penetrating peptides. Database URL: http://crdd.osdd.net/raghava/cppsite/.",poster,cp31
Medicine,p1958,d1,b71ac5caa3a4335c311122cbacada6b17a199060,j397,Methods in Enzymology,Saccharomyces Genome Database.,Abstract,fullPaper,jv397
Biology,p1958,d5,b71ac5caa3a4335c311122cbacada6b17a199060,j397,Methods in Enzymology,Saccharomyces Genome Database.,Abstract,fullPaper,jv397
Medicine,p1960,d1,f836da820f53f5bbc890647ecbf00e1031f200c7,j343,Proteomics,The International Protein Index: An integrated database for proteomics experiments,"Despite the complete determination of the genome sequence of several higher eukaryotes, their proteomes remain relatively poorly defined. Information about proteins identified by different experimental and computational methods is stored in different databases, meaning that no single resource offers full coverage of known and predicted proteins. IPI (the International Protein Index) has been developed to address these issues and offers complete nonredundant data sets representing the human, mouse and rat proteomes, built from the Swiss‐Prot, TrEMBL, Ensembl and RefSeq databases.",fullPaper,jv343
Biology,p1960,d5,f836da820f53f5bbc890647ecbf00e1031f200c7,j343,Proteomics,The International Protein Index: An integrated database for proteomics experiments,"Despite the complete determination of the genome sequence of several higher eukaryotes, their proteomes remain relatively poorly defined. Information about proteins identified by different experimental and computational methods is stored in different databases, meaning that no single resource offers full coverage of known and predicted proteins. IPI (the International Protein Index) has been developed to address these issues and offers complete nonredundant data sets representing the human, mouse and rat proteomes, built from the Swiss‐Prot, TrEMBL, Ensembl and RefSeq databases.",fullPaper,jv343
Medicine,p1962,d1,dacc0018d0a0c45d93599751e53c91f88fcd45b8,c53,International Conference on Learning Representations,"The InterPro Database, 2003 brings increased coverage and new features","InterPro, an integrated documentation resource of protein families, domains and functional sites, was created in 1999 as a means of amalgamating the major protein signature databases into one comprehensive resource. PROSITE, Pfam, PRINTS, ProDom, SMART and TIGRFAMs have been manually integrated and curated and are available in InterPro for text- and sequence-based searching. The results are provided in a single format that rationalises the results that would be obtained by searching the member databases individually. The latest release of InterPro contains 5629 entries describing 4280 families, 1239 domains, 95 repeats and 15 post-translational modifications. Currently, the combined signatures in InterPro cover more than 74% of all proteins in SWISS-PROT and TrEMBL, an increase of nearly 15% since the inception of InterPro. New features of the database include improved searching capabilities and enhanced graphical user interfaces for visualisation of the data. The database is available via a webserver (http://www.ebi.ac.uk/interpro) and anonymous FTP (ftp://ftp.ebi.ac.uk/pub/databases/interpro).",poster,cp53
Computer Science,p1962,d3,dacc0018d0a0c45d93599751e53c91f88fcd45b8,c53,International Conference on Learning Representations,"The InterPro Database, 2003 brings increased coverage and new features","InterPro, an integrated documentation resource of protein families, domains and functional sites, was created in 1999 as a means of amalgamating the major protein signature databases into one comprehensive resource. PROSITE, Pfam, PRINTS, ProDom, SMART and TIGRFAMs have been manually integrated and curated and are available in InterPro for text- and sequence-based searching. The results are provided in a single format that rationalises the results that would be obtained by searching the member databases individually. The latest release of InterPro contains 5629 entries describing 4280 families, 1239 domains, 95 repeats and 15 post-translational modifications. Currently, the combined signatures in InterPro cover more than 74% of all proteins in SWISS-PROT and TrEMBL, an increase of nearly 15% since the inception of InterPro. New features of the database include improved searching capabilities and enhanced graphical user interfaces for visualisation of the data. The database is available via a webserver (http://www.ebi.ac.uk/interpro) and anonymous FTP (ftp://ftp.ebi.ac.uk/pub/databases/interpro).",poster,cp53
Biology,p1962,d5,dacc0018d0a0c45d93599751e53c91f88fcd45b8,c53,International Conference on Learning Representations,"The InterPro Database, 2003 brings increased coverage and new features","InterPro, an integrated documentation resource of protein families, domains and functional sites, was created in 1999 as a means of amalgamating the major protein signature databases into one comprehensive resource. PROSITE, Pfam, PRINTS, ProDom, SMART and TIGRFAMs have been manually integrated and curated and are available in InterPro for text- and sequence-based searching. The results are provided in a single format that rationalises the results that would be obtained by searching the member databases individually. The latest release of InterPro contains 5629 entries describing 4280 families, 1239 domains, 95 repeats and 15 post-translational modifications. Currently, the combined signatures in InterPro cover more than 74% of all proteins in SWISS-PROT and TrEMBL, an increase of nearly 15% since the inception of InterPro. New features of the database include improved searching capabilities and enhanced graphical user interfaces for visualisation of the data. The database is available via a webserver (http://www.ebi.ac.uk/interpro) and anonymous FTP (ftp://ftp.ebi.ac.uk/pub/databases/interpro).",poster,cp53
Medicine,p1963,d1,97232c7bba5bef3ac970bf82966a8ea97cb4fa14,c77,Visualization for Computer Security,WHO global database on child growth and malnutrition,"ii The designations employed and the presentation of material do not imply the expression of any opinion whatsoever on the part of the World Health Organization concerning the legal status of any country, territory or area, its authorities, its current or former official name or the delimitation of its frontiers or boundaries. We are guilty of many errors and many faults, but our worst crime is abandoning the children, neglecting the foundation of life. Many of the things we need can wait. The child cannot. Right now is the time his bones are being formed, his blood is being made and his senses are being developed. To him we cannot answer "" Tomorrow "". His name is "" Today "". We dedicate this work to the world's children in the hope that it will alert decision-makers to how much remains to be done to ensure children's healthy growth and development. "" "" WHO/NUT/97.4 iv Acknowledgements The Programme of Nutrition appreciates the strong support from numerous individuals, institutions, governments, and nongovernmental and international organizations, without whose continual collaboration this compilation would not have been possible. A special note of gratitude is due to all those who provided standardized information and reanalyses of original data sets to conform to the database requirements. Thanks to such international cooperation in keeping the Global Database up-to-date, the Programme of Nutrition is able to present this vast compilation of data on worldwide patterns and trends in child growth and malnutrition. SD Standard deviation WHO World Health Organization Z-score (or SD-score) The deviation of an individual's value from the median value of a reference population, divided by the standard deviation of the reference population.",poster,cp77
Medicine,p1964,d1,6ecb2f55ae787363712adf6e7ba6c2812d3f0b32,c66,International Conference on Web and Social Media,miRBase: the microRNA sequence database.,Abstract,poster,cp66
Computer Science,p1964,d3,6ecb2f55ae787363712adf6e7ba6c2812d3f0b32,c66,International Conference on Web and Social Media,miRBase: the microRNA sequence database.,Abstract,poster,cp66
Medicine,p1968,d1,c7a77164a8ede4f536c2779f32aeb6bf98eff766,j398,Biophysical Journal,The nucleic acid database. A comprehensive relational database of three-dimensional structures of nucleic acids.,Abstract,fullPaper,jv398
Biology,p1968,d5,c7a77164a8ede4f536c2779f32aeb6bf98eff766,j398,Biophysical Journal,The nucleic acid database. A comprehensive relational database of three-dimensional structures of nucleic acids.,Abstract,fullPaper,jv398
Medicine,p1969,d1,c9e151ba8e59422320013d64307a17a94e018a98,j399,Environmental Health,Scopus database: a review,Abstract,fullPaper,jv399
Environmental Science,p1969,d14,c9e151ba8e59422320013d64307a17a94e018a98,j399,Environmental Health,Scopus database: a review,Abstract,fullPaper,jv399
Medicine,p1970,d1,3bee5cc2d0b6bfcd564158cd0cf1bd311dae68b7,c31,Information Security Solutions Europe,PHOSIDA 2011: the posttranslational modification database,"The primary purpose of PHOSIDA (http://www.phosida.com) is to manage posttranslational modification sites of various species ranging from bacteria to human. Since its last report, PHOSIDA has grown significantly in size and evolved in scope. It comprises more than 80 000 phosphorylated, N-glycosylated or acetylated sites from nine different species. All sites are obtained from high-resolution mass spectrometric data using the same stringent quality criteria. One of the main distinguishing features of PHOSIDA is the provision of a wide range of analysis tools. PHOSIDA is comprised of three main components: the database environment, the prediction platform and the toolkit section. The database environment integrates and combines high-resolution proteomic data with multiple annotations. High-accuracy species-specific phosphorylation and acetylation site predictors, trained on the modification sites contained in PHOSIDA, allow the in silico determination of modified sites on any protein on the basis of the primary sequence. The toolkit section contains methods that search for sequence motif matches or identify de novo consensus, sequences from large scale data sets.",poster,cp31
Computer Science,p1970,d3,3bee5cc2d0b6bfcd564158cd0cf1bd311dae68b7,c31,Information Security Solutions Europe,PHOSIDA 2011: the posttranslational modification database,"The primary purpose of PHOSIDA (http://www.phosida.com) is to manage posttranslational modification sites of various species ranging from bacteria to human. Since its last report, PHOSIDA has grown significantly in size and evolved in scope. It comprises more than 80 000 phosphorylated, N-glycosylated or acetylated sites from nine different species. All sites are obtained from high-resolution mass spectrometric data using the same stringent quality criteria. One of the main distinguishing features of PHOSIDA is the provision of a wide range of analysis tools. PHOSIDA is comprised of three main components: the database environment, the prediction platform and the toolkit section. The database environment integrates and combines high-resolution proteomic data with multiple annotations. High-accuracy species-specific phosphorylation and acetylation site predictors, trained on the modification sites contained in PHOSIDA, allow the in silico determination of modified sites on any protein on the basis of the primary sequence. The toolkit section contains methods that search for sequence motif matches or identify de novo consensus, sequences from large scale data sets.",poster,cp31
Biology,p1970,d5,3bee5cc2d0b6bfcd564158cd0cf1bd311dae68b7,c31,Information Security Solutions Europe,PHOSIDA 2011: the posttranslational modification database,"The primary purpose of PHOSIDA (http://www.phosida.com) is to manage posttranslational modification sites of various species ranging from bacteria to human. Since its last report, PHOSIDA has grown significantly in size and evolved in scope. It comprises more than 80 000 phosphorylated, N-glycosylated or acetylated sites from nine different species. All sites are obtained from high-resolution mass spectrometric data using the same stringent quality criteria. One of the main distinguishing features of PHOSIDA is the provision of a wide range of analysis tools. PHOSIDA is comprised of three main components: the database environment, the prediction platform and the toolkit section. The database environment integrates and combines high-resolution proteomic data with multiple annotations. High-accuracy species-specific phosphorylation and acetylation site predictors, trained on the modification sites contained in PHOSIDA, allow the in silico determination of modified sites on any protein on the basis of the primary sequence. The toolkit section contains methods that search for sequence motif matches or identify de novo consensus, sequences from large scale data sets.",poster,cp31
Medicine,p1974,d1,b26281050ffcdf52ac24fe2b4d21482fba78deaa,j400,Epilepsia,The EPILEPSIAE database: An extensive electroencephalography database of epilepsy patients,"From the very beginning the seizure prediction community faced problems concerning evaluation, standardization, and reproducibility of its studies. One of the main reasons for these shortcomings was the lack of access to high‐quality long‐term electroencephalography (EEG) data. In this article we present the EPILEPSIAE database, which was made publicly available in 2012. We illustrate its content and scope. The EPILEPSIAE database provides long‐term EEG recordings of 275 patients as well as extensive metadata and standardized annotation of the data sets. It will adhere to the current standards in the field of prediction and facilitate reproducibility and comparison of those studies. Beyond seizure prediction, it may also be of considerable benefit for studies focusing on seizure detection, basic neurophysiology, and other fields.",fullPaper,jv400
Computer Science,p1974,d3,b26281050ffcdf52ac24fe2b4d21482fba78deaa,j400,Epilepsia,The EPILEPSIAE database: An extensive electroencephalography database of epilepsy patients,"From the very beginning the seizure prediction community faced problems concerning evaluation, standardization, and reproducibility of its studies. One of the main reasons for these shortcomings was the lack of access to high‐quality long‐term electroencephalography (EEG) data. In this article we present the EPILEPSIAE database, which was made publicly available in 2012. We illustrate its content and scope. The EPILEPSIAE database provides long‐term EEG recordings of 275 patients as well as extensive metadata and standardized annotation of the data sets. It will adhere to the current standards in the field of prediction and facilitate reproducibility and comparison of those studies. Beyond seizure prediction, it may also be of considerable benefit for studies focusing on seizure detection, basic neurophysiology, and other fields.",fullPaper,jv400
Medicine,p1976,d1,d5ae5a965ac5ad79128082d7d1edf9d7ab1d840b,j365,Plant Physiology,"ARAMEMNON, a Novel Database for Arabidopsis Integral Membrane Proteins1","A specialized database (DB) for Arabidopsis membrane proteins, ARAMEMNON, was designed that facilitates the interpretation of gene and protein sequence data by integrating features that are presently only available from individual sources. Using several publicly available prediction programs, putative integral membrane proteins were identified among the approximately 25,500 proteins in the Arabidopsis genome DBs. By averaging the predictions from seven programs, approximately 6,500 proteins were classified as transmembrane (TM) candidate proteins. Some 1,800 of these contain at least four TM spans and are possibly linked to transport functions. The ARAMEMNON DB enables direct comparison of the predictions of seven different TM span computation programs and the predictions of subcellular localization by eight signal peptide recognition programs. A special function displays the proteins related to the query and dynamically generates a protein family structure. As a first set of proteins from other organisms, all of the approximately 700 putative membrane proteins were extracted from the genome of the cyanobacterium Synechocystis sp. and incorporated in the ARAMEMNON DB. The ARAMEMNON DB is accessible at the URL http://aramemnon.botanik.uni-koeln.de.",fullPaper,jv365
Biology,p1976,d5,d5ae5a965ac5ad79128082d7d1edf9d7ab1d840b,j365,Plant Physiology,"ARAMEMNON, a Novel Database for Arabidopsis Integral Membrane Proteins1","A specialized database (DB) for Arabidopsis membrane proteins, ARAMEMNON, was designed that facilitates the interpretation of gene and protein sequence data by integrating features that are presently only available from individual sources. Using several publicly available prediction programs, putative integral membrane proteins were identified among the approximately 25,500 proteins in the Arabidopsis genome DBs. By averaging the predictions from seven programs, approximately 6,500 proteins were classified as transmembrane (TM) candidate proteins. Some 1,800 of these contain at least four TM spans and are possibly linked to transport functions. The ARAMEMNON DB enables direct comparison of the predictions of seven different TM span computation programs and the predictions of subcellular localization by eight signal peptide recognition programs. A special function displays the proteins related to the query and dynamically generates a protein family structure. As a first set of proteins from other organisms, all of the approximately 700 putative membrane proteins were extracted from the genome of the cyanobacterium Synechocystis sp. and incorporated in the ARAMEMNON DB. The ARAMEMNON DB is accessible at the URL http://aramemnon.botanik.uni-koeln.de.",fullPaper,jv365
Medicine,p1978,d1,e9a1699735aff36cdd1fa385165426dd18b0d9ec,c8,Frontiers in Education Conference,New developments in the InterPro database,"InterPro is an integrated resource for protein families, domains and functional sites, which integrates the following protein signature databases: PROSITE, PRINTS, ProDom, Pfam, SMART, TIGRFAMs, PIRSF, SUPERFAMILY, Gene3D and PANTHER. The latter two new member databases have been integrated since the last publication in this journal. There have been several new developments in InterPro, including an additional reading field, new database links, extensions to the web interface and additional match XML files. InterPro has always provided matches to UniProtKB proteins on the website and in the match XML file on the FTP site. Additional matches to proteins in UniParc (UniProt archive) are now available for download in the new match XML files only. The latest InterPro release (13.0) contains more than 13 000 entries, covering over 78% of all proteins in UniProtKB. The database is available for text- and sequence-based searches via a webserver (), and for download by anonymous FTP (). The InterProScan search tool is now also available via a web service at .",poster,cp8
Computer Science,p1978,d3,e9a1699735aff36cdd1fa385165426dd18b0d9ec,c8,Frontiers in Education Conference,New developments in the InterPro database,"InterPro is an integrated resource for protein families, domains and functional sites, which integrates the following protein signature databases: PROSITE, PRINTS, ProDom, Pfam, SMART, TIGRFAMs, PIRSF, SUPERFAMILY, Gene3D and PANTHER. The latter two new member databases have been integrated since the last publication in this journal. There have been several new developments in InterPro, including an additional reading field, new database links, extensions to the web interface and additional match XML files. InterPro has always provided matches to UniProtKB proteins on the website and in the match XML file on the FTP site. Additional matches to proteins in UniParc (UniProt archive) are now available for download in the new match XML files only. The latest InterPro release (13.0) contains more than 13 000 entries, covering over 78% of all proteins in UniProtKB. The database is available for text- and sequence-based searches via a webserver (), and for download by anonymous FTP (). The InterProScan search tool is now also available via a web service at .",poster,cp8
Biology,p1978,d5,e9a1699735aff36cdd1fa385165426dd18b0d9ec,c8,Frontiers in Education Conference,New developments in the InterPro database,"InterPro is an integrated resource for protein families, domains and functional sites, which integrates the following protein signature databases: PROSITE, PRINTS, ProDom, Pfam, SMART, TIGRFAMs, PIRSF, SUPERFAMILY, Gene3D and PANTHER. The latter two new member databases have been integrated since the last publication in this journal. There have been several new developments in InterPro, including an additional reading field, new database links, extensions to the web interface and additional match XML files. InterPro has always provided matches to UniProtKB proteins on the website and in the match XML file on the FTP site. Additional matches to proteins in UniParc (UniProt archive) are now available for download in the new match XML files only. The latest InterPro release (13.0) contains more than 13 000 entries, covering over 78% of all proteins in UniProtKB. The database is available for text- and sequence-based searches via a webserver (), and for download by anonymous FTP (). The InterProScan search tool is now also available via a web service at .",poster,cp8
Medicine,p1981,d1,a5c4c2b5719eff7160334259b018809dc9c4ab4b,j104,Science,Exhaustive matching of the entire protein sequence database.,"The entire protein sequence database has been exhaustively matched. Definitive mutation matrices and models for scoring gaps were obtained from the matching and used to organize the sequence database as sets of evolutionarily connected components. The methods developed are general and can be used to manage sequence data generated by major genome sequencing projects. The alignments made possible by the exhaustive matching are the starting point for successful de novo prediction of the folded structures of proteins, for reconstructing sequences of ancient proteins and metabolisms in ancient organisms, and for obtaining new perspectives in structural biochemistry.",fullPaper,jv104
Biology,p1981,d5,a5c4c2b5719eff7160334259b018809dc9c4ab4b,j104,Science,Exhaustive matching of the entire protein sequence database.,"The entire protein sequence database has been exhaustively matched. Definitive mutation matrices and models for scoring gaps were obtained from the matching and used to organize the sequence database as sets of evolutionarily connected components. The methods developed are general and can be used to manage sequence data generated by major genome sequencing projects. The alignments made possible by the exhaustive matching are the starting point for successful de novo prediction of the folded structures of proteins, for reconstructing sequences of ancient proteins and metabolisms in ancient organisms, and for obtaining new perspectives in structural biochemistry.",fullPaper,jv104
Medicine,p1982,d1,64acb315b6129061c62bfabef2ac06d1a6fff95b,c31,Information Security Solutions Europe,Human protein reference database as a discovery resource for proteomics,"The rapid pace at which genomic and proteomic data is being generated necessitates the development of tools and resources for managing data that allow integration of information from disparate sources. The Human Protein Reference Database (http://www.hprd.org) is a web-based resource based on open source technologies for protein information about several aspects of human proteins including protein-protein interactions, post-translational modifications, enzyme-substrate relationships and disease associations. This information was derived manually by a critical reading of the published literature by expert biologists and through bioinformatics analyses of the protein sequence. This database will assist in biomedical discoveries by serving as a resource of genomic and proteomic information and providing an integrated view of sequence, structure, function and protein networks in health and disease.",poster,cp31
Computer Science,p1982,d3,64acb315b6129061c62bfabef2ac06d1a6fff95b,c31,Information Security Solutions Europe,Human protein reference database as a discovery resource for proteomics,"The rapid pace at which genomic and proteomic data is being generated necessitates the development of tools and resources for managing data that allow integration of information from disparate sources. The Human Protein Reference Database (http://www.hprd.org) is a web-based resource based on open source technologies for protein information about several aspects of human proteins including protein-protein interactions, post-translational modifications, enzyme-substrate relationships and disease associations. This information was derived manually by a critical reading of the published literature by expert biologists and through bioinformatics analyses of the protein sequence. This database will assist in biomedical discoveries by serving as a resource of genomic and proteomic information and providing an integrated view of sequence, structure, function and protein networks in health and disease.",poster,cp31
Biology,p1982,d5,64acb315b6129061c62bfabef2ac06d1a6fff95b,c31,Information Security Solutions Europe,Human protein reference database as a discovery resource for proteomics,"The rapid pace at which genomic and proteomic data is being generated necessitates the development of tools and resources for managing data that allow integration of information from disparate sources. The Human Protein Reference Database (http://www.hprd.org) is a web-based resource based on open source technologies for protein information about several aspects of human proteins including protein-protein interactions, post-translational modifications, enzyme-substrate relationships and disease associations. This information was derived manually by a critical reading of the published literature by expert biologists and through bioinformatics analyses of the protein sequence. This database will assist in biomedical discoveries by serving as a resource of genomic and proteomic information and providing an integrated view of sequence, structure, function and protein networks in health and disease.",poster,cp31
Medicine,p1983,d1,4aa6aaeb14e5f881100c97cd5d06306f16ab80d0,c35,"International Conference on Internet of Things, Big Data and Security",Current Status of the Digital Database for Screening Mammography,Abstract,poster,cp35
Computer Science,p1983,d3,4aa6aaeb14e5f881100c97cd5d06306f16ab80d0,c35,"International Conference on Internet of Things, Big Data and Security",Current Status of the Digital Database for Screening Mammography,Abstract,poster,cp35
Medicine,p1986,d1,daeabbe2ac3aa90aabf10527090f548fc125e9e6,j401,Journal of Medicinal Chemistry,The PDBbind database: methodologies and updates.,"We have developed the PDBbind database to provide a comprehensive collection of binding affinities for the protein-ligand complexes in the Protein Data Bank (PDB). This paper gives a full description of the latest version, i.e., version 2003, which is an update to our recently reported work. Out of 23 790 entries in the PDB release No.107 (January 2004), 5897 entries were identified as protein-ligand complexes that meet our definition. Experimentally determined binding affinities (K(d), K(i), and IC(50)) for 1622 of these were retrieved from the references associated with these complexes. A total of 900 complexes were selected to form a ""refined set"", which is of particular value as a standard data set for docking and scoring studies. All of the final data, including binding affinity data, reference citations, and processed structural files, have been incorporated into the PDBbind database accessible on-line at http:// www.pdbbind.org/.",fullPaper,jv401
Chemistry,p1986,d8,daeabbe2ac3aa90aabf10527090f548fc125e9e6,j401,Journal of Medicinal Chemistry,The PDBbind database: methodologies and updates.,"We have developed the PDBbind database to provide a comprehensive collection of binding affinities for the protein-ligand complexes in the Protein Data Bank (PDB). This paper gives a full description of the latest version, i.e., version 2003, which is an update to our recently reported work. Out of 23 790 entries in the PDB release No.107 (January 2004), 5897 entries were identified as protein-ligand complexes that meet our definition. Experimentally determined binding affinities (K(d), K(i), and IC(50)) for 1622 of these were retrieved from the references associated with these complexes. A total of 900 complexes were selected to form a ""refined set"", which is of particular value as a standard data set for docking and scoring studies. All of the final data, including binding affinity data, reference citations, and processed structural files, have been incorporated into the PDBbind database accessible on-line at http:// www.pdbbind.org/.",fullPaper,jv401
Medicine,p1995,d1,2fe0dea4a9a243ebeaae37fec9cbbaa28b5f72a7,j402,Applied Optics,The HITRAN database: 1986 edition.,"A description and summary of the latest edition of the AFGL HITRAN molecular absorption parameters database are presented. This new database combines the information for the seven principal atmospheric absorbers and twenty-one additional molecular species previously contained on the AFGL atmospheric absorption line parameter compilation and on the trace gas compilation. In addition to updating the parameters on earlier editions of the compilation, new parameters have been added to this edition such as the self-broadened halfwidth, the temperature dependence of the air-broadened halfwidth, and the transition probability. The database contains 348043 entries between 0 and 17,900 cm(-1). A FORTRAN program is now furnished to allow rapid access to the molecular transitions and for the creation of customized output. A separate file of molecular cross sections of eleven heavy molecular species, applicable for qualitative simulation of transmission and emission in the atmosphere, has also been provided.",fullPaper,jv402
Computer Science,p1995,d3,2fe0dea4a9a243ebeaae37fec9cbbaa28b5f72a7,j402,Applied Optics,The HITRAN database: 1986 edition.,"A description and summary of the latest edition of the AFGL HITRAN molecular absorption parameters database are presented. This new database combines the information for the seven principal atmospheric absorbers and twenty-one additional molecular species previously contained on the AFGL atmospheric absorption line parameter compilation and on the trace gas compilation. In addition to updating the parameters on earlier editions of the compilation, new parameters have been added to this edition such as the self-broadened halfwidth, the temperature dependence of the air-broadened halfwidth, and the transition probability. The database contains 348043 entries between 0 and 17,900 cm(-1). A FORTRAN program is now furnished to allow rapid access to the molecular transitions and for the creation of customized output. A separate file of molecular cross sections of eleven heavy molecular species, applicable for qualitative simulation of transmission and emission in the atmosphere, has also been provided.",fullPaper,jv402
Medicine,p2032,d1,ef4784551509456237a2ba2926e30eecf485a456,j404,Journal of Applied Psychology,"Business-unit-level relationship between employee satisfaction, employee engagement, and business outcomes: a meta-analysis.","Based on 7,939 business units in 36 companies, this study used meta-analysis to examine the relationship at the business-unit level between employee satisfaction-engagement and the business-unit outcomes of customer satisfaction, productivity, profit, employee turnover, and accidents. Generalizable relationships large enough to have substantial practical value were found between unit-level employee satisfaction-engagement and these business-unit outcomes. One implication is that changes in management practices that increase employee satisfaction may increase business-unit outcomes, including profit.",fullPaper,jv404
Psychology,p2032,d10,ef4784551509456237a2ba2926e30eecf485a456,j404,Journal of Applied Psychology,"Business-unit-level relationship between employee satisfaction, employee engagement, and business outcomes: a meta-analysis.","Based on 7,939 business units in 36 companies, this study used meta-analysis to examine the relationship at the business-unit level between employee satisfaction-engagement and the business-unit outcomes of customer satisfaction, productivity, profit, employee turnover, and accidents. Generalizable relationships large enough to have substantial practical value were found between unit-level employee satisfaction-engagement and these business-unit outcomes. One implication is that changes in management practices that increase employee satisfaction may increase business-unit outcomes, including profit.",fullPaper,jv404
Medicine,p2046,d1,f641bc5e199571643c319ba8f675ef6bab9697b2,j23,Proceedings of the National Academy of Sciences of the United States of America,The impact of COVID-19 on small business outcomes and expectations,"Significance Drawing on a survey of more than 5,800 small businesses, this paper provides insight into the economic impact of coronavirus 2019 (COVID-19) on small businesses. The results shed light on both the financial fragility of many small businesses, and the significant impact COVID-19 had on these businesses in the weeks after the COVID-19–related disruptions began. The results also provide evidence on businesses’ expectations about the longer-term impact of COVID-19, as well as their perceptions of relief programs offered by the government. To explore the impact of coronavirus disease 2019 (COVID-19) on small businesses, we conducted a survey of more than 5,800 small businesses between March 28 and April 4, 2020. Several themes emerged. First, mass layoffs and closures had already occurred—just a few weeks into the crisis. Second, the risk of closure was negatively associated with the expected length of the crisis. Moreover, businesses had widely varying beliefs about the likely duration of COVID-related disruptions. Third, many small businesses are financially fragile: The median business with more than $10,000 in monthly expenses had only about 2 wk of cash on hand at the time of the survey. Fourth, the majority of businesses planned to seek funding through the Coronavirus Aid, Relief, and Economic Security (CARES) Act. However, many anticipated problems with accessing the program, such as bureaucratic hassles and difficulties establishing eligibility. Using experimental variation, we also assess take-up rates and business resilience effects for loans relative to grants-based programs.",fullPaper,jv23
Business,p2046,d9,f641bc5e199571643c319ba8f675ef6bab9697b2,j23,Proceedings of the National Academy of Sciences of the United States of America,The impact of COVID-19 on small business outcomes and expectations,"Significance Drawing on a survey of more than 5,800 small businesses, this paper provides insight into the economic impact of coronavirus 2019 (COVID-19) on small businesses. The results shed light on both the financial fragility of many small businesses, and the significant impact COVID-19 had on these businesses in the weeks after the COVID-19–related disruptions began. The results also provide evidence on businesses’ expectations about the longer-term impact of COVID-19, as well as their perceptions of relief programs offered by the government. To explore the impact of coronavirus disease 2019 (COVID-19) on small businesses, we conducted a survey of more than 5,800 small businesses between March 28 and April 4, 2020. Several themes emerged. First, mass layoffs and closures had already occurred—just a few weeks into the crisis. Second, the risk of closure was negatively associated with the expected length of the crisis. Moreover, businesses had widely varying beliefs about the likely duration of COVID-related disruptions. Third, many small businesses are financially fragile: The median business with more than $10,000 in monthly expenses had only about 2 wk of cash on hand at the time of the survey. Fourth, the majority of businesses planned to seek funding through the Coronavirus Aid, Relief, and Economic Security (CARES) Act. However, many anticipated problems with accessing the program, such as bureaucratic hassles and difficulties establishing eligibility. Using experimental variation, we also assess take-up rates and business resilience effects for loans relative to grants-based programs.",fullPaper,jv23
Medicine,p2047,d1,564a8fb25e2cf940cbe381b4f57e88d55632df28,j241,Harvard Business Review,Why business models matter.,"""Business model"" was one of the great buzz-words of the Internet boom. A company didn't need a strategy, a special competence, or even any customers--all it needed was a Web-based business model that promised wild profits in some distant, ill-defined future. Many people--investors, entrepreneurs, and executives alike--fell for the fantasy and got burned. And as the inevitable counterreaction played out, the concept of the business model fell out of fashion nearly as quickly as the .com appendage itself. That's a shame. As Joan Magretta explains, a good business model remains essential to every successful organization, whether it's a new venture or an established player. To help managers apply the concept successfully, she defines what a business model is and how it complements a smart competitive strategy. Business models are, at heart, stories that explain how enterprises work. Like a good story, a robust business model contains precisely delineated characters, plausible motivations, and a plot that turns on an insight about value. It answers certain questions: Who is the customer? How do we make money? What underlying economic logic explains how we can deliver value to customers at an appropriate cost? Every viable organization is built on a sound business model, but a business model isn't a strategy, even though many people use the terms interchangeably. Business models describe, as a system, how the pieces of a business fit together. But they don't factor in one critical dimension of performance: competition. That's the job of strategy. Illustrated with examples from companies like American Express, EuroDisney, WalMart, and Dell Computer, this article clarifies the concepts of business models and strategy, which are fundamental to every company's performance.",fullPaper,jv241
Business,p2047,d9,564a8fb25e2cf940cbe381b4f57e88d55632df28,j241,Harvard Business Review,Why business models matter.,"""Business model"" was one of the great buzz-words of the Internet boom. A company didn't need a strategy, a special competence, or even any customers--all it needed was a Web-based business model that promised wild profits in some distant, ill-defined future. Many people--investors, entrepreneurs, and executives alike--fell for the fantasy and got burned. And as the inevitable counterreaction played out, the concept of the business model fell out of fashion nearly as quickly as the .com appendage itself. That's a shame. As Joan Magretta explains, a good business model remains essential to every successful organization, whether it's a new venture or an established player. To help managers apply the concept successfully, she defines what a business model is and how it complements a smart competitive strategy. Business models are, at heart, stories that explain how enterprises work. Like a good story, a robust business model contains precisely delineated characters, plausible motivations, and a plot that turns on an insight about value. It answers certain questions: Who is the customer? How do we make money? What underlying economic logic explains how we can deliver value to customers at an appropriate cost? Every viable organization is built on a sound business model, but a business model isn't a strategy, even though many people use the terms interchangeably. Business models describe, as a system, how the pieces of a business fit together. But they don't factor in one critical dimension of performance: competition. That's the job of strategy. Illustrated with examples from companies like American Express, EuroDisney, WalMart, and Dell Computer, this article clarifies the concepts of business models and strategy, which are fundamental to every company's performance.",fullPaper,jv241
Medicine,p2074,d1,b2c2fa0267c8431e2a7161808ff6e067881c20e3,j405,Journal of Economics and Management Strategy,The impact of COVID‐19 on small business owners: Evidence from the first three months after widespread social‐distancing restrictions,"Abstract Social‐distancing restrictions and health‐ and economic‐driven demand shifts from COVID‐19 are expected to shutter many small businesses and entrepreneurial ventures, but there is very little early evidence on impacts. This paper provides the first analysis of impacts of the pandemic on the number of active small businesses in the United States using nationally representative data from the April 2020 Current Population Survey—the first month fully capturing early effects. The number of active business owners in the United States plummeted by 3.3 million or 22% over the crucial 2‐month window from February to April 2020. The drop in active business owners was the largest on record, and losses to business activity were felt across nearly all industries. African‐American businesses were hit especially hard experiencing a 41% drop in business activity. Latinx business owner activity fell by 32%, and Asian business owner activity dropped by 26%. Simulations indicate that industry compositions partly placed these groups at a higher risk of business activity losses. Immigrant business owners experienced substantial losses in business activity of 36%. Female business owners were also disproportionately affected (25% drop in business activity). Continuing the analysis in May and June, the number of active business owners remained low—down by 15% and 8%, respectively. The continued losses in May and June, and partial rebounds from April were felt across all demographic groups and most industries. These findings of early‐stage losses to small business activity have important implications for policy, income losses, and future economic inequality.",fullPaper,jv405
Business,p2074,d9,b2c2fa0267c8431e2a7161808ff6e067881c20e3,j405,Journal of Economics and Management Strategy,The impact of COVID‐19 on small business owners: Evidence from the first three months after widespread social‐distancing restrictions,"Abstract Social‐distancing restrictions and health‐ and economic‐driven demand shifts from COVID‐19 are expected to shutter many small businesses and entrepreneurial ventures, but there is very little early evidence on impacts. This paper provides the first analysis of impacts of the pandemic on the number of active small businesses in the United States using nationally representative data from the April 2020 Current Population Survey—the first month fully capturing early effects. The number of active business owners in the United States plummeted by 3.3 million or 22% over the crucial 2‐month window from February to April 2020. The drop in active business owners was the largest on record, and losses to business activity were felt across nearly all industries. African‐American businesses were hit especially hard experiencing a 41% drop in business activity. Latinx business owner activity fell by 32%, and Asian business owner activity dropped by 26%. Simulations indicate that industry compositions partly placed these groups at a higher risk of business activity losses. Immigrant business owners experienced substantial losses in business activity of 36%. Female business owners were also disproportionately affected (25% drop in business activity). Continuing the analysis in May and June, the number of active business owners remained low—down by 15% and 8%, respectively. The continued losses in May and June, and partial rebounds from April were felt across all demographic groups and most industries. These findings of early‐stage losses to small business activity have important implications for policy, income losses, and future economic inequality.",fullPaper,jv405
Medicine,p2101,d1,ca1f3605d2c4a497e38a0ab641e2f5ebf867d341,j29,Small Business Economics,The early impacts of the COVID-19 pandemic on business sales,Abstract,fullPaper,jv29
Medicine,p2132,d1,01012412146d66fdc39377378a08097d00db92eb,j413,The Veterinary Record,Business.,"Social cognitivism illuminates two of the most important processes in business, leadership and marketing, which turn out to have remarkable similarities. Effectiveness in both endeavors requires understanding cognitive and emotional mechanisms operating in the minds of individuals, and also social mechanisms by which thoughts and emotions are communicated. Both leadership and marketing require elicitation of multimodal semantic pointers that combine verbal, sensory, motor, and emotional information. Leaders and marketers all need to understand the emotional processes of their followers and customers by using theories of emotion (based on semantic pointers, not folk psychology) and modes of empathy ranging from verbal analogy to multimodal rule simulation. For employees and purchasers, emotion is a major contributor to motivations that produce intentions that lead to action.",fullPaper,jv413
Medicine,p2144,d1,80ab9cd8c50743cd05b433152c6563780eeee784,j241,Harvard Business Review,How business schools lost their way.,"Business schools are facing intense criticism for failing to impart useful skills, failing to prepare leaders, failing to instill norms of ethical behavior--and even failing to lead graduates to good corporate jobs. These criticisms come not just from students, employers, and the media but also from deans of some of America's most prestigious B schools. The root cause oftoday's crisis in management education, assert Warren G. Bennis and James O'Toole, is that business schools have adopted an inappropriate--and ultimately self-defeating--model of academic excellence. Instead of measuring themselves in terms of the competence of their graduates, or by how well their faculty members understand important drivers of business performance, they assess themselves almost solely by the rigor of their scientific research. This scientific model is predicated on the faulty assumption that business is an academic discipline like chemistry or geology when, in fact, business is a profession and business schools are professional schools--or should be. Business school deans may claim that their schools remain focused on practice, but they nevertheless hire and promote research-oriented professors who haven't spent time working in companies and are more comfortable teaching methodology than messy, multidisciplinary issues--the very stuff of management. The authors don't advocate a return to the days when business schools were glorified trade schools. But to regain relevancy, they say, business schools must rediscover the practice of business and find a way to balance the dual mission of educating practitioners and creating knowledge through research.",fullPaper,jv241
Medicine,p2147,d1,099d531217e8fab22844d9ab073e5f820e896b18,j60,Nature,Business culture and dishonesty in the banking industry,Abstract,fullPaper,jv60
Business,p2147,d9,099d531217e8fab22844d9ab073e5f820e896b18,j60,Nature,Business culture and dishonesty in the banking industry,Abstract,fullPaper,jv60
Medicine,p2263,d1,dc5df9d6c1fb1bda4ef92395ba31040e68ade395,j241,Harvard Business Review,Increasing returns and the new world of business.,"Our understanding of how markets and businesses operate was passed down to us more than a century ago by English economist Alfred Marshall. It is based on the assumption of diminishing returns: products or companies that get ahead in a market eventually run into limitations so that a predictable equilibrium of prices and market shares is reached. The theory was valid for the bulk-processing, smokestack economy of Marshall's day. But in this century, Western economies have gone from processing resources to processing information, from the application of raw energy to the application of ideas. The mechanisms that determine economic behavior have also shifted--from diminishing returns to increasing returns. Increasing returns are the tendency for that which is ahead to get further ahead and for that which is losing advantage to lose further advantage. If a product gets ahead, increasing returns can magnify the advantage, and the product can go on to lock in the market. Mechanisms of increasing returns exist alongside those of diminishing returns in all industries. But, in general, diminishing returns hold sway in the traditional, resource-processing industries. Increasing returns reign in the newer, knowledge-based industries. Modern economies have split into two interrelated worlds of business corresponding to the two types of returns. The two worlds have different economics. They differ in behavior, style, and culture. They call for different management techniques, strategies, and codes of government regulation. The author illuminates those differences by explaining how increasing returns operate in high tech and in service industries. He also offers advice to managers in knowledge-based markets.",fullPaper,jv241
Business,p2263,d9,dc5df9d6c1fb1bda4ef92395ba31040e68ade395,j241,Harvard Business Review,Increasing returns and the new world of business.,"Our understanding of how markets and businesses operate was passed down to us more than a century ago by English economist Alfred Marshall. It is based on the assumption of diminishing returns: products or companies that get ahead in a market eventually run into limitations so that a predictable equilibrium of prices and market shares is reached. The theory was valid for the bulk-processing, smokestack economy of Marshall's day. But in this century, Western economies have gone from processing resources to processing information, from the application of raw energy to the application of ideas. The mechanisms that determine economic behavior have also shifted--from diminishing returns to increasing returns. Increasing returns are the tendency for that which is ahead to get further ahead and for that which is losing advantage to lose further advantage. If a product gets ahead, increasing returns can magnify the advantage, and the product can go on to lock in the market. Mechanisms of increasing returns exist alongside those of diminishing returns in all industries. But, in general, diminishing returns hold sway in the traditional, resource-processing industries. Increasing returns reign in the newer, knowledge-based industries. Modern economies have split into two interrelated worlds of business corresponding to the two types of returns. The two worlds have different economics. They differ in behavior, style, and culture. They call for different management techniques, strategies, and codes of government regulation. The author illuminates those differences by explaining how increasing returns operate in high tech and in service industries. He also offers advice to managers in knowledge-based markets.",fullPaper,jv241
Economics,p2263,d11,dc5df9d6c1fb1bda4ef92395ba31040e68ade395,j241,Harvard Business Review,Increasing returns and the new world of business.,"Our understanding of how markets and businesses operate was passed down to us more than a century ago by English economist Alfred Marshall. It is based on the assumption of diminishing returns: products or companies that get ahead in a market eventually run into limitations so that a predictable equilibrium of prices and market shares is reached. The theory was valid for the bulk-processing, smokestack economy of Marshall's day. But in this century, Western economies have gone from processing resources to processing information, from the application of raw energy to the application of ideas. The mechanisms that determine economic behavior have also shifted--from diminishing returns to increasing returns. Increasing returns are the tendency for that which is ahead to get further ahead and for that which is losing advantage to lose further advantage. If a product gets ahead, increasing returns can magnify the advantage, and the product can go on to lock in the market. Mechanisms of increasing returns exist alongside those of diminishing returns in all industries. But, in general, diminishing returns hold sway in the traditional, resource-processing industries. Increasing returns reign in the newer, knowledge-based industries. Modern economies have split into two interrelated worlds of business corresponding to the two types of returns. The two worlds have different economics. They differ in behavior, style, and culture. They call for different management techniques, strategies, and codes of government regulation. The author illuminates those differences by explaining how increasing returns operate in high tech and in service industries. He also offers advice to managers in knowledge-based markets.",fullPaper,jv241
Medicine,p2274,d1,099f73585f23b1c923f4b68c03c064928e0ac616,j425,Nature Reviews Neuroscience,Neuromarketing: the hope and hype of neuroimaging in business,Abstract,fullPaper,jv425
Psychology,p2274,d10,099f73585f23b1c923f4b68c03c064928e0ac616,j425,Nature Reviews Neuroscience,Neuromarketing: the hope and hype of neuroimaging in business,Abstract,fullPaper,jv425
Medicine,p2300,d1,f924946ced43cc3462f2a7fc8232bcce4d8f7cf6,j409,Business Horizons,Applications of Business Analytics in Healthcare.,Abstract,fullPaper,jv409
Business,p2300,d9,f924946ced43cc3462f2a7fc8232bcce4d8f7cf6,j409,Business Horizons,Applications of Business Analytics in Healthcare.,Abstract,fullPaper,jv409
Medicine,p2393,d1,9da56d686d2037bda1eeae86116b78a9f6e33cc1,j241,Harvard Business Review,Customer value propositions in business markets.,"Examples of consumer value propositions that resonate with customers are exceptionally difficult to find. When properly constructed, value propositions force suppliers to focus on what their offerings are really worth. Once companies become disciplined about understanding their customers, they can make smarter choices about where to allocate scarce resources. The authors illuminate the pitfalls of current approaches, then present a systematic method for developing value propositions that are meaningful to target customers and that focus suppliers' efforts on creating superior value. When managers construct a customer value proposition, they often simply list all the benefits their offering might deliver. But the relative simplicity of this all-benefits approach may have a major drawback: benefit assertion. In other words, managers may claim advantages for features their customers don't care about in the least. Other suppliers try to answer the question, Why should our firm purchase your offering instead of your competitor's? But without a detailed understanding of the customer's requirements and preferences, suppliers can end up stressing points of difference that deliver relatively little value to the target customer. The pitfall with this approach is value presumption: assuming that any favorable points of difference must be valuable for the customer. Drawing on the best practices of a handful of suppliers in business markets, the authors advocate a resonating focus approach. Suppliers can provide simple, yet powerfully captivating, consumer value propositions by making their offerings superior on the few elements that matter most to target customers, demonstrating and documenting the value of this superior performance, and communicating it in a way that conveys a sophisticated understanding of the customer's business priorities.",fullPaper,jv241
Business,p2393,d9,9da56d686d2037bda1eeae86116b78a9f6e33cc1,j241,Harvard Business Review,Customer value propositions in business markets.,"Examples of consumer value propositions that resonate with customers are exceptionally difficult to find. When properly constructed, value propositions force suppliers to focus on what their offerings are really worth. Once companies become disciplined about understanding their customers, they can make smarter choices about where to allocate scarce resources. The authors illuminate the pitfalls of current approaches, then present a systematic method for developing value propositions that are meaningful to target customers and that focus suppliers' efforts on creating superior value. When managers construct a customer value proposition, they often simply list all the benefits their offering might deliver. But the relative simplicity of this all-benefits approach may have a major drawback: benefit assertion. In other words, managers may claim advantages for features their customers don't care about in the least. Other suppliers try to answer the question, Why should our firm purchase your offering instead of your competitor's? But without a detailed understanding of the customer's requirements and preferences, suppliers can end up stressing points of difference that deliver relatively little value to the target customer. The pitfall with this approach is value presumption: assuming that any favorable points of difference must be valuable for the customer. Drawing on the best practices of a handful of suppliers in business markets, the authors advocate a resonating focus approach. Suppliers can provide simple, yet powerfully captivating, consumer value propositions by making their offerings superior on the few elements that matter most to target customers, demonstrating and documenting the value of this superior performance, and communicating it in a way that conveys a sophisticated understanding of the customer's business priorities.",fullPaper,jv241
Medicine,p2403,d1,693af4a5c84e768684d4c1885308e10cd25342ae,c18,International Conference on Exploring Services Science,The structural and environmental correlates of business strategy,"An attempt is made to relate several common dimensions of business-level strategy to their organizational contexts. A model is developed that predicts the structural and environmental correlates of a strategy on the basis of the number and uncertainty of its contingencies. It is shown that strategies of complex product innovation, marketing differentiation, market breadth and conservative cost control each have pronounced but very different relationships with bureaucratic and organic structural devices of uncertainty reduction, differentiation and integration, and with environmental dynamism, heterogeneity and hostility.",poster,cp18
Medicine,p2416,d1,81809ea142aa2d43ac5adb7b24ccda5a7f1b2773,j241,Harvard Business Review,Business marketing: understand what customers value.,"How do you define the value of your market offering? Can you measure it? Few suppliers in business markets are able to answer those questions, and yet the ability to pinpoint the value of a product or service for one's customers has never been more important. By creating and using what the authors call customer value models, suppliers are able to figure out exactly what their offerings are worth to customers. Field value assessments--the most commonly used method for building customer value models--call for suppliers to gather data about their customers firsthand whenever possible. Through these assessments, a supplier can build a value model for an individual customer or for a market segment, drawing on data gathered form several customers in that segment. Suppliers can use customer value models to create competitive advantage in several ways. First, they can capitalize on the inevitable variation in customers' requirements by providing flexible market offerings. Second, they can use value models to demonstrate how a new product or service they are offering will provide greater value. Third, they can use their knowledge of how their market offerings specifically deliver value to craft persuasive value propositions. And fourth, they can use value models to provide evidence to customers of their accomplishments. Doing business based on value delivered gives companies the means to get an equitable return for their efforts. Once suppliers truly understand value, they will be able to realize the benefits of measuring and monitoring it for their customers.",fullPaper,jv241
Business,p2416,d9,81809ea142aa2d43ac5adb7b24ccda5a7f1b2773,j241,Harvard Business Review,Business marketing: understand what customers value.,"How do you define the value of your market offering? Can you measure it? Few suppliers in business markets are able to answer those questions, and yet the ability to pinpoint the value of a product or service for one's customers has never been more important. By creating and using what the authors call customer value models, suppliers are able to figure out exactly what their offerings are worth to customers. Field value assessments--the most commonly used method for building customer value models--call for suppliers to gather data about their customers firsthand whenever possible. Through these assessments, a supplier can build a value model for an individual customer or for a market segment, drawing on data gathered form several customers in that segment. Suppliers can use customer value models to create competitive advantage in several ways. First, they can capitalize on the inevitable variation in customers' requirements by providing flexible market offerings. Second, they can use value models to demonstrate how a new product or service they are offering will provide greater value. Third, they can use their knowledge of how their market offerings specifically deliver value to craft persuasive value propositions. And fourth, they can use value models to provide evidence to customers of their accomplishments. Doing business based on value delivered gives companies the means to get an equitable return for their efforts. Once suppliers truly understand value, they will be able to realize the benefits of measuring and monitoring it for their customers.",fullPaper,jv241
Medicine,p2441,d1,5ff443850cfcd7563aea613691d18e936c369b45,c111,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,"Business strategy, technology policy and firm performance","This paper presents a study of the relationships among select business strategy dimensions, technology policy dimensions, and firm performance. The research sought to identify how these variables interrelate at the bivariate and multivariate levels. Data were collected from 103 manufacturing-based firms representing 28 mature industries. Results show that technology policy choices vary widely across firms with different business strategies, and that business strategy affects the strength of the relationship between firm performance and particular technology policies.",poster,cp111
Medicine,p2453,d1,adc0d86a718b249639c0fb858f5d88b74f702166,j241,Harvard Business Review,From spare change to real change. The social sector as beta site for business innovation.,"Corporations are continually looking for new sources of innovation. Today several leading companies are beginning to find inspiration in an unexpected place: the social sector. That includes public schools, welfare-to-work programs, and the inner city. Indeed, a new paradigm for innovation is emerging: a partnership between private enterprise and public interest that produces profitable and sustainable change for both sides. In this article, the author shows how some companies are moving beyond corporate social responsibility to corporate social innovation. Traditionally, companies viewed the social sector as a dumping ground for their spare cash, obsolete equipment, and tired executives. But that mind-set hardly created lasting change. Now companies are viewing community needs as opportunities to develop ideas and demonstrate business technologies; find and serve new markets; and solve long-standing business problems. They focus on inventing sophisticated solutions through a hands-on approach. This is not charity; it is R & D, a strategic business investment. The author concedes that it isn't easy to make the new paradigm work. But she has found that successful private-public partnerships share six characteristics: a clear business agenda, strong partners committed to change, investment by both parties, rootedness in the user community, links to other organizations, and a commitment to sustain and replicate the results. Drawing on examples of successful companies such as IBM and Bell Atlantic, the author illustrates how this paradigm has produced innovations that have both business and community payoffs.",fullPaper,jv241
Business,p2453,d9,adc0d86a718b249639c0fb858f5d88b74f702166,j241,Harvard Business Review,From spare change to real change. The social sector as beta site for business innovation.,"Corporations are continually looking for new sources of innovation. Today several leading companies are beginning to find inspiration in an unexpected place: the social sector. That includes public schools, welfare-to-work programs, and the inner city. Indeed, a new paradigm for innovation is emerging: a partnership between private enterprise and public interest that produces profitable and sustainable change for both sides. In this article, the author shows how some companies are moving beyond corporate social responsibility to corporate social innovation. Traditionally, companies viewed the social sector as a dumping ground for their spare cash, obsolete equipment, and tired executives. But that mind-set hardly created lasting change. Now companies are viewing community needs as opportunities to develop ideas and demonstrate business technologies; find and serve new markets; and solve long-standing business problems. They focus on inventing sophisticated solutions through a hands-on approach. This is not charity; it is R & D, a strategic business investment. The author concedes that it isn't easy to make the new paradigm work. But she has found that successful private-public partnerships share six characteristics: a clear business agenda, strong partners committed to change, investment by both parties, rootedness in the user community, links to other organizations, and a commitment to sustain and replicate the results. Drawing on examples of successful companies such as IBM and Bell Atlantic, the author illustrates how this paradigm has produced innovations that have both business and community payoffs.",fullPaper,jv241
Medicine,p2454,d1,4048d49d30217d73515d9dcb61502f272c9aa068,j328,Health Affairs,Disruptive innovation in health care delivery: a framework for business-model innovation.,"Disruptive innovation has brought affordability and convenience to customers in a variety of industries. However, health care remains expensive and inaccessible to many because of the lack of business-model innovation. This paper explains the theory of disruptive innovation and describes how disruptive technologies must be matched with innovative business models. The authors present a framework for categorizing and developing business models in health care, followed by a discussion of some of the reasons why disruptive innovation in health care delivery has been slow.",fullPaper,jv328
Medicine,p2486,d1,442f890d3512622bb867de96ebcc7900164d1332,j241,Harvard Business Review,E-hubs: the new B2B (business-to-business) marketplaces.,"Electronic hubs--Internet-based intermediaries that host electronic marketplaces and mediate transactions among businesses--are generating a lot of interest. Companies like Ariba, Chemdex, and Commerce One have already attained breathtaking stock market capitalizations. Venture capitalists are pouring money into more business-to-business start-ups. Even industrial stalwarts like GM and Ford are making plans to set up their own Web markets. As new entrants with new business models pour into the business-to-business space, it's increasingly difficult to make sense of the landscape. This article provides a blueprint of the e-hub arena. The authors start by looking at the two dimensions of purchasing: what businesses buy--manufacturing inputs or operating inputs--and how they buy--through systematic sourcing or spot sourcing. They classify B2B e-hubs into four categories: MRO hubs, yield managers, exchanges, and catalog hubs, and they discuss each type in detail. Drilling deeper into this B2B matrix, the authors look at how e-hubs create value--through aggregation and matching--and explain when each mechanism works best. They also examine the biases of e-hubs. Although many e-hubs are neutral--they're operated by independent third parties--some favor the buyers or sellers. The authors explain the differences and discuss the pros and cons of each position. The B2B marketplace is changing rapidly. This framework helps buyers, sellers, and market makers navigate the landscape by explaining what the different hubs do and how they add the most value.",fullPaper,jv241
Business,p2486,d9,442f890d3512622bb867de96ebcc7900164d1332,j241,Harvard Business Review,E-hubs: the new B2B (business-to-business) marketplaces.,"Electronic hubs--Internet-based intermediaries that host electronic marketplaces and mediate transactions among businesses--are generating a lot of interest. Companies like Ariba, Chemdex, and Commerce One have already attained breathtaking stock market capitalizations. Venture capitalists are pouring money into more business-to-business start-ups. Even industrial stalwarts like GM and Ford are making plans to set up their own Web markets. As new entrants with new business models pour into the business-to-business space, it's increasingly difficult to make sense of the landscape. This article provides a blueprint of the e-hub arena. The authors start by looking at the two dimensions of purchasing: what businesses buy--manufacturing inputs or operating inputs--and how they buy--through systematic sourcing or spot sourcing. They classify B2B e-hubs into four categories: MRO hubs, yield managers, exchanges, and catalog hubs, and they discuss each type in detail. Drilling deeper into this B2B matrix, the authors look at how e-hubs create value--through aggregation and matching--and explain when each mechanism works best. They also examine the biases of e-hubs. Although many e-hubs are neutral--they're operated by independent third parties--some favor the buyers or sellers. The authors explain the differences and discuss the pros and cons of each position. The B2B marketplace is changing rapidly. This framework helps buyers, sellers, and market makers navigate the landscape by explaining what the different hubs do and how they add the most value.",fullPaper,jv241
Physics,p1,d2,fa5853fdef7d2f6bb68203d187ddacbbddc63a8b,c79,IEEE Symposium on Computational Intelligence for Engineering Solutions,High-Dimensional Probability: An Introduction with Applications in Data Science,"© 2018, Cambridge University Press Let us summarize our findings. A random projection of a set T in R n onto an m-dimensional subspace approximately preserves the geometry of T if m ⪆ d ( T ) . For...",poster,cp79
Physics,p57,d2,62a9d4f1763c5071cb2476c100614ba9741f036b,c116,International Society for Music Information Retrieval Conference,Data science applications to string theory,Abstract,poster,cp116
Physics,p86,d2,4b3ce468e0dd4d77d9abb7fc7edfd437202f504d,c85,IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering,Data Science,Abstract,poster,cp85
Computer Science,p86,d3,4b3ce468e0dd4d77d9abb7fc7edfd437202f504d,c85,IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering,Data Science,Abstract,poster,cp85
Physics,p114,d2,a2d7efb8b174702111e713765cbf741dff2bf9b8,c10,Americas Conference on Information Systems,Searching for Hidden Perovskite Materials for Photovoltaic Systems by Combining Data Science and First Principle Calculations,"Undiscovered perovskite materials for applications in capturing solar lights are explored through the implementation of data science. In particular, 15000 perovskite materials data is analyzed where visualization of the data reveals hidden trends and clustering of data. Random forest classification within machine learning is used in order to predict the band gap of perovskite materials where 18 physical descriptors are revealed to determine the band gap. With trained random forest, 9328 perovskite materials with potential for applications in solar cell materials are predicted. The selected Li and Na based perovskite materials within predicted 9328 perovskite materials are evaluated with first principle calculations where 11 undiscovered Li(Na) based perovskite materials fall into the ideal band gap and formation energy ranges for solar cell applications. Thus, the implementation of data science accelerates the discovery of hidden perovskite materials and the approach can be applied to the materials scienc...",poster,cp10
Physics,p127,d2,fde0b586e3bc9e5139a14493044bce9ff61706d4,c12,The Compass,Inverse statistical problems: from the inverse Ising problem to data science,"Inverse problems in statistical physics are motivated by the challenges of ‘big data’ in different fields, in particular high-throughput experiments in biology. In inverse problems, the usual procedure of statistical physics needs to be reversed: Instead of calculating observables on the basis of model parameters, we seek to infer parameters of a model based on observations. In this review, we focus on the inverse Ising problem and closely related problems, namely how to infer the coupling strengths between spins given observed spin correlations, magnetizations, or other data. We review applications of the inverse Ising problem, including the reconstruction of neural connections, protein structure determination, and the inference of gene regulatory networks. For the inverse Ising problem in equilibrium, a number of controlled and uncontrolled approximate solutions have been developed in the statistical mechanics community. A particularly strong method, pseudolikelihood, stems from statistics. We also review the inverse Ising problem in the non-equilibrium case, where the model parameters must be reconstructed based on non-equilibrium statistics.",poster,cp12
Biology,p127,d5,fde0b586e3bc9e5139a14493044bce9ff61706d4,c12,The Compass,Inverse statistical problems: from the inverse Ising problem to data science,"Inverse problems in statistical physics are motivated by the challenges of ‘big data’ in different fields, in particular high-throughput experiments in biology. In inverse problems, the usual procedure of statistical physics needs to be reversed: Instead of calculating observables on the basis of model parameters, we seek to infer parameters of a model based on observations. In this review, we focus on the inverse Ising problem and closely related problems, namely how to infer the coupling strengths between spins given observed spin correlations, magnetizations, or other data. We review applications of the inverse Ising problem, including the reconstruction of neural connections, protein structure determination, and the inference of gene regulatory networks. For the inverse Ising problem in equilibrium, a number of controlled and uncontrolled approximate solutions have been developed in the statistical mechanics community. A particularly strong method, pseudolikelihood, stems from statistics. We also review the inverse Ising problem in the non-equilibrium case, where the model parameters must be reconstructed based on non-equilibrium statistics.",poster,cp12
Physics,p226,d2,2081508c05ebe4fc0b7b2a1fd6a356a0e933186b,c19,International Conference on Conceptual Structures,Teaching Data Science,Abstract,fullPaper,cp19
Computer Science,p226,d3,2081508c05ebe4fc0b7b2a1fd6a356a0e933186b,c19,International Conference on Conceptual Structures,Teaching Data Science,Abstract,fullPaper,cp19
Mathematics,p226,d6,2081508c05ebe4fc0b7b2a1fd6a356a0e933186b,c19,International Conference on Conceptual Structures,Teaching Data Science,Abstract,fullPaper,cp19
Physics,p285,d2,ee5825861645ec9b9d11a2882f3aa15ec9e6e4dd,c21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,"ENDF/B-VII.1 Nuclear Data for Science and Technology: Cross Sections, Covariances, Fission Product Yields and Decay Data",Abstract,poster,cp21
Physics,p289,d2,8e981ddb4877615f7d5f944a8d64789d1388ee87,j90,Astrophysical Journal,LSST: From Science Drivers to Reference Design and Anticipated Data Products,"We describe here the most ambitious survey currently planned in the optical, the Large Synoptic Survey Telescope (LSST). The LSST design is driven by four main science themes: probing dark energy and dark matter, taking an inventory of the solar system, exploring the transient optical sky, and mapping the Milky Way. LSST will be a large, wide-field ground-based system designed to obtain repeated images covering the sky visible from Cerro Pachón in northern Chile. The telescope will have an 8.4 m (6.5 m effective) primary mirror, a 9.6 deg2 field of view, a 3.2-gigapixel camera, and six filters (ugrizy) covering the wavelength range 320–1050 nm. The project is in the construction phase and will begin regular survey operations by 2022. About 90% of the observing time will be devoted to a deep-wide-fast survey mode that will uniformly observe a 18,000 deg2 region about 800 times (summed over all six bands) during the anticipated 10 yr of operations and will yield a co-added map to r ∼ 27.5. These data will result in databases including about 32 trillion observations of 20 billion galaxies and a similar number of stars, and they will serve the majority of the primary science programs. The remaining 10% of the observing time will be allocated to special projects such as Very Deep and Very Fast time domain surveys, whose details are currently under discussion. We illustrate how the LSST science drivers led to these choices of system parameters, and we describe the expected data products and their characteristics.",fullPaper,jv90
Computer Science,p289,d3,8e981ddb4877615f7d5f944a8d64789d1388ee87,j90,Astrophysical Journal,LSST: From Science Drivers to Reference Design and Anticipated Data Products,"We describe here the most ambitious survey currently planned in the optical, the Large Synoptic Survey Telescope (LSST). The LSST design is driven by four main science themes: probing dark energy and dark matter, taking an inventory of the solar system, exploring the transient optical sky, and mapping the Milky Way. LSST will be a large, wide-field ground-based system designed to obtain repeated images covering the sky visible from Cerro Pachón in northern Chile. The telescope will have an 8.4 m (6.5 m effective) primary mirror, a 9.6 deg2 field of view, a 3.2-gigapixel camera, and six filters (ugrizy) covering the wavelength range 320–1050 nm. The project is in the construction phase and will begin regular survey operations by 2022. About 90% of the observing time will be devoted to a deep-wide-fast survey mode that will uniformly observe a 18,000 deg2 region about 800 times (summed over all six bands) during the anticipated 10 yr of operations and will yield a co-added map to r ∼ 27.5. These data will result in databases including about 32 trillion observations of 20 billion galaxies and a similar number of stars, and they will serve the majority of the primary science programs. The remaining 10% of the observing time will be allocated to special projects such as Very Deep and Very Fast time domain surveys, whose details are currently under discussion. We illustrate how the LSST science drivers led to these choices of system parameters, and we describe the expected data products and their characteristics.",fullPaper,jv90
Physics,p291,d2,7ec947261f5a3eabdaddb8e53d58a36b986c4e71,c92,International Symposium on Computer Architecture,ENDF/B-VII.0: Next Generation Evaluated Nuclear Data Library for Nuclear Science and Technology,Abstract,poster,cp92
Physics,p299,d2,fff51615943e08d05080682009c9c656321ef0b2,j96,MRS bulletin,NOMAD: The FAIR concept for big data-driven materials science,"Data are a crucial raw material of this century. The amount of data that have been created in materials science thus far and that continues to be created every day is immense. Without a proper infrastructure that allows for collecting and sharing data, the envisioned success of big data-driven materials science will be hampered. For the field of computational materials science, the NOMAD (Novel Materials Discovery) Center of Excellence (CoE) has changed the scientific culture toward comprehensive and findable, accessible, interoperable, and reusable (FAIR) data, opening new avenues for mining materials science big data. Novel data-analytics concepts and tools turn data into knowledge and help in the prediction of new materials and in the identification of new properties of already known materials.",fullPaper,jv96
Materials Science,p299,d7,fff51615943e08d05080682009c9c656321ef0b2,j96,MRS bulletin,NOMAD: The FAIR concept for big data-driven materials science,"Data are a crucial raw material of this century. The amount of data that have been created in materials science thus far and that continues to be created every day is immense. Without a proper infrastructure that allows for collecting and sharing data, the envisioned success of big data-driven materials science will be hampered. For the field of computational materials science, the NOMAD (Novel Materials Discovery) Center of Excellence (CoE) has changed the scientific culture toward comprehensive and findable, accessible, interoperable, and reusable (FAIR) data, opening new avenues for mining materials science big data. Novel data-analytics concepts and tools turn data into knowledge and help in the prediction of new materials and in the identification of new properties of already known materials.",fullPaper,jv96
Physics,p303,d2,8a9f26a4cee210e51c96f4016737605e31d490ee,j97,MRS Communications,A data ecosystem to support machine learning in materials science,"Facilitating the application of machine learning (ML) to materials science problems requires enhancing the data ecosystem to enable discovery and collection of data from many sources, automated dissemination of new data across the ecosystem, and the connecting of data with materials-specific ML models. Here, we present two projects, the Materials Data Facility (MDF) and the Data and Learning Hub for Science (DLHub), that address these needs. We use examples to show how MDF and DLHub capabilities can be leveraged to link data with ML models and how users can access those capabilities through web and programmatic interfaces.",fullPaper,jv97
Materials Science,p303,d7,8a9f26a4cee210e51c96f4016737605e31d490ee,j97,MRS Communications,A data ecosystem to support machine learning in materials science,"Facilitating the application of machine learning (ML) to materials science problems requires enhancing the data ecosystem to enable discovery and collection of data from many sources, automated dissemination of new data across the ecosystem, and the connecting of data with materials-specific ML models. Here, we present two projects, the Materials Data Facility (MDF) and the Data and Learning Hub for Science (DLHub), that address these needs. We use examples to show how MDF and DLHub capabilities can be leveraged to link data with ML models and how users can access those capabilities through web and programmatic interfaces.",fullPaper,jv97
Physics,p346,d2,daaf02de10f338d98ed6f58c13987b63b275825a,j114,Astronomy & Astrophysics,Gaia Early Data Release 3,"Context. Since July 2014, the Gaia mission has been engaged in a high-spatial-resolution, time-resolved, precise, accurate astrometric, and photometric survey of the entire sky.
Aims. We present the Gaia Science Alerts project, which has been in operation since 1 June 2016. We describe the system which has been developed to enable the discovery and publication of transient photometric events as seen by Gaia.
Methods. We outline the data handling, timings, and performances, and we describe the transient detection algorithms and filtering procedures needed to manage the high false alarm rate. We identify two classes of events: (1) sources which are new to Gaia and (2) Gaia sources which have undergone a significant brightening or fading. Validation of the Gaia transit astrometry and photometry was performed, followed by testing of the source environment to minimise contamination from Solar System objects, bright stars, and fainter near-neighbours.
Results. We show that the Gaia Science Alerts project suffers from very low contamination, that is there are very few false-positives. We find that the external completeness for supernovae, CE = 0.46, is dominated by the Gaia scanning law and the requirement of detections from both fields-of-view. Where we have two or more scans the internal completeness is CI = 0.79 at 3 arcsec or larger from the centres of galaxies, but it drops closer in, especially within 1 arcsec.
Conclusions. The per-transit photometry for Gaia transients is precise to 1% at G = 13, and 3% at G = 19. The per-transit astrometry is accurate to 55 mas when compared to Gaia DR2. The Gaia Science Alerts project is one of the most homogeneous and productive transient surveys in operation, and it is the only survey which covers the whole sky at high spatial resolution (subarcsecond), including the Galactic plane and bulge.",fullPaper,jv114
Physics,p394,d2,cff7b1b98da6de583bf2d5ffd496c2e6d70a794c,c24,International Conference on Data Technologies and Applications,From DFT to machine learning: recent approaches to materials science–a review,"Recent advances in experimental and computational methods are increasing the quantity and complexity of generated data. This massive amount of raw data needs to be stored and interpreted in order to advance the materials science field. Identifying correlations and patterns from large amounts of complex data is being performed by machine learning algorithms for decades. Recently, the materials science community started to invest in these methodologies to extract knowledge and insights from the accumulated data. This review follows a logical sequence starting from density functional theory as the representative instance of electronic structure methods, to the subsequent high-throughput approach, used to generate large amounts of data. Ultimately, data-driven strategies which include data mining, screening, and machine learning techniques, employ the data generated. We show how these approaches to modern computational materials science are being used to uncover complexities and design novel materials with enhanced properties. Finally, we point to the present research problems, challenges, and potential future perspectives of this new exciting field.",poster,cp24
Physics,p405,d2,e281464d9a558cc1d25084687efb75683e65d4f0,c102,ACM SIGMOD Conference,Growth rates of modern science: A bibliometric analysis based on the number of publications and cited references,"Many studies (in information science) have looked at the growth of science. In this study, we reexamine the question of the growth of science. To do this we (a) use current data up to publication year 2012 and (b) analyze the data across all disciplines and also separately for the natural sciences and for the medical and health sciences. Furthermore, the data were analyzed with an advanced statistical technique—segmented regression analysis—which can identify specific segments with similar growth rates in the history of science. The study is based on two different sets of bibliometric data: (a) the number of publications held as source items in the Web of Science (WoS, Thomson Reuters) per publication year and (b) the number of cited references in the publications of the source items per cited reference year. We looked at the rate at which science has grown since the mid‐1600s. In our analysis of cited references we identified three essential growth phases in the development of science, which each led to growth rates tripling in comparison with the previous phase: from less than 1% up to the middle of the 18th century, to 2 to 3% up to the period between the two world wars, and 8 to 9% to 2010.",poster,cp102
Computer Science,p405,d3,e281464d9a558cc1d25084687efb75683e65d4f0,c102,ACM SIGMOD Conference,Growth rates of modern science: A bibliometric analysis based on the number of publications and cited references,"Many studies (in information science) have looked at the growth of science. In this study, we reexamine the question of the growth of science. To do this we (a) use current data up to publication year 2012 and (b) analyze the data across all disciplines and also separately for the natural sciences and for the medical and health sciences. Furthermore, the data were analyzed with an advanced statistical technique—segmented regression analysis—which can identify specific segments with similar growth rates in the history of science. The study is based on two different sets of bibliometric data: (a) the number of publications held as source items in the Web of Science (WoS, Thomson Reuters) per publication year and (b) the number of cited references in the publications of the source items per cited reference year. We looked at the rate at which science has grown since the mid‐1600s. In our analysis of cited references we identified three essential growth phases in the development of science, which each led to growth rates tripling in comparison with the previous phase: from less than 1% up to the middle of the 18th century, to 2 to 3% up to the period between the two world wars, and 8 to 9% to 2010.",poster,cp102
Mathematics,p405,d6,e281464d9a558cc1d25084687efb75683e65d4f0,c102,ACM SIGMOD Conference,Growth rates of modern science: A bibliometric analysis based on the number of publications and cited references,"Many studies (in information science) have looked at the growth of science. In this study, we reexamine the question of the growth of science. To do this we (a) use current data up to publication year 2012 and (b) analyze the data across all disciplines and also separately for the natural sciences and for the medical and health sciences. Furthermore, the data were analyzed with an advanced statistical technique—segmented regression analysis—which can identify specific segments with similar growth rates in the history of science. The study is based on two different sets of bibliometric data: (a) the number of publications held as source items in the Web of Science (WoS, Thomson Reuters) per publication year and (b) the number of cited references in the publications of the source items per cited reference year. We looked at the rate at which science has grown since the mid‐1600s. In our analysis of cited references we identified three essential growth phases in the development of science, which each led to growth rates tripling in comparison with the previous phase: from less than 1% up to the middle of the 18th century, to 2 to 3% up to the period between the two world wars, and 8 to 9% to 2010.",poster,cp102
Geography,p405,d13,e281464d9a558cc1d25084687efb75683e65d4f0,c102,ACM SIGMOD Conference,Growth rates of modern science: A bibliometric analysis based on the number of publications and cited references,"Many studies (in information science) have looked at the growth of science. In this study, we reexamine the question of the growth of science. To do this we (a) use current data up to publication year 2012 and (b) analyze the data across all disciplines and also separately for the natural sciences and for the medical and health sciences. Furthermore, the data were analyzed with an advanced statistical technique—segmented regression analysis—which can identify specific segments with similar growth rates in the history of science. The study is based on two different sets of bibliometric data: (a) the number of publications held as source items in the Web of Science (WoS, Thomson Reuters) per publication year and (b) the number of cited references in the publications of the source items per cited reference year. We looked at the rate at which science has grown since the mid‐1600s. In our analysis of cited references we identified three essential growth phases in the development of science, which each led to growth rates tripling in comparison with the previous phase: from less than 1% up to the middle of the 18th century, to 2 to 3% up to the period between the two world wars, and 8 to 9% to 2010.",poster,cp102
Physics,p420,d2,846883b7761cb5fe4468d42bf9d328b5d1030175,j138,Publications of the Astronomical Society of the Pacific,"The Zwicky Transient Facility: Data Processing, Products, and Archive","The Zwicky Transient Facility (ZTF) is a new robotic time-domain survey currently in progress using the Palomar 48-inch Schmidt Telescope. ZTF uses a 47 square degree field with a 600 megapixel camera to scan the entire northern visible sky at rates of ∼3760 square degrees/hour to median depths of g ∼ 20.8 and r ∼ 20.6 mag (AB, 5σ in 30 sec). We describe the Science Data System that is housed at IPAC, Caltech. This comprises the data-processing pipelines, alert production system, data archive, and user interfaces for accessing and analyzing the products. The real-time pipeline employs a novel image-differencing algorithm, optimized for the detection of point-source transient events. These events are vetted for reliability using a machine-learned classifier and combined with contextual information to generate data-rich alert packets. The packets become available for distribution typically within 13 minutes (95th percentile) of observation. Detected events are also linked to generate candidate moving-object tracks using a novel algorithm. Objects that move fast enough to streak in the individual exposures are also extracted and vetted. We present some preliminary results of the calibration performance delivered by the real-time pipeline. The reconstructed astrometric accuracy per science image with respect to Gaia DR1 is typically 45 to 85 milliarcsec. This is the RMS per-axis on the sky for sources extracted with photometric S/N ≥ 10 and hence corresponds to the typical astrometric uncertainty down to this limit. The derived photometric precision (repeatability) at bright unsaturated fluxes varies between 8 and 25 millimag. The high end of these ranges corresponds to an airmass approaching ∼2—the limit of the public survey. Photometric calibration accuracy with respect to Pan-STARRS1 is generally better than 2%. The products support a broad range of scientific applications: fast and young supernovae; rare flux transients; variable stars; eclipsing binaries; variability from active galactic nuclei; counterparts to gravitational wave sources; a more complete census of Type Ia supernovae; and solar-system objects.",fullPaper,jv138
Computer Science,p420,d3,846883b7761cb5fe4468d42bf9d328b5d1030175,j138,Publications of the Astronomical Society of the Pacific,"The Zwicky Transient Facility: Data Processing, Products, and Archive","The Zwicky Transient Facility (ZTF) is a new robotic time-domain survey currently in progress using the Palomar 48-inch Schmidt Telescope. ZTF uses a 47 square degree field with a 600 megapixel camera to scan the entire northern visible sky at rates of ∼3760 square degrees/hour to median depths of g ∼ 20.8 and r ∼ 20.6 mag (AB, 5σ in 30 sec). We describe the Science Data System that is housed at IPAC, Caltech. This comprises the data-processing pipelines, alert production system, data archive, and user interfaces for accessing and analyzing the products. The real-time pipeline employs a novel image-differencing algorithm, optimized for the detection of point-source transient events. These events are vetted for reliability using a machine-learned classifier and combined with contextual information to generate data-rich alert packets. The packets become available for distribution typically within 13 minutes (95th percentile) of observation. Detected events are also linked to generate candidate moving-object tracks using a novel algorithm. Objects that move fast enough to streak in the individual exposures are also extracted and vetted. We present some preliminary results of the calibration performance delivered by the real-time pipeline. The reconstructed astrometric accuracy per science image with respect to Gaia DR1 is typically 45 to 85 milliarcsec. This is the RMS per-axis on the sky for sources extracted with photometric S/N ≥ 10 and hence corresponds to the typical astrometric uncertainty down to this limit. The derived photometric precision (repeatability) at bright unsaturated fluxes varies between 8 and 25 millimag. The high end of these ranges corresponds to an airmass approaching ∼2—the limit of the public survey. Photometric calibration accuracy with respect to Pan-STARRS1 is generally better than 2%. The products support a broad range of scientific applications: fast and young supernovae; rare flux transients; variable stars; eclipsing binaries; variability from active galactic nuclei; counterparts to gravitational wave sources; a more complete census of Type Ia supernovae; and solar-system objects.",fullPaper,jv138
Physics,p421,d2,3859aef8d52ef1bad6351ec25c4fe4009b184689,c43,European Conference on Machine Learning,Characterization of the LIGO detectors during their sixth science run,"In 2009-2010, the Laser Interferometer Gravitational-wave Observa- tory (LIGO) operated together with international partners Virgo and GEO600 as a network to search for gravitational waves of astrophysical origin. The sensitiv- ity of these detectors was limited by a combination of noise sources inherent to the instrumental design and its environment, often localized in time or frequency, that couple into the gravitational-wave readout. Here we review the performance of the LIGO instruments during this epoch, the work done to characterize the de- tectors and their data, and the effect that transient and continuous noise artefacts have on the sensitivity of LIGO to a variety of astrophysical sources.",poster,cp43
Physics,p455,d2,edacaedb1b2312023c4b0cf1d42bbdbed2793c65,c6,Annual Conference on Genetic and Evolutionary Computation,The Electric and Magnetic Field Instrument Suite and Integrated Science (EMFISIS) on RBSP,Abstract,poster,cp6
Physics,p460,d2,40f19bdaa4e869ab9784880fec5e9e229a2a61ab,j147,Astrophysical Journal Supplement Series,The Pan-STARRS1 Database and Data Products,"This paper describes the organization of the database and the catalog data products from the Pan-STARRS1 3π Steradian Survey. The catalog data products are available in the form of an SQL-based relational database from MAST, the Mikulski Archive for Space Telescopes at STScI. The database is described in detail, including the construction of the database, the provenance of the data, the schema, and how the database tables are related. Examples of queries for a range of science goals are included.",fullPaper,jv147
Physics,p562,d2,ca38d68c87a2f3734ca2d806ec2daceac7dbbfb4,c22,Grid Computing Environments,2022 roadmap on neuromorphic computing and engineering,"Modern computation based on von Neumann architecture is now a mature cutting-edge science. In the von Neumann architecture, processing and memory units are implemented as separate blocks interchanging data intensively and continuously. This data transfer is responsible for a large part of the power consumption. The next generation computer technology is expected to solve problems at the exascale with 1018 calculations each second. Even though these future computers will be incredibly powerful, if they are based on von Neumann type architectures, they will consume between 20 and 30 megawatts of power and will not have intrinsic physically built-in capabilities to learn or deal with complex data as our brain does. These needs can be addressed by neuromorphic computing systems which are inspired by the biological concepts of the human brain. This new generation of computers has the potential to be used for the storage and processing of large amounts of digital information with much lower power consumption than conventional processors. Among their potential future applications, an important niche is moving the control from data centers to edge devices. The aim of this roadmap is to present a snapshot of the present state of neuromorphic technology and provide an opinion on the challenges and opportunities that the future holds in the major areas of neuromorphic technology, namely materials, devices, neuromorphic circuits, neuromorphic algorithms, applications, and ethics. The roadmap is a collection of perspectives where leading researchers in the neuromorphic community provide their own view about the current state and the future challenges for each research area. We hope that this roadmap will be a useful resource by providing a concise yet comprehensive introduction to readers outside this field, for those who are just entering the field, as well as providing future perspectives for those who are well established in the neuromorphic computing community.",poster,cp22
Computer Science,p562,d3,ca38d68c87a2f3734ca2d806ec2daceac7dbbfb4,c22,Grid Computing Environments,2022 roadmap on neuromorphic computing and engineering,"Modern computation based on von Neumann architecture is now a mature cutting-edge science. In the von Neumann architecture, processing and memory units are implemented as separate blocks interchanging data intensively and continuously. This data transfer is responsible for a large part of the power consumption. The next generation computer technology is expected to solve problems at the exascale with 1018 calculations each second. Even though these future computers will be incredibly powerful, if they are based on von Neumann type architectures, they will consume between 20 and 30 megawatts of power and will not have intrinsic physically built-in capabilities to learn or deal with complex data as our brain does. These needs can be addressed by neuromorphic computing systems which are inspired by the biological concepts of the human brain. This new generation of computers has the potential to be used for the storage and processing of large amounts of digital information with much lower power consumption than conventional processors. Among their potential future applications, an important niche is moving the control from data centers to edge devices. The aim of this roadmap is to present a snapshot of the present state of neuromorphic technology and provide an opinion on the challenges and opportunities that the future holds in the major areas of neuromorphic technology, namely materials, devices, neuromorphic circuits, neuromorphic algorithms, applications, and ethics. The roadmap is a collection of perspectives where leading researchers in the neuromorphic community provide their own view about the current state and the future challenges for each research area. We hope that this roadmap will be a useful resource by providing a concise yet comprehensive introduction to readers outside this field, for those who are just entering the field, as well as providing future perspectives for those who are well established in the neuromorphic computing community.",poster,cp22
Physics,p580,d2,cd74acb268404cde24f5131a22d04d48776b283e,j169,Annual Review of Fluid Mechanics,Turbulence Modeling in the Age of Data,"Data from experiments and direct simulations of turbulence have historically been used to calibrate simple engineering models such as those based on the Reynolds-averaged Navier–Stokes (RANS) equations. In the past few years, with the availability of large and diverse data sets, researchers have begun to explore methods to systematically inform turbulence models with data, with the goal of quantifying and reducing model uncertainties. This review surveys recent developments in bounding uncertainties in RANS models via physical constraints, in adopting statistical inference to characterize model coefficients and estimate discrepancy, and in using machine learning to improve turbulence models. Key principles, achievements, and challenges are discussed. A central perspective advocated in this review is that by exploiting foundational knowledge in turbulence modeling and physical constraints, researchers can use data-driven approaches to yield useful predictive models.",fullPaper,jv169
Computer Science,p580,d3,cd74acb268404cde24f5131a22d04d48776b283e,j169,Annual Review of Fluid Mechanics,Turbulence Modeling in the Age of Data,"Data from experiments and direct simulations of turbulence have historically been used to calibrate simple engineering models such as those based on the Reynolds-averaged Navier–Stokes (RANS) equations. In the past few years, with the availability of large and diverse data sets, researchers have begun to explore methods to systematically inform turbulence models with data, with the goal of quantifying and reducing model uncertainties. This review surveys recent developments in bounding uncertainties in RANS models via physical constraints, in adopting statistical inference to characterize model coefficients and estimate discrepancy, and in using machine learning to improve turbulence models. Key principles, achievements, and challenges are discussed. A central perspective advocated in this review is that by exploiting foundational knowledge in turbulence modeling and physical constraints, researchers can use data-driven approaches to yield useful predictive models.",fullPaper,jv169
Physics,p614,d2,d843192295fc6f9d2e3e2a883de6b48abc86815d,c116,International Society for Music Information Retrieval Conference,Initial Data Engineering,Abstract,poster,cp116
Mathematics,p614,d6,d843192295fc6f9d2e3e2a883de6b48abc86815d,c116,International Society for Music Information Retrieval Conference,Initial Data Engineering,Abstract,poster,cp116
Physics,p734,d2,b83a7f29f7ac72b7b604b9f94e47d07539e7f1b3,c12,The Compass,Handbook of Accelerator Physics and Engineering,"Concerned with the design and operation of modern accelerators including linacs, synchrotrons and storage rings, this text includes both theoretical and practical matters. Chapters on beam dynamics and electromagnetic and nuclear interactions deals with linear and nonlinear single particle and collective effects including spin motion, beam-environment, beam-beam and intrabeam interactions. The impedance concept and calculations are covered along with the instabilities associated with the various interactions mentioned. A chapter on operational considerations deals with orbit error assessment and correction. Chapters on mechanical and electrical considerations present material data and aspects of component design including heat transfer and refrigeration. Hardware systems for particle sources, feedback systems, confinement and acceleration (both normal conduction and superconducting) receive detailed treatment in a subsystems chapter, which also covers beam measurement techniques and apparatus. The closing chapter gives data and methods for radiation protection computations as well as much data on radiation damage to various materials and devices.",poster,cp12
Physics,p829,d2,5e3eb22c476b889eecbb380d012231d819edf156,c112,British Machine Vision Conference,Introduction to Fourier optics,"The second edition of this respected text considerably expands the original and reflects the tremendous advances made in the discipline since 1968. All material has been thoroughly updated and several new sections explore recent progress in important areas, such as wavelength modulation, analog information processing, and holography. Fourier analysis is a ubiquitous tool with applications in diverse areas of physics and engineering. This book explores these applications in the field of optics with a special emphasis on applications to diffraction, imaging, optical data processing, and holography. This book can be used as a textbook to satisfy the needs of several different types of courses, and it is directed toward both engineers ad physicists. By varying the emphasis on different topics and specific applications, the book can be used successfully in a wide range of basic Fourier Optics or Optical Signal Processing courses.",poster,cp112
Physics,p868,d2,501608b011a1b0c6a6d1f92153c4c0f54f3d1882,c26,Decision Support Systems,DIRECT NUMERICAL SIMULATION: A Tool in Turbulence Research,"▪ Abstract We review the direct numerical simulation (DNS) of turbulent flows. We stress that DNS is a research tool, and not a brute-force solution to the Navier-Stokes equations for engineering problems. The wide range of scales in turbulent flows requires that care be taken in their numerical solution. We discuss related numerical issues such as boundary conditions and spatial and temporal discretization. Significant insight into turbulence physics has been gained from DNS of certain idealized flows that cannot be easily attained in the laboratory. We discuss some examples. Further, we illustrate the complementary nature of experiments and computations in turbulence research. Examples are provided where DNS data has been used to evaluate measurement accuracy. Finally, we consider how DNS has impacted turbulence modeling and provided further insight into the structure of turbulent boundary layers.",poster,cp26
Physics,p908,d2,0cd130421ed3aa531ebc5932875aca1861f5e26d,c0,International Conference on Machine Learning,Sensing global Birkeland currents with iridium® engineering magnetometer data,"The Iridium system consists of >70 satellites in low altitude, 780km, polar orbits in six equally spaced orbit planes with at least eleven satellites in each plane. Each satellite carries an engineering magnetometer with 48nT resolution. Techniques have been developed at JHU/APL to process Iridium magnetic field data and obtain global maps of magnetic perturbations due to field aligned currents (FACs). The noise level in the processed data is typically 70 to 100 nT and readings above the noise level occur at high magnetic latitudes consistent with an auroral signature. Time series also display well known features characteristic of FACs. Synoptic maps derived using three hours of data from 17 February, 2000, AE ∼100–300nT, show patterns consistent with the Region 1/2 currents previously determined statistically. The Indium data set provides new global measurements of the Birkeland currents in both hemispheres on time scales of a few hours or less.",poster,cp0
Physics,p940,d2,7b7e46eb83df36cb3665a96c23d2547340ed43a9,j129,Space Science Reviews,The Magnetic Electron Ion Spectrometer (MagEIS) Instruments Aboard the Radiation Belt Storm Probes (RBSP) Spacecraft,Abstract,fullPaper,jv129
Physics,p977,d2,1fb42852a47f77d26efdbacbf75cddcb294b5c3d,c58,Extreme Science and Engineering Discovery Environment,Deep-STORM: super-resolution single-molecule microscopy by deep learning,"We present an ultra-fast, precise, parameter-free method, which we term Deep-STORM, for obtaining super-resolution images from stochastically-blinking emitters, such as fluorescent molecules used for localization microscopy. Deep-STORM uses a deep convolutional neural network that can be trained on simulated data or experimental measurements, both of which are demonstrated. The method achieves state-of-the-art resolution under challenging signal-to-noise conditions and high emitter densities, and is significantly faster than existing approaches. Additionally, no prior information on the shape of the underlying structure is required, making the method applicable to any blinking data-set. We validate our approach by super-resolution image reconstruction of simulated and experimentally obtained data.",poster,cp58
Computer Science,p977,d3,1fb42852a47f77d26efdbacbf75cddcb294b5c3d,c58,Extreme Science and Engineering Discovery Environment,Deep-STORM: super-resolution single-molecule microscopy by deep learning,"We present an ultra-fast, precise, parameter-free method, which we term Deep-STORM, for obtaining super-resolution images from stochastically-blinking emitters, such as fluorescent molecules used for localization microscopy. Deep-STORM uses a deep convolutional neural network that can be trained on simulated data or experimental measurements, both of which are demonstrated. The method achieves state-of-the-art resolution under challenging signal-to-noise conditions and high emitter densities, and is significantly faster than existing approaches. Additionally, no prior information on the shape of the underlying structure is required, making the method applicable to any blinking data-set. We validate our approach by super-resolution image reconstruction of simulated and experimentally obtained data.",poster,cp58
Physics,p1087,d2,b4c9f49f726f87791768c37e0b29f5e7c8719f6d,j271,Environmental Research Letters,"How can Big Data and machine learning benefit environment and water management: a survey of methods, applications, and future directions","Big Data and machine learning (ML) technologies have the potential to impact many facets of environment and water management (EWM). Big Data are information assets characterized by high volume, velocity, variety, and veracity. Fast advances in high-resolution remote sensing techniques, smart information and communication technologies, and social media have contributed to the proliferation of Big Data in many EWM fields, such as weather forecasting, disaster management, smart water and energy management systems, and remote sensing. Big Data brings about new opportunities for data-driven discovery in EWM, but it also requires new forms of information processing, storage, retrieval, as well as analytics. ML, a subdomain of artificial intelligence (AI), refers broadly to computer algorithms that can automatically learn from data. ML may help unlock the power of Big Data if properly integrated with data analytics. Recent breakthroughs in AI and computing infrastructure have led to the fast development of powerful deep learning (DL) algorithms that can extract hierarchical features from data, with better predictive performance and less human intervention. Collectively Big Data and ML techniques have shown great potential for data-driven decision making, scientific discovery, and process optimization. These technological advances may greatly benefit EWM, especially because (1) many EWM applications (e.g. early flood warning) require the capability to extract useful information from a large amount of data in autonomous manner and in real time, (2) EWM researches have become highly multidisciplinary, and handling the ever increasing data volume/types using the traditional workflow is simply not an option, and last but not least, (3) the current theoretical knowledge about many EWM processes is still incomplete, but which may now be complemented through data-driven discovery. A large number of applications on Big Data and ML have already appeared in the EWM literature in recent years. The purposes of this survey are to (1) examine the potential and benefits of data-driven research in EWM, (2) give a synopsis of key concepts and approaches in Big Data and ML, (3) provide a systematic review of current applications, and finally (4) discuss major issues and challenges, and recommend future research directions. EWM includes a broad range of research topics. Instead of attempting to survey each individual area, this review focuses on areas of nexus in EWM, with an emphasis on elucidating the potential benefits of increased data availability and predictive analytics to improving the EWM research.",fullPaper,jv271
Computer Science,p1087,d3,b4c9f49f726f87791768c37e0b29f5e7c8719f6d,j271,Environmental Research Letters,"How can Big Data and machine learning benefit environment and water management: a survey of methods, applications, and future directions","Big Data and machine learning (ML) technologies have the potential to impact many facets of environment and water management (EWM). Big Data are information assets characterized by high volume, velocity, variety, and veracity. Fast advances in high-resolution remote sensing techniques, smart information and communication technologies, and social media have contributed to the proliferation of Big Data in many EWM fields, such as weather forecasting, disaster management, smart water and energy management systems, and remote sensing. Big Data brings about new opportunities for data-driven discovery in EWM, but it also requires new forms of information processing, storage, retrieval, as well as analytics. ML, a subdomain of artificial intelligence (AI), refers broadly to computer algorithms that can automatically learn from data. ML may help unlock the power of Big Data if properly integrated with data analytics. Recent breakthroughs in AI and computing infrastructure have led to the fast development of powerful deep learning (DL) algorithms that can extract hierarchical features from data, with better predictive performance and less human intervention. Collectively Big Data and ML techniques have shown great potential for data-driven decision making, scientific discovery, and process optimization. These technological advances may greatly benefit EWM, especially because (1) many EWM applications (e.g. early flood warning) require the capability to extract useful information from a large amount of data in autonomous manner and in real time, (2) EWM researches have become highly multidisciplinary, and handling the ever increasing data volume/types using the traditional workflow is simply not an option, and last but not least, (3) the current theoretical knowledge about many EWM processes is still incomplete, but which may now be complemented through data-driven discovery. A large number of applications on Big Data and ML have already appeared in the EWM literature in recent years. The purposes of this survey are to (1) examine the potential and benefits of data-driven research in EWM, (2) give a synopsis of key concepts and approaches in Big Data and ML, (3) provide a systematic review of current applications, and finally (4) discuss major issues and challenges, and recommend future research directions. EWM includes a broad range of research topics. Instead of attempting to survey each individual area, this review focuses on areas of nexus in EWM, with an emphasis on elucidating the potential benefits of increased data availability and predictive analytics to improving the EWM research.",fullPaper,jv271
Physics,p1179,d2,fff51615943e08d05080682009c9c656321ef0b2,j96,MRS bulletin,NOMAD: The FAIR concept for big data-driven materials science,"Data are a crucial raw material of this century. The amount of data that have been created in materials science thus far and that continues to be created every day is immense. Without a proper infrastructure that allows for collecting and sharing data, the envisioned success of big data-driven materials science will be hampered. For the field of computational materials science, the NOMAD (Novel Materials Discovery) Center of Excellence (CoE) has changed the scientific culture toward comprehensive and findable, accessible, interoperable, and reusable (FAIR) data, opening new avenues for mining materials science big data. Novel data-analytics concepts and tools turn data into knowledge and help in the prediction of new materials and in the identification of new properties of already known materials.",fullPaper,jv96
Materials Science,p1179,d7,fff51615943e08d05080682009c9c656321ef0b2,j96,MRS bulletin,NOMAD: The FAIR concept for big data-driven materials science,"Data are a crucial raw material of this century. The amount of data that have been created in materials science thus far and that continues to be created every day is immense. Without a proper infrastructure that allows for collecting and sharing data, the envisioned success of big data-driven materials science will be hampered. For the field of computational materials science, the NOMAD (Novel Materials Discovery) Center of Excellence (CoE) has changed the scientific culture toward comprehensive and findable, accessible, interoperable, and reusable (FAIR) data, opening new avenues for mining materials science big data. Novel data-analytics concepts and tools turn data into knowledge and help in the prediction of new materials and in the identification of new properties of already known materials.",fullPaper,jv96
Physics,p1388,d2,504bcd9e5e67d5f258fccca9303b2ff54e274c52,c103,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,Geospatial Big Data Handling Theory and Methods: A Review and Research Challenges,"Big data has now become a strong focus of global interest that is increasingly attracting the attention of academia, industry, government and other organizations. Big data can be situated in the disciplinary area of traditional geospatial data handling theory and methods. The increasing volume and varying format of collected geospatial big data presents challenges in storing, managing, processing, analyzing, visualizing and verifying the quality of data. This has implications for the quality of decisions made with big data. Consequently, this position paper of the International Society for Photogrammetry and Remote Sensing (ISPRS) Technical Commission II (TC II) revisits the existing geospatial data handling methods and theories to determine if they are still capable of handling emerging geospatial big data. Further, the paper synthesises problems, major issues and challenges with current developments as well as recommending what needs to be developed further in the near future. Keywords: Big data, Geospatial, Data handling, Analytics, Spatial Modeling, Review",poster,cp103
Computer Science,p1388,d3,504bcd9e5e67d5f258fccca9303b2ff54e274c52,c103,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,Geospatial Big Data Handling Theory and Methods: A Review and Research Challenges,"Big data has now become a strong focus of global interest that is increasingly attracting the attention of academia, industry, government and other organizations. Big data can be situated in the disciplinary area of traditional geospatial data handling theory and methods. The increasing volume and varying format of collected geospatial big data presents challenges in storing, managing, processing, analyzing, visualizing and verifying the quality of data. This has implications for the quality of decisions made with big data. Consequently, this position paper of the International Society for Photogrammetry and Remote Sensing (ISPRS) Technical Commission II (TC II) revisits the existing geospatial data handling methods and theories to determine if they are still capable of handling emerging geospatial big data. Further, the paper synthesises problems, major issues and challenges with current developments as well as recommending what needs to be developed further in the near future. Keywords: Big data, Geospatial, Data handling, Analytics, Spatial Modeling, Review",poster,cp103
Physics,p1393,d2,d74624e8a5c8e8227615895c40e2b3ea8367de27,c66,International Conference on Web and Social Media,"Big Questions for Social Media Big Data: Representativeness, Validity and Other Methodological Pitfalls","
 
 Large-scale databases of human activity in social media have captured scientific and policy attention, producing a flood of research and discussion. This paper considers methodological and conceptual challenges for this emergent field, with special attention to the validity and representativeness of social media big data analyses. Persistent issues include the over-emphasis of a single platform, Twitter, sampling biases arising from selection by hashtags, and vague and unrepresentative sampling frames. The socio-cultural complexity of user behavior aimed at algorithmic invisibility (such as subtweeting, mock-retweeting, use of “screen captures” for text, etc.) further complicate interpretation of big data social media. Other challenges include accounting for field effects, i.e. broadly consequential events that do not diffuse only through the network under study but affect the whole society. The application of network methods from other fields to the study of human social activity may not always be appropriate. The paper concludes with a call to action on practical steps to improve our analytic capacity in this promising, rapidly-growing field.
 
",fullPaper,cp66
Computer Science,p1393,d3,d74624e8a5c8e8227615895c40e2b3ea8367de27,c66,International Conference on Web and Social Media,"Big Questions for Social Media Big Data: Representativeness, Validity and Other Methodological Pitfalls","
 
 Large-scale databases of human activity in social media have captured scientific and policy attention, producing a flood of research and discussion. This paper considers methodological and conceptual challenges for this emergent field, with special attention to the validity and representativeness of social media big data analyses. Persistent issues include the over-emphasis of a single platform, Twitter, sampling biases arising from selection by hashtags, and vague and unrepresentative sampling frames. The socio-cultural complexity of user behavior aimed at algorithmic invisibility (such as subtweeting, mock-retweeting, use of “screen captures” for text, etc.) further complicate interpretation of big data social media. Other challenges include accounting for field effects, i.e. broadly consequential events that do not diffuse only through the network under study but affect the whole society. The application of network methods from other fields to the study of human social activity may not always be appropriate. The paper concludes with a call to action on practical steps to improve our analytic capacity in this promising, rapidly-growing field.
 
",fullPaper,cp66
Physics,p1540,d2,298d799da82395a64a3bda38ef9d2a4646828ccb,c99,Symposium on the Theory of Computing,A fast quantum mechanical algorithm for database search,"were proposed in the early 1980’s [Benioff80] and shown to be at least as powerful as classical computers an important but not surprising result, since classical computers, at the deepest level, ultimately follow the laws of quantum mechanics. The description of quantum mechanical computers was formalized in the late 80’s and early 90’s [Deutsch85][BB92] [BV93] [Yao93] and they were shown to be more powerful than classical computers on various specialized problems. In early 1994, [Shor94] demonstrated that a quantum mechanical computer could efficiently solve a well-known problem for which there was no known efficient algorithm using classical computers. This is the problem of integer factorization, i.e. testing whether or not a given integer, N, is prime, in a time which is a finite power of o (logN) . ----------------------------------------------",fullPaper,cp99
Computer Science,p1540,d3,298d799da82395a64a3bda38ef9d2a4646828ccb,c99,Symposium on the Theory of Computing,A fast quantum mechanical algorithm for database search,"were proposed in the early 1980’s [Benioff80] and shown to be at least as powerful as classical computers an important but not surprising result, since classical computers, at the deepest level, ultimately follow the laws of quantum mechanics. The description of quantum mechanical computers was formalized in the late 80’s and early 90’s [Deutsch85][BB92] [BV93] [Yao93] and they were shown to be more powerful than classical computers on various specialized problems. In early 1994, [Shor94] demonstrated that a quantum mechanical computer could efficiently solve a well-known problem for which there was no known efficient algorithm using classical computers. This is the problem of integer factorization, i.e. testing whether or not a given integer, N, is prime, in a time which is a finite power of o (logN) . ----------------------------------------------",fullPaper,cp99
Physics,p1620,d2,e57683f3eea6176441230c2b30bec2fda4984697,c58,Extreme Science and Engineering Discovery Environment,The Open Quantum Materials Database (OQMD): assessing the accuracy of DFT formation energies,Abstract,poster,cp58
Physics,p1658,d2,dc0e51b870dba980b79a3c34a044e491e7cfd5c4,c76,Group,CHIANTI - an atomic database for emission lines - I. Wavelengths greater than 50 Å,"CHIANTI consists of a critically evaluated set of atomic data and transition probabilities necessary to calculate the emission line spectrum of astrophysical plasmas. The data consist of atomic energy levels, atomic radiative data such as wavelengths, weighted oscillator strengths and A values, and electron collisional excitation rates. A set of programs that use these data to calculate the spectrum in a desired wavelength range as a function of temperature and density is also provided. A suite of programs has been developed to carry out plasma diagnostics of astrophysical plasmas. The state-of-the-art contents of the CHIANTI database will be described and some of the most important results obtained from the use of the CHIANTI database will be reviewed.",poster,cp76
Physics,p1664,d2,7fa10285fd3532aea2e64d0d6de06608266b4366,c80,International Symposium on Software Engineering for Adaptive and Self-Managing Systems,An Overview of the Global Historical Climatology Network-Daily Database,"AbstractA database is described that has been designed to fulfill the need for daily climate data over global land areas. The dataset, known as Global Historical Climatology Network (GHCN)-Daily, was developed for a wide variety of potential applications, including climate analysis and monitoring studies that require data at a daily time resolution (e.g., assessments of the frequency of heavy rainfall, heat wave duration, etc.). The dataset contains records from over 80 000 stations in 180 countries and territories, and its processing system produces the official archive for U.S. daily data. Variables commonly include maximum and minimum temperature, total daily precipitation, snowfall, and snow depth; however, about two-thirds of the stations report precipitation only. Quality assurance checks are routinely applied to the full dataset, but the data are not homogenized to account for artifacts associated with the various eras in reporting practice at any particular station (i.e., for changes in systematic...",poster,cp80
Physics,p1676,d2,34284e452a59db6a3d0a867cb01626151a7781b4,c47,EUROMICRO Conference on Software Engineering and Advanced Applications,The Dartmouth Stellar Evolution Database,"The ever-expanding depth and quality of photometric and spectroscopic observations of stellar populations increase the need for theoretical models in regions of age-composition parameter space that are largely unexplored at present. Stellar evolution models that employ the most advanced physics and cover a wide range of compositions are needed to extract the most information from current observations of both resolved and unresolved stellar populations. The Dartmouth Stellar Evolution Database is a collection of stellar evolution tracks and isochrones that spans a range of [Fe/H] from –2.5 to +0.5, [α/Fe] from –0.2 to +0.8 (for [Fe/H] ⩽ 0) or +0.2 (for [Fe/H] > 0), and initial He mass fractions from Y = 0.245 to 0.40. Stellar evolution tracks were computed for masses between 0.1 and 4 M☉, allowing isochrones to be generated for ages as young as 250 Myr. For the range in masses where the core He flash occurs, separate He-burning tracks were computed starting from the zero age horizontal branch. The tracks and isochrones have been transformed to the observational plane in a variety of photometric systems including standard UBV(RI)C, Stromgren uvby, SDSS ugriz, 2MASS JHKs, and HST ACS/WFC and WFPC2. The Dartmouth Stellar Evolution Database is accessible through a Web site at http://stellar.dartmouth.edu/~models/ where all tracks, isochrones, and additional files can be downloaded.",poster,cp47
Physics,p1692,d2,40f19bdaa4e869ab9784880fec5e9e229a2a61ab,j147,Astrophysical Journal Supplement Series,The Pan-STARRS1 Database and Data Products,"This paper describes the organization of the database and the catalog data products from the Pan-STARRS1 3π Steradian Survey. The catalog data products are available in the form of an SQL-based relational database from MAST, the Mikulski Archive for Space Telescopes at STScI. The database is described in detail, including the construction of the database, the provenance of the data, the schema, and how the database tables are related. Examples of queries for a range of science goals are included.",fullPaper,jv147
Physics,p1709,d2,99a4acc4de2097d5ed6c4ec257f284c9dc86b3b8,c5,Technical Symposium on Computer Science Education,A major upgrade of the VALD database,"Vienna atomic line database (VALD) is a collection of critically evaluated laboratory parameters for individual atomic transitions, complemented by theoretical calculations. VALD is actively used by astronomers for stellar spectroscopic studies—model atmosphere calculations, atmospheric parameter determinations, abundance analysis etc. The two first VALD releases contained parameters for atomic transitions only. In a major upgrade of VALD—VALD3, publically available from spring 2014, atomic data was complemented with parameters of molecular lines. The diatomic molecules C2, CH, CN, CO, OH, MgH, SiH, TiO are now included. For each transition VALD provides species name, wavelength, energy, quantum number J and Landé-factor of the lower and upper levels, radiative, Stark and van der Waals damping factors and a full description of electronic configurarion and term information of both levels. Compared to the previous versions we have revised and verify all of the existing data and added new measurements and calculations for transitions in the range between 20 Å and 200 microns. All transitions were complemented with term designations in a consistent way and electron configurations when available. All data were checked for consistency: listed wavelength versus Ritz, selection rules etc. A new bibliographic system keeps track of literature references for each parameter in a given transition throughout the merging process so that every selected data entry can be traced to the original source. The query language and the extraction tools can now handle various units, vacuum and air wavelengths. In the upgrade process we had an intensive interaction with data producers, which was very helpful for improving the quality of the VALD content.",poster,cp5
Physics,p1734,d2,987a42cc5a8d7c8536e7e5a308b1ba6aa15d454f,c45,IEEE Symposium on Security and Privacy,The UMIST database for astrochemistry 2012,"We present the fifth release of the UMIST Database for Astrochemistry (UDfA). The new reaction network contains 6173 gas-phase reactions, involving 467 species, 47 of which are new to this release. We have updated rate coefficients across all reaction types. We have included 1171 new anion reactions and updated and reviewed all photorates. In addition to the usual reaction network, we also now include, for download, state-specific deuterated rate coefficients, deuterium exchange reactions and a list of surface binding energies for many neutral species. Where possible, we have referenced the original source of all new and existing data. We have tested the main reaction network using a dark cloud model and a carbon-rich circumstellar envelope model. We present and briefly discuss the results of these models.",poster,cp45
Physics,p1786,d2,e606ccf581b507149e4bdaba972ab58682eef57b,c62,International Conference on Advanced Data and Information Engineering,The SIMBAD astronomical database. The CDS reference database for astronomical objects,"Simbad is the reference database for identification and bibliography of astronomical objects. It contains identifications, “basic data”, bibliography, and selected observational measurements for several million astronomical objects.  Simbad is developed and maintained by CDS, Strasbourg. Building the database contents is achieved with the help of several contributing institutes. Scanning the bibliography is the result of the collaboration of CDS with bibliographers in Observatoire de Paris (DASGAL), Institut d'Astrophysique de Paris, and Observatoire de Bordeaux. When selecting catalogues and tables for inclusion, priority is given to optimal multi-wavelength coverage of the database, and to support of research developments linked to large projects. In parallel, the systematic scanning of the bibliography reflects the diversity and general trends of astronomical research. A WWW interface to Simbad is available at: http://simbad.u-strasbg.fr/Simbad.",poster,cp62
Physics,p1834,d2,13bc458634865e23ed8ecd54473a34e705f7da10,c108,IEEE International Conference on Multimedia and Expo,THE HITRAN MOLECULAR DATABASE: EDITIONS OF 1991 AND 1992,Abstract,poster,cp108
Physics,p1856,d2,bf9e27a62e100e46c5060c7ea79a0d97ce6c1a79,c11,European Conference on Modelling and Simulation,An atomic and molecular database for analysis of submillimetre line observations,"Atomic and molecular data for the transitions of a number of astrophysically interesting species are summarized, in- cluding energy levels, statistical weights, Einstein A-coefficients and collisional rate coefficients. Available collisional data from quantum chemical calculations and experiments are extrapolated to higher energies (up to E/k ∼ 1000 K). These data, which are made publically available through the WWW at http://www.strw.leidenuniv.nl/∼moldata, are essential input for non-LTE line radiative transfer programs. An online version of a computer program for performing statistical equilibrium calcu- lations is also made available as part of the database. Comparisons of calculated emission lines using different sets of collisional rate coefficients are presented. This database should form an important tool in analyzing observations from current and future (sub)millimetre and infrared telescopes.",poster,cp11
Physics,p1880,d2,4d130f394bd16320ac41112eebd3af74a129c6be,c24,International Conference on Data Technologies and Applications,A KINETIC DATABASE FOR ASTROCHEMISTRY (KIDA),"We present a novel chemical database for gas-phase astrochemistry. Named the KInetic Database for Astrochemistry (KIDA), this database consists of gas-phase reactions with rate coefficients and uncertainties that will be vetted to the greatest extent possible. Submissions of measured and calculated rate coefficients are welcome, and will be studied by experts before inclusion into the database. Besides providing kinetic information for the interstellar medium, KIDA is planned to contain such data for planetary atmospheres and for circumstellar envelopes. Each year, a subset of the reactions in the database (kida.uva) will be provided as a network for the simulation of the chemistry of dense interstellar clouds with temperatures between 10 K and 300 K. We also provide a code, named Nahoon, to study the time-dependent gas-phase chemistry of zero-dimensional and one-dimensional interstellar sources.",poster,cp24
Chemistry,p1880,d8,4d130f394bd16320ac41112eebd3af74a129c6be,c24,International Conference on Data Technologies and Applications,A KINETIC DATABASE FOR ASTROCHEMISTRY (KIDA),"We present a novel chemical database for gas-phase astrochemistry. Named the KInetic Database for Astrochemistry (KIDA), this database consists of gas-phase reactions with rate coefficients and uncertainties that will be vetted to the greatest extent possible. Submissions of measured and calculated rate coefficients are welcome, and will be studied by experts before inclusion into the database. Besides providing kinetic information for the interstellar medium, KIDA is planned to contain such data for planetary atmospheres and for circumstellar envelopes. Each year, a subset of the reactions in the database (kida.uva) will be provided as a network for the simulation of the chemistry of dense interstellar clouds with temperatures between 10 K and 300 K. We also provide a code, named Nahoon, to study the time-dependent gas-phase chemistry of zero-dimensional and one-dimensional interstellar sources.",poster,cp24
Physics,p1883,d2,351bbaa6d0b597175a17f59f822c8e0d1fdebe03,c89,Conference on Uncertainty in Artificial Intelligence,Defining and cataloging exoplanets: the exoplanet.eu database,"We describe an online database for extrasolar planetary-mass candidates, which is updated regularly as new data are available. We first discuss criteria for inclusion of objects in the catalog: “definition” of a planet and several aspects of the confidence level of planet candidates. We are led to point out the contradiction between the sharpness of criteria for belonging to a catalog and the fuzziness of the confidence level for an object to be a planet. We then describe the different tables of extrasolar planetary systems, including unconfirmed candidates (which will ultimately be confirmed, or not, by direct imaging). It also provides online tools: histograms of planet and host star data, cross-correlations between these parameters, and some Virtual Observatory services. Future evolutions of the database are presented.",poster,cp89
Physics,p1886,d2,dd79f74b9f5537ceecd097563a20546dd60937f6,c106,International Conference on Biometrics,The VizieR database of astronomical catalogues,"VizieR is a database grouping in an homoge- neous way thousands of astronomical catalogues gath- ered for decades by the Centre de Donn ees de Strasbourg (CDS) and participating institutes. The history and cur- rent status of this large collection is briefly presented, and the way these catalogues are being standardized to t in the VizieR system is described. The architecture of the database is then presented, with emphasis on the man- agement of links and of accesses to very large catalogues. Several query interfaces are currently available, making use of the ASU protocol, for browsing purposes or for use by other data processing systems such as visualisa- tion tools.",poster,cp106
Physics,p1887,d2,48671641597e73e7e2ba3067a029d7e632fc5e59,c46,Ideal,An Overview of the Global Historical Climatology Network Temperature Database,"Abstract The Global Historical Climatology Network version 2 temperature database was released in May 1997. This century-scale dataset consists of monthly surface observations from ∼7000 stations from around the world. This archive breaks considerable new ground in the field of global climate databases. The enhancements include 1) data for additional stations to improve regional-scale analyses, particularly in previously data-sparse areas; 2) the addition of maximum–minimum temperature data to provide climate information not available in mean temperature data alone; 3) detailed assessments of data quality to increase the confidence in research results; 4) rigorous and objective homogeneity adjustments to decrease the effect of nonclimatic factors on the time series; 5) detailed metadata (e.g., population, vegetation, topography) that allow more detailed analyses to be conducted; and 6) an infrastructure for updating the archive at regular intervals so that current climatic conditions can constantly be put...",poster,cp46
Physics,p1899,d2,9f37bd6500bcc8ed946c0fd3dc9deb6334c24c12,c102,ACM SIGMOD Conference,CHIANTI—AN ATOMIC DATABASE FOR EMISSION LINES. XII. VERSION 7 OF THE DATABASE,"The CHIANTI spectral code consists of an atomic database and a suite of computer programs to calculate the optically thin spectrum of astrophysical objects and carry out spectroscopic plasma diagnostics. The database includes atomic energy levels, wavelengths, radiative transition probabilities, collision excitation rate coefficients, and ionization and recombination rate coefficients, as well as data to calculate free–free, free–bound, and two-photon continuum emission. Version 7 has been released, which includes several new ions, significant updates to existing ions, as well as Chianti-Py, the implementation of CHIANTI software in the Python programming language. All data and programs are freely available at http://www.chiantidatabase.org, while the Python interface to CHIANTI can be found at http://chiantipy.sourceforge.net.",poster,cp102
Physics,p1900,d2,711d5205c29fd772e22521cc4f9150db2f338d8e,c73,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,NIST Atomic Spectra Database,"Accurate atomic data have great importance in astrophysics, plasma research, and other fields of physics. For more than 10 years, the Atomic Spectra Database (ASD) at the National Institute of Standards and Technology has served as a convenient and robust source of critically evaluated data on tens of thousands of spectral lines and energy levels. The recent upgrade of the ASD represents a significant new step in the development of dynamic databases providing powerful tools for data analysis and manipulation. We present a detailed description of ASD 3.0 emphasizing numerous advanced features and options for data search and presentation.",poster,cp73
Computer Science,p1900,d3,711d5205c29fd772e22521cc4f9150db2f338d8e,c73,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,NIST Atomic Spectra Database,"Accurate atomic data have great importance in astrophysics, plasma research, and other fields of physics. For more than 10 years, the Atomic Spectra Database (ASD) at the National Institute of Standards and Technology has served as a convenient and robust source of critically evaluated data on tens of thousands of spectral lines and energy levels. The recent upgrade of the ASD represents a significant new step in the development of dynamic databases providing powerful tools for data analysis and manipulation. We present a detailed description of ASD 3.0 emphasizing numerous advanced features and options for data search and presentation.",poster,cp73
Materials Science,p1900,d7,711d5205c29fd772e22521cc4f9150db2f338d8e,c73,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,NIST Atomic Spectra Database,"Accurate atomic data have great importance in astrophysics, plasma research, and other fields of physics. For more than 10 years, the Atomic Spectra Database (ASD) at the National Institute of Standards and Technology has served as a convenient and robust source of critically evaluated data on tens of thousands of spectral lines and energy levels. The recent upgrade of the ASD represents a significant new step in the development of dynamic databases providing powerful tools for data analysis and manipulation. We present a detailed description of ASD 3.0 emphasizing numerous advanced features and options for data search and presentation.",poster,cp73
Physics,p1929,d2,369e98d934881e9cfd464a73e56011cb807ab104,c115,International Conference on Information Integration and Web-based Applications & Services,"THE COLOGNE DATABASE FOR MOLECULAR SPECTROSCOPY, CDMS",Abstract,poster,cp115
Chemistry,p1929,d8,369e98d934881e9cfd464a73e56011cb807ab104,c115,International Conference on Information Integration and Web-based Applications & Services,"THE COLOGNE DATABASE FOR MOLECULAR SPECTROSCOPY, CDMS",Abstract,poster,cp115
Physics,p1951,d2,a68f8e1f7b9f4144049537c766be3faec5b54786,c102,ACM SIGMOD Conference,The Exoplanet Orbit Database,"We present a database of well-determined orbital parameters of exoplanets, and their host stars’ properties. This database comprises spectroscopic orbital elements measured for 427 planets orbiting 363 stars from radial velocity and transit measurements as reported in the literature. We have also compiled fundamental transit parameters, stellar parameters, and the method used for the planets discovery. This Exoplanet Orbit Database includes all planets with robust, well measured orbital parameters reported in peer-reviewed articles. The database is available in a searchable, filterable, and sortable form online through the Exoplanets Data Explorer table, and the data can be plotted and explored through the Exoplanet Data Explorer plotter. We use the Data Explorer to generate publication-ready plots, giving three examples of the signatures of exoplanet migration and dynamical evolution: We illustrate the character of the apparent correlation between mass and period in exoplanet orbits, the different selection biases between radial velocity and transit surveys, and that the multiplanet systems show a distinct semimajor-axis distribution from apparently singleton systems.",poster,cp102
Physics,p1959,d2,1337f14678d80f22df094a3a9ad09a695d5f86ee,c65,International Symposium on Empirical Software Engineering and Measurement,THE EXTRAGALACTIC DISTANCE DATABASE,"A database can be accessed on the Web at http://edd.ifa.hawaii.edu that was developed to promote access to information related to galaxy distances. The database has three functional components. First, tables from many literature sources have been gathered and enhanced with links through a distinct galaxy naming convention. Second, comparisons of results both at the levels of parameters and of techniques have begun and are continuing, leading to increasing homogeneity and consistency of distance measurements. Third, new material is presented arising from ongoing observational programs at the University of Hawaii 2.2 m telescope, radio telescopes at Green Bank, Arecibo, and Parkes and with the Hubble Space Telescope. This new observational material is made available in tandem with related material drawn from archives and passed through common analysis pipelines.",poster,cp65
Computer Science,p2,d3,c082ccfcfe1afc696e371374146ba9380b84061e,c4,Conference on Innovative Data Systems Research,The Role of ChatGPT in Data Science: How AI-Assisted Conversational Interfaces Are Revolutionizing the Field,"ChatGPT, a conversational AI interface that utilizes natural language processing and machine learning algorithms, is taking the world by storm and is the buzzword across many sectors today. Given the likely impact of this model on data science, through this perspective article, we seek to provide an overview of the potential opportunities and challenges associated with using ChatGPT in data science, provide readers with a snapshot of its advantages, and stimulate interest in its use for data science projects. The paper discusses how ChatGPT can assist data scientists in automating various aspects of their workflow, including data cleaning and preprocessing, model training, and result interpretation. It also highlights how ChatGPT has the potential to provide new insights and improve decision-making processes by analyzing unstructured data. We then examine the advantages of ChatGPT’s architecture, including its ability to be fine-tuned for a wide range of language-related tasks and generate synthetic data. Limitations and issues are also addressed, particularly around concerns about bias and plagiarism when using ChatGPT. Overall, the paper concludes that the benefits outweigh the costs and ChatGPT has the potential to greatly enhance the productivity and accuracy of data science workflows and is likely to become an increasingly important tool for intelligence augmentation in the field of data science. ChatGPT can assist with a wide range of natural language processing tasks in data science, including language translation, sentiment analysis, and text classification. However, while ChatGPT can save time and resources compared to training a model from scratch, and can be fine-tuned for specific use cases, it may not perform well on certain tasks if it has not been specifically trained for them. Additionally, the output of ChatGPT may be difficult to interpret, which could pose challenges for decision-making in data science applications.",poster,cp4
Computer Science,p3,d3,ed6473fd5294a0639d661e02092768f364d80f39,c87,International Conference on Big Data Research,What is Data Science?,"The Communications website, https://cacm.acm.org, features more than a dozen bloggers in the BLOG@CACM community. In each issue of Communications, we'll publish selected posts or excerpts. twitter Follow us on Twitter at http://twitter.com/blogCACM https://cacm.acm.org/blogs/blog-cacm Koby Mike and Orit Hazzan consider why multiple definitions are needed to pin down data science.",poster,cp87
Sociology,p3,d4,ed6473fd5294a0639d661e02092768f364d80f39,c87,International Conference on Big Data Research,What is Data Science?,"The Communications website, https://cacm.acm.org, features more than a dozen bloggers in the BLOG@CACM community. In each issue of Communications, we'll publish selected posts or excerpts. twitter Follow us on Twitter at http://twitter.com/blogCACM https://cacm.acm.org/blogs/blog-cacm Koby Mike and Orit Hazzan consider why multiple definitions are needed to pin down data science.",poster,cp87
Computer Science,p4,d3,8a4fc5f00cd4aca61e148e46a2125c3a406719f1,c0,International Conference on Machine Learning,DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation,"We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas. Compared to prior works, DS-1000 incorporates three core features. First, our problems reflect diverse, realistic, and practical use cases since we collected them from StackOverflow. Second, our automatic evaluation is highly specific (reliable) -- across all Codex-002-predicted solutions that our evaluation accept, only 1.8% of them are incorrect; we achieve this with multi-criteria metrics, checking both functional correctness by running test cases and surface-form constraints by restricting API usages or keywords. Finally, we proactively defend against memorization by slightly modifying our problems to be different from the original StackOverflow source; consequently, models cannot answer them correctly by memorizing the solutions from pre-training. The current best public system (Codex-002) achieves 43.3% accuracy, leaving ample room for improvement. We release our benchmark at https://ds1000-code-gen.github.io.",fullPaper,cp0
Computer Science,p5,d3,f38a7b19bcb2aa9246543e3f224a5f509906d43c,c25,IEEE International Parallel and Distributed Processing Symposium,"Data Science: 8th International Conference of Pioneering Computer Scientists, Engineers and Educators, ICPCSEE 2022, Chengdu, China, August 19–22, 2022, Proceedings, Part II",Abstract,poster,cp25
Computer Science,p7,d3,8bb6a6802027c7f2489accb0559e6f02984535c9,j1,Journal of Organizational and End User Computing,"Smart Health Intelligent Healthcare Systems in the Metaverse, Artificial Intelligence, and Data Science Era","In recent decades, healthcare organizations around the world have increasingly appreciated the value of information technologies for a variety of applications. Three of the new technological advancements that are impacting smart health are metaverse, artificial intelligence (AI), and data science. The metaverse is the intersection of three major technologies — AI, augmented reality (AR), and virtual reality (VR). Metaverse provides new possibilities and potential that are still emerging. The increased work efficiency enabled by artificial intelligence and data science in hospitals not only improves patient care but also cuts costs and workload for healthcare providers.The availability of big data enables data scientists to use the data for descriptive, predictive, and prescriptive analytics. This article reviews multiple case studies and the literature on AI and data science applications in hospital administration. The article also presents unresolved research questions and challenges in the applications of the metaverse, AI, and data science in the smart health context.",fullPaper,jv1
Computer Science,p10,d3,88ca84ce36ddc1bf7b6593b7f73fe2663e2365ad,c75,International Conference on Predictive Models in Software Engineering,Foundations of Data Science,"Computer science as an academic discipline began in the 1960’s. Emphasis was on programming languages, compilers, operating systems, and the mathematical theory that supported these areas. Courses in theoretical computer science covered finite automata, regular expressions, context-free languages, and computability. In the 1970’s, the study of algorithms was added as an important component of theory. The emphasis was on making computers useful. Today, a fundamental change is taking place and the focus is more on applications. There are many reasons for this change. The merging of computing and communications has played an important role. The enhanced ability to observe, collect, and store data in the natural sciences, in commerce, and in other fields calls for a change in our understanding of data and how to handle it in the modern setting. The emergence of the web and social networks as central aspects of daily life presents both opportunities and challenges for theory.",poster,cp75
Computer Science,p12,d3,4c6e31458b0b44c1e8bd6e58f7d7e0767f7fde44,j4,IEEE Transactions on Knowledge and Data Engineering,CRISP-DM Twenty Years Later: From Data Mining Processes to Data Science Trajectories,"CRISP-DM(CRoss-Industry Standard Process for Data Mining) has its origins in the second half of the nineties and is thus about two decades old. According to many surveys and user polls it is still the de facto standard for developing data mining and knowledge discovery projects. However, undoubtedly the field has moved on considerably in twenty years, with data science now the leading term being favoured over data mining. In this paper we investigate whether, and in what contexts, CRISP-DM is still fit for purpose for data science projects. We argue that if the project is goal-directed and process-driven the process model view still largely holds. On the other hand, when data science projects become more exploratory the paths that the project can take become more varied, and a more flexible model is called for. We suggest what the outlines of such a trajectory-based model might look like and how it can be used to categorise data science projects (goal-directed, exploratory or data management). We examine seven real-life exemplars where exploratory activities play an important role and compare them against 51 use cases extracted from the NIST Big Data Public Working Group. We anticipate this categorisation can help project planning in terms of time and cost characteristics.",fullPaper,jv4
Computer Science,p15,d3,72d3ddf1f7210d7e70144bbc09f770ec411fe909,c73,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,"Machine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence","Smarter applications are making better use of the insights gleaned from data, having an impact on every industry and research discipline. At the core of this revolution lies the tools and the methods that are driving it, from processing the massive piles of data generated each day to learning from and taking useful action. Deep neural networks, along with advancements in classical machine learning and scalable general-purpose graphics processing unit (GPU) computing, have become critical components of artificial intelligence, enabling many of these astounding breakthroughs and lowering the barrier to adoption. Python continues to be the most preferred language for scientific computing, data science, and machine learning, boosting both performance and productivity by enabling the use of low-level libraries and clean high-level APIs. This survey offers insight into the field of machine learning with Python, taking a tour through important topics to identify some of the core hardware and software paradigms that have enabled it. We cover widely-used libraries and concepts, collected together for holistic comparison, with the goal of educating the reader and driving the field of Python machine learning forward.",poster,cp73
Mathematics,p15,d6,72d3ddf1f7210d7e70144bbc09f770ec411fe909,c73,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,"Machine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence","Smarter applications are making better use of the insights gleaned from data, having an impact on every industry and research discipline. At the core of this revolution lies the tools and the methods that are driving it, from processing the massive piles of data generated each day to learning from and taking useful action. Deep neural networks, along with advancements in classical machine learning and scalable general-purpose graphics processing unit (GPU) computing, have become critical components of artificial intelligence, enabling many of these astounding breakthroughs and lowering the barrier to adoption. Python continues to be the most preferred language for scientific computing, data science, and machine learning, boosting both performance and productivity by enabling the use of low-level libraries and clean high-level APIs. This survey offers insight into the field of machine learning with Python, taking a tour through important topics to identify some of the core hardware and software paradigms that have enabled it. We cover widely-used libraries and concepts, collected together for holistic comparison, with the goal of educating the reader and driving the field of Python machine learning forward.",poster,cp73
Computer Science,p16,d3,ede0a8039a561905f40777ec2ae66c2010e3f2bc,j7,Journal of Big Data,Cybersecurity data science: an overview from machine learning perspective,Abstract,fullPaper,jv7
Computer Science,p19,d3,7282f5c9d84cd47c516a6a66c5a6b8f1e2cf44b6,c1,International Conference on Human Factors in Computing Systems,AutoDS: Towards Human-Centered Automation of Data Science,"Data science (DS) projects often follow a lifecycle that consists of laborious tasks for data scientists and domain experts (e.g., data exploration, model training, etc.). Only till recently, machine learning(ML) researchers have developed promising automation techniques to aid data workers in these tasks. This paper introduces AutoDS, an automated machine learning (AutoML) system that aims to leverage the latest ML automation techniques to support data science projects. Data workers only need to upload their dataset, then the system can automatically suggest ML configurations, preprocess data, select algorithm, and train the model. These suggestions are presented to the user via a web-based graphical user interface and a notebook-based programming user interface. Our goal is to offer a systematic investigation of user interaction and perceptions of using an AutoDS system in solving a data science task. We studied AutoDS with 30 professional data scientists, where one group used AutoDS, and the other did not, to complete a data science project. As expected, AutoDS improves productivity; Yet surprisingly, we find that the models produced by the AutoDS group have higher quality and less errors, but lower human confidence scores. We reflect on the findings by presenting design implications for incorporating automation techniques into human work in the data science lifecycle.",fullPaper,cp1
Computer Science,p23,d3,f9d403c58db99e2214f43e5b1740694b9c79002f,c2,International Conference on Software Engineering,"The Art and Practice of Data Science Pipelines: A Comprehensive Study of Data Science Pipelines In Theory, In-The-Small, and In-The-Large","Increasingly larger number of software systems today are including data science components for descriptive, predictive, and prescriptive analytics. The collection of data science stages from acquisition, to cleaning/curation, to modeling, and so on are referred to as data science pipelines. To facilitate research and practice on data science pipelines, it is essential to understand their nature. What are the typical stages of a data science pipeline? How are they connected? Do the pipelines differ in the theoretical representations and that in the practice? Today we do not fully understand these architectural characteristics of data science pipelines. In this work, we present a three-pronged comprehensive study to answer this for the state-of-the-art, data science in-the-small, and data science in-the-large, Our study analyzes three datasets: a collection of 71 proposals for data science pipelines and related concepts in theory, a collection of over 105 implementations of curated data science pipelines from Kaggle competitions to understand data science in-the-small, and a collection of 21 mature data science projects from GitHub to understand data science in-the-large. Our study has led to three representations of data science pipelines that capture the essence of our subjects in theory, in-the-small, and in-the-large.",fullPaper,cp2
Computer Science,p26,d3,f9b0b10713044c146caa84704b66804aa1e82d5e,j12,Communications of the ACM,Automating data science,"Given the complexity of data science projects and related demand for human expertise, automation has the potential to transform the data science process.",fullPaper,jv12
Computer Science,p27,d3,c13147ef0b86d5ec833c272840f8f3bdacf96e7f,j13,International Journal of Data Science and Analysis,Data science: a game changer for science and innovation,Abstract,fullPaper,jv13
Computer Science,p30,d3,ab8ba0f2d290a8e56eb61e10027d0b2e57d2d544,c103,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,"How do Data Science Workers Collaborate? Roles, Workflows, and Tools","Today, the prominence of data science within organizations has given rise to teams of data science workers collaborating on extracting insights from data, as opposed to individual data scientists working alone. However, we still lack a deep understanding of how data science workers collaborate in practice. In this work, we conducted an online survey with 183 participants who work in various aspects of data science. We focused on their reported interactions with each other (e.g., managers with engineers) and with different tools (e.g., Jupyter Notebook). We found that data science teams are extremely collaborative and work with a variety of stakeholders and tools during the six common steps of a data science workflow (e.g., clean data and train model). We also found that the collaborative practices workers employ, such as documentation, vary according to the kinds of tools they use. Based on these findings, we discuss design implications for supporting data science team collaborations and future research directions.",poster,cp103
Mathematics,p30,d6,ab8ba0f2d290a8e56eb61e10027d0b2e57d2d544,c103,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,"How do Data Science Workers Collaborate? Roles, Workflows, and Tools","Today, the prominence of data science within organizations has given rise to teams of data science workers collaborating on extracting insights from data, as opposed to individual data scientists working alone. However, we still lack a deep understanding of how data science workers collaborate in practice. In this work, we conducted an online survey with 183 participants who work in various aspects of data science. We focused on their reported interactions with each other (e.g., managers with engineers) and with different tools (e.g., Jupyter Notebook). We found that data science teams are extremely collaborative and work with a variety of stakeholders and tools during the six common steps of a data science workflow (e.g., clean data and train model). We also found that the collaborative practices workers employ, such as documentation, vary according to the kinds of tools they use. Based on these findings, we discuss design implications for supporting data science team collaborations and future research directions.",poster,cp103
Computer Science,p32,d3,1ba044d3d501dddd94b479aa9dbe55a93bfa9d5f,c14,Hawaii International Conference on System Sciences,"QIIME 2: Reproducible, interactive, scalable, and extensible microbiome data science","We present QIIME 2, an open-source microbiome data science platform accessible to users spanning the microbiome research ecosystem, from scientists and engineers to clinicians and policy makers. QIIME 2 provides new features that will drive the next generation of microbiome research. These include interactive spatial and temporal analysis and visualization tools, support for metabolomics and shotgun metagenomics analysis, and automated data provenance tracking to ensure reproducible, transparent microbiome data science.",poster,cp14
Computer Science,p33,d3,306627dfa6a7f595676e7d0ac74f6162fdcb37f1,j15,Journal on spesial topics in mobile networks and applications,"Mobile Data Science and Intelligent Apps: Concepts, AI-Based Modeling and Research Directions",Abstract,fullPaper,jv15
Computer Science,p35,d3,5b9ea2abf1c5a04b3024367409284edceb741ef2,c55,Design Automation Conference,A new paradigm for accelerating clinical data science at Stanford Medicine,"Stanford Medicine is building a new data platform for our academic research community to do better clinical data science. Hospitals have a large amount of patient data and researchers have demonstrated the ability to reuse that data and AI approaches to derive novel insights, support patient care, and improve care quality. However, the traditional data warehouse and Honest Broker approaches that are in current use, are not scalable. We are establishing a new secure Big Data platform that aims to reduce time to access and analyze data. In this platform, data is anonymized to preserve patient data privacy and made available preparatory to Institutional Review Board (IRB) submission. Furthermore, the data is standardized such that analysis done at Stanford can be replicated elsewhere using the same analytical code and clinical concepts. Finally, the analytics data warehouse integrates with a secure data science computational facility to support large scale data analytics. The ecosystem is designed to bring the modern data science community to highly sensitive clinical data in a secure and collaborative big data analytics environment with a goal to enable bigger, better and faster science.",poster,cp55
Computer Science,p38,d3,a4b6f802b3f416fb1af6d723e0549c5e6d34faae,c109,Computer Vision and Pattern Recognition,Data science in economics: comprehensive review of advanced machine learning and deep learning methods,"This paper provides a state-of-the-art investigation of advances in data science in emerging economic applications. The analysis was performed on novel data science methods in four individual classes of deep learning models, hybrid deep learning models, hybrid machine learning, and ensemble models. Application domains include a wide and diverse range of economics research from the stock market, marketing, and e-commerce to corporate banking and cryptocurrency. Prisma method, a systematic literature review methodology, was used to ensure the quality of the survey. The findings reveal that the trends follow the advancement of hybrid models, which, based on the accuracy metric, outperform other learning algorithms. It is further expected that the trends will converge toward the advancements of sophisticated hybrid deep learning models.",poster,cp109
Computer Science,p39,d3,648ba966b63975c6859e1948ae3ddc30053884e4,j17,Big Data & Society,Making data science systems work,"How are data science systems made to work? It may seem that whether a system works is a function of its technical design, but it is also accomplished through ongoing forms of discretionary work by many actors. Based on six months of ethnographic fieldwork with a corporate data science team, we describe how actors involved in a corporate project negotiated what work the system should do, how it should work, and how to assess whether it works. These negotiations laid the foundation for how, why, and to what extent the system ultimately worked. We describe three main findings. First, how already-existing technologies are essential reference points to determine how and whether systems work. Second, how the situated resolution of development challenges continually reshapes the understanding of how and whether systems work. Third, how business goals, and especially their negotiated balance with data science imperatives, affect a system’s working. We conclude with takeaways for critical data studies, orienting researchers to focus on the organizational and cultural aspects of data science, the third-party platforms underlying data science systems, and ways to engage with practitioners’ imagination of how systems can and should work.",fullPaper,jv17
Computer Science,p40,d3,deb4e0c46f2e389ec5e4528f9dcee643bb6a15fa,j18,IEEE Transactions on Signal Processing,Fixed Point Strategies in Data Science,"The goal of this article is to promote the use of fixed point strategies in data science by showing that they provide a simplifying and unifying framework to model, analyze, and solve a great variety of problems. They are seen to constitute a natural environment to explain the behavior of advanced convex optimization methods as well as of recent nonlinear methods in data science which are formulated in terms of paradigms that go beyond minimization concepts and involve constructs such as Nash equilibria or monotone inclusions. We review the pertinent tools of fixed point theory and describe the main state-of-the-art algorithms for provenly convergent fixed point construction. We also incorporate additional ingredients such as stochasticity, block-implementations, and non-Euclidean metrics, which provide further enhancements. Applications to signal and image processing, machine learning, statistics, neural networks, and inverse problems are discussed.",fullPaper,jv18
Mathematics,p40,d6,deb4e0c46f2e389ec5e4528f9dcee643bb6a15fa,j18,IEEE Transactions on Signal Processing,Fixed Point Strategies in Data Science,"The goal of this article is to promote the use of fixed point strategies in data science by showing that they provide a simplifying and unifying framework to model, analyze, and solve a great variety of problems. They are seen to constitute a natural environment to explain the behavior of advanced convex optimization methods as well as of recent nonlinear methods in data science which are formulated in terms of paradigms that go beyond minimization concepts and involve constructs such as Nash equilibria or monotone inclusions. We review the pertinent tools of fixed point theory and describe the main state-of-the-art algorithms for provenly convergent fixed point construction. We also incorporate additional ingredients such as stochasticity, block-implementations, and non-Euclidean metrics, which provide further enhancements. Applications to signal and image processing, machine learning, statistics, neural networks, and inverse problems are discussed.",fullPaper,jv18
Computer Science,p42,d3,8ece479b5dfed4727d2d9b9763f777bb9a94096e,c31,Information Security Solutions Europe,Human-AI Collaboration in Data Science,"The rapid advancement of artificial intelligence (AI) is changing our lives in many ways. One application domain is data science. New techniques in automating the creation of AI, known as AutoAI or AutoML, aim to automate the work practices of data scientists. AutoAI systems are capable of autonomously ingesting and pre-processing data, engineering new features, and creating and scoring models based on a target objectives (e.g. accuracy or run-time efficiency). Though not yet widely adopted, we are interested in understanding how AutoAI will impact the practice of data science. We conducted interviews with 20 data scientists who work at a large, multinational technology company and practice data science in various business settings. Our goal is to understand their current work practices and how these practices might change with AutoAI. Reactions were mixed: while informants expressed concerns about the trend of automating their jobs, they also strongly felt it was inevitable. Despite these concerns, they remained optimistic about their future job security due to a view that the future of data science work will be a collaboration between humans and AI systems, in which both automation and human expertise are indispensable.",poster,cp31
Psychology,p42,d10,8ece479b5dfed4727d2d9b9763f777bb9a94096e,c31,Information Security Solutions Europe,Human-AI Collaboration in Data Science,"The rapid advancement of artificial intelligence (AI) is changing our lives in many ways. One application domain is data science. New techniques in automating the creation of AI, known as AutoAI or AutoML, aim to automate the work practices of data scientists. AutoAI systems are capable of autonomously ingesting and pre-processing data, engineering new features, and creating and scoring models based on a target objectives (e.g. accuracy or run-time efficiency). Though not yet widely adopted, we are interested in understanding how AutoAI will impact the practice of data science. We conducted interviews with 20 data scientists who work at a large, multinational technology company and practice data science in various business settings. Our goal is to understand their current work practices and how these practices might change with AutoAI. Reactions were mixed: while informants expressed concerns about the trend of automating their jobs, they also strongly felt it was inevitable. Despite these concerns, they remained optimistic about their future job security due to a view that the future of data science work will be a collaboration between humans and AI systems, in which both automation and human expertise are indispensable.",poster,cp31
Computer Science,p43,d3,82620503cacf8ff6f8f3490e7bdf7508f1ab2021,j20,Journal of Geographical Systems,Opening practice: supporting reproducibility and critical spatial data science,Abstract,fullPaper,jv20
Mathematics,p43,d6,82620503cacf8ff6f8f3490e7bdf7508f1ab2021,j20,Journal of Geographical Systems,Opening practice: supporting reproducibility and critical spatial data science,Abstract,fullPaper,jv20
Computer Science,p44,d3,398b154013db9d8025bf60f910bc156dedd9b40e,c1,International Conference on Human Factors in Computing Systems,"How Data Science Workers Work with Data: Discovery, Capture, Curation, Design, Creation","With the rise of big data, there has been an increasing need for practitioners in this space and an increasing opportunity for researchers to understand their workflows and design new tools to improve it. Data science is often described as data-driven, comprising unambiguous data and proceeding through regularized steps of analysis. However, this view focuses more on abstract processes, pipelines, and workflows, and less on how data science workers engage with the data. In this paper, we build on the work of other CSCW and HCI researchers in describing the ways that scientists, scholars, engineers, and others work with their data, through analyses of interviews with 21 data science professionals. We set five approaches to data along a dimension of interventions: Data as given; as captured; as curated; as designed; and as created. Data science workers develop an intuitive sense of their data and processes, and actively shape their data. We propose new ways to apply these interventions analytically, to make sense of the complex activities around data practices.",fullPaper,cp1
Computer Science,p45,d3,a11e157cb828b800426223f0a3d79e8fb122c8cc,c91,International Symposium on High-Performance Computer Architecture,Process Mining for Python (PM4Py): Bridging the Gap Between Process- and Data Science,"Process mining, i.e., a sub-field of data science focusing on the analysis of event data generated during the execution of (business) processes, has seen a tremendous change over the past two decades. Starting off in the early 2000's, with limited to no tool support, nowadays, several software tools, i.e., both open-source, e.g., ProM and Apromore, and commercial, e.g., Disco, Celonis, ProcessGold, etc., exist. The commercial process mining tools provide limited support for implementing custom algorithms. Moreover, both commercial and open-source process mining tools are often only accessible through a graphical user interface, which hampers their usage in large-scale experimental settings. Initiatives such as RapidProM provide process mining support in the scientific workflow-based data science suite RapidMiner. However, these offer limited to no support for algorithmic customization. In the light of the aforementioned, in this paper, we present a novel process mining library, i.e. Process Mining for Python (PM4Py) that aims to bridge this gap, providing integration with state-of-the-art data science libraries, e.g., pandas, numpy, scipy and scikit-learn. We provide a global overview of the architecture and functionality of PM4Py, accompanied by some representative examples of its usage.",poster,cp91
Computer Science,p47,d3,e57f360d4ffd1d3aa1dfbcc92d35b506f46f3afd,j13,International Journal of Data Science and Analysis,Data science and AI in FinTech: an overview,Abstract,fullPaper,jv13
Economics,p47,d11,e57f360d4ffd1d3aa1dfbcc92d35b506f46f3afd,j13,International Journal of Data Science and Analysis,Data science and AI in FinTech: an overview,Abstract,fullPaper,jv13
Computer Science,p48,d3,3b16bcb226bb1c87a6e63e0658be30067ed03f57,c113,International Conference on Mobile Data Management,A Systematic Review on Supervised and Unsupervised Machine Learning Algorithms for Data Science,Abstract,poster,cp113
Computer Science,p49,d3,68bee44cc58b1853c7ddcb41aa3c6d29f363637a,c3,Knowledge Discovery and Data Mining,Vamsa: Automated Provenance Tracking in Data Science Scripts,"There has recently been a lot of ongoing research in the areas of fairness, bias and explainability of machine learning (ML) models due to the self-evident or regulatory requirements of various ML applications. We make the following observation: All of these approaches require a robust understanding of the relationship between ML models and the data used to train them. In this work, we introduce the ML provenance tracking problem: the fundamental idea is to automatically track which columns in a dataset have been used to derive the features/labels of an ML model. We discuss the challenges in capturing such information in the context of Python, the most common language used by data scientists. We then present Vamsa, a modular system that extracts provenance from Python scripts without requiring any changes to the users' code. Using 26K real data science scripts, we verify the effectiveness of Vamsa in terms of coverage, and performance. We also evaluate Vamsa's accuracy on a smaller subset of manually labeled data. Our analysis shows that Vamsa's precision and recall range from 90.4% to 99.1% and its latency is in the order of milliseconds for average size scripts. Drawing from our experience in deploying ML models in production, we also present an example in which Vamsa helps automatically identify models that are affected by data corruption issues.",fullPaper,cp3
Computer Science,p52,d3,d88d39dd9c910105e7503aa43698c806d42d5198,c5,Technical Symposium on Computer Science Education,Statistical Foundations of Data Science,Abstract,poster,cp5
Computer Science,p54,d3,579b64d2179a58a8bc586c30850ea238d3c14164,j4,IEEE Transactions on Knowledge and Data Engineering,A Survey on Data Pricing: From Economics to Data Science,"Data are invaluable. How can we assess the value of data objectively, systematically and quantitatively? Pricing data, or information goods in general, has been studied and practiced in dispersed areas and principles, such as economics, marketing, electronic commerce, data management, data mining and machine learning. In this article, we present a unified, interdisciplinary and comprehensive overview of this important direction. We examine various motivations behind data pricing, understand the economics of data pricing and review the development and evolution of pricing models according to a series of fundamental principles. We discuss both digital products and data products. We also consider a series of challenges and directions for future work.",fullPaper,jv4
Economics,p54,d11,579b64d2179a58a8bc586c30850ea238d3c14164,j4,IEEE Transactions on Knowledge and Data Engineering,A Survey on Data Pricing: From Economics to Data Science,"Data are invaluable. How can we assess the value of data objectively, systematically and quantitatively? Pricing data, or information goods in general, has been studied and practiced in dispersed areas and principles, such as economics, marketing, electronic commerce, data management, data mining and machine learning. In this article, we present a unified, interdisciplinary and comprehensive overview of this important direction. We examine various motivations behind data pricing, understand the economics of data pricing and review the development and evolution of pricing models according to a series of fundamental principles. We discuss both digital products and data products. We also consider a series of challenges and directions for future work.",fullPaper,jv4
Computer Science,p58,d3,e2055b85dab66c922ccf25a28046e8e559074824,j24,Computer/law journal,Algorithmic Government: Automating Public Services and Supporting Civil Servants in using Data Science Technologies,"The data science technologies of artificial intelligence (AI), Internet of Things (IoT), big data and behavioral/predictive analytics, and blockchain are poised to revolutionize government and create a new generation of GovTech start-ups. The impact from the ‘smartification’ of public services and the national infrastructure will be much more significant in comparison to any other sector given government's function and importance to every institution and individual. Potential GovTech systems include Chatbots and intelligent assistants for public engagement, Robo-advisors to support civil servants, real-time management of the national infrastructure using IoT and blockchain, automated compliance/regulation, public records securely stored in blockchain distributed ledgers, online judicial and dispute resolution systems, and laws/statutes encoded as blockchain smart contracts. Government is potentially the major ‘client’ and also ‘public champion’ for these new data technologies. This review paper uses our simple taxonomy of government services to provide an overview of data science automation being deployed by governments world-wide. The goal of this review paper is to encourage the Computer Science community to engage with government to develop these new systems to transform public services and support the work of civil servants.",fullPaper,jv24
Computer Science,p60,d3,b017bf6879e57077b4b4e180a02747b89878d7a1,j21,Journal of Statistics and Data Science Education,A Fresh Look at Introductory Data Science,"ABSTRACT The proliferation of vast quantities of available datasets that are large and complex in nature has challenged universities to keep up with the demand for graduates trained in both the statistical and the computational set of skills required to effectively plan, acquire, manage, analyze, and communicate the findings of such data. To keep up with this demand, attracting students early on to data science as well as providing them a solid foray into the field becomes increasingly important. We present a case study of an introductory undergraduate course in data science that is designed to address these needs. Offered at Duke University, this course has no prerequisites and serves a wide audience of aspiring statistics and data science majors as well as humanities, social sciences, and natural sciences students. We discuss the unique set of challenges posed by offering such a course, and in light of these challenges, we present a detailed discussion into the pedagogical design elements, content, structure, computational infrastructure, and the assessment methodology of the course. We also offer a repository containing all teaching materials that are open-source, along with supplementary materials and the R code for reproducing the figures found in the article.",fullPaper,jv21
Mathematics,p60,d6,b017bf6879e57077b4b4e180a02747b89878d7a1,j21,Journal of Statistics and Data Science Education,A Fresh Look at Introductory Data Science,"ABSTRACT The proliferation of vast quantities of available datasets that are large and complex in nature has challenged universities to keep up with the demand for graduates trained in both the statistical and the computational set of skills required to effectively plan, acquire, manage, analyze, and communicate the findings of such data. To keep up with this demand, attracting students early on to data science as well as providing them a solid foray into the field becomes increasingly important. We present a case study of an introductory undergraduate course in data science that is designed to address these needs. Offered at Duke University, this course has no prerequisites and serves a wide audience of aspiring statistics and data science majors as well as humanities, social sciences, and natural sciences students. We discuss the unique set of challenges posed by offering such a course, and in light of these challenges, we present a detailed discussion into the pedagogical design elements, content, structure, computational infrastructure, and the assessment methodology of the course. We also offer a repository containing all teaching materials that are open-source, along with supplementary materials and the R code for reproducing the figures found in the article.",fullPaper,jv21
Computer Science,p61,d3,28cc044d5ba938472bc53d87240583982ad21663,c4,Conference on Innovative Data Systems Research,Data Management for Data Science - Towards Embedded Analytics,"textabstractThe rise of Data Science has caused an influx of new usersin need of data management solutions. However, insteadof utilizing existing RDBMS solutions they are opting touse a stack of independent solutions for data storage andprocessing glued together by scripting languages. This is notbecause they do not need the functionality that an integratedRDBMS provides, but rather because existing RDBMS im-plementations do not cater to their use case. To solve theseissues, we propose a new class of data management systems:embedded analytical systems. These systems are tightlyintegrated with analytical tools, and provide fast and effi-cient access to the data stored within them. In this work,we describe the unique challenges and opportunities w.r.tworkloads, resilience and cooperation that are faced by thisnew class of systems and the steps we have taken towardsaddressing them in the DuckDB system.",fullPaper,cp4
Computer Science,p64,d3,3746152e023e79b7d03cf12a560e473de2945d67,c98,Vision,Interrogating Data Science,"Data science provides powerful tools and methods. CSCW researchers have contributed insightfulstudies of conventional work-practices in data science - and particularly machine learning. However,recent research has shown that human skills and collaborative decision-making, play important rolesin defining data, acquiring data, curating data, designing data, and creating data. This workshopgathers researchers and practitioners together to take a collective and critical look at data sciencework-practices, and at how those work-practices make crucial and often invisible impacts on theformal work of data science. When we understand the human and social contributions to data sciencepipelines, we can constructively redesign both work and technologies for new insights, theories, andchallenges.",poster,cp98
Computer Science,p66,d3,4271faaa82eb722d079222211c30ab642bc734be,j12,Communications of the ACM,The data science life cycle,A cycle that traces ways to define the landscape of data science.,fullPaper,jv12
Computer Science,p67,d3,9f2b2111cd65cc33c0c440f4f8e548b58d8dd851,j27,Engineering,The State of the Art of Data Science and Engineering in Structural Health Monitoring,Abstract,fullPaper,jv27
Computer Science,p69,d3,1ec4d0e29455e47245edaa17368257df3efb6562,c1,International Conference on Human Factors in Computing Systems,"Practitioners Teaching Data Science in Industry and Academia: Expectations, Workflows, and Challenges","Data science has been growing in prominence across both academia and industry, but there is still little formal consensus about how to teach it. Many people who currently teach data science are practitioners such as computational researchers in academia or data scientists in industry. To understand how these practitioner-instructors pass their knowledge onto novices and how that contrasts with teaching more traditional forms of programming, we interviewed 20 data scientists who teach in settings ranging from small-group workshops to large online courses. We found that: 1) they must empathize with a diverse array of student backgrounds and expectations, 2) they teach technical workflows that integrate authentic practices surrounding code, data, and communication, 3) they face challenges involving authenticity versus abstraction in software setup, finding and curating pedagogically-relevant datasets, and acclimating students to live with uncertainty in data analysis. These findings can point the way toward better tools for data science education and help bring data literacy to more people around the world.",fullPaper,cp1
Computer Science,p70,d3,2ad13329d44c74041626a60898ccf921b0bdacd3,c4,Conference on Innovative Data Systems Research,SystemDS: A Declarative Machine Learning System for the End-to-End Data Science Lifecycle,"Machine learning (ML) applications become increasingly common in many domains. ML systems to execute these workloads include numerical computing frameworks and libraries, ML algorithm libraries, and specialized systems for deep neural networks and distributed ML. These systems focus primarily on efficient model training and scoring. However, the data science process is exploratory, and deals with underspecified objectives and a wide variety of heterogeneous data sources. Therefore, additional tools are employed for data engineering and debugging, which requires boundary crossing, unnecessary manual effort, and lacks optimization across the lifecycle. In this paper, we introduce SystemDS, an open source ML system for the end-to-end data science lifecycle from data integration, cleaning, and preparation, over local, distributed, and federated ML model training, to debugging and serving. To this end, we aim to provide a stack of declarative languages with R-like syntax for the different lifecycle tasks, and users with different expertise. We describe the overall system architecture, explain major design decisions (motivated by lessons learned from Apache SystemML), and discuss key features and research directions. Finally, we provide preliminary results that show the potential of end-to-end lifecycle optimization.",fullPaper,cp4
Computer Science,p71,d3,678da221aa156807bc2c191ed5f4bcbb0b25d421,j28,Ethics and Information Technology,Data science ethical considerations: a systematic literature review and proposed project framework,Abstract,fullPaper,jv28
Computer Science,p76,d3,2081ed6854290a479f796f2432c7951ff24232fe,c104,North American Chapter of the Association for Computational Linguistics,Human-Centered Study of Data Science Work Practices,"With the rise of big data, there has been an increasing need to understand who is working in data science and how they are doing their work. HCI and CSCW researchers have begun to examine these questions. In this workshop, we invite researchers to share their observations, experiences, hypotheses, and insights, in the hopes of developing a taxonomy of work practices and open issues in the behavioral and social study of data science and data science workers.",poster,cp104
Sociology,p76,d4,2081ed6854290a479f796f2432c7951ff24232fe,c104,North American Chapter of the Association for Computational Linguistics,Human-Centered Study of Data Science Work Practices,"With the rise of big data, there has been an increasing need to understand who is working in data science and how they are doing their work. HCI and CSCW researchers have begun to examine these questions. In this workshop, we invite researchers to share their observations, experiences, hypotheses, and insights, in the hopes of developing a taxonomy of work practices and open issues in the behavioral and social study of data science and data science workers.",poster,cp104
Computer Science,p77,d3,e564e3656395782d0ab9f801bfbe9f9f1a5d34a7,j33,Journal of Library and Information Sciences,Data science in data librarianship: Core competencies of a data librarian,"Currently, data are stored in an always-on condition, and can be globally accessed at any point, by any user. Data librarianship has its origins in the social sciences. In particular, the creation of data services and data archives, in the United Kingdom (Data Archives Services) and in the United States and Canada (Data Library Services), is a key factor for the emergence of data librarianship. The focus of data librarianship nowadays is on the creation of new library services. Data librarians are concerned with the proposition of services for data management and curation in academic libraries and other research organizations. The purpose of this paper is to understand how the complexity of the data can serve as the basis for identifying the technical skills required by data librarians. This essay is systematically divided, first introducing the concepts of data and research data in data librarianship, followed by an overview of data science as a theory, method, and technology to assess data. Next, the identification of the competencies and skills required by data scientists and data librarians are discussed. Our final remarks highlight that data librarians should understand that the complexity and novelty associated with data science praxis. Data science provides new methods and practices for data librarianship. A data librarian need not become a programmer, statistician, or database manager, but should be interested in learning about the languages and programming logic of computers, databases, and information retrieval tools. We believe that numerous kinds of scientific data research provide opportunities for a data librarian to engage with data science.",fullPaper,jv33
Computer Science,p80,d3,e799d31e1c2d80a971c1f956d62b98c0a9f27031,j36,British Journal of Educational Technology,Big Data and data science: A critical review of issues for educational research,"Big Data refers to large and disparate volumes of data generated by people, applications and machines. It is gaining increasing attention from a variety of domains, including education. What are the challenges of engaging with Big Data research in education? This paper identifies a wide range of critical issues that researchers need to consider when working with Big Data in education. The issues identified include diversity in the conception and meaning of Big Data in education, ontological, epistemological disparity, technical challenges, ethics and privacy, digital divide and digital dividend, lack of expertise and academic development opportunities to prepare educational researchers to leverage opportunities afforded by Big Data. The goal of this paper is to raise awareness on these issues and initiate a dialogue. The paper was inspired partly by insights drawn from the literature but mostly informed by experience researching into Big Data in education. [ABSTRACT FROM AUTHOR]",fullPaper,jv36
Sociology,p80,d4,e799d31e1c2d80a971c1f956d62b98c0a9f27031,j36,British Journal of Educational Technology,Big Data and data science: A critical review of issues for educational research,"Big Data refers to large and disparate volumes of data generated by people, applications and machines. It is gaining increasing attention from a variety of domains, including education. What are the challenges of engaging with Big Data research in education? This paper identifies a wide range of critical issues that researchers need to consider when working with Big Data in education. The issues identified include diversity in the conception and meaning of Big Data in education, ontological, epistemological disparity, technical challenges, ethics and privacy, digital divide and digital dividend, lack of expertise and academic development opportunities to prepare educational researchers to leverage opportunities afforded by Big Data. The goal of this paper is to raise awareness on these issues and initiate a dialogue. The paper was inspired partly by insights drawn from the literature but mostly informed by experience researching into Big Data in education. [ABSTRACT FROM AUTHOR]",fullPaper,jv36
Computer Science,p82,d3,eaa3bbe9e3c52781fd84149d8ee6e2670c90e5ec,c36,International Conference on Information Technology Based Higher Education and Training,Bayesian Optimization and Data Science,Abstract,poster,cp36
Computer Science,p88,d3,590ead4aeddbf8fea8414998b2dc3b74576a71cb,j39,CHANCE : New Directions for Statistics and Computing,A Second Chance to Get Causal Inference Right: A Classification of Data Science Tasks,"Causal inference from observational data is the goal of many data analyses in the health and social sciences. However, academic statistics has often frowned upon data analyses with a causal objective. The introduction of the term ""data science"" provides a historic opportunity to redefine data analysis in such a way that it naturally accommodates causal inference from observational data. Like others before, we organize the scientific contributions of data science into three classes of tasks: Description, prediction, and counterfactual prediction (which includes causal inference). An explicit classification of data science tasks is necessary to discuss the data, assumptions, and analytics required to successfully accomplish each task. We argue that a failure to adequately describe the role of subject-matter expert knowledge in data analysis is a source of widespread misunderstandings about data science. Specifically, causal analyses typically require not only good data and algorithms, but also domain expert knowledge. We discuss the implications for the use of data science to guide decision-making in the real world and to train data scientists.",fullPaper,jv39
Computer Science,p90,d3,8d446e7af03d7c7f9fe5828b2d9939e23a3ed7b0,j38,Geographical Analysis,A Data Science Framework for Movement,"Author(s): Dodge, S | Abstract: © 2019 The Ohio State University Movement is the driving force behind the form and function of many ecological and human systems. Identification and analysis of movement patterns that may relate to the behavior of individuals and their interactions is a fundamental first step in understanding these systems. With advances in IoT and the ubiquity of smart connected sensors to collect movement and contextual data, we now have access to a wealth of geo-enriched high-resolution tracking data. These data promise new forms of knowledge and insight into movement of humans, animals, and goods, and hence can increase our understanding of complex spatiotemporal processes such as disease outbreak, urban mobility, migration, and human-species interaction. To take advantage of the evolution in our data, we need a revolution in how we visualize, model, and analyze movement as a multidimensional process that involves space, time, and context. This paper introduces a data science paradigm with the aim of advancing research on movement.",fullPaper,jv38
Computer Science,p94,d3,daec8baf1740a09725b375729d95caebc42f61c8,c5,Technical Symposium on Computer Science Education,ACM Task Force on Data Science Education: Draft Report and Opportunity for Feedback,"The ACM Data Science Task Force was established by the ACM Education Council and tasked with articulating the role of computing discipline-specific contributions to this emerging field. This special session seeks to introduce the work of the ACM Data Science Task Force as well as to engage the SIGCSE community in this effort. Members of the task force will introduce key components of a draft report, including a summary of data science curricular efforts to date, results of ACM academic and industry surveys on data science, as well as the initial articulation of computing competencies for undergraduate programs in data science. This session should be of interest to all SIGCSE attendees, but especially faculty developing college-level curricula in Data Science.",fullPaper,cp5
Computer Science,p95,d3,4f0218eb9ed62d5acc03f02bfa24b388a66067e8,j42,TOP - An Official Journal of the Spanish Society of Statistics and Operations Research,Distance geometry and data science,Abstract,fullPaper,jv42
Mathematics,p95,d6,4f0218eb9ed62d5acc03f02bfa24b388a66067e8,j42,TOP - An Official Journal of the Spanish Society of Statistics and Operations Research,Distance geometry and data science,Abstract,fullPaper,jv42
Computer Science,p96,d3,36708c11c2fde2efb50e75d81f174b2c205082c8,j17,Big Data & Society,What is responsible and sustainable data science?,"In the expansion of health ecosystems, issues of responsibility and sustainability of the data science involved are central. The idea that these values should be central to the practice of data science is increasingly gaining traction, yet there is no agreement on what exactly makes data science responsible or sustainable because these concepts prove slippery when applied to a global field involving commercial, academic and governmental actors. This lack of clarity is causing problems in setting goals and boundaries for data scientific practice, and risks fundamental disagreement on governance principles for this emerging field. We will argue in this commentary for a commons analytical framework as one approach to this problem, since it offers useful signposts for how to establish governance principles for shared resources.",fullPaper,jv17
Business,p96,d9,36708c11c2fde2efb50e75d81f174b2c205082c8,j17,Big Data & Society,What is responsible and sustainable data science?,"In the expansion of health ecosystems, issues of responsibility and sustainability of the data science involved are central. The idea that these values should be central to the practice of data science is increasingly gaining traction, yet there is no agreement on what exactly makes data science responsible or sustainable because these concepts prove slippery when applied to a global field involving commercial, academic and governmental actors. This lack of clarity is causing problems in setting goals and boundaries for data scientific practice, and risks fundamental disagreement on governance principles for this emerging field. We will argue in this commentary for a commons analytical framework as one approach to this problem, since it offers useful signposts for how to establish governance principles for shared resources.",fullPaper,jv17
Computer Science,p97,d3,46f1c45c62b7dbf77af405f5ddcf137b5e1ddde9,c76,Group,Data science from a library and information science perspective,"
Purpose
Data science is a relatively new field which has gained considerable attention in recent years. This new field requires a wide range of knowledge and skills from different disciplines including mathematics and statistics, computer science and information science. The purpose of this paper is to present the results of the study that explored the field of data science from the library and information science (LIS) perspective.


Design/methodology/approach
Analysis of research publications on data science was made on the basis of papers published in the Web of Science database. The following research questions were proposed: What are the main tendencies in publication years, document types, countries of origin, source titles, authors of publications, affiliations of the article authors and the most cited articles related to data science in the field of LIS? What are the main themes discussed in the publications from the LIS perspective?


Findings
The highest contribution to data science comes from the computer science research community. The contribution of information science and library science community is quite small. However, there has been continuous increase in articles from the year 2015. The main document types are journal articles, followed by conference proceedings and editorial material. The top three journals that publish data science papers from the LIS perspective are the Journal of the American Medical Informatics Association, the International Journal of Information Management and the Journal of the Association for Information Science and Technology. The top five countries publishing are USA, China, England, Australia and India. The most cited article has got 112 citations. The analysis revealed that the data science field is quite interdisciplinary by nature. In addition to the field of LIS the papers belonged to several other research areas. The reviewed articles belonged to the six broad categories: data science education and training; knowledge and skills of the data professional; the role of libraries and librarians in the data science movement; tools, techniques and applications of data science; data science from the knowledge management perspective; and data science from the perspective of health sciences.


Research limitations/implications
The limitations of this research are that this study only analyzed research papers in the Web of Science database and therefore only covers a certain amount of scientific papers published in the field of LIS. In addition, only publications with the term “data science” in the topic area of the Web of Science database were analyzed. Therefore, several relevant studies are not discussed in this paper that are not reflected in the Web of Science database or were related to other keywords such as “e-science,” “e-research,” “data service,” “data curation” or “research data management.”


Originality/value
The field of data science has not been explored using bibliographic analysis of publications from the perspective of the LIS. This paper helps to better understand the field of data science and the perspectives for information professionals.
",poster,cp76
Computer Science,p98,d3,4aeda303fa0b9beae3f6d65e052dace9d4540116,j43,Journal of Library Administration,Data Science Support at the Academic Library,"Abstract Data science is a rapidly growing field with applications across all scientific domains. The demand for support in data science literacy is outpacing available resources at college campuses. The academic library is uniquely positioned to provide training and guidance in a number of areas relevant to data science. The University of Arizona Libraries has built a successful data science support program, focusing on computational literacy, geographic information systems, and reproducible science. Success of the program has largely been due to the strength of library personnel and strategic partnerships with units outside of the library. Academic libraries can support campus data science needs through professional development of current staff and recruitment of new personnel with expertise in data-intensive domains.",fullPaper,jv43
Computer Science,p102,d3,e1c8f86668d3e37e430f187b7fd91d1643a0a0ff,j4,IEEE Transactions on Knowledge and Data Engineering,Theory-Guided Data Science: A New Paradigm for Scientific Discovery from Data,"Data science models, although successful in a number of commercial domains, have had limited applicability in scientific problems involving complex physical phenomena. Theory-guided data science (TGDS) is an emerging paradigm that aims to leverage the wealth of scientific knowledge for improving the effectiveness of data science models in enabling scientific discovery. The overarching vision of TGDS is to introduce scientific consistency as an essential component for learning generalizable models. Further, by producing scientifically interpretable models, TGDS aims to advance our scientific understanding by discovering novel domain insights. Indeed, the paradigm of TGDS has started to gain prominence in a number of scientific disciplines such as turbulence modeling, material discovery, quantum chemistry, bio-medical science, bio-marker discovery, climate science, and hydrology. In this paper, we formally conceptualize the paradigm of TGDS and present a taxonomy of research themes in TGDS. We describe several approaches for integrating domain knowledge in different research themes using illustrative examples from different disciplines. We also highlight some of the promising avenues of novel research for realizing the full potential of theory-guided data science.",fullPaper,jv4
Mathematics,p102,d6,e1c8f86668d3e37e430f187b7fd91d1643a0a0ff,j4,IEEE Transactions on Knowledge and Data Engineering,Theory-Guided Data Science: A New Paradigm for Scientific Discovery from Data,"Data science models, although successful in a number of commercial domains, have had limited applicability in scientific problems involving complex physical phenomena. Theory-guided data science (TGDS) is an emerging paradigm that aims to leverage the wealth of scientific knowledge for improving the effectiveness of data science models in enabling scientific discovery. The overarching vision of TGDS is to introduce scientific consistency as an essential component for learning generalizable models. Further, by producing scientifically interpretable models, TGDS aims to advance our scientific understanding by discovering novel domain insights. Indeed, the paradigm of TGDS has started to gain prominence in a number of scientific disciplines such as turbulence modeling, material discovery, quantum chemistry, bio-medical science, bio-marker discovery, climate science, and hydrology. In this paper, we formally conceptualize the paradigm of TGDS and present a taxonomy of research themes in TGDS. We describe several approaches for integrating domain knowledge in different research themes using illustrative examples from different disciplines. We also highlight some of the promising avenues of novel research for realizing the full potential of theory-guided data science.",fullPaper,jv4
Computer Science,p103,d3,bf12943b1862cbdf556ba1ddcdbc685d4f38a6c3,j12,Communications of the ACM,Realizing the potential of data science,"Data science promises new insights, helping transform information into knowledge that can drive science and industry.",fullPaper,jv12
Computer Science,p104,d3,ffdb6039a5d82f8edd70b2d177074c2f2c89e97f,j45,Journal of Social Computing,Data Science as Political Action: Grounding Data Science in a Politics of Justice,"In response to recent controversies, the field of data science has rushed to adopt codes of ethics. Such professional codes, however, are ill-equipped to address broad matters of social justice. Instead of ethics codes, I argue, the field must embrace politics. Data scientists must recognize themselves as political actors engaged in normative constructions of society and, as befits political work, evaluate their work according to its downstream material impacts on people's lives. I justify this notion in two parts: first, by articulating why data scientists must recognize themselves as political actors, and second, by describing how the field can evolve toward a deliberative and rigorous grounding in a politics of social justice. Part 1 responds to three arguments that are commonly invoked by data scientists when they are challenged to take political positions regarding their work. In confronting these arguments, I will demonstrate why attempting to remain apolitical is itself a political stance--a fundamentally conservative one--and why the field's current attempts to promote ""social good"" dangerously rely on vague and unarticulated political assumptions. Part 2 proposes a framework for what a politically-engaged data science could look like and how to achieve it, recognizing the challenge of reforming the field in this manner. I conceptualize the process of incorporating politics into data science in four stages: becoming interested in directly addressing social issues, recognizing the politics underlying these issues, redirecting existing methods toward new applications, and, finally, developing new practices and methods that orient data science around a mission of social justice. The path ahead does not require data scientists to abandon their technical expertise, but it does entail expanding their notions of what problems to work on and how to engage with society.",fullPaper,jv45
Political Science,p104,d15,ffdb6039a5d82f8edd70b2d177074c2f2c89e97f,j45,Journal of Social Computing,Data Science as Political Action: Grounding Data Science in a Politics of Justice,"In response to recent controversies, the field of data science has rushed to adopt codes of ethics. Such professional codes, however, are ill-equipped to address broad matters of social justice. Instead of ethics codes, I argue, the field must embrace politics. Data scientists must recognize themselves as political actors engaged in normative constructions of society and, as befits political work, evaluate their work according to its downstream material impacts on people's lives. I justify this notion in two parts: first, by articulating why data scientists must recognize themselves as political actors, and second, by describing how the field can evolve toward a deliberative and rigorous grounding in a politics of social justice. Part 1 responds to three arguments that are commonly invoked by data scientists when they are challenged to take political positions regarding their work. In confronting these arguments, I will demonstrate why attempting to remain apolitical is itself a political stance--a fundamentally conservative one--and why the field's current attempts to promote ""social good"" dangerously rely on vague and unarticulated political assumptions. Part 2 proposes a framework for what a politically-engaged data science could look like and how to achieve it, recognizing the challenge of reforming the field in this manner. I conceptualize the process of incorporating politics into data science in four stages: becoming interested in directly addressing social issues, recognizing the politics underlying these issues, redirecting existing methods toward new applications, and, finally, developing new practices and methods that orient data science around a mission of social justice. The path ahead does not require data scientists to abandon their technical expertise, but it does entail expanding their notions of what problems to work on and how to engage with society.",fullPaper,jv45
Computer Science,p106,d3,140a6476f7b8dde9e7bbcd199d248fc629721faa,c24,International Conference on Data Technologies and Applications,Trust in Data Science,"The trustworthiness of data science systems in applied and real-world settings emerges from the resolution of specific tensions through situated, pragmatic, and ongoing forms of work. Drawing on research in CSCW, critical data studies, and history and sociology of science, and six months of immersive ethnographic fieldwork with a corporate data science team, we describe four common tensions in applied data science work: (un)equivocal numbers, (counter)intuitive knowledge, (in)credible data, and (in)scrutable models. We show how organizational actors establish and re-negotiate trust under messy and uncertain analytic conditions through practices of skepticism, assessment, and credibility. Highlighting the collaborative and heterogeneous nature of real-world data science, we show how the management of trust in applied corporate data science settings depends not only on pre-processing and quantification, but also on negotiation and translation. We conclude by discussing the implications of our findings for data science research and practice, both within and beyond CSCW.",poster,cp24
Sociology,p106,d4,140a6476f7b8dde9e7bbcd199d248fc629721faa,c24,International Conference on Data Technologies and Applications,Trust in Data Science,"The trustworthiness of data science systems in applied and real-world settings emerges from the resolution of specific tensions through situated, pragmatic, and ongoing forms of work. Drawing on research in CSCW, critical data studies, and history and sociology of science, and six months of immersive ethnographic fieldwork with a corporate data science team, we describe four common tensions in applied data science work: (un)equivocal numbers, (counter)intuitive knowledge, (in)credible data, and (in)scrutable models. We show how organizational actors establish and re-negotiate trust under messy and uncertain analytic conditions through practices of skepticism, assessment, and credibility. Highlighting the collaborative and heterogeneous nature of real-world data science, we show how the management of trust in applied corporate data science settings depends not only on pre-processing and quantification, but also on negotiation and translation. We conclude by discussing the implications of our findings for data science research and practice, both within and beyond CSCW.",poster,cp24
Computer Science,p107,d3,3335c340c20609b4e6de481c9eaf67ecd6c960dc,c6,Annual Conference on Genetic and Evolutionary Computation,Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science,"As the field of data science continues to grow, there will be an ever-increasing demand for tools that make machine learning accessible to non-experts. In this paper, we introduce the concept of tree-based pipeline optimization for automating one of the most tedious parts of machine learning--pipeline design. We implement an open source Tree-based Pipeline Optimization Tool (TPOT) in Python and demonstrate its effectiveness on a series of simulated and real-world benchmark data sets. In particular, we show that TPOT can design machine learning pipelines that provide a significant improvement over a basic machine learning analysis while requiring little to no input nor prior knowledge from the user. We also address the tendency for TPOT to design overly complex pipelines by integrating Pareto optimization, which produces compact pipelines without sacrificing classification accuracy. As such, this work represents an important step toward fully automating machine learning pipeline design.",fullPaper,cp6
Computer Science,p109,d3,0a4b3c33e830d8cde364443a52e673c2c07dcfe8,c7,International Symposium on Intelligent Data Analysis,Open Data Science,Abstract,fullPaper,cp7
Computer Science,p110,d3,305600f3cba8a63bad1bedeab34a299bf748754b,j46,Proceedings of the VLDB Endowment,Northstar: An Interactive Data Science System,"In order to democratize data science, we need to fundamentally rethink the current analytics stack, from the user interface to the ""guts."" Most importantly, enabling a broader range of users to unfold the potential of (their) data requires a change in the interface and the ""protection"" we offer them. On the one hand, visual interfaces for data science have to be intuitive, easy, and interactive to reach users without a strong background in computer science or statistics. On the other hand, we need to protect users from making false discoveries. Furthermore, it requires that technically involved (and often boring) tasks have to be automatically done by the system so that the user can focus on contributing their domain expertise to the problem. In this paper, we present Northstar, the Interactive Data Science System, which we have developed over the last 4 years to explore designs that make advanced analytics and model building more accessible.",fullPaper,jv46
Computer Science,p113,d3,6bf9d589f80823735084956f056728ae1a7bcfa8,j48,BioScience,"Situating Ecology as a Big-Data Science: Current Advances, Challenges, and Solutions","Ecology has joined a world of big data. Two complementary frameworks define big data: data that exceed the analytical capacities of individuals or disciplines or the “Four Vs” axes of volume, variety, veracity, and velocity. Variety predominates in ecoinformatics and limits the scalability of ecological science. Volume varies widely. Ecological velocity is low but growing as data throughput and societal needs increase. Ecological big-data systems include in situ and remote sensors, community data resources, biodiversity databases, citizen science, and permanent stations. Technological solutions include the development of open code- and data-sharing platforms, flexible statistical models that can handle heterogeneous data and sources of uncertainty, and cloud-computing delivery of high-velocity computing to large-volume analytics. Cultural solutions include training targeted to early and current scientific workforce and strengthening collaborations among ecologists and data scientists. The broader goal is to maximize the power, scalability, and timeliness of ecological insights and forecasting.",fullPaper,jv48
Computer Science,p115,d3,ff6586ab32e9ed45d20a486ec7c5be02da5d3f1f,j13,International Journal of Data Science and Analysis,Data Science: the impact of statistics,Abstract,fullPaper,jv13
Computer Science,p116,d3,2a85f034ae7a6119ae6b718c8f73a58dc1fbd7b4,c75,International Conference on Predictive Models in Software Engineering,Curriculum Guidelines for Undergraduate Programs in Data Science,"The Park City Math Institute (PCMI) 2016 Summer Undergraduate Faculty Program met for the purpose of composing guidelines for undergraduate programs in Data Science. The group consisted of 25 undergraduate faculty from a variety of institutions in the U.S., primarily from the disciplines of mathematics, statistics and computer science. These guidelines are meant to provide some structure for institutions planning for or revising a major in Data Science.",poster,cp75
Mathematics,p116,d6,2a85f034ae7a6119ae6b718c8f73a58dc1fbd7b4,c75,International Conference on Predictive Models in Software Engineering,Curriculum Guidelines for Undergraduate Programs in Data Science,"The Park City Math Institute (PCMI) 2016 Summer Undergraduate Faculty Program met for the purpose of composing guidelines for undergraduate programs in Data Science. The group consisted of 25 undergraduate faculty from a variety of institutions in the U.S., primarily from the disciplines of mathematics, statistics and computer science. These guidelines are meant to provide some structure for institutions planning for or revising a major in Data Science.",poster,cp75
Computer Science,p117,d3,2146edb37621d80f53c1261c8a53c94d3dda84c8,c8,Frontiers in Education Conference,Smart Blockchain Badges for Data Science Education,"Blockchain technology has the potential to revolutionise education in a number of ways. In this paper, we explore the applications of Smart Blockchain Badges on data science education. In particular, we investigate how Smart Blockchain Badges can support learners that want to advance their careers in data science, by offering them personalised recommendations based on their learning achievements. This work aims at enhancing data science accreditation by introducing a robust system based on the Blockchain technology. Learners will benefit from a sophisticated, open and transparent accreditation system, as well as from receiving job recommendations that match their skills and can potentially progress their careers. As a result, this work contributes towards closing the data science skills gap by linking data science education to the industry.",fullPaper,cp8
Computer Science,p121,d3,79be83a308a9a75ef4e64f63a938b201531c0bbf,j50,ACM Computing Surveys,Data Science,"The 21st century has ushered in the age of big data and data economy, in which data DNA, which carries important knowledge, insights, and potential, has become an intrinsic constituent of all data-based organisms. An appropriate understanding of data DNA and its organisms relies on the new field of data science and its keystone, analytics. Although it is widely debated whether big data is only hype and buzz, and data science is still in a very early phase, significant challenges and opportunities are emerging or have been inspired by the research, innovation, business, profession, and education of data science. This article provides a comprehensive survey and tutorial of the fundamental aspects of data science: the evolution from data analysis to data science, the data science concepts, a big picture of the era of data science, the major challenges and directions in data innovation, the nature of data analytics, new industrialization and service opportunities in the data economy, the profession and competency of data education, and the future of data science. This article is the first in the field to draw a comprehensive big picture, in addition to offering rich observations, lessons, and thinking about data science and analytics.",fullPaper,jv50
Sociology,p121,d4,79be83a308a9a75ef4e64f63a938b201531c0bbf,j50,ACM Computing Surveys,Data Science,"The 21st century has ushered in the age of big data and data economy, in which data DNA, which carries important knowledge, insights, and potential, has become an intrinsic constituent of all data-based organisms. An appropriate understanding of data DNA and its organisms relies on the new field of data science and its keystone, analytics. Although it is widely debated whether big data is only hype and buzz, and data science is still in a very early phase, significant challenges and opportunities are emerging or have been inspired by the research, innovation, business, profession, and education of data science. This article provides a comprehensive survey and tutorial of the fundamental aspects of data science: the evolution from data analysis to data science, the data science concepts, a big picture of the era of data science, the major challenges and directions in data innovation, the nature of data analytics, new industrialization and service opportunities in the data economy, the profession and competency of data education, and the future of data science. This article is the first in the field to draw a comprehensive big picture, in addition to offering rich observations, lessons, and thinking about data science and analytics.",fullPaper,jv50
Computer Science,p123,d3,a1dbdc2ce338d694a720163f591e4eb5c4070140,c54,ACM SIGCOMM Symposium on Software Defined Networking Research,Deep Learning in Biomedical Data Science,"Since the 1980s, deep learning and biomedical data have been coevolving and feeding each other. The breadth, complexity, and rapidly expanding size of biomedical data have stimulated the development of novel deep learning methods, and application of these methods to biomedical data have led to scientific discoveries and practical solutions. This overview provides technical and historical pointers to the field, and surveys current applications of deep learning to biomedical data organized around five subareas, roughly of increasing spatial scale: chemoinformatics, proteomics, genomics and transcriptomics, biomedical imaging, and health care. The black box problem of deep learning methods is also briefly discussed.",poster,cp54
Computer Science,p125,d3,b154d9ce0a551be90557d7a24a49b1988add2a81,c26,Decision Support Systems,"Three principles of data science: predictability, computability, and stability (PCS)","In this talk, I'd like to discuss the intertwining importance and connections of three principles of data science in the title and the PCS workflow that is built on the three principles. The principles will be demonstrated in the context of two collaborative projects in neuroscience and genomics for interpretable data results and testable hypothesis generation.",poster,cp26
Mathematics,p125,d6,b154d9ce0a551be90557d7a24a49b1988add2a81,c26,Decision Support Systems,"Three principles of data science: predictability, computability, and stability (PCS)","In this talk, I'd like to discuss the intertwining importance and connections of three principles of data science in the title and the PCS workflow that is built on the three principles. The principles will be demonstrated in the context of two collaborative projects in neuroscience and genomics for interpretable data results and testable hypothesis generation.",poster,cp26
Computer Science,p126,d3,3c51a892ce5a8fc78d57ea290c6e5144ee9db579,c5,Technical Symposium on Computer Science Education,Key Concepts for a Data Science Ethics Curriculum,"Data science is a new field that integrates aspects of computer science, statistics and information management. As a new field, ethical issues a data scientist may encounter have received little attention to date, and ethics training within a data science curriculum has received even less attention. To address this gap, this article explores the different codes of conduct and ethics frameworks related to data science. We compare this analysis with the results of a systematic literature review focusing on ethics in data science. Our analysis identified twelve key ethics areas that should be included within a data science ethics curriculum. Our research notes that none of the existing codes or frameworks covers all of the identified themes. Data science educators and program coordinators can use our results as a way to identify key ethical concepts that can be introduced within a data science program.",fullPaper,cp5
Computer Science,p129,d3,cdd5d0a3e2ba0e1f4b5bcd3115e5f5b6536e24f9,c87,International Conference on Big Data Research,The Data Science Design Manual,Abstract,poster,cp87
Computer Science,p130,d3,f4e66bd035e195f539f1b65a5aaec0e873cdee29,j53,Computer Applications in Engineering Education,Data science in education: Big data and learning analytics,This paper considers the data science and the summaries significance of Big Data and Learning Analytics in education. The widespread platform of making high‐quality benefits that could be achieved by exhausting big data techniques in the field of education is considered. One principal architecture framework to support education research is proposed.,fullPaper,jv53
Computer Science,p131,d3,b0fbdffb9733e7857afbb21ccbcd9cd74803ca1d,j54,Data Science,"Data Science and symbolic AI: Synergies, challenges and opportunities","Symbolic approaches to Artificial Intelligence (AI) represent things within a domain of knowledge through physical symbols, combine symbols into symbol expressions, and manipulate symbols and symbol expressions through inference processes. While a large part of Data Science relies on statistics and applies statistical approaches to AI, there is an increasing potential for successfully applying symbolic approaches as well. Symbolic representations and symbolic inference are close to human cognitive representations and therefore comprehensible and interpretable; they are widely used to represent data and metadata, and their specific semantic content must be taken into account for analysis of such information; and human communication largely relies on symbols, making symbolic representations a crucial part in the analysis of natural language. Here we discuss the role symbolic representations and inference can play in Data Science, highlight the research challenges from the perspective of the data scientist, and argue that symbolic methods should become a crucial component of the data scientists’ toolbox.",fullPaper,jv54
Computer Science,p133,d3,d2f83aa22def149095f1dd89b4cf36d09a748a87,c113,International Conference on Mobile Data Management,Data science is science's second chance to get causal inference right: A classification of data science tasks,"Causal inference from observational data is the goal of many data analyses in the health and social sciences. However, academic statistics has often frowned upon data analyses with a causal objective. The introduction of the term""data science""provides a historic opportunity to redefine data analysis in such a way that it naturally accommodates causal inference from observational data. Like others before, we organize the scientific contributions of data science into three classes of tasks: Description, prediction, and counterfactual prediction (which includes causal inference). An explicit classification of data science tasks is necessary to discuss the data, assumptions, and analytics required to successfully accomplish each task. We argue that a failure to adequately describe the role of subject-matter expert knowledge in data analysis is a source of widespread misunderstandings about data science. Specifically, causal analyses typically require not only good data and algorithms, but also domain expert knowledge. We discuss the implications for the use of data science to guide decision-making in the real world and to train data scientists.",poster,cp113
Mathematics,p133,d6,d2f83aa22def149095f1dd89b4cf36d09a748a87,c113,International Conference on Mobile Data Management,Data science is science's second chance to get causal inference right: A classification of data science tasks,"Causal inference from observational data is the goal of many data analyses in the health and social sciences. However, academic statistics has often frowned upon data analyses with a causal objective. The introduction of the term""data science""provides a historic opportunity to redefine data analysis in such a way that it naturally accommodates causal inference from observational data. Like others before, we organize the scientific contributions of data science into three classes of tasks: Description, prediction, and counterfactual prediction (which includes causal inference). An explicit classification of data science tasks is necessary to discuss the data, assumptions, and analytics required to successfully accomplish each task. We argue that a failure to adequately describe the role of subject-matter expert knowledge in data analysis is a source of widespread misunderstandings about data science. Specifically, causal analyses typically require not only good data and algorithms, but also domain expert knowledge. We discuss the implications for the use of data science to guide decision-making in the real world and to train data scientists.",poster,cp113
Computer Science,p136,d3,798e5e09c20b0270701b194a3198427fec6a4fcd,j13,International Journal of Data Science and Analysis,What makes Data Science different? A discussion involving Statistics2.0 and Computational Sciences,Abstract,fullPaper,jv13
Computer Science,p137,d3,dd1f93c3faae464d50d2e97c2bf4ac8d43681cb1,c59,Australian Software Engineering Conference,Twinning data science with information science in schools of library and information science,"As an emerging discipline, data science represents a vital new current of school of library and information science (LIS) education. However, it remains unclear how it relates to information science within LIS schools. The purpose of this paper is to clarify this issue.,Mission statement and nature of both data science and information science are analyzed by reviewing existing work in the two disciplines and drawing DIKW hierarchy. It looks at the ways in which information science theories bring new insights and shed new light on fundamentals of data science.,Data science and information science are twin disciplines by nature. The mission, task and nature of data science are consistent with those of information science. They greatly overlap and share similar concerns. Furthermore, they can complement each other. LIS school should integrate both sciences and develop organizational ambidexterity. Information science can make unique contributions to data science research, including conception of data, data quality control, data librarianship and theory dualism. Document theory, as a promising direction of unified information science, should be introduced to data science to solve the disciplinary divide.,The results of this paper may contribute to the integration of data science and information science within LIS schools and iSchools. It has particular value for LIS school development and reform in the age of big data.",poster,cp59
Computer Science,p138,d3,5de20ffb7852ae0665c382084c8a56918f23dc0b,c11,European Conference on Modelling and Simulation,Drafting a Data Science Curriculum for Secondary Schools,"Data science as the art of generating information and knowledge from data is increasingly becoming an important part of most operational processes. But up to now, data science is hardly an issue in German computer science education at secondary schools. For this reason, we are developing a data science curriculum for German secondary schools, which first guidelines and ideas we present in this paper. The curriculum is designed as interdisciplinary approach between maths and computer science education, with also a strong focus on societal aspects. After a brief discussion of important concepts and challenges in data science, a first draft of the curriculum and an outline of a data science course for upper secondary schools accompanying the development are presented.",fullPaper,cp11
Computer Science,p139,d3,e36022198f21f46d066007ee5cf901ea55080e21,c110,Biometrics and Identity Management,"Introduction to Data Science: A Python Approach to Concepts, Techniques and Applications","This accessible and classroom-tested textbook/reference presents an introduction to the fundamentals of the emerging and interdisciplinary field of data science. The coverage spans key concepts adopted from statistics and machine learning, useful techniques for graph analysis and parallel programming, and the practical application of data science for such tasks as building recommender systems or performing sentiment analysis. Topics and features: provides numerous practical case studies using real-world data throughout the book; supports understanding through hands-on experience of solving data science problems using Python; describes techniques and tools for statistical analysis, machine learning, graph analysis, and parallel programming; reviews a range of applications of data science, including recommender systems and sentiment analysis of text data; provides supplementary code resources and data at an associated website.",poster,cp110
Computer Science,p144,d3,04d7b3457dc78b2d2282e6af2c787308f75c9b26,c12,The Compass,Care and the Practice of Data Science for Social Good,"Data science is an interdisciplinary field that extracts insights from data through a multi-stage process of data collection, analysis and use. When data science is applied for social good, a variety of stakeholders are introduced to the process with an intention to inform policies or programs to improve well-being. Our goal in this paper is to propose an orientation to care in the practice of data science for social good. When applied to data science, a logic of care can improve the data science process and reveal outcomes of ""good"" throughout. Consideration of care in practice has its origins in Science and Technology Studies (STS) and has recently been applied by Human Computer Interaction (HCI) researchers to understand technology repair and use in under-served environments as well as care in remote health monitoring. We bring care to the practice of data science through a detailed examination of our engaged research with a community group that uses data as a strategy to advocate for permanently affordable housing. We identify opportunities and experiences of care throughout the stages of the data science process. We bring greater detail to the notion of human-centered systems for data science and begin to describe what these look like.",fullPaper,cp12
Sociology,p144,d4,04d7b3457dc78b2d2282e6af2c787308f75c9b26,c12,The Compass,Care and the Practice of Data Science for Social Good,"Data science is an interdisciplinary field that extracts insights from data through a multi-stage process of data collection, analysis and use. When data science is applied for social good, a variety of stakeholders are introduced to the process with an intention to inform policies or programs to improve well-being. Our goal in this paper is to propose an orientation to care in the practice of data science for social good. When applied to data science, a logic of care can improve the data science process and reveal outcomes of ""good"" throughout. Consideration of care in practice has its origins in Science and Technology Studies (STS) and has recently been applied by Human Computer Interaction (HCI) researchers to understand technology repair and use in under-served environments as well as care in remote health monitoring. We bring care to the practice of data science through a detailed examination of our engaged research with a community group that uses data as a strategy to advocate for permanently affordable housing. We identify opportunities and experiences of care throughout the stages of the data science process. We bring greater detail to the notion of human-centered systems for data science and begin to describe what these look like.",fullPaper,cp12
Computer Science,p145,d3,843793928e308b5414d2883ac869e813ec16f65d,c88,International Conference on Big Data Computing and Communications,Progressive Data Science: Potential and Challenges,"Data science requires time-consuming iterative manual activities. In particular, activities such as data selection, preprocessing, transformation, and mining, highly depend on iterative trial-and-error processes that could be sped up significantly by providing quick feedback on the impact of changes. The idea of progressive data science is to compute the results of changes in a progressive manner, returning a first approximation of results quickly and allow iterative refinements until converging to a final result. Enabling the user to interact with the intermediate results allows an early detection of erroneous or suboptimal choices, the guided definition of modifications to the pipeline and their quick assessment. In this paper, we discuss the progressiveness challenges arising in different steps of the data science pipeline. We describe how changes in each step of the pipeline impact the subsequent steps and outline why progressive data science will help to make the process more effective. Computing progressive approximations of outcomes resulting from changes creates numerous research challenges, especially if the changes are made in the early steps of the pipeline. We discuss these challenges and outline first steps towards progressiveness, which, we argue, will ultimately help to significantly speed-up the overall data science process.",poster,cp88
Computer Science,p146,d3,a0ef3467c09acc3106b915258b7b8db7bb663b77,c105,International Conference on Automatic Face and Gesture Recognition,Data Science Methodology for Cybersecurity Projects,"Cyber-security solutions are traditionally static and signature-based. The traditional solutions along with the use of analytic models, machine learning and big data could be improved by automatically trigger mitigation or provide relevant awareness to control or limit consequences of threats. This kind of intelligent solutions is covered in the context of Data Science for Cyber-security. Data Science provides a significant role in cyber-security by utilising the power of data (and big data), high-performance computing and data mining (and machine learning) to protect users against cyber-crimes. For this purpose, a successful data science project requires an effective methodology to cover all issues and provide adequate resources. In this paper, we are introducing popular data science methodologies and will compare them in accordance with cyber-security challenges. A comparison discussion has also delivered to explain methodologies strengths and weaknesses in case of cyber-security projects.",poster,cp105
Computer Science,p147,d3,9b54c9a7d2060f800961c2f9195fcf5408288f17,c66,International Conference on Web and Social Media,Data Science for Undergraduates,Abstract,poster,cp66
Computer Science,p148,d3,1d045f4f347409f0635c9d15d538dbfabb6b38fa,j57,Environmental Modelling & Software,Environmental Data Science,Abstract,fullPaper,jv57
Computer Science,p149,d3,818c9fd2df1229f962af3c50ef493e8633433fb9,c92,International Symposium on Computer Architecture,Data Science Thinking,Abstract,poster,cp92
Computer Science,p151,d3,ea07f64ad84542e04acc41db6b171007f344efd7,c13,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,Milo: A visual programming environment for Data Science Education,"Most courses on Data Science offered at universities or online require students to have familiarity with at least one programming language. In this paper, we present, “Milo”, a web-based visual programming environment for Data Science Education, designed as a pedagogical tool that can be used by students without prior-programming experience. To that end, Milo uses graphical blocks as abstractions of language specific implementations of Data Science and Machine Learning(ML) concepts along with creation of interactive visualizations. Using block definitions created by a user, Milo generates equivalent source code in JavaScript to run entirely in the browser. Based on a preliminary user study with a focus group of undergraduate computer science students, Milo succeeds as an effective tool for novice learners in the field of Data Science.",fullPaper,cp13
Computer Science,p152,d3,010f65dd2fa979892a8229db825954871652fb8f,c57,IEEE International Geoscience and Remote Sensing Symposium,Defining Data Science by a Data-Driven Quantification of the Community,"Data science is a new academic field that has received much attention in recent years. One reason for this is that our increasingly digitalized society generates more and more data in all areas of our lives and science and we are desperately seeking for solutions to deal with this problem. In this paper, we investigate the academic roots of data science. We are using data of scientists and their citations from Google Scholar, who have an interest in data science, to perform a quantitative analysis of the data science community. Furthermore, for decomposing the data science community into its major defining factors corresponding to the most important research fields, we introduce a statistical regression model that is fully automatic and robust with respect to a subsampling of the data. This statistical model allows us to define the ‘importance’ of a field as its predictive abilities. Overall, our method provides an objective answer to the question ‘What is data science?’.",poster,cp57
Computer Science,p153,d3,00b1fa3c7170563567fb22a9bb6ff4c7b2e8853e,c14,Hawaii International Conference on System Sciences,Comparing Data Science Project Management Methodologies via a Controlled Experiment,"Data Science is an emerging field with a significant research focus on improving the techniques available to analyze data. However, there has been much less focus on how people should work together on a data science project. In this paper, we report on the results of an experiment comparing four different methodologies to manage and coordinate a data science project. We first introduce a model to compare different project management methodologies and then report on the results of our experiment. The results from our experiment demonstrate that there are significant differences based on the methodology used, with an Agile Kanban methodology being the most effective and surprisingly, an Agile Scrum methodology being the least effective.",fullPaper,cp14
Computer Science,p156,d3,8d89159249e0faf5deae508cc8533010898bbda5,c95,Cyber ..,Data Science in Action,Abstract,poster,cp95
Computer Science,p158,d3,a1dc9a3df54ac24712fc47ac5f0b116f1043b95e,c26,Decision Support Systems,R – Data Science,Abstract,poster,cp26
Computer Science,p159,d3,bfd6caddec8a98d531ee9f1f7ebf5833797cd5e3,c5,Technical Symposium on Computer Science Education,Introducing Data Science to School Kids,"Data-driven decision making is fast becoming a necessary skill in jobs across the board. The industry today uses analytics and machine learning to get useful insights from a wealth of digital information in order to make decisions. With data science becoming an important skill needed in varying degrees of complexity by the workforce of the near future, we felt the need to expose school-goers to its power through a hands-on exercise. We organized a half-day long data science tutorial for kids in grades 5 through 9 (10-15 years old). Our aim was to expose them to the full cycle of a typical supervised learning approach - data collection, data entry, data visualization, feature engineering, model building, model testing and data permissions. We discuss herein the design choices made while developing the dataset, the method and the pedagogy for the tutorial. These choices aimed to maximize student engagement while ensuring minimal pre-requisite knowledge. This was a challenging task given that we limited the pre-requisites for the kids to the knowledge of counting, addition, percentages, comparisons and a basic exposure to operating computers. By designing an exercise with the stated principles, we were able to provide to kids an exciting, hands-on introduction to data science, as confirmed by their experiences. To the best of the authors' knowledge, the tutorial was the first of its kind. Considering the positive reception of such a tutorial, we hope that educators across the world are encouraged to introduce data science in their respective curricula for high-schoolers and are able to use the principles laid out in this work to build full-fledged courses.",fullPaper,cp5
Computer Science,p161,d3,796d70a6eb0428ae19f1187ae1c81185d4ae6701,c76,Group,Automating Biomedical Data Science Through Tree-Based Pipeline Optimization,Abstract,poster,cp76
Computer Science,p162,d3,da63f30bd5b3a1b16c261f75ca1b1daddfc5b44d,c46,Ideal,Big Data and Data Science Methods for Management Research,"The recent advent of remote sensing, mobile technologies, novel transaction systems, and highperformance computing offers opportunities to understand trends, behaviors, and actions in a manner that has not been previously possible. Researchers can thus leverage “big data” that are generated from a plurality of sources including mobile transactions, wearable technologies, social media, ambient networks, andbusiness transactions.An earlierAcademy of Management Journal (AMJ) editorial explored the potential implications for data science inmanagement research and highlighted questions for management scholarship as well as the attendant challenges of data sharing and privacy (George, Haas, & Pentland, 2014). This nascent field is evolving rapidly and at a speed that leaves scholars and practitioners alike attempting to make sense of the emergent opportunities that big datahold.With thepromiseof bigdata comequestions about the analytical value and thus relevance of these data for theory development—including concerns over the context-specific relevance, its reliability and its validity. To address this challenge, data science is emerging as an interdisciplinary field that combines statistics, data mining, machine learning, and analytics to understand and explainhowwecan generate analytical insights and prediction models from structured and unstructured big data. Data science emphasizes the systematic study of the organization, properties, and analysis of data and their role in inference, including our confidence in the inference (Dhar, 2013).Whereas both big data and data science terms are often used interchangeably, “big data” refer to large and varied data that can be collected and managed, whereas “data science” develops models that capture, visualize, andanalyze theunderlyingpatterns in thedata. In this editorial, we address both the collection and handling of big data and the analytical tools provided by data science for management scholars. At the current time, practitioners suggest that data science applications tackle the three core elements of big data: volume, velocity, and variety (McAfee & Brynjolfsson, 2012; Zikopoulos & Eaton, 2011). “Volume” represents the sheer size of the dataset due to the aggregation of a large number of variables and an even larger set of observations for each variable. “Velocity” reflects the speed atwhich these data are collected and analyzed, whether in real time or near real time from sensors, sales transactions, social media posts, and sentiment data for breaking news and social trends. “Variety” in big data comes from the plurality of structured and unstructured data sources such as text, videos, networks, and graphics among others. The combinations of volume, velocity, and variety reveal the complex task of generating knowledge from big data, which often runs into millions of observations, and deriving theoretical contributions from such data. In this editorial, we provide a primer or a “starter kit” for potential data science applications inmanagement research. We do so with a caveat that emerging fields outdate and improve uponmethodologies while often supplanting them with new applications. Nevertheless, this primer can guide management scholars who wish to use data science techniques to reach better answers to existing questions or explore completely new research questions.",poster,cp46
Computer Science,p164,d3,f447afeccbdb9ed5df15c44011aec9c018d4b2c4,j59,Journal of Data and Information Science,Big Data and Data Science: Opportunities and Challenges of iSchools,"Abstract Due to the recent explosion of big data, our society has been rapidly going through digital transformation and entering a new world with numerous eye-opening developments. These new trends impact the society and future jobs, and thus student careers. At the heart of this digital transformation is data science, the discipline that makes sense of big data. With many rapidly emerging digital challenges ahead of us, this article discusses perspectives on iSchools’ opportunities and suggestions in data science education. We argue that iSchools should empower their students with “information computing” disciplines, which we define as the ability to solve problems and create values, information, and knowledge using tools in application domains. As specific approaches to enforcing information computing disciplines in data science education, we suggest the three foci of user-based, tool-based, and application-based. These three foci will serve to differentiate the data science education of iSchools from that of computer science or business schools. We present a layered Data Science Education Framework (DSEF) with building blocks that include the three pillars of data science (people, technology, and data), computational thinking, data-driven paradigms, and data science lifecycles. Data science courses built on the top of this framework should thus be executed with user-based, tool-based, and application-based approaches. This framework will help our students think about data science problems from the big picture perspective and foster appropriate problem-solving skills in conjunction with broad perspectives of data science lifecycles. We hope the DSEF discussed in this article will help fellow iSchools in their design of new data science curricula.",fullPaper,jv59
Computer Science,p168,d3,d83a99bfb6f81565a186e0eb86858864568c1327,j12,Communications of the ACM,Data science,"While it may not be possible to build a data brain identical to a human, data science can still aspire to imaginative machine thinking.",fullPaper,jv12
Computer Science,p169,d3,021865bb9fcc59814d2ce84d086554e5e0259779,j59,Journal of Data and Information Science,"Big Metadata, Smart Metadata, and Metadata Capital: Toward Greater Synergy Between Data Science and Metadata","Abstract Purpose The purpose of the paper is to provide a framework for addressing the disconnect between metadata and data science. Data science cannot progress without metadata research. This paper takes steps toward advancing the synergy between metadata and data science, and identifies pathways for developing a more cohesive metadata research agenda in data science. Design/methodology/approach This paper identifies factors that challenge metadata research in the digital ecosystem, defines metadata and data science, and presents the concepts big metadata, smart metadata, and metadata capital as part of a metadata lingua franca connecting to data science. Findings The “utilitarian nature” and “historical and traditional views” of metadata are identified as two intersecting factors that have inhibited metadata research. Big metadata, smart metadata, and metadata capital are presented as part of a metadata lingua franca to help frame research in the data science research space. Research limitations There are additional, intersecting factors to consider that likely inhibit metadata research, and other significant metadata concepts to explore. Practical implications The immediate contribution of this work is that it may elicit response, critique, revision, or, more significantly, motivate research. The work presented can encourage more researchers to consider the significance of metadata as a research worthy topic within data science and the larger digital ecosystem. Originality/value Although metadata research has not kept pace with other data science topics, there is little attention directed to this problem. This is surprising, given that metadata is essential for data science endeavors. This examination synthesizes original and prior scholarship to provide new grounding for metadata research in data science.",fullPaper,jv59
Computer Science,p170,d3,97a3726b3f9395c8919c6271540d87d1c44e10ac,c16,International Conference on Data Science and Advanced Analytics,Deep feature synthesis: Towards automating data science endeavors,"In this paper, we develop the Data Science Machine, which is able to derive predictive models from raw data automatically. To achieve this automation, we first propose and develop the Deep Feature Synthesis algorithm for automatically generating features for relational datasets. The algorithm follows relationships in the data to a base field, and then sequentially applies mathematical functions along that path to create the final feature. Second, we implement a generalizable machine learning pipeline and tune it using a novel Gaussian Copula process based approach. We entered the Data Science Machine in 3 data science competitions that featured 906 other data science teams. Our approach beats 615 teams in these data science competitions. In 2 of the 3 competitions we beat a majority of competitors, and in the third, we achieved 94% of the best competitor's score. In the best case, with an ongoing competition, we beat 85.6% of the teams and achieved 95.7% of the top submissions score.",fullPaper,cp16
Computer Science,p171,d3,96f5a9360ccfd1c5c4210dc62948baac234c372d,c80,International Symposium on Software Engineering for Adaptive and Self-Managing Systems,Predicting data science sociotechnical execution challenges by categorizing data science projects,"The challenge in executing a data science project is more than just identifying the best algorithm and tool set to use. Additional sociotechnical challenges include items such as how to define the project goals and how to ensure the project is effectively managed. This paper reports on a set of case studies where researchers were embedded within data science teams and where the researcher observations and analysis was focused on the attributes that can help describe data science projects and the challenges faced by the teams executing these projects, as opposed to the algorithms and technologies that were used to perform the analytics. Based on our case studies, we identified 14 characteristics that can help describe a data science project. We then used these characteristics to create a model that defines two key dimensions of the project. Finally, by clustering the projects within these two dimensions, we identified four types of data science projects, and based on the type of project, we identified some of the sociotechnical challenges that project teams should expect to encounter when executing data science projects.",poster,cp80
Computer Science,p172,d3,94c52a7516ef8955f76c3ee1319ff4fd8bf071fd,c100,IEEE International Conference on Computer Vision,"Computer Age Statistical Inference: Algorithms, Evidence, and Data Science","The twenty-first century has seen a breathtaking expansion of statistical methodology, both in scope and in influence. 'Big data', 'data science', and 'machine learning' have become familiar terms in the news, as statistical methods are brought to bear upon the enormous data sets of modern science and commerce. How did we get here? And where are we going? This book takes us on an exhilarating journey through the revolution in data analysis following the introduction of electronic computation in the 1950s. Beginning with classical inferential theories - Bayesian, frequentist, Fisherian - individual chapters take up a series of influential topics: survival analysis, logistic regression, empirical Bayes, the jackknife and bootstrap, random forests, neural networks, Markov chain Monte Carlo, inference after model selection, and dozens more. The distinctly modern approach integrates methodology and algorithms with statistical inference. The book ends with speculation on the future direction of statistics and data science.",poster,cp100
Computer Science,p173,d3,3a8da09a87f06273c19fb61573b299388f8d1673,j61,Japanese Journal of Statistics and Data Science,Data science vs. statistics: two cultures?,Abstract,fullPaper,jv61
Mathematics,p173,d6,3a8da09a87f06273c19fb61573b299388f8d1673,j61,Japanese Journal of Statistics and Data Science,Data science vs. statistics: two cultures?,Abstract,fullPaper,jv61
Computer Science,p174,d3,427a613d349d305726e1c4c7935b33c79de5850a,c80,International Symposium on Software Engineering for Adaptive and Self-Managing Systems,Python Data Science Handbook: Essential Tools for Working with Data,"For many researchers, Python is a first-class tool mainly because of its libraries for storing, manipulating, and gaining insight from data. Several resources exist for individual pieces of this data science stack, but only with the Python Data Science Handbook do you get them all IPython, NumPy, Pandas, Matplotlib, Scikit-Learn, and other related tools. Working scientists and data crunchers familiar with reading and writing Python code will find this comprehensive desk reference ideal for tackling day-to-day issues: manipulating, transforming, and cleaning data; visualizing different types of data; and using data to build statistical or machine learning models. Quite simply, this is the must-have reference for scientific computing in Python. With this handbook, youll learn how to use:IPython and Jupyter: provide computational environments for data scientists using PythonNumPy: includes the ndarray for efficient storage and manipulation of dense data arrays in PythonPandas: features the DataFrame for efficient storage and manipulation of labeled/columnar data in PythonMatplotlib: includes capabilities for a flexible range of data visualizations in PythonScikit-Learn: for efficient and clean Python implementations of the most important and established machine learning algorithms",poster,cp80
Computer Science,p175,d3,9141efc0d91ab0bda9b264ff6d1df5f20fd1dbb0,c0,International Conference on Machine Learning,Transdisciplinary Foundations of Geospatial Data Science,"Recent developments in data mining and machine learning approaches have brought lots of excitement in providing solutions for challenging tasks (e.g., computer vision). However, many approaches have limited interpretability, so their success and failure modes are difficult to understand and their scientific robustness is difficult to evaluate. Thus, there is an urgent need for better understanding of the scientific reasoning behind data mining and machine learning approaches. This requires taking a transdisciplinary view of data science and recognizing its foundations in mathematics, statistics, and computer science. Focusing on the geospatial domain, we apply this crucial transdisciplinary perspective to five common geospatial techniques (hotspot detection, colocation detection, prediction, outlier detection and teleconnection detection). We also describe challenges and opportunities for future advancement.",poster,cp0
Computer Science,p177,d3,5b42e8ab6542fbbc11d84b07b34443a6853f96f1,j62,Business & Information Systems Engineering,Responsible Data Science,Abstract,fullPaper,jv62
Computer Science,p178,d3,af1fed4f5226292afffc0b736ddfa777acb8eb86,c54,ACM SIGCOMM Symposium on Software Defined Networking Research,An Introduction to Data Science,"An Introduction to Data Scienceby Jeffrey S. Saltz and Jeffrey M. Stanton is an easy-to-read, gentle introduction for people with a wide range of backgrounds into the world of data science. Needing no prior coding experience or a deep understanding of statistics, this book uses the R programming language and RStudio platform to make data science welcoming and accessible for all learners. After introducing the basics of data science, the book builds on each previous concept to explain R programming from the ground up. Readers will learn essential skills in data science through demonstrations of how to use data to construct models, predict outcomes, and visualize data.",poster,cp54
Computer Science,p179,d3,4a6d46962d3f58d278cfb46d3ddebbb30bf275f5,j63,IEEE Computer Graphics and Applications,Geographic Data Science,"Data science methods and approaches address all stages of transition from data to knowledge and action. Visualization of this data is essential for human understanding of the subject under study, analytical reasoning about it, and generating new knowledge. Geographic data science deals with data that incorporates spatial and, often, temporal elements. The articles selected for this special issue represent a mix of theoretical approaches and novel applications of geographic data science.",fullPaper,jv63
Computer Science,p181,d3,0a9b30386408595ff0b3155d4de4a56dad80a97b,c52,Workshop on Applied Computational Geometry,The ambiguity of data science team roles and the need for a data science workforce framework,"This paper first reviews the benefits of well-defined roles and then discusses the current lack of standardized roles within the data science community, perhaps due to the newness of the field. Specifically, the paper reports on five case studies exploring five different attempts to define a standard set of roles. These case studies explore the usage of roles from an industry perspective as well as from national standard big data committee efforts. The paper then leverages the results of these case studies to explore the use of data science roles within online job postings. While some roles appeared frequently, such as data scientist and data engineer, no role was consistently used across all five case studies. Hence, the paper concludes by noting the need to create a data science workforce framework that could be used by students, employers, and academic institutions. This framework would enable organizations to staff their data science teams more accurately with the desired skillsets.",poster,cp52
Computer Science,p182,d3,cd247b7830fb58c6f019a79ae9679251176e8342,c111,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,Game Theory for Data Science: Eliciting Truthful Information,Abstract,poster,cp111
Computer Science,p183,d3,9a552de12d0b5a1561cf741f3170a0864f2b15d2,c48,"Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing",Ushering in a New Frontier in Geospace Through Data Science,"Our understanding and specification of solar‐terrestrial interactions benefit from taking advantage of comprehensive data‐intensive approaches. These data‐driven methods are taking on new importance in light of the shifting data landscape of the geospace system, which extends from the near Earth space environment, through the magnetosphere and interplanetary space, to the Sun. The space physics community faces both an exciting opportunity and an important imperative to create a new frontier built at the intersection of traditional approaches and state‐of‐the‐art data‐driven sciences and technologies. This brief commentary addresses the current paradigm of geospace science and the emerging need for data science innovation, discusses the meaning of data science in the context of geospace, and highlights community efforts to respond to the changing landscape.",poster,cp48
Geology,p183,d16,9a552de12d0b5a1561cf741f3170a0864f2b15d2,c48,"Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing",Ushering in a New Frontier in Geospace Through Data Science,"Our understanding and specification of solar‐terrestrial interactions benefit from taking advantage of comprehensive data‐intensive approaches. These data‐driven methods are taking on new importance in light of the shifting data landscape of the geospace system, which extends from the near Earth space environment, through the magnetosphere and interplanetary space, to the Sun. The space physics community faces both an exciting opportunity and an important imperative to create a new frontier built at the intersection of traditional approaches and state‐of‐the‐art data‐driven sciences and technologies. This brief commentary addresses the current paradigm of geospace science and the emerging need for data science innovation, discusses the meaning of data science in the context of geospace, and highlights community efforts to respond to the changing landscape.",poster,cp48
Computer Science,p184,d3,dd340315c44a9c68391d8d2f600a0adc76b70c09,c17,International Conference on Statistical and Scientific Database Management,Fides: Towards a Platform for Responsible Data Science,"Issues of responsible data analysis and use are coming to the forefront of the discourse in data science research and practice, with most significant efforts to date on the part of the data mining, machine learning, and security and privacy communities. In these fields, the research has been focused on analyzing the fairness, accountability and transparency (FAT) properties of specific algorithms and their outputs. Although these issues are most apparent in the social sciences where fairness is interpreted in terms of the distribution of resources across protected groups, management of bias in source data affects a variety of fields. Consider climate change studies that require representative data from geographically diverse regions, or supply chain analyses that require data that represents the diversity of products and customers. Any domain that involves sparse or sampled data has exposure to potential bias. In this vision paper, we argue that FAT properties must be considered as database system issues, further upstream in the data science lifecycle: bias in source data goes unnoticed, and bias may be introduced during pre-processing (fairness), spurious correlations lead to reproducibility problems (accountability), and assumptions made during pre-processing have invisible but significant effects on decisions (transparency). As machine learning methods continue to be applied broadly by non-experts, the potential for misuse increases. We see a need for a data sharing and collaborative analytics platform with features to encourage (and in some cases, enforce) best practices at all stages of the data science lifecycle. We describe features of such a platform, which we term Fides, in the context of urban analytics, outlining a systems research agenda in responsible data science.",fullPaper,cp17
Computer Science,p186,d3,38fadf7c21c32b183fa3dcf32da1044e8441b813,c111,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,The Data Science Handbook,"microbial community dynamics, Support Vector Machines, a robust prediction method with applications in bioinformatics, Bayesian Model Selection for Data with High Dimension, High dimensional statistical inference: theoretical development to data analytics, Big data challenges in genomics, Analysis of microarray gene expression data using information theory and stochastic algorithm, Hybrid Models, Markov Chain Monte Carlo Methods: Theory and Practice, and more. Provides the authority and expertise of leading contributors from an international board of authors Presents the latest release in the Handbook of Statistics series Updated release includes the latest information on",poster,cp111
Computer Science,p187,d3,ce0b7ee60920f9b37f88cab785cb8b4dc337e89f,c103,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,Educational data science in massive open online courses,"The current massive open online course (MOOC) euphoria is revolutionizing online education. Despite its expediency, there is considerable skepticism over various concerns. In order to resolve some of these problems, educational data science (EDS) has been used with success. MOOCs provide a wealth of information about the way in which a large number of learners interact with educational platforms and engage with the courses offered. This extensive amount of data provided by MOOCs concerning students' usage information is a gold mine for EDS. This paper aims to provide the reader with a complete and comprehensive review of the existing literature that helps us understand the application of EDS in MOOCs. The main works in this area are described and grouped by task or issue to be solved, along with the techniques used. WIREs Data Mining Knowl Discov 2017, 7:e1187. doi: 10.1002/widm.1187",poster,cp103
Computer Science,p189,d3,31485e1213dd886fa2b668eefcd9b13533d8a9fe,c83,"International Convention on Information and Communication Technology, Electronics and Microelectronics",Big data and data science: what should we teach?,"The era of big data has arrived. Big data bring us the data‐driven paradigm and enlighten us to challenge new classes of problems we were not able to solve in the past. We are beginning to see the impacts of big data in every aspect of our lives and society. We need a science that can address these big data problems. Data science is a new emerging discipline that was termed to address challenges that we are facing and going to face in the big data era. Thus, education in data science is the key to success, and we need concrete strategies and approaches to better educate future data scientists. In this paper, we discuss general concepts on big data, data science, and data scientists and show the results of an extensive survey on current data science education in United States. Finally, we propose various approaches that data science education should aim to accomplish.",poster,cp83
Computer Science,p190,d3,aeede2d75d7cb3e10bc3b732a897ca1a7bfc12c5,c25,IEEE International Parallel and Distributed Processing Symposium,"Data Science and Big Data Analytics: Discovering, Analyzing, Visualizing and Presenting Data","Data Science and Big Data Analytics: Discovering, Analyzing, Visualizing and Presenting Data By EMC Education Services Data Science and Big Data Analytics is about harnessing the power of data for new insights. The book covers the breadth of activities and methods and tools that Data Scientists use. The content focuses on concepts, principles and practical applications that are applicable to any industry and technology environment, and the learning is supported and explained with examples that you can replicate using open-source software. This book will help you: Become a contributor on a data science team ●",poster,cp25
Computer Science,p191,d3,e1a1ad4025e2c7a82882c7389d937cbdfd10b799,j66,Data Science Journal,Towards Data Science,"Currently, a huge amount of data is being rapidly generated in cyberspace. Datanature (all data in cyberspace) is forming due to a data explosion. Exploring the patterns and rules in datanature is necessary but difficult. A new discipline called Data Science is coming. It provides a type of novel research method (a data-intensive method) for natural and social sciences and goes beyond computer science in researching data. This paper presents the challenges presented by data and discusses what differentiates data science from the established sciences, data technologies, and big data. Our goal is to encourage data related researchers to transfer their focus towards this new science.",fullPaper,jv66
Computer Science,p193,d3,0ec1992151e28c5678832c0923e56aeb58caad53,c18,International Conference on Exploring Services Science,From Data Science to Value Creation,Abstract,fullPaper,cp18
Computer Science,p194,d3,5a56bbd762e9dd70dd20afe8740a6d09ec85ffed,c98,Vision,Data science from scratch,"This is a first-principles-based, practical introduction to the fundamentals of data science aimed at the mathematically-comfortable reader with some programming skills. The book covers: The important parts of Python to know The important parts of Math / Probability / Statistics to know The basics of data science How commonly-used data science techniques work (learning by implementing them) What is Map-Reduce and how to do it in Python Other applications such as NLP, Network Analysis, and more",poster,cp98
Computer Science,p196,d3,87a7e55b4c3116751edb4b0f74e0484eaf7a853d,j67,Information systems research,"Editorial - Big Data, Data Science, and Analytics: The Opportunity and Challenge for IS Research","We address key questions related to the explosion of interest in the emerging fields of big data, analytics, and data science. We discuss the novelty of the fields and whether the underlying questions are fundamentally different, the strengths that the information systems IS community brings to this discourse, interesting research questions for IS scholars, the role of predictive and explanatory modeling, and how research in this emerging area should be evaluated for contribution and significance.",fullPaper,jv67
Economics,p196,d11,87a7e55b4c3116751edb4b0f74e0484eaf7a853d,j67,Information systems research,"Editorial - Big Data, Data Science, and Analytics: The Opportunity and Challenge for IS Research","We address key questions related to the explosion of interest in the emerging fields of big data, analytics, and data science. We discuss the novelty of the fields and whether the underlying questions are fundamentally different, the strengths that the information systems IS community brings to this discourse, interesting research questions for IS scholars, the role of predictive and explanatory modeling, and how research in this emerging area should be evaluated for contribution and significance.",fullPaper,jv67
Computer Science,p197,d3,ae118a88ada51dfdb2296cbaa948eb4a467942b6,c102,ACM SIGMOD Conference,"Computer Age Statistical Inference: Algorithms, Evidence, and Data Science","The twenty-first century has seen a breathtaking expansion of statistical methodology, both in scope and in influence. 'Big data', 'data science', and 'machine learning' have become familiar terms in the news, as statistical methods are brought to bear upon the enormous data sets of modern science and commerce. How did we get here? And where are we going? This book takes us on an exhilarating journey through the revolution in data analysis following the introduction of electronic computation in the 1950s. Beginning with classical inferential theories - Bayesian, frequentist, Fisherian - individual chapters take up a series of influential topics: survival analysis, logistic regression, empirical Bayes, the jackknife and bootstrap, random forests, neural networks, Markov chain Monte Carlo, inference after model selection, and dozens more. The distinctly modern approach integrates methodology and algorithms with statistical inference. The book ends with speculation on the future direction of statistics and data science.",poster,cp102
Computer Science,p198,d3,b26c93eba9e1d99a5c99b07d2476714b386c4d54,c11,European Conference on Modelling and Simulation,Agile big data analytics: AnalyticsOps for data science,"Big data analytic (BDA) systems leverage data distribution and parallel processing across a cluster of resources. This introduces a number of new challenges specifically for analytics. The analytics portion of the complete lifecycle has typically followed a waterfall process — completing one step before beginning the next. While efforts have been made to map different types of analytics to an agile methodology, the steps are often described as breaking activities into smaller tasks while the overall process is still consistent with step-by-step waterfall. BDA changes a number of the activities in the analytics lifecycle, as well as their ordering. The goal of agile analytics — to reach a point of optimality between generating value from data and the time spent getting there. This paper discusses the implications of an agile process for BDA in cleansing, transformation, and analytics.",poster,cp11
Computer Science,p199,d3,62806c60226d54ba1a4455bb1d7d2f034ef7c29a,c105,International Conference on Automatic Face and Gesture Recognition,"Introducing Data Science: Big Data, Machine Learning, and more, using Python tools","Summary Introducing Data Science teaches you how to accomplish the fundamental tasks that occupy data scientists. Using the Python language and common Python libraries, you'll experience firsthand the challenges of dealing with data at scale and gain a solid foundation in data science. Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications. About the Technology Many companies need developers with data science skills to work on projects ranging from social media marketing to machine learning. Discovering what you need to learn to begin a career as a data scientist can seem bewildering. This book is designed to help you get started. About the BookIntroducing Data Science Introducing Data Science explains vital data science concepts and teaches you how to accomplish the fundamental tasks that occupy data scientists. Youll explore data visualization, graph databases, the use of NoSQL, and the data science process. Youll use the Python language and common Python libraries as you experience firsthand the challenges of dealing with data at scale. Discover how Python allows you to gain insights from data sets so big that they need to be stored on multiple machines, or from data moving so quickly that no single machine can handle it. This book gives you hands-on experience with the most popular Python data science libraries, Scikit-learn and Stats Models. After reading this book, youll have the solid foundation you need to start a career in data science. Whats Inside Handling large data Introduction to machine learning Using Python to work with data Writing data science algorithms About the ReaderThis book assumes you're comfortable reading code in Python or a similar language, such as C, Ruby, or JavaScript. No prior experience with data science is required. About the Authors Davy Cielen, Arno D. B. Meysman, and Mohamed Ali are the founders and managing partners of Optimately and Maiton, where they focus on developing data science projects and solutions in various sectors.",poster,cp105
Computer Science,p200,d3,52ff64f7f26b28447af255fedeb2216a70b48d66,c3,Knowledge Discovery and Data Mining,Large Scale Distributed Data Science using Apache Spark,"Apache Spark is an open-source cluster computing framework for big data processing. It has emerged as the next generation big data processing engine, overtaking Hadoop MapReduce which helped ignite the big data revolution. Spark maintains MapReduce's linear scalability and fault tolerance, but extends it in a few important ways: it is much faster (100 times faster for certain applications), much easier to program in due to its rich APIs in Python, Java, Scala (and shortly R), and its core data abstraction, the distributed data frame, and it goes far beyond batch applications to support a variety of compute-intensive tasks, including interactive queries, streaming, machine learning, and graph processing. This tutorial will provide an accessible introduction to Spark and its potential to revolutionize academic and commercial data science practices.",fullPaper,cp3
Computer Science,p202,d3,4c05d4410c0023e14f2bb0cbcf7613468855430b,c7,International Symposium on Intelligent Data Analysis,A Data Science Course for Undergraduates: Thinking With Data,"Data science is an emerging interdisciplinary field that combines elements of mathematics, statistics, computer science, and knowledge in a particular application domain for the purpose of extracting meaningful information from the increasingly sophisticated array of data available in many settings. These data tend to be nontraditional, in the sense that they are often live, large, complex, and/or messy. A first course in statistics at the undergraduate level typically introduces students to a variety of techniques to analyze small, neat, and clean datasets. However, whether they pursue more formal training in statistics or not, many of these students will end up working with data that are considerably more complex, and will need facility with statistical computing techniques. More importantly, these students require a framework for thinking structurally about data. We describe an undergraduate course in a liberal arts environment that provides students with the tools necessary to apply data science. The course emphasizes modern, practical, and useful skills that cover the full data analysis spectrum, from asking an interesting question to acquiring, managing, manipulating, processing, querying, analyzing, and visualizing data, as well communicating findings in written, graphical, and oral forms. Supplementary materials for this article are available online. [Received June 2014. Revised July 2015.]",poster,cp7
Mathematics,p202,d6,4c05d4410c0023e14f2bb0cbcf7613468855430b,c7,International Symposium on Intelligent Data Analysis,A Data Science Course for Undergraduates: Thinking With Data,"Data science is an emerging interdisciplinary field that combines elements of mathematics, statistics, computer science, and knowledge in a particular application domain for the purpose of extracting meaningful information from the increasingly sophisticated array of data available in many settings. These data tend to be nontraditional, in the sense that they are often live, large, complex, and/or messy. A first course in statistics at the undergraduate level typically introduces students to a variety of techniques to analyze small, neat, and clean datasets. However, whether they pursue more formal training in statistics or not, many of these students will end up working with data that are considerably more complex, and will need facility with statistical computing techniques. More importantly, these students require a framework for thinking structurally about data. We describe an undergraduate course in a liberal arts environment that provides students with the tools necessary to apply data science. The course emphasizes modern, practical, and useful skills that cover the full data analysis spectrum, from asking an interesting question to acquiring, managing, manipulating, processing, querying, analyzing, and visualizing data, as well communicating findings in written, graphical, and oral forms. Supplementary materials for this article are available online. [Received June 2014. Revised July 2015.]",poster,cp7
Computer Science,p203,d3,0442b04b4e8741900b65de0721f0c3e152e044ef,c44,Italian National Conference on Sensors,Materials Data Science: Current Status and Future Outlook,"The field of materials science and engineering is on the cusp of a digital data revolution. After reviewing the nature of data science and Big Data, we discuss the features of materials data that distinguish them from data in other fields. We introduce the concept of process-structure-property (PSP) linkages and illustrate how the determination of PSPs is one of the main objectives of materials data science. Then we review a selection of materials databases, as well as important aspects of materials data management, such as storage hardware, archiving strategies, and data access strategies. We introduce the emerging field of materials data analytics, which focuses on data-driven approaches to extract and curate materials knowledge from available data sets. The critical need for materials e-collaboration platforms is highlighted, and we conclude the article with a number of suggestions regarding the near-term future of the materials data science field.",poster,cp44
Computer Science,p204,d3,259d81ced1837bb74f3eeeb30ca3217d535e0c31,c32,International Conference on Smart Data Services,Introduction to HPC with MPI for Data Science,Abstract,poster,cp32
Computer Science,p205,d3,c740a6816155fd123081d2f78926a0d3819926e7,c46,Ideal,LibGuides: *Data Science: Data Science Resources,"Data science resources, from finding ebooks and blogs, to finding raw datasets and analysis. Learn about data science resources, analysis, communities and data management. Also learn about hte datasets openly available and dataset purchase program.",poster,cp46
Computer Science,p206,d3,37095b714dad5895d946b1f8435a3a38dee1be8b,c58,Extreme Science and Engineering Discovery Environment,"Data quality for data science, predictive analytics, and big data in supply chain management: An introduction to the problem and suggestions for research and applications",Abstract,poster,cp58
Computer Science,p210,d3,c04aaf36c8587e40747212e316d9bf44186ef64a,c24,International Conference on Data Technologies and Applications,Developing a Research Agenda for Human-Centered Data Science,"The study and analysis of large and complex data sets offer a wealth of insights in a variety of applications. Computational approaches provide researchers access to broad assemblages of data, but the insights extracted may lack the rich detail that qualitative approaches have brought to the understanding of sociotechnical phenomena. How do we preserve the richness associated with traditional qualitative methods while utilizing the power of large data sets? How do we uncover social nuances or consider ethics and values in data use? These and other questions are explored by human-centered data science, an emerging field at the intersection of human-computer interaction (HCI), computer-supported cooperative work (CSCW), human computation, and the statistical and computational techniques of data science. This workshop, the first of its kind at CSCW, seeks to bring together researchers interested in human-centered approaches to data science to collaborate, define a research agenda, and form a community.",poster,cp24
Computer Science,p211,d3,b9888bb70d6f246c7ffb53dcb9498bfafe113d8f,c80,International Symposium on Software Engineering for Adaptive and Self-Managing Systems,Thinking by classes in data science: the symbolic data analysis paradigm,"Data Science, considered as a science by itself, is in general terms, the extraction of knowledge from data. Symbolic data analysis (SDA) gives a new way of thinking in Data Science by extending the standard input to a set of classes of individual entities. Hence, classes of a given population are considered to be units of a higher level population to be studied. Such classes often represent the real units of interest. In order to take variability between the members of each class into account, classes are described by intervals, distributions, set of categories or numbers sometimes weighted and the like. In that way, we obtain new kinds of data, called ‘symbolic’ as they cannot be reduced to numbers without losing much information. The first step in SDA is to build the symbolic data table where the rows are classes and the variables can take symbolic values. The second step is to study and extract new knowledge from these new kinds of data by at least an extension of Computer Statistics and Data Mining to symbolic data. SDA is a new paradigm which opens up a vast domain of research and applications by giving complementary results to classical methods applied to standard data. SDA also gives answers to big data and complex data challenges as big data can be reduced and summarized by classes and as complex data with multiple unstructured data tables and unpaired variables can be transformed into a structured data table with paired symbolic‐valued variables. WIREs Comput Stat 2016, 8:172–205. doi: 10.1002/wics.1384",poster,cp80
Computer Science,p213,d3,6b705d7ef453d42d87a9099b31344adad2367f40,c36,International Conference on Information Technology Based Higher Education and Training,EDISON Data Science Framework: A Foundation for Building Data Science Profession for Research and Industry,"Data Science is an emerging field of science, which requires a multi-disciplinary approach and should be built with a strong link to emerging Big Data and data driven technologies, and consequently needs re-thinking and re-design of both traditional educational models and existing courses. The education and training of Data Scientists currently lacks a commonly accepted, harmonized instructional model that reflects by design the whole lifecycle of data handling in modern, data driven research and the digital economy. This paper presents the EDISON Data Science Framework (EDSF) that is intended to create a foundation for the Data Science profession definition. The EDSF includes the following core components: Data Science Competence Framework (CF-DS), Data Science Body of Knowledge (DS-BoK), Data Science Model Curriculum (MC-DS), and Data Science Professional profiles (DSP profiles). The MC-DS is built based on CF-DS and DS-BoK, where Learning Outcomes are defined based on CF-DS competences and Learning Units are mapped to Knowledge Units in DS-BoK. In its own turn, Learning Units are defined based on the ACM Classification of Computer Science (CCS2012) and reflect typical courses naming used by universities in their current programmes. The paper provides example how the proposed EDSF can be used for designing effective Data Science curricula and reports the experience of implementing EDSF by the Champion Universities that cooperate with the EDISON project.",poster,cp36
Computer Science,p214,d3,071036abe55e7247d7e6ec28a4afc8ef2670f479,c59,Australian Software Engineering Conference,A Comparison of Open Source Tools for Data Science,"The next decade of competitive advantage revolves around the ability to make predictions and discover patterns in data. Data science is at the center of this revolution. Data science has been termed the sexiest job of the 21st century. Data science combines data mining, machine learning, and statistical methodologies to extract knowledge and leverage predictions from data. Given the need for data science in organizations, many small or medium organizations are not adequately funded to acquire expensive data science tools. Open source tools may provide the solution to this issue. While studies comparing open source tools for data mining or business intelligence exist, an update on the current state of the art is necessary. This work explores and compares common open source data science tools. Implications include an overview of the state of the art and knowledge for practitioners and academics to select an open source data science tool that suits the requirements of specific data science projects.",poster,cp59
Computer Science,p215,d3,dc86a7295d737fc2f195b67a273e90b549bd6272,j62,Business & Information Systems Engineering,Business Analytics and Data Science: Once Again?,Abstract,fullPaper,jv62
Computer Science,p216,d3,8ffe80d758a78810c7d5a33a088cd4529b8a6a4b,j68,Journal of Decision Systems,Data science: supporting decision-making,"Abstract Data science is a new academic trans-discipline that builds on 60 years of research about supporting decision-making in organisations. It is an important and potentially significant concept and practice. Contemplating the need for data scientists encourages academics and managers to examine issues of decision-maker rationality, data and data analysis needs, analytical tools, job skills and academic preparation. This article explores data science and the data professionals who will use new data streams and analytics to support decision-making. It also examines the dimensions that are changing in the data stream and the skills needed by data scientists to analyse the new data streams. Organisations need data scientists, but academics need to understand the new data science jobs to prepare more people to support decision-making.",fullPaper,jv68
Computer Science,p217,d3,ca9f74a1a7b69214c670202bb4f66eb16194f836,c5,Technical Symposium on Computer Science Education,Datathons: An Experience Report of Data Hackathons for Data Science Education,Large amounts of data are becoming increasingly available through open data repositories as well as companies and governments collecting data to improve decision making and efficiencies. Consequently there is a need to increase the data literacy of computer science students. Data science is a relatively new area within computer science and the curriculum is rapidly evolving along with the tools required to perform analytics which students need to learn how to effectively use. To address the needs of students learning key data science and analytics skills we propose augmenting existing data science curriculums with hackathon events that focus on data also known as datathons. In this paper we present our experience at hosting and running four datathons that involved students and members from the community coming together to solve challenging problems with data from not-for-profit social good organizations and publicly open data. Our reported experience from our datathons will help inform other academics and community groups who also wish to host datathons to help facilitate their students and members to learn key data science and analytics skills.,fullPaper,cp5
Computer Science,p218,d3,3e209c705350761fe676ac330503e8662279fbf2,j69,IEEE Transactions on Services Computing,Processes Meet Big Data: Connecting Data Science with Process Science,"As more and more companies are embracing Big data, it has become apparent that the ultimate challenge is to relate massive amounts of event data to processes that are highly dynamic. To unleash the value of event data, events need to be tightly connected to the control and management of operational processes. However, the primary focus of Big data technologies is currently on storage, processing, and rather simple analytical tasks. Big data initiatives rarely focus on the improvement of end-to-end processes. To address this mismatch, we advocate a better integration of data science, data technology and process science. Data science approaches tend to be process agonistic whereas process science approaches tend to be model-driven without considering the “evidence” hidden in the data. Process mining aims to bridge this gap. This editorial discusses the interplay between data science and process science and relates process mining to Big data technologies, service orientation, and cloud computing.",fullPaper,jv69
Computer Science,p221,d3,f152a4008f114ac19076ee6b98d431268f4aea9e,c5,Technical Symposium on Computer Science Education,A Practical and Sustainable Model for Learning and Teaching Data Science,"This paper details our experiences with design and implementation of data science curriculum at University at Buffalo (UB). We discuss (i) briefly the history of project, (ii) a certificate program that we created, (iii) a data-intensive computing course that forms the core of the curriculum and (iv) some of the challenges we faced and how we addressed them. Major goal of the project was to improve the preparedness of our workforce for the emerging data-intensive computing area. We measured this through assessment of student learning on various concepts and topics related to data-intensive computing. We also discuss the best practices in building a data science program. We highlight the importance of external funding support and multi-disciplinary collaborations in the success of the project. The pedagogical resources created for the project are freely available to help educators and other learners navigate the path to learning data science. We expect this paper about our experience will provide a road map for educators who desire to introduce data science in their curriculum.",fullPaper,cp5
Computer Science,p224,d3,b0150dd118ebedbc3ece68726e065f9afaaf3b18,c63,International Conference on Evaluation & Assessment in Software Engineering,Big data analytics and big data science: a survey,"Big data has attracted much attention from academia and industry. But the discussion of big data is disparate, fragmented and distributed among different outlets. This paper conducts a systematic and extensive review on 186 journal publications about big data from 2011 to 2015 in the Science Citation Index (SCI) and the Social Science Citation Index (SSCI) database aiming to provide scholars and practitioners with a comprehensive overview and big picture about research on big data. The selected papers are grouped into 20 research categories. The contents of the paper(s) in each research category are summarized. Research directions for each category are outlined as well. The results in this study indicate that the selected papers were mainly published between 2013 and 2015 and focus on technological issues regarding big data. Diverse new approaches, methods, frameworks and systems are proposed for data collection, storage, transport, processing and analysis in the selected papers. Possible directions for f...",poster,cp63
Computer Science,p225,d3,49522df4fab1ebbeb831fc265196c2c129bf6087,c64,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",Survey on data science with population-based algorithms,Abstract,poster,cp64
Computer Science,p229,d3,061c3291d817076dbb3e5a41c51f99800a390e94,c33,Workshop on Python for High-Performance and Scientific Computing,"Data Science and Big Data Analytics: Discovering, Analyzing, Visualizing and Presenting Data","Data Science and Big Data Analytics: Discovering, Analyzing, Visualizing and Presenting Data By EMC Education Services Data Science and Big Data Analytics is about harnessing the power of data for new insights. The book covers the breadth of activities and methods and tools that Data Scientists use. The content focuses on concepts, principles and practical applications that are applicable to any industry and technology environment, and the learning is supported and explained with examples that you can replicate using open-source software. This book will help you: Become a contributor on a data science team ●",poster,cp33
Computer Science,p230,d3,fa15d626d8905d08953abe646a75a31417ad61fa,c51,International Conference on Engineering Education,"Data science on the ground: Hype, criticism, and everyday work","Modern organizations often employ data scientists to improve business processes using diverse sets of data. Researchers and practitioners have both touted the benefits and warned of the drawbacks associated with data science and big data approaches, but few studies investigate how data science is carried out “on the ground.” In this paper, we first review the hype and criticisms surrounding data science and big data approaches. We then present the findings of semistructured interviews with 18 data analysts from various industries and organizational roles. Using qualitative coding techniques, we evaluated these interviews in light of the hype and criticisms surrounding data science in the popular discourse. We found that although the data analysts we interviewed were sensitive to both the allure and the potential pitfalls of data science, their motivations and evaluations of their work were more nuanced. We conclude by reflecting on the relationship between data analysts' work and the discourses around data science and big data, suggesting how future research can better account for the everyday practices of this profession.",poster,cp51
Computer Science,p231,d3,de84e808462b8240c75987364a6d518eff7d8813,c56,International Conference on Automated Software Engineering,Statistics: a data science for the 21st century,"The rise of data science could be seen as a potental threat to the long‐term status of the statistics discipline. I first argue that, although there is a threat, there is also a much greater opportunity to re‐emphasize the universal relevance of statistical method to the interpretation of data, and I give a short historical outline of the increasingly important links between statistics and information technology. The core of the paper is a summary of several recent research projects, through which I hope to demonstrate that statistics makes an essential, but incomplete, contribution to the emerging field of ‘electronic health’ research. Finally, I offer personal thoughts on how statistics might best be organized in a research‐led university, on what we should teach our students and on some issues broadly related to data science where the Royal Statistical Society can take a lead.",poster,cp56
Computer Science,p232,d3,e926ef463fa96b3d06a321fcbcccdab9ff5f3da0,j70,Statistics and computing,Statistics and computing: the genesis of data science,Abstract,fullPaper,jv70
Computer Science,p233,d3,8f6a4609531ca9ff35915c32dae5cd146fc57c40,c81,ACM Symposium on Applied Computing,HEALTH BANK - A Workbench for Data Science Applications in Healthcare,"The enormous amounts of data that are generated in the healthcare process and stored in electronic health record (EHR) systems are an underutilized resource that, with the use of data science applica- tions, can be exploited to improve healthcare. To foster the development and use of data science applications in healthcare, there is a fundamen- tal need for access to EHR data, which is typically not readily available to researchers and developers. A relatively rare exception is the large EHR database, the Stockholm EPR Corpus, comprising data from more than two million patients, that has been been made available to a lim- ited group of researchers at Stockholm University. Here, we describe a number of data science applications that have been developed using this database, demonstrating the potential reuse of EHR data to support healthcare and public health activities, as well as facilitate medical re- search. However, in order to realize the full potential of this resource, it needs to be made available to a larger community of researchers, as well as to industry actors. To that end, we envision the provision of an in- frastructure around this database called HEALTH BANK – the Swedish Health Record Research Bank. It will function both as a workbench for the development of data science applications and as a data explo- ration tool, allowing epidemiologists, pharmacologists and other medical researchers to generate and evaluate hypotheses. Aggregated data will be fed into a pipeline for open e-access, while non-aggregated data will be provided to researchers within an ethical permission framework. We believe that HEALTH BANK has the potential to promote a growing industry around the development of data science applications that will ultimately increase the efficiency and effectiveness of healthcare.",poster,cp81
Computer Science,p235,d3,e78d7fa72a5dbe5f3bc93f6e200826004f23530b,j72,IEEE Intelligent Systems,Data Science: Nature and Pitfalls,Data science is creating exciting trends as well as significant controversy. A critical matter for the healthy development of data science in its early stages is to deeply understand the nature of data and data science and discuss the various pitfalls. These important issues motivate the discussions in this article.,fullPaper,jv72
Computer Science,p237,d3,d343c9823bcacf31ea4aca105d0366f3f18a75e5,j72,IEEE Intelligent Systems,The Role of Data Science in Web Science,"Web science relies on an interdisciplinary approach that seeks to go beyond what any one subject can say about the World Wide Web. By incorporating numerous disciplinary perspectives and relying heavily on domain knowledge and expertise, data science has emerged as an important new area that integrates statistics with computational knowledge, data collection, cleaning and processing, analysis methods, and visualization to produce actionable insights from big data. As a discipline to use within Web science research, data science offers significant opportunities for uncovering trends in large Web-based datasets. A Web science observatory exemplifies this relationship by offering an online platform of tools for carrying out Web science research, allowing users to carry out data science techniques to produce insights into Web science issues such as community development, online behavior, and information propagation. The authors outline the similarities and differences of these two growing subject areas to demonstrate the important relationship developing between them.",fullPaper,jv72
Computer Science,p238,d3,bff0d1d3a3251cb7bcbeb424ae0580c3085649f7,j74,International Journal of System Dynamics Applications,Integrating Systems Modelling and Data Science: The Joint Future of Simulation and 'Big Data' Science,"Although System Dynamics modelling is sometimes referred to as data-poor modelling, it often is -or could be-applied in a data-rich manner. However, more can be done in the era of 'big data'. Big data refers here to situations with much more available data than was until recently manageable. The field of data science makes bigger data manageable. This paper provides a perspective on the future of System Dynamics with a prominent place for bigger data and data science. It discusses different approaches for dealing with bigger data. It reviews methods, techniques and tools for dealing with bigger data in System Dynamics, and sheds light on the modelling phases for which data science is most useful. Finally, it provides several examples of current applications in which big data, data science, and System Dynamics modelling and simulation are being merged.",fullPaper,jv74
Computer Science,p239,d3,def43235dba7eb98659fb8879fa9d27695029df2,j75,IEEE Geoscience and Remote Sensing Magazine,Recent Activities in Earth Data Science [Technical Committees],"Recent trends on big Earth-observing (EO) data lead to some questions that the Earth science community needs to address. Are we experiencing a paradigm shift in Earth science research now? How can we better utilize the explosion of technology maturation to create new forms of EO data processing? Can we summarize the existing methodologies and technologies scaling to big EO data as a new field named earth data science? Big data technologies are being widely practiced in Earth sciences and remote sensing communities to support EO data access, processing, and knowledge discovery. The data-intensive scientific discovery, named the fourth paradigm, leads to data science in the big data era [1]. According to the definition by the U.S. National Institute of Standards and Technology, the data science paradigm is the ""extraction of actionable knowledge directly from data through a process of discovery, hypothesis, and hypothesis testing"" [2]. Earth data science is the art and science of applying the data science paradigm to EO data.",fullPaper,jv75
Computer Science,p241,d3,5ea0821f37481dafab363a47bf9b904e986f5a20,j13,International Journal of Data Science and Analysis,Data science and analytics: a new era,Abstract,fullPaper,jv13
Computer Science,p243,d3,682b105746238d3c39bd4f6cd0baa375dc0c2534,c43,European Conference on Machine Learning,Perspectives on Data Science for Software Engineering,Abstract,poster,cp43
Computer Science,p244,d3,520515cfffcd2f439469398d7c959f8baa9ccc8b,c44,Italian National Conference on Sensors,Philosophy of Big Data: Expanding the Human-Data Relation with Big Data Science Services,"Big data is growing as an area of information technology, service, and science, and so too is the need for its intellectual understanding and interpretation from a theoretical, philosophical, and societal perspective. The Philosophy of Big Data is the branch of philosophy concerned with the foundations, methods, and implications of big data, the definitions, meaning, conceptualization, knowledge possibilities, truth standards, and practices in situations involving very-large data sets that are big in volume, velocity, variety, veracity, and variability. The Philosophy of Big Data is evolving into a discipline at two levels, one internal to the field as a generalized articulation of the concepts, theory, and systems that comprise the overall conduct of big data science. The other is external to the field, as a consideration of the impact of big data science more broadly on individuals, society, and the world. Methods, tools, and concepts are evaluated at both the level of industry practice theory and social impact. Three aspects are considered: what might constitute a Philosophy of Big Data, how the disciplines of the Philosophy of Information and the Philosophy of Big Data are developing, and an example of the Philosophy of Big Data in application in the data-intensive science field of Synthetic Biology. Overall a Philosophy of Big Data might helpful in conceptualizing and realizing big data science as a service practice, and also in transitioning to data-rich futures with human and data entities more productively co-existing in mutual growth and collaboration.",poster,cp44
Computer Science,p246,d3,04831fedd16110da4cbd0798d16e21fbbc34ad06,j77,International Journal of Intelligent Computing and Cybernetics,A survey of open source data science tools,"Purpose – Data science is the study of the generalizable extraction of knowledge from data. It includes a variety of components and develops on methods and concepts from many domains, containing mathematics, probability models, machine learning, statistical learning, computer programming, data engineering, pattern recognition and learning, visualization and data warehousing aiming to extract value from data. The purpose of this paper is to provide an overview of open source (OS) data science tools, proposing a classification scheme that can be used to study OS data science software. Design/methodology/approach – The proposed classification scheme is based on general characteristics, project activity, operational characteristics and data mining characteristics. The authors then use the proposed scheme to examine 70 identified Open Source Software. From this the authors provide insight about the current status of OS data science tools and reveal the state-of-the-art tools. Findings – The features of 70 OS t...",fullPaper,jv77
Computer Science,p247,d3,57c82a005ae353f4683938b15a52e1b0561f6e43,c35,"International Conference on Internet of Things, Big Data and Security","R for Data Science: Import, Tidy, Transform, Visualize, and Model Data","Learn how to use R to turn raw data into insight, knowledge, and understanding. This book introduces you to R, RStudio, and the tidyverse, a collection of R packages designed to work together to make data science fast, fluent, and fun. Suitable for readers with no previous programming experience, R for Data Science is designed to get you doing data science as quickly as possible. Authors Hadley Wickham and Garrett Grolemund guide you through the steps of importing, wrangling, exploring, and modeling your data and communicating the results. Youll get a complete, big-picture understanding of the data science cycle, along with basic tools you need to manage the details. Each section of the book is paired with exercises to help you practice what youve learned along the way. Youll learn how to: Wrangletransform your datasets into a form convenient for analysisProgramlearn powerful R tools for solving data problems with greater clarity and easeExploreexamine your data, generate hypotheses, and quickly test themModelprovide a low-dimensional summary that captures true ""signals"" in your datasetCommunicatelearn R Markdown for integrating prose, code, and results",poster,cp35
Computer Science,p250,d3,6f989651c4f592613e92c9e37a8c4ac205998cfe,c57,IEEE International Geoscience and Remote Sensing Symposium,Data Science in Statistics Curricula: Preparing Students to “Think with Data”,"A growing number of students are completing undergraduate degrees in statistics and entering the workforce as data analysts. In these positions, they are expected to understand how to use databases and other data warehouses, scrape data from Internet sources, program solutions to complex problems in multiple languages, and think algorithmically as well as statistically. These data science topics have not traditionally been a major component of undergraduate programs in statistics. Consequently, a curricular shift is needed to address additional learning outcomes. The goal of this article is to motivate the importance of data science proficiency and to provide examples and resources for instructors to implement data science in their own statistics curricula. We provide case studies from seven institutions. These varied approaches to teaching data science demonstrate curricular innovations to address new needs. Also included here are examples of assignments designed for courses that foster engagement of undergraduates with data and data science. [Received November 2014. Revised July 2015.]",poster,cp57
Mathematics,p250,d6,6f989651c4f592613e92c9e37a8c4ac205998cfe,c57,IEEE International Geoscience and Remote Sensing Symposium,Data Science in Statistics Curricula: Preparing Students to “Think with Data”,"A growing number of students are completing undergraduate degrees in statistics and entering the workforce as data analysts. In these positions, they are expected to understand how to use databases and other data warehouses, scrape data from Internet sources, program solutions to complex problems in multiple languages, and think algorithmically as well as statistically. These data science topics have not traditionally been a major component of undergraduate programs in statistics. Consequently, a curricular shift is needed to address additional learning outcomes. The goal of this article is to motivate the importance of data science proficiency and to provide examples and resources for instructors to implement data science in their own statistics curricula. We provide case studies from seven institutions. These varied approaches to teaching data science demonstrate curricular innovations to address new needs. Also included here are examples of assignments designed for courses that foster engagement of undergraduates with data and data science. [Received November 2014. Revised July 2015.]",poster,cp57
Computer Science,p252,d3,92efba7c622f54b8cd7b0d70d7cc09063e17b4f3,c5,Technical Symposium on Computer Science Education,An undergraduate degree in data science: curriculum and a decade of implementation experience,"We describe Data Science, a four-year undergraduate program in predictive analytics, machine learning, and data mining implemented at the College of Charleston, Charleston, South Carolina, USA. We present a ten-year status report detailing the program's origins, successes, and challenges. Our experience demonstrates that education and training for big data concepts are possible and practical at the undergraduate level. The development of this program parallels the growing demand for finding utility in data sets and streaming data. The curriculum is a seventy-seven credit-hour program that has been successfully implemented in a liberal arts and sciences institution by the faculties of computer science and mathematics.",fullPaper,cp5
Computer Science,p253,d3,c7d7d579d94b7fc67c75b68361e01ba8f59b1d40,j78,Future generations computer systems,Intelligent services for Big Data science,Abstract,fullPaper,jv78
Computer Science,p255,d3,516a53c59a53b0a471cd8a277b229925e0582114,c4,Conference on Innovative Data Systems Research,DataHub: Collaborative Data Science & Dataset Version Management at Scale,"Relational databases have limited support for data collaboration, where teams collaboratively curate and analyze large datasets. Inspired by software version control systems like git, we propose (a) a dataset version control system, giving users the ability to create, branch, merge, difference and search large, divergent collections of datasets, and (b) a platform, DATAHUB, that gives users the ability to perform collaborative data analysis building on this version control system. We outline the challenges in providing dataset version control at scale.",fullPaper,cp4
Computer Science,p256,d3,f707f6d7c3f874cb1a8aa961a50e50706731cd2d,c20,ACM Conference on Economics and Computation,Mechanism design for data science,"The promise of data science is that if data from a system can be recorded and understood then this understanding can potentially be utilized to improve the system. Behavioral and economic data, however, is different from scientific data in that it is subjective to the system. Behavior changes when the system changes, and to predict behavior for any given system change or to optimize over system changes, the behavioral model that generates the data must be inferred from the data. The ease with which this inference can be performed generally also depends on the system. Trivially, a system that ignores behavior does not admit any inference of a behavior generating model that can be used to predict behavior in a system that is responsive to behavior. To realize the promise of data science in economic systems, a theory for the design of such systems must also incorporate the desired inference properties. Consider as an example the revenue-maximizing auctioneer. If the auctioneer has knowledge of the distribution of bidder values then she can run the first-price auction with a reserve price that is tuned to the distribution. Under some mild distributional assumptions, with the appropriate reserve price the first-price auction is revenue optimal [Myerson 1981]. Notice that the historical bid data for the first-price auction with a reserve price will in most cases not have bids for bidders whose values are below the reserve. Therefore, there is no data analysis that the auctioneer can perform that will enable properties of the distribution of bidder values below the reserve price to be inferred. It could be, nonetheless, that over time the population of potential bidders evolves and the optimal reserve price lowers. This change could go completely unnoticed in the auctioneer's data. The two main tools for optimizing revenue in an auction are reserve prices (as above) and ironing. Both of these tools cause pooling behavior (i.e., bidders with distinct values take the same action) and economic inference cannot thereafter differentiate these pooled bidders. In order to maintain the distributional knowledge necessary to be able to run a good auction in the long term, the auctioneer must sacrifice the short-term revenue by running a non-revenue-optimal auction.",fullPaper,cp20
Economics,p256,d11,f707f6d7c3f874cb1a8aa961a50e50706731cd2d,c20,ACM Conference on Economics and Computation,Mechanism design for data science,"The promise of data science is that if data from a system can be recorded and understood then this understanding can potentially be utilized to improve the system. Behavioral and economic data, however, is different from scientific data in that it is subjective to the system. Behavior changes when the system changes, and to predict behavior for any given system change or to optimize over system changes, the behavioral model that generates the data must be inferred from the data. The ease with which this inference can be performed generally also depends on the system. Trivially, a system that ignores behavior does not admit any inference of a behavior generating model that can be used to predict behavior in a system that is responsive to behavior. To realize the promise of data science in economic systems, a theory for the design of such systems must also incorporate the desired inference properties. Consider as an example the revenue-maximizing auctioneer. If the auctioneer has knowledge of the distribution of bidder values then she can run the first-price auction with a reserve price that is tuned to the distribution. Under some mild distributional assumptions, with the appropriate reserve price the first-price auction is revenue optimal [Myerson 1981]. Notice that the historical bid data for the first-price auction with a reserve price will in most cases not have bids for bidders whose values are below the reserve. Therefore, there is no data analysis that the auctioneer can perform that will enable properties of the distribution of bidder values below the reserve price to be inferred. It could be, nonetheless, that over time the population of potential bidders evolves and the optimal reserve price lowers. This change could go completely unnoticed in the auctioneer's data. The two main tools for optimizing revenue in an auction are reserve prices (as above) and ironing. Both of these tools cause pooling behavior (i.e., bidders with distinct values take the same action) and economic inference cannot thereafter differentiate these pooled bidders. In order to maintain the distributional knowledge necessary to be able to run a good auction in the long term, the auctioneer must sacrifice the short-term revenue by running a non-revenue-optimal auction.",fullPaper,cp20
Computer Science,p258,d3,9d653160d048eecf1a8138407994bfc69952324b,c75,International Conference on Predictive Models in Software Engineering,Practical Data Science with R,"Summary Practical Data Science with R lives up to its name. It explains basic principles without the theoretical mumbo-jumbo and jumps right to the real use cases you'll face as you collect, curate, and analyze the data crucial to the success of your business. You'll apply the R programming language and statistical analysis techniques to carefully explained examples based in marketing, business intelligence, and decision support. Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications. About the Book Business analysts and developers are increasingly collecting, curating, analyzing, and reporting on crucial business data. The R language and its associated tools provide a straightforward way to tackle day-to-day data science tasks without a lot of academic theory or advanced mathematics. Practical Data Science with R shows you how to apply the R programming language and useful statistical techniques to everyday business situations. Using examples from marketing, business intelligence, and decision support, it shows you how to design experiments (such as A/B tests), build predictive models, and present results to audiences of all levels. This book is accessible to readers without a background in data science. Some familiarity with basic statistics, R, or another scripting language is assumed. What's Inside Data science for the business professional Statistical analysis using the R language Project lifecycle, from planning to delivery Numerous instantly familiar use cases Keys to effective data presentations About the Authors Nina Zumel and John Mount are cofounders of a San Francisco-based data science consulting firm. Both hold PhDs from Carnegie Mellon and blog on statistics, probability, and computer science at win-vector.com.",poster,cp75
Computer Science,p259,d3,0e341b2a181f71dea088dbba800e70262f91a79e,c61,"International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems","Color Science: Concepts and Methods, Quantitative Data and Formulae, 2nd Edition",Physical Data. The Eye. Colorimetry. Photometry. Visual Equivalence and Visual Matching. Uniform Color Scales. Visual Thresholds. Theories and Models of Color Vision. Appendix. References. Author and Subject Indexes.,poster,cp61
Computer Science,p260,d3,5a92ccd20e551c191ff19bdd8e75bf1b64faa54b,j79,College and Research Libraries,Dealing with Data: Science Librarians' Participation in Data Management at Association of Research Libraries Institutions,"As long as empirical research has existed, researchers have been doing “data management” in one form or another. However, funding agency mandates for doing formal data management are relatively recent, and academic libraries’ involvement has been concentrated mainly in the last few years. The National Science Foundation implemented a new mandate in January 2011, requiring researchers to include a data management plan with their proposals for funding. This has prompted many academic libraries to work more actively than before in data management, and science librarians in particular are uniquely poised to step into new roles to meet researchers’ data management needs. This study, a survey of science librarians at institutions affiliated with the Association of Research Libraries, investigates science librarians’ awareness of and involvement in institutional repositories, data repositories, and data management support services at their institutions. The study also explores the roles and responsibilities, both new and traditional, that science librarians have assumed related to data management, and the skills that science librarians believe are necessary to meet the demands of data management work. The results reveal themes of both uncertainty and optimism—uncertainty about the roles of librarians, libraries, and other campus entities; uncertainty about the skills that will be required; but also optimism about applying “traditional” librarian skills to this emerging field of academic librarianship.",fullPaper,jv79
Political Science,p260,d15,5a92ccd20e551c191ff19bdd8e75bf1b64faa54b,j79,College and Research Libraries,Dealing with Data: Science Librarians' Participation in Data Management at Association of Research Libraries Institutions,"As long as empirical research has existed, researchers have been doing “data management” in one form or another. However, funding agency mandates for doing formal data management are relatively recent, and academic libraries’ involvement has been concentrated mainly in the last few years. The National Science Foundation implemented a new mandate in January 2011, requiring researchers to include a data management plan with their proposals for funding. This has prompted many academic libraries to work more actively than before in data management, and science librarians in particular are uniquely poised to step into new roles to meet researchers’ data management needs. This study, a survey of science librarians at institutions affiliated with the Association of Research Libraries, investigates science librarians’ awareness of and involvement in institutional repositories, data repositories, and data management support services at their institutions. The study also explores the roles and responsibilities, both new and traditional, that science librarians have assumed related to data management, and the skills that science librarians believe are necessary to meet the demands of data management work. The results reveal themes of both uncertainty and optimism—uncertainty about the roles of librarians, libraries, and other campus entities; uncertainty about the skills that will be required; but also optimism about applying “traditional” librarian skills to this emerging field of academic librarianship.",fullPaper,jv79
Computer Science,p261,d3,0a7dd279ee312c9ef9c6fe04cd6f4f5e974abae3,c10,Americas Conference on Information Systems,A Data Science Solution for Mining Interesting Patterns from Uncertain Big Data,"Nowadays, high volumes of valuable uncertain data can be easily collected or generated at high velocity in many real-life applications. Mining these uncertain Big data is computationally intensive due to the presence of existential probability values associated with items in every transaction in the uncertain data. Each existential probability value expresses the likelihood of that item to be present in a particular transaction in the Big data. In some situations, users may be interested in mining all frequent patterns from these uncertain Big data, in other situations, users may be interested in only a tiny portion of these mined patterns. To reduce the computation and to focus the mining for the latter situations, we propose a tree-based algorithm that (i) allows users to express the patterns to be mined according to their intention via the use of constraints and (ii) uses MapReduce to mine uncertain Big data for only those frequent patterns that satisfy user-specified constraints. Experimental results show the effectiveness of our algorithm in mining interesting patterns from uncertain Big data.",poster,cp10
Computer Science,p262,d3,92b68d5a59262971d0f4a563c6abe1be6f2dab56,c88,International Conference on Big Data Computing and Communications,Data Science,Abstract,poster,cp88
Computer Science,p263,d3,5bbb90ae23803b8bb115d5d7f60c8defc5376e2a,c21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,"Intrinsic Relations between Data Science, Big Data, Business Analytics and Datafication","Data recording and storage have evolved over the past decades from manual gathering of data by using simple writing materials to the automation of data collection. Data storage has evolved significantly in the past decades and today databases no longer suffice as the only medium for the storage and management of data. This is due to the emergence of the Big Data and Data Science concepts. Previous studies have indicated that the multiplication of processing power of computers and the availability of larger data storage at reduced cost are part of the catalysts for the volume and rate at which data is now made available and captured.
 In this paper, the concepts of Big Data, Data Science and Business Analytics are reviewed. This paper discusses datafication of different aspects of life as the fundamental concept behind the growth of Big Data and Data Science. A review of the characteristics and value of Big Data and Data Science suggests that these emerging concepts will bring a paradigm change to a number of areas. Big Data was described as the basis for Data Science and Business Analytics which are tools employed in Data Science. Because these fields are still developing, there are diverse opinions, especially on the definition of Data Science. This paper provides a revised definition of Data Science, based on the review of available literature and proposes a schematic representation of the concepts.",fullPaper,cp21
Computer Science,p265,d3,1163c2996dfd0a46639b094e34ad783e969a0692,c69,Neural Information Processing Systems,Data science and prediction,Big data promises automated actionable knowledge creation and predictive models for use by both humans and computers.,poster,cp69
Computer Science,p266,d3,3858f600d0187c28f381b034a70226213e82a54e,j80,Nature Reviews Methods Primers,Network analysis of multivariate data in psychological science,Abstract,fullPaper,jv80
Computer Science,p267,d3,f3eda875e14bf933759f3b777131a4a9973537b4,c92,International Symposium on Computer Architecture,"Data-driven science and engineering: machine learning, dynamical systems, and control",Abstract,poster,cp92
Computer Science,p268,d3,3402835f33e3e1342eb86b4d13907e3c9121c82b,c46,Ideal,Data science for business,"Written by renowned data science experts Foster Provost and Tom Fawcett, Data Science for Business introduces the fundamental principles of data science, and walks you through the ""data-analytic thinking"" necessary for extracting useful knowledge and business value from the data you collect. This guide also helps you understand the many data-mining techniques in use today. Based on an MBA course Provost has taught at New York University over the past ten years, Data Science for Business provides examples of real-world business problems to illustrate these principles. You'll not only learn how to improve communication between business stakeholders and data scientists, but also how participate intelligently in your company's data science projects. You'll also discover how to think data-analytically, and fully appreciate how data science methods can support business decision-making. Understand how data science fits in your organization - and how you can use it for competitive advantage Treat data as a business asset that requires careful investment if you're to gain real value Approach business problems data-analytically, using the data-mining process to gather good data in the most appropriate way Learn general concepts for actually extracting knowledge from data Apply data science principles when interviewing data science job candidates",poster,cp46
Computer Science,p269,d3,c2fb0ded7b21a23cd0931558b52ddbc98fc4f934,c102,ACM SIGMOD Conference,Doing Data Science: Straight Talk from the Frontline,"Now that people are aware that data can make the difference in an election or a business model, data science as an occupation is gaining ground. But how can you get started working in a wide-ranging, interdisciplinary field thats so clouded in hype? This insightful book, based on Columbia Universitys Introduction to Data Science class, tells you what you need to know. In many of these chapter-long lectures, data scientists from companies such as Google, Microsoft, and eBay share new algorithms, methods, and models by presenting case studies and the code they use. If youre familiar with linear algebra, probability, and statistics, and have programming experience, this book is an ideal introduction to data science. Topics include:Statistical inference, exploratory data analysis, and the data science process Algorithms Spam filters, Naive Bayes, and data wrangling Logistic regression Financial modeling Recommendation engines and causality Data visualization Social networks and data journalism Data engineering, MapReduce, Pregel, and Hadoop Doing Data Science is collaboration between course instructor Rachel Schutt, Senior VP of Data Science at News Corp, and data science consultant Cathy ONeil, a senior data scientist at Johnson Research Labs, who attended and blogged about the course.",poster,cp102
Computer Science,p270,d3,0636653b82e152ba99b1d921b0aa2798aa845d1e,j81,Quantitative Science Studies,"Scopus as a curated, high-quality bibliometric data source for academic research in quantitative science studies","Abstract Scopus is among the largest curated abstract and citation databases, with a wide global and regional coverage of scientific journals, conference proceedings, and books, while ensuring only the highest quality data are indexed through rigorous content selection and re-evaluation by an independent Content Selection and Advisory Board. Additionally, extensive quality assurance processes continuously monitor and improve all data elements in Scopus. Besides enriched metadata records of scientific articles, Scopus offers comprehensive author and institution profiles, obtained from advanced profiling algorithms and manual curation, ensuring high precision and recall. The trustworthiness of Scopus has led to its use as bibliometric data source for large-scale analyses in research assessments, research landscape studies, science policy evaluations, and university rankings. Scopus data have been offered for free for selected studies by the academic research community, such as through application programming interfaces, which have led to many publications employing Scopus data to investigate topics such as researcher mobility, network visualizations, and spatial bibliometrics. In June 2019, the International Center for the Study of Research was launched, with an advisory board consisting of bibliometricians, aiming to work with the scientometric research community and offering a virtual laboratory where researchers will be able to utilize Scopus data.",fullPaper,jv81
Computer Science,p273,d3,e9931ea8ae9b8db38b519ef9ae32ec41a06d8445,c51,International Conference on Engineering Education,Doing Data Science,"Now that people are aware that data can make the difference in an election or a business model, data science as an occupation is gaining ground. But how can you get started working in a wide-ranging, interdisciplinary field that's so clouded in hype? This insightful book, based on Columbia University's Introduction to Data Science class, tells you what you need to know. In many of these chapter-long lectures, data scientists from companies such as Google, Microsoft, and eBay share new algorithms, methods, and models by presenting case studies and the code they use. If you're familiar with linear algebra, probability, and statistics, and have programming experience, this book is an ideal introduction to data science. Topics include: Statistical inference, exploratory data analysis, and the data science process Algorithms Spam filters, Naive Bayes, and data wrangling Logistic regression Financial modeling Recommendation engines and causality Data visualization Social networks and data journalism Data engineering, MapReduce, Pregel, and Hadoop Doing Data Science is collaboration between course instructor Rachel Schutt, Senior VP of Data Science at News Corp, and data science consultant Cathy O'Neil, a senior data scientist at Johnson Research Labs, who attended and blogged about the course.",poster,cp51
Computer Science,p277,d3,e084f4021f30c483564dcccc29d1230ab213ce70,j81,Quantitative Science Studies,"Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic","We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.",fullPaper,jv81
Computer Science,p278,d3,30c9e3fcb1ead2a827f91ff5cd203aa0d8058bff,c101,Interspeech,Data-Driven Science and Engineering,Abstract,poster,cp101
Computer Science,p282,d3,1110da1c238a7b09258136e7a2e7d558fb16f272,j85,Nuclear Data Sheets,TENDL: Complete Nuclear Data Library for Innovative Nuclear Science and Technology,Abstract,fullPaper,jv85
Computer Science,p287,d3,ee013b1477e8f81cb5c66a9a93a342281f740042,j89,bioRxiv,Assessing data quality in citizen science (preprint),"Ecological and environmental citizen science projects have enormous potential to advance science, influence policy, and guide resource management by producing datasets that are otherwise infeasible to generate. This potential can only be realized, though, if the datasets are of high quality. While scientists are often skeptical of the ability of unpaid volunteers to produce accurate datasets, a growing body of publications clearly shows that diverse types of citizen science projects can produce data with accuracy equal to or surpassing that of professionals. Successful projects rely on a suite of methods to boost data accuracy and account for bias, including iterative project development, volunteer training and testing, expert validation, replication across volunteers, and statistical modeling of systematic error. Each citizen science dataset should therefore be judged individually, according to project design and application, rather than assumed to be substandard simply because volunteers generated it.",fullPaper,jv89
Biology,p287,d5,ee013b1477e8f81cb5c66a9a93a342281f740042,j89,bioRxiv,Assessing data quality in citizen science (preprint),"Ecological and environmental citizen science projects have enormous potential to advance science, influence policy, and guide resource management by producing datasets that are otherwise infeasible to generate. This potential can only be realized, though, if the datasets are of high quality. While scientists are often skeptical of the ability of unpaid volunteers to produce accurate datasets, a growing body of publications clearly shows that diverse types of citizen science projects can produce data with accuracy equal to or surpassing that of professionals. Successful projects rely on a suite of methods to boost data accuracy and account for bias, including iterative project development, volunteer training and testing, expert validation, replication across volunteers, and statistical modeling of systematic error. Each citizen science dataset should therefore be judged individually, according to project design and application, rather than assumed to be substandard simply because volunteers generated it.",fullPaper,jv89
Computer Science,p288,d3,e257edf34abd9a191fea1023a423abb497cca70f,c77,Visualization for Computer Security,The data science education dilemma,"The need for people fluent in working with data is growing rapidly and enormously, but U.S. K–12 education does not provide meaningful learning experiences designed to develop understanding of data science concepts or a fluency with data science skills. Data science is inherently inter- disciplinary, so it makes sense to integrate it with existing content areas, but difficulties abound. Consideration of the work involved in doing data science and the habits of mind that lie behind it leads to a way of thinking about integrating data science with mathematics and science. Examples drawn from current activity development in the Data Games project shed some light on what technology-based, data-driven might be like. The project’s ongoing research on learners’ conceptions of organizing data and the relevance to data science education is explained.",poster,cp77
Computer Science,p294,d3,81e1dbe1e8152d0ddbf89e861468d799dbebe367,j92,Biological Conservation,Social media data for conservation science: A methodological overview,Abstract,fullPaper,jv92
Computer Science,p297,d3,adc22a722f2ad1d972de507779041e340f20a6a2,j66,Data Science Journal,Kadi4Mat: A Research Data Infrastructure for Materials Science,"The concepts and current developments of a research data infrastructure for materials science are presented, extending and combining the features of an electronic lab notebook and a repository. The objective of this infrastructure is to incorporate the possibility of structured data storage and data exchange with documented and reproducible data analysis and visualization, which finally leads to the publication of the data. This way, researchers can be supported throughout the entire research process. The software is being developed as a web-based and desktop-based system, offering both a graphical user interface and a programmatic interface. The focus of the development is on the integration of technologies and systems based on both established as well as new concepts. Due to the heterogeneous nature of materials science data, the current features are kept mostly generic, and the structuring of the data is largely left to the users. As a result, an extension of the research data infrastructure to other disciplines is possible in the future. The source code of the project is publicly available under a permissive Apache 2.0 license.",fullPaper,jv66
Computer Science,p304,d3,3fe3924a5315fbb5b5cd0edf98533b8c61a3bbdf,j98,ICES Journal of Marine Science,Machine intelligence and the data-driven future of marine science,"
 Oceans constitute over 70% of the earth's surface, and the marine environment and ecosystems are central to many global challenges. Not only are the oceans an important source of food and other resources, but they also play a important roles in the earth's climate and provide crucial ecosystem services. To monitor the environment and ensure sustainable exploitation of marine resources, extensive data collection and analysis efforts form the backbone of management programmes on global, regional, or national levels. Technological advances in sensor technology, autonomous platforms, and information and communications technology now allow marine scientists to collect data in larger volumes than ever before. But our capacity for data analysis has not progressed comparably, and the growing discrepancy is becoming a major bottleneck for effective use of the available data, as well as an obstacle to scaling up data collection further. Recent years have seen rapid advances in the fields of artificial intelligence and machine learning, and in particular, so-called deep learning systems are now able to solve complex tasks that previously required human expertise. This technology is directly applicable to many important data analysis problems and it will provide tools that are needed to solve many complex challenges in marine science and resource management. Here we give a brief review of recent developments in deep learning, and highlight the many opportunities and challenges for effective adoption of this technology across the marine sciences.",fullPaper,jv98
Computer Science,p305,d3,652c77a90d84df639622efdc9cd7475e96a248c9,j99,Comptes rendus. Mecanique,Data-driven modeling and learning in science and engineering,Abstract,fullPaper,jv99
Computer Science,p307,d3,4a6e74d4bf4fd0106891e5518692a77c7aa8811d,j100,Regular Issue,Outlier Detection in High Dimensional Data,"Artificial intelligence (AI) is the science that allows
computers to replicate human intelligence in areas such as
decision-making, text processing, visual perception. Artificial
Intelligence is the broader field that contains several subfields
such as machine learning, robotics, and computer vision.
Machine Learning is a branch of Artificial Intelligence that
allows a machine to learn and improve at a task over time. Deep
Learning is a subset of machine learning that makes use of deep
artificial neural networks for training. The paper proposed on
outlier detection for multivariate high dimensional data for
Autoencoder unsupervised model.",fullPaper,jv100
Computer Science,p311,d3,0db731c99879bb74c3850c53923d1df2c510f8c3,j103,IEEE Transactions on Geoscience and Remote Sensing,"AIRS/AMSU/HSB on the Aqua mission: design, science objectives, data products, and processing systems","The Atmospheric Infrared Sounder (AIRS), the Advanced Microwave Sounding Unit (AMSU), and the Humidity Sounder for Brazil (HSB) form an integrated cross-track scanning temperature and humidity sounding system on the Aqua satellite of the Earth Observing System (EOS). AIRS is an infrared spectrometer/radiometer that covers the 3.7-15.4-/spl mu/m spectral range with 2378 spectral channels. AMSU is a 15-channel microwave radiometer operating between 23 and 89 GHz. HSB is a four-channel microwave radiometer that makes measurements between 150 and 190 GHz. In addition to supporting the National Aeronautics and Space Administration's interest in process study and climate research, AIRS is the first hyperspectral infrared radiometer designed to support the operational requirements for medium-range weather forecasting of the National Ocean and Atmospheric Administration's National Centers for Environmental Prediction (NCEP) and other numerical weather forecasting centers. AIRS, together with the AMSU and HSB microwave radiometers, will achieve global retrieval accuracy of better than 1 K in the lower troposphere under clear and partly cloudy conditions. This paper presents an overview of the science objectives, AIRS/AMSU/HSB data products, retrieval algorithms, and the ground-data processing concepts. The EOS Aqua was launched on May 4, 2002 from Vandenberg AFB, CA, into a 705-km-high, sun-synchronous orbit. Based on the excellent radiometric and spectral performance demonstrated by AIRS during prelaunch testing, which has by now been verified during on-orbit testing, we expect the assimilation of AIRS data into the numerical weather forecast to result in significant forecast range and reliability improvements.",fullPaper,jv103
Environmental Science,p311,d14,0db731c99879bb74c3850c53923d1df2c510f8c3,j103,IEEE Transactions on Geoscience and Remote Sensing,"AIRS/AMSU/HSB on the Aqua mission: design, science objectives, data products, and processing systems","The Atmospheric Infrared Sounder (AIRS), the Advanced Microwave Sounding Unit (AMSU), and the Humidity Sounder for Brazil (HSB) form an integrated cross-track scanning temperature and humidity sounding system on the Aqua satellite of the Earth Observing System (EOS). AIRS is an infrared spectrometer/radiometer that covers the 3.7-15.4-/spl mu/m spectral range with 2378 spectral channels. AMSU is a 15-channel microwave radiometer operating between 23 and 89 GHz. HSB is a four-channel microwave radiometer that makes measurements between 150 and 190 GHz. In addition to supporting the National Aeronautics and Space Administration's interest in process study and climate research, AIRS is the first hyperspectral infrared radiometer designed to support the operational requirements for medium-range weather forecasting of the National Ocean and Atmospheric Administration's National Centers for Environmental Prediction (NCEP) and other numerical weather forecasting centers. AIRS, together with the AMSU and HSB microwave radiometers, will achieve global retrieval accuracy of better than 1 K in the lower troposphere under clear and partly cloudy conditions. This paper presents an overview of the science objectives, AIRS/AMSU/HSB data products, retrieval algorithms, and the ground-data processing concepts. The EOS Aqua was launched on May 4, 2002 from Vandenberg AFB, CA, into a 705-km-high, sun-synchronous orbit. Based on the excellent radiometric and spectral performance demonstrated by AIRS during prelaunch testing, which has by now been verified during on-orbit testing, we expect the assimilation of AIRS data into the numerical weather forecast to result in significant forecast range and reliability improvements.",fullPaper,jv103
Computer Science,p313,d3,2ff6d7e05b1f74e0b17dbf97a59ac0d75ef65efc,j105,Data Intelligence,FAIR Data and Services in Biodiversity Science and Geoscience,"We examine the intersection of the FAIR principles (Findable, Accessible, Interoperable and Reusable), the challenges and opportunities presented by the aggregation of widely distributed and heterogeneous data about biological and geological specimens, and the use of the Digital Object Architecture (DOA) data model and components as an approach to solving those challenges that offers adherence to the FAIR principles as an integral characteristic. This approach will be prototyped in the Distributed System of Scientific Collections (DiSSCo) project, the pan-European Research Infrastructure which aims to unify over 110 natural science collections across 21 countries. We take each of the FAIR principles, discuss them as requirements in the creation of a seamless virtual collection of bio/geo specimen data, and map those requirements to Digital Object components and facilities such as persistent identification, extended data typing, and the use of an additional level of abstraction to normalize existing heterogeneous data structures. The FAIR principles inform and motivate the work and the DO Architecture provides the technical vision to create the seamless virtual collection vitally needed to address scientific questions of societal importance.",fullPaper,jv105
Computer Science,p315,d3,bd8a307efcffbf57d2e5c3c23577de44d883d865,c23,International Conference on Open and Big Data,MedRec: Using Blockchain for Medical Data Access and Permission Management,"Years of heavy regulation and bureaucratic inefficiency have slowed innovation for electronic medical records (EMRs). We now face a critical need for such innovation, as personalization and data science prompt patients to engage in the details of their healthcare and restore agency over their medical data. In this paper, we propose MedRec: a novel, decentralized record management system to handle EMRs, using blockchain technology. Our system gives patients a comprehensive, immutable log and easy access to their medical information across providers and treatment sites. Leveraging unique blockchain properties, MedRec manages authentication, confidentiality, accountability and data sharing- crucial considerations when handling sensitive information. A modular design integrates with providers' existing, local data storage solutions, facilitating interoperability and making our system convenient and adaptable. We incentivize medical stakeholders (researchers, public health authorities, etc.) to participate in the network as blockchain “miners”. This provides them with access to aggregate, anonymized data as mining rewards, in return for sustaining and securing the network via Proof of Work. MedRec thus enables the emergence of data economics, supplying big data to empower researchers while engaging patients and providers in the choice to release metadata. The purpose of this short paper is to expose, prior to field tests, a working prototype through which we analyze and discuss our approach.",fullPaper,cp23
Computer Science,p316,d3,2660fbc3b666145a87f05de10066fc2a3e7467dd,c108,IEEE International Conference on Multimedia and Expo,The Science Of Real Time Data Capture Self Reports In Health Research,"The National Cancer Institute (NCI) has designated the topic of real-time data capture as an important and innovative research area. As such, the NCI sponsored a national meeting of distinguished research scientists to discuss the state of the science in this emerging and burgeoning field. This book reflects the findings of the conference and discusses the state of the science of real-time data capture and its application to health and cancer research. It provides a conceptual framework for minute-by-minute data captureecological momentary assessments (EMA)and discusses health-related topics where these assessements have been applied. In addition, future directions in real-time data capture assessment, interventions, methodology, and technology are discussed.",poster,cp108
Computer Science,p317,d3,02a9428b5b28d85ea330033fb990dc10cd15cc4e,j106,Methods in Ecology and Evolution,Occupancy models for citizen‐science data,"Large‐scale citizen‐science projects, such as atlases of species distribution, are an important source of data for macroecological research, for understanding the effects of climate change and other drivers on biodiversity, and for more applied conservation tasks, such as early‐warning systems for biodiversity loss. However, citizen‐science data are challenging to analyse because the observation process has to be taken into account. Typically, the observation process leads to heterogeneous and non‐random sampling, false absences, false detections, and spatial correlations in the data. Increasingly, occupancy models are being used to analyse atlas data. We advocate a dual approach to strengthen inference from citizen science data for the questions the programme is intended to address: (a) the survey design should be chosen with a particular set of questions and associated analysis strategy in mind and (b) the statistical methods should be tailored not only to those questions but also to the specific characteristics of the data. We review the consequences of particular survey design choices that typically need to be made in atlas‐style citizen‐science projects. These include spatial resolution of the sampling units, allocation of effort in space, and collection of information about the observation process. On the analysis side, we review extensions of the basic occupancy models that are frequently necessary with atlas data, including methods for dealing with heterogeneity, non‐independent detections, false detections, and violation of the closure assumption. New technologies, such as cell‐phone apps and fixed remote detection devices, are revolutionizing citizen‐science projects. There is an opportunity to maximize the usefulness of the resulting datasets if the protocols are rooted in robust statistical designs and data analysis issues are being considered. Our review provides guidelines for designing new projects and an overview of the current methods that can be used to analyse data from such projects.",fullPaper,jv106
Computer Science,p321,d3,87f7c170aecf8f3465b26a11b9a384fef934337b,c39,Online World Conference on Soft Computing in Industrial Applications,Measurement and Data Analysis for Engineering and Science,"Fundamentals of Experimentation Introduction Experiments Chapter Overview Experimental Approach Role of Experiments The Experiment Classification of Experiments Plan for Successful Experimentation Hypothesis Testing* Design of Experiments* Factorial Design* Problems Bibliography Fundamental Electronics Chapter Overview Concepts and Definitions Circuit Elements RLC Combinations Elementary DC Circuit Analysis Elementary AC Circuit Analysis Equivalent Circuits* Meters* Impedance Matching and Loading Error* Electrical Noise* Problems Bibliography Measurement Systems: Sensors and Transducers Chapter Overview Measurement System Overview Sensor Domains Sensor Characteristics Physical Principles of Sensors Electric Piezoelectric Fluid Mechanic Optic Photoelastic Thermoelectric Electrochemical Sensor Scaling* Problems Bibliography Measurement Systems: Other Components Chapter Overview Signal Conditioning, Processing, and Recording Amplifiers Filters Analog-to-Digital Converters Smart Measurement Systems Other Example Measurement Systems Problems Bibliography Measurement Systems: Calibration and Response Chapter Overview Static Response Characterization by Calibration Dynamic Response Characterization Zero-Order System Dynamic Response First-Order System Dynamic Response Second-Order System Dynamic Response Measurement System Dynamic Response Problems Bibliography Measurement Systems: Design-Stage Uncertainty Chapter Overview Design-Stage Uncertainty Analysis Design-Stage Uncertainty Estimate of a Measurand Design-Stage Uncertainty Estimate of a Result Problems Bibliography Signal Characteristics Chapter Overview Signal Classification Signal Variables Signal Statistical Parameters Problems Bibliography The Fourier Transform Chapter Overview Fourier Series of a Periodic Signal Complex Numbers and Waves Exponential Fourier Series Spectral Representations Continuous Fourier Transform Continuous Fourier Transform Properties* Discrete Fourier Transform Fast Fourier Transform Problems Bibliography Digital Signal Analysis Chapter Overview Digital Sampling Digital Sampling Errors Windowing* Determining a Sample Period Problems Bibliography Probability Chapter Overview Relation to Measurements Basic Probability Concepts Sample versus Population Plotting Statistical Information Probability Density Function Various Probability Density Functions Central Moments Probability Distribution Function Problems Bibliography Statistics Chapter Overview Normal Distribution Normalized Variables Student's t Distribution Rejection of Data Standard Deviation of the Means Chi-Square Distribution Pooling Samples* Problems Bibliography Uncertainty Analysis Chapter Overview Modeling and Experimental Uncertainties Probabilistic Basis of Uncertainty Identifying Sources of Error Systematic and Random Errors Quantifying Systematic and Random Errors Measurement Uncertainty Analysis Uncertainty Analysis of a Multiple-Measurement Result Uncertainty Analyses for Other Measurement Situations Uncertainty Analysis Summary Finite-Difference Uncertainties* Uncertainty Based upon Interval Statistics* Problems Bibliography Regression and Correlation Chapter Overview Least-Squares Approach Least-Squares Regression Analysis Linear Analysis Higher-Order Analysis* Multi-Variable Linear Analysis* Determining the Appropriate Fit Regression Confidence Intervals Regression Parameters Linear Correlation Analysis Signal Correlations in Time* Problems Bibliography Units and Significant Figures Chapter Overview English and Metric Systems Systems of Units SI Standards Technical English and SI Conversion Factors Prefixes Significant Figures Problems Bibliography Technical Communication Chapter Overview Guidelines for Writing Technical Memo Technical Report Oral Technical Presentation Problems Bibliography A Glossary B Symbols C Review Problem Answers Index",poster,cp39
Computer Science,p322,d3,2809d4876e34b8c64fc1783fe6a0a278770505b0,c2,International Conference on Software Engineering,A survey of data provenance in e-science,"Data management is growing in complexity as large-scale applications take advantage of the loosely coupled resources brought together by grid middleware and by abundant storage capacity. Metadata describing the data products used in and generated by these applications is essential to disambiguate the data and enable reuse. Data provenance, one kind of metadata, pertains to the derivation history of a data product starting from its original sources.In this paper we create a taxonomy of data provenance characteristics and apply it to current research efforts in e-science, focusing primarily on scientific workflow approaches. The main aspect of our taxonomy categorizes provenance systems based on why they record provenance, what they describe, how they represent and store provenance, and ways to disseminate it. The survey culminates with an identification of open research problems in the field.",poster,cp2
Computer Science,p325,d3,16f4135a229c79e60fa25259100c8cdcedfab8cc,c75,International Conference on Predictive Models in Software Engineering,Patent citation data in social science research: Overview and best practices,"The last 2 decades have witnessed a dramatic increase in the use of patent citation data in social science research. Facilitated by digitization of the patent data and increasing computing power, a community of practice has grown up that has developed methods for using these data to: measure attributes of innovations such as impact and originality; to trace flows of knowledge across individuals, institutions and regions; and to map innovation networks. The objective of this article is threefold. First, it takes stock of these main uses. Second, it discusses 4 pitfalls associated with patent citation data, related to office, time and technology, examiner, and strategic effects. Third, it highlights gaps in our understanding and offers directions for future research.",poster,cp75
Political Science,p325,d15,16f4135a229c79e60fa25259100c8cdcedfab8cc,c75,International Conference on Predictive Models in Software Engineering,Patent citation data in social science research: Overview and best practices,"The last 2 decades have witnessed a dramatic increase in the use of patent citation data in social science research. Facilitated by digitization of the patent data and increasing computing power, a community of practice has grown up that has developed methods for using these data to: measure attributes of innovations such as impact and originality; to trace flows of knowledge across individuals, institutions and regions; and to map innovation networks. The objective of this article is threefold. First, it takes stock of these main uses. Second, it discusses 4 pitfalls associated with patent citation data, related to office, time and technology, examiner, and strategic effects. Third, it highlights gaps in our understanding and offers directions for future research.",poster,cp75
Computer Science,p326,d3,d9a668e849fe780081b9d133d84c72039283f30b,j109,Earth and Space Science,From Open Data to Open Science,"The open science movement continues to gain momentum, attention, and discussion. However, there are a number of different interpretations, viewpoints, and perspectives as to what the term “open science” means. In this study, we define open science as a collaborative culture enabled by technology that empowers the open sharing of data, information, and knowledge within the scientific community and the wider public to accelerate scientific research and understanding. As science has become increasingly data driven, data programs now play a critical role in enabling and accelerating open science.",fullPaper,jv109
Computer Science,p330,d3,362f50f59a280d7cc526fb626fdf44ad382cee57,j110,Scientometrics,The journal coverage of Web of Science and Scopus: a comparative analysis,Abstract,fullPaper,jv110
Political Science,p330,d15,362f50f59a280d7cc526fb626fdf44ad382cee57,j110,Scientometrics,The journal coverage of Web of Science and Scopus: a comparative analysis,Abstract,fullPaper,jv110
Computer Science,p331,d3,1e4709c0b8fe3bf759cd64dc1ede695d6e5316f0,j7,Journal of Big Data,Deep learning applications and challenges in big data analytics,Abstract,fullPaper,jv7
Computer Science,p333,d3,06a81f63fc4ccfcf02934647a7c17454b91853b0,c83,"International Convention on Information and Communication Technology, Electronics and Microelectronics",Machine Learning - The Art and Science of Algorithms that Make Sense of Data,"As one of the most comprehensive machine learning texts around, this book does justice to the field's incredible richness, but without losing sight of the unifying principles. Peter Flach's clear, example-based approach begins by discussing how a spam filter works, which gives an immediate introduction to machine learning in action, with a minimum of technical fuss. Flach provides case studies of increasing complexity and variety with well-chosen examples and illustrations throughout. He covers a wide range of logical, geometric and statistical models and state-of-the-art topics such as matrix factorisation and ROC analysis. Particular attention is paid to the central role played by features. The use of established terminology is balanced with the introduction of new and useful concepts, and summaries of relevant background material are provided with pointers for revision if necessary. These features ensure Machine Learning will set a new standard as an introductory textbook.",poster,cp83
Computer Science,p335,d3,ad3d83248eae66580d4deada76e72e3be9a9b44c,c9,Big Data,Named data networking,"Named Data Networking (NDN) is one of five projects funded by the U.S. National Science Foundation under its Future Internet Architecture Program. NDN has its roots in an earlier project, Content-Centric Networking (CCN), which Van Jacobson first publicly presented in 2006. The NDN project investigates Jacobson's proposed evolution from today's host-centric network architecture (IP) to a data-centric network architecture (NDN). This conceptually simple shift has far-reaching implications for how we design, develop, deploy, and use networks and applications. We describe the motivation and vision of this new architecture, and its basic components and operations. We also provide a snapshot of its current design, development status, and research challenges. More information about the project, including prototype implementations, publications, and annual reports, is available on named-data.net.",poster,cp9
Biology,p335,d5,ad3d83248eae66580d4deada76e72e3be9a9b44c,c9,Big Data,Named data networking,"Named Data Networking (NDN) is one of five projects funded by the U.S. National Science Foundation under its Future Internet Architecture Program. NDN has its roots in an earlier project, Content-Centric Networking (CCN), which Van Jacobson first publicly presented in 2006. The NDN project investigates Jacobson's proposed evolution from today's host-centric network architecture (IP) to a data-centric network architecture (NDN). This conceptually simple shift has far-reaching implications for how we design, develop, deploy, and use networks and applications. We describe the motivation and vision of this new architecture, and its basic components and operations. We also provide a snapshot of its current design, development status, and research challenges. More information about the project, including prototype implementations, publications, and annual reports, is available on named-data.net.",poster,cp9
Computer Science,p336,d3,391a5f286f814d852dddcab1b2b68e5c1af6c79e,j4,IEEE Transactions on Knowledge and Data Engineering,Data mining with big data,"Big Data concern large-volume, complex, growing data sets with multiple, autonomous sources. With the fast development of networking, data storage, and the data collection capacity, Big Data are now rapidly expanding in all science and engineering domains, including physical, biological and biomedical sciences. This paper presents a HACE theorem that characterizes the features of the Big Data revolution, and proposes a Big Data processing model, from the data mining perspective. This data-driven model involves demand-driven aggregation of information sources, mining and analysis, user interest modeling, and security and privacy considerations. We analyze the challenging issues in the data-driven model and also in the Big Data revolution.",fullPaper,jv4
Computer Science,p339,d3,d4825585c5b036bb789ad183635cc2d4d89ff394,c50,Conference on Emerging Network Experiment and Technology,Opening the archive: How free data has enabled the science and monitoring promise of Landsat,Abstract,poster,cp50
Computer Science,p343,d3,c36991759325bedd19f69264f72d1cbf59a6158c,c78,ACM/IEEE International Conference on Model Driven Engineering Languages and Systems,Data Mining: Concepts and Techniques,"The increasing volume of data in modern business and science calls for more complex and sophisticated tools. Although advances in data mining technology have made extensive data collection much easier, it's still always evolving and there is a constant need for new techniques and tools that can help us transform this data into useful information and knowledge. Since the previous edition's publication, great advances have been made in the field of data mining. Not only does the third of edition of Data Mining: Concepts and Techniques continue the tradition of equipping you with an understanding and application of the theory and practice of discovering patterns hidden in large data sets, it also focuses on new, important topics in the field: data warehouses and data cube technology, mining stream, mining social networks, and mining spatial, multimedia and other complex data. Each chapter is a stand-alone guide to a critical topic, presenting proven algorithms and sound implementations ready to be used directly or with strategic modification against live data. This is the resource you need if you want to apply today's most powerful data mining techniques to meet real business challenges. * Presents dozens of algorithms and implementation examples, all in pseudo-code and suitable for use in real-world, large-scale data mining projects. * Addresses advanced topics such as mining object-relational databases, spatial databases, multimedia databases, time-series databases, text databases, the World Wide Web, and applications in several fields. *Provides a comprehensive, practical look at the concepts and techniques you need to get the most out of real business data",poster,cp78
Computer Science,p347,d3,b48a917258f4e7e2b78a41289d005513db1de8c9,c24,International Conference on Data Technologies and Applications,Earth Observation Open Science: Enhancing Reproducible Science Using Data Cubes,"Earth Observation Data Cubes (EODC) have emerged as a promising solution to efficiently and effectively handle Big Earth Observation (EO) Data generated by satellites and made freely and openly available from different data repositories. The aim of this Special Issue, “Earth Observation Data Cube”, in Data, is to present the latest advances in EODC development and implementation, including innovative approaches for the exploitation of satellite EO data using multi-dimensional (e.g., spatial, temporal, spectral) approaches. This Special Issue contains 14 articles covering a wide range of topics such as Synthetic Aperture Radar (SAR), Analysis Ready Data (ARD), interoperability, thematic applications (e.g., land cover, snow cover mapping), capacity development, semantics, processing techniques, as well as national implementations and best practices. These papers made significant contributions to the advancement of a more Open and Reproducible Earth Observation Science, reducing the gap between users’ expectations for decision-ready products and current Big Data analytical capabilities, and ultimately unlocking the information power of EO data by transforming them into actionable knowledge.",fullPaper,cp24
Computer Science,p348,d3,3aa1b70fdc97ae96091c5fb39cd911015ac5253e,c36,International Conference on Information Technology Based Higher Education and Training,Novel methods improve prediction of species' distributions from occurrence data,"Prediction of species' distributions is central to diverse applications in ecology, evolution and conservation science. There is increasing electronic access to vast sets of occurrence records in museums and herbaria, yet little effective guidance on how best to use this information in the context of numerous approaches for modelling distributions. To meet this need, we compared 16 modelling methods over 226 species from 6 regions of the world, creating the most comprehensive set of model comparisons to date. We used presence-only data to fit models, and independent presence-absence data to evaluate the predictions. Along with well-established modelling methods such as generalised additive models and GARP and BIOCLIM, we explored methods that either have been developed recently or have rarely been applied to modelling species' distributions. These include machine-learning methods and community models, both of which have features that may make them particularly well suited to noisy or sparse information, as is typical of species' occurrence data. Presence-only data were effective for modelling species' distributions for many species and regions. The novel methods consistently outperformed more established methods. The results of our analysis are promising for the use of data from museums and herbaria, especially as methods suited to the noise inherent in such data improve.",poster,cp36
Computer Science,p349,d3,88bcdfd021d935a28f245e178792207881b14794,j115,Cambridge International Law Journal,Learning from Imbalanced Data Sets,Abstract,fullPaper,jv115
Computer Science,p351,d3,7bd598f6a7c6eb4265fe5a9ca64504d1d639684a,c77,Visualization for Computer Security,Educational data mining and learning analytics: An updated survey,"This survey is an updated and improved version of the previous one published in 2013 in this journal with the title “data mining in education”. It reviews in a comprehensible and very general way how Educational Data Mining and Learning Analytics have been applied over educational data. In the last decade, this research area has evolved enormously and a wide range of related terms are now used in the bibliography such as Academic Analytics, Institutional Analytics, Teaching Analytics, Data‐Driven Education, Data‐Driven Decision‐Making in Education, Big Data in Education, and Educational Data Science. This paper provides the current state of the art by reviewing the main publications, the key milestones, the knowledge discovery cycle, the main educational environments, the specific tools, the free available datasets, the most used methods, the main objectives, and the future trends in this research area.",poster,cp77
Computer Science,p352,d3,e0634f2945b43d4c13a0aa2ff31f2c1c5fe597b9,c23,International Conference on Open and Big Data,The Role of Anomalous Data in Knowledge Acquisition: A Theoretical Framework and Implications for Science Instruction,"Understanding how science students respond to anomalous data is essential to understanding knowledge acquisition in science classrooms. This article presents a detailed analysis of the ways in which scientists and science students respond to such data. We postulate that there are seven distinct forms of response to anomalous data, only one of which is to accept the data and change theories. The other six responses involve discounting the data in various ways in order to protect the preinstructional theory. We analyze the factors that influence which of these seven forms of response a scientist or student will choose, giving special attention to the factors that make theory change more likely. Finally, we discuss the implications of our framework for science instruction.",poster,cp23
Psychology,p352,d10,e0634f2945b43d4c13a0aa2ff31f2c1c5fe597b9,c23,International Conference on Open and Big Data,The Role of Anomalous Data in Knowledge Acquisition: A Theoretical Framework and Implications for Science Instruction,"Understanding how science students respond to anomalous data is essential to understanding knowledge acquisition in science classrooms. This article presents a detailed analysis of the ways in which scientists and science students respond to such data. We postulate that there are seven distinct forms of response to anomalous data, only one of which is to accept the data and change theories. The other six responses involve discounting the data in various ways in order to protect the preinstructional theory. We analyze the factors that influence which of these seven forms of response a scientist or student will choose, giving special attention to the factors that make theory change more likely. Finally, we discuss the implications of our framework for science instruction.",poster,cp23
Environmental Science,p352,d14,e0634f2945b43d4c13a0aa2ff31f2c1c5fe597b9,c23,International Conference on Open and Big Data,The Role of Anomalous Data in Knowledge Acquisition: A Theoretical Framework and Implications for Science Instruction,"Understanding how science students respond to anomalous data is essential to understanding knowledge acquisition in science classrooms. This article presents a detailed analysis of the ways in which scientists and science students respond to such data. We postulate that there are seven distinct forms of response to anomalous data, only one of which is to accept the data and change theories. The other six responses involve discounting the data in various ways in order to protect the preinstructional theory. We analyze the factors that influence which of these seven forms of response a scientist or student will choose, giving special attention to the factors that make theory change more likely. Finally, we discuss the implications of our framework for science instruction.",poster,cp23
Computer Science,p353,d3,0870c1ea2b7d5a515c7b5b954f1433b379fe1e02,j117,Earth-Science Reviews,Principles and methods of scaling geospatial Earth science data,Abstract,fullPaper,jv117
Computer Science,p356,d3,5703617b9d9d40e90b6c8ffa21a52734d9822d60,c30,PS,Defining Computational Thinking for Mathematics and Science Classrooms,Abstract,poster,cp30
Computer Science,p357,d3,da619a6c524f5ab800b44c8728db3cef3d3b25d9,j17,Big Data & Society,"Big Data, new epistemologies and paradigm shifts","This article examines how the availability of Big Data, coupled with new data analytics, challenges established epistemologies across the sciences, social sciences and humanities, and assesses the extent to which they are engendering paradigm shifts across multiple disciplines. In particular, it critically explores new forms of empiricism that declare ‘the end of theory’, the creation of data-driven rather than knowledge-driven science, and the development of digital humanities and computational social sciences that propose radically different ways to make sense of culture, history, economy and society. It is argued that: (1) Big Data and new data analytics are disruptive innovations which are reconfiguring in many instances how research is conducted; and (2) there is an urgent need for wider critical reflection within the academy on the epistemological implications of the unfolding data revolution, a task that has barely begun to be tackled despite the rapid changes in research practices presently taking place. After critically reviewing emerging epistemological positions, it is contended that a potentially fruitful approach would be the development of a situated, reflexive and contextually nuanced epistemology.",fullPaper,jv17
Sociology,p357,d4,da619a6c524f5ab800b44c8728db3cef3d3b25d9,j17,Big Data & Society,"Big Data, new epistemologies and paradigm shifts","This article examines how the availability of Big Data, coupled with new data analytics, challenges established epistemologies across the sciences, social sciences and humanities, and assesses the extent to which they are engendering paradigm shifts across multiple disciplines. In particular, it critically explores new forms of empiricism that declare ‘the end of theory’, the creation of data-driven rather than knowledge-driven science, and the development of digital humanities and computational social sciences that propose radically different ways to make sense of culture, history, economy and society. It is argued that: (1) Big Data and new data analytics are disruptive innovations which are reconfiguring in many instances how research is conducted; and (2) there is an urgent need for wider critical reflection within the academy on the epistemological implications of the unfolding data revolution, a task that has barely begun to be tackled despite the rapid changes in research practices presently taking place. After critically reviewing emerging epistemological positions, it is contended that a potentially fruitful approach would be the development of a situated, reflexive and contextually nuanced epistemology.",fullPaper,jv17
Computer Science,p359,d3,88dde718acafeaedbe9768883d274a81fd8313d7,c25,IEEE International Parallel and Distributed Processing Symposium,DLHub: Model and Data Serving for Science,"While the Machine Learning (ML) landscape is evolving rapidly, there has been a relative lag in the development of the ""learning systems"" needed to enable broad adoption. Furthermore, few such systems are designed to support the specialized requirements of scientific ML. Here we present the Data and Learning Hub for science (DLHub), a multi-tenant system that provides both model repository and serving capabilities with a focus on science applications. DLHub addresses two significant shortcomings in current systems. First, its self-service model repository allows users to share, publish, verify, reproduce, and reuse models, and addresses concerns related to model reproducibility by packaging and distributing models and all constituent components. Second, it implements scalable and low-latency serving capabilities that can leverage parallel and distributed computing resources to democratize access to published models through a simple web interface. Unlike other model serving frameworks, DLHub can store and serve any Python 3-compatible model or processing function, plus multiple-function pipelines. We show that relative to other model serving systems including TensorFlow Serving, SageMaker, and Clipper, DLHub provides greater capabilities, comparable performance without memoization and batching, and significantly better performance when the latter two techniques can be employed. We also describe early uses of DLHub for scientific applications.",fullPaper,cp25
Mathematics,p359,d6,88dde718acafeaedbe9768883d274a81fd8313d7,c25,IEEE International Parallel and Distributed Processing Symposium,DLHub: Model and Data Serving for Science,"While the Machine Learning (ML) landscape is evolving rapidly, there has been a relative lag in the development of the ""learning systems"" needed to enable broad adoption. Furthermore, few such systems are designed to support the specialized requirements of scientific ML. Here we present the Data and Learning Hub for science (DLHub), a multi-tenant system that provides both model repository and serving capabilities with a focus on science applications. DLHub addresses two significant shortcomings in current systems. First, its self-service model repository allows users to share, publish, verify, reproduce, and reuse models, and addresses concerns related to model reproducibility by packaging and distributing models and all constituent components. Second, it implements scalable and low-latency serving capabilities that can leverage parallel and distributed computing resources to democratize access to published models through a simple web interface. Unlike other model serving frameworks, DLHub can store and serve any Python 3-compatible model or processing function, plus multiple-function pipelines. We show that relative to other model serving systems including TensorFlow Serving, SageMaker, and Clipper, DLHub provides greater capabilities, comparable performance without memoization and batching, and significantly better performance when the latter two techniques can be employed. We also describe early uses of DLHub for scientific applications.",fullPaper,cp25
Computer Science,p362,d3,f2b66923db74a16169d040a51ada555d5b6f8851,c12,The Compass,Data Mining and Analysis: Fundamental Concepts and Algorithms,"The fundamental algorithms in data mining and analysis form the basis for the emerging field of data science, which includes automated methods to analyze patterns and models for all kinds of data, with applications ranging from scientific discovery to business intelligence and analytics. This textbook for senior undergraduate and graduate data mining courses provides a broad yet in-depth overview of data mining, integrating related concepts from machine learning and statistics. The main parts of the book include exploratory data analysis, pattern mining, clustering, and classification. The book lays the basic foundations of these tasks, and also covers cutting-edge topics such as kernel methods, high-dimensional data analysis, and complex graphs and networks. With its comprehensive coverage, algorithmic perspective, and wealth of examples, this book offers solid guidance in data mining for students, researchers, and practitioners alike. Key features: Covers both core methods and cutting-edge research Algorithmic approach with open-source implementations Minimal prerequisites: all key mathematical concepts are presented, as is the intuition behind the formulas Short, self-contained chapters with class-tested examples and exercises allow for flexibility in designing a course and for easy reference Supplementary website with lecture slides, videos, project ideas, and more",poster,cp12
Computer Science,p363,d3,fb3140c9766a5bc92400ac8ce9d48a4272bba69e,j120,Artificial Life,A New Kind of Science,"nationwide data set of losses from 1975 to 1998 was compiled to assess the trends. Temporal patterns of deaths and injuries, monetary damages, and—in some cases—the number of events are systematically examined by year in chapter 5, and the authors undertake a systematic spatial assessment of the statewide totals in chapter 6. Explanations for some of the patterns are offered, particularly for the most significant disasters and for the states with most events or the greatest losses. Further refinement and evaluation of patterns of economic losses and death are undertaken by normalizing losses by population, land area, and gross domestic product (GDP). The authors advance the discussion from simple descriptions of loss patterns to explanations of the patterns of disaster-loss burden, and some surprises emerge from the arithmetic. For instance, North Dakota, Iowa, andMississippi not only suffered the greatest monetary losses per capita during the period, but also suffered the greatest losses of property and crops compared to their state GDP!For afinal analysis, the authors created an overall hazard score (averaged proportion of the states’ contributions to the national totals of events, deaths, and damages) and used it to rank the states. Using this ranking, states were assigned to categories of ‘‘proneness,’’ from highest (Florida, Texas, andCalifornia) to lowest (Rhode Island, Delaware, Alaska and other small or lightly populated states). The conclusion we are to draw is that the amount of loss a state has experienced indicates its disaster proneness. Finally, ‘‘Charting a Course for theNext Two Decades’’ by Cutter describes what is needed to produce the models and data appropriate for mitigation and planning assessments. In order for an effective assessment of events and losses to occur, progress is required in several areas: development of vulnerability science, the creation of a national hazard events and losses database, and the establishment of a national loss inventory and events clearinghouse. To do so, Cutter argues, we need to rethink thewaywe monitor, assess, andmanage our vulnerabilities. She briefly describes the shifts needed in data gathering and provision, sustainability and distributive justice, strategic planning, research funding, and societal awareness of issues that influence the prospects for disaster. While American Hazardscapes is intended to provide a broadunderstanding of the geography of loss due to hazards in the United States, it suffers from its openly acknowledged limitations. Though criticizing the quality of currently available data, the authors use those data to indicate the prospects for future disasters. The elimination of extreme events is no longer believed tobe the key loss reduction. Instead,we must identify and avoid places too dynamic for permanent occupation and adjust to the inevitable events in ways that limit prospects for loss. Mitigation must address the vulnerabilities that cause greater exposure and profound upset of our social systems and create more complex catastrophes. The data employed in this assessment describe (however imperfectly) the losses suffered over two and a half decades. The largest disasters overwhelm the patterns of loss in their analysis. The authors imply, based on proneness rankings, that those who lost the most are the most prone to loss. But in reality, losses are byproducts of the interplay of two dynamic geographies: the pattern of extreme events and the pattern of human use of the landscape. The former is often poorly understood, may not behave consistently, andmay operate on greater than twenty-five-year cycles. The latter may change so rapidly that it surpasses our capacity to measure it and map it, and postdisaster land use and human perception may be radically changed. These geographies were outside the scope of this book, however, and given new homeland security efforts and reorganization of the Federal Emergency Management Agency, the past is an even poorer indicator of the future.",fullPaper,jv120
Computer Science,p365,d3,24931dc3ddedfc2db5405af236e3ca84944d66d7,c71,International Joint Conference on Artificial Intelligence,Big Data and Social Science: A Practical Guide to Methods and Tools,"Both Traditional Students and Working Professionals Acquire the Skills to Analyze Social Problems. Big Data and Social Science: A Practical Guide to Methods and Tools shows how to apply data science to real-world problems in both research and the practice. The book provides practical guidance on combining methods and tools from computer science, statistics, and social science. This concrete approach is illustrated throughout using an important national problem, the quantitative study of innovation. The text draws on the expertise of prominent leaders in statistics, the social sciences, data science, and computer science to teach students how to use modern social science research principles as well as the best analytical and computational tools. It uses a real-world challenge to introduce how these tools are used to identify and capture appropriate data, apply data science models and tools to that data, and recognize and respond to data errors and limitations. For more information, including sample chapters and news, please visit the author's website.",poster,cp71
Computer Science,p367,d3,b970f9c088beee99666a40374dd5ccb06eeda112,c26,Decision Support Systems,Understanding the paradigm shift to computational social science in the presence of big data,Abstract,fullPaper,cp26
Computer Science,p372,d3,500b73ecdf8ff5590718edb03367e3836a368485,c119,International Conference on Business Process Management,Secondary Data Analysis: A Method of which the Time Has Come,"Technological advances have led to vast amounts of data that has been collected, compiled, and archived, and that is now easily accessible for research. As a result, utilizing existing data for research is becoming more prevalent, and therefore secondary data analysis. While secondary analysis is flexible and can be utilized in several ways, it is also an empirical exercise and a systematic method with procedural and evaluative steps, just as in collecting and evaluating primary data. This paper asserts that secondary data analysis is a viable method to utilize in the process of inquiry when a systematic procedure is followed and presents an illustrative research application utilizing secondary data analysis in library and information science research.",poster,cp119
Computer Science,p374,d3,46d71d947231f86e1f9d4581e61212385debbe14,c28,International Conference on Contemporary Computing,OpenML: networked science in machine learning,"Many sciences have made significant breakthroughs by adopting online tools that help organize, structure and mine information that is too detailed to be printed in journals. In this paper, we introduce OpenML, a place for machine learning researchers to share and organize data in fine detail, so that they can work more effectively, be more visible, and collaborate with others to tackle harder problems. We discuss how OpenML relates to other examples of networked science and what benefits it brings for machine learning research, individual scientists, as well as students and practitioners.",poster,cp28
Computer Science,p376,d3,b8f75b848b6cef0f2b5a1a11b794332ca9bccb45,j125,Environmental Reviews,A review of machine learning applications in wildfire science and management,"Artificial intelligence has been applied in wildfire science and management since the 1990s, with early applications including neural networks and expert systems. Since then, the field has rapidly progressed congruently with the wide adoption of machine learning (ML) methods in the environmental sciences. Here, we present a scoping review of ML applications in wildfire science and management. Our overall objective is to improve awareness of ML methods among wildfire researchers and managers, as well as illustrate the diverse and challenging range of problems in wildfire science available to ML data scientists. To that end, we first present an overview of popular ML approaches used in wildfire science to date and then review the use of ML in wildfire science as broadly categorized into six problem domains, including (i) fuels characterization, fire detection, and mapping; (ii) fire weather and climate change; (iii) fire occurrence, susceptibility, and risk; (iv) fire behavior prediction; (v) fire effects; and (vi) fire management. Furthermore, we discuss the advantages and limitations of various ML approaches relating to data size, computational requirements, generalizability, and interpretability, as well as identify opportunities for future advances in the science and management of wildfires within a data science context. In total, to the end of 2019, we identified 300 relevant publications in which the most frequently used ML methods across problem domains included random forests, MaxEnt, artificial neural networks, decision trees, support vector machines, and genetic algorithms. As such, there exists opportunities to apply more current ML methods — including deep learning and agent-based learning — in the wildfire sciences, especially in instances involving very large multivariate datasets. We must recognize, however, that despite the ability of ML models to learn on their own, expertise in wildfire science is necessary to ensure realistic modelling of fire processes across multiple scales, while the complexity of some ML methods such as deep learning requires a dedicated and sophisticated knowledge of their application. Finally, we stress that the wildfire research and management communities play an active role in providing relevant, high-quality, and freely available wildfire data for use by practitioners of ML methods.",fullPaper,jv125
Mathematics,p376,d6,b8f75b848b6cef0f2b5a1a11b794332ca9bccb45,j125,Environmental Reviews,A review of machine learning applications in wildfire science and management,"Artificial intelligence has been applied in wildfire science and management since the 1990s, with early applications including neural networks and expert systems. Since then, the field has rapidly progressed congruently with the wide adoption of machine learning (ML) methods in the environmental sciences. Here, we present a scoping review of ML applications in wildfire science and management. Our overall objective is to improve awareness of ML methods among wildfire researchers and managers, as well as illustrate the diverse and challenging range of problems in wildfire science available to ML data scientists. To that end, we first present an overview of popular ML approaches used in wildfire science to date and then review the use of ML in wildfire science as broadly categorized into six problem domains, including (i) fuels characterization, fire detection, and mapping; (ii) fire weather and climate change; (iii) fire occurrence, susceptibility, and risk; (iv) fire behavior prediction; (v) fire effects; and (vi) fire management. Furthermore, we discuss the advantages and limitations of various ML approaches relating to data size, computational requirements, generalizability, and interpretability, as well as identify opportunities for future advances in the science and management of wildfires within a data science context. In total, to the end of 2019, we identified 300 relevant publications in which the most frequently used ML methods across problem domains included random forests, MaxEnt, artificial neural networks, decision trees, support vector machines, and genetic algorithms. As such, there exists opportunities to apply more current ML methods — including deep learning and agent-based learning — in the wildfire sciences, especially in instances involving very large multivariate datasets. We must recognize, however, that despite the ability of ML models to learn on their own, expertise in wildfire science is necessary to ensure realistic modelling of fire processes across multiple scales, while the complexity of some ML methods such as deep learning requires a dedicated and sophisticated knowledge of their application. Finally, we stress that the wildfire research and management communities play an active role in providing relevant, high-quality, and freely available wildfire data for use by practitioners of ML methods.",fullPaper,jv125
Computer Science,p383,d3,2daffab3ebd3fc034f8f78d6a546606c33a5d398,j110,Scientometrics,"Google Scholar, Scopus and the Web of Science: a longitudinal and cross-disciplinary comparison",Abstract,fullPaper,jv110
Computer Science,p384,d3,5b5332e79aefa3b913d42a434b8ddb09b31b5b2e,c72,Workshop on Research on Enterprise Networking,Voronoi diagrams—a survey of a fundamental geometric data structure,"Computational geometry is concerned with the design and analysis of algorithms for geometrical problems. In addition, other more practically oriented, areas of computer science— such as computer graphics, computer-aided design, robotics, pattern recognition, and operations research—give rise to problems that inherently are geometrical. This is one reason computational geometry has attracted enormous research interest in the past decade and is a well-established area today. (For standard sources, we refer to the survey article by Lee and Preparata [19841 and to the textbooks by Preparata and Shames [1985] and Edelsbrunner [1987bl.) Readers familiar with the literature of computational geometry will have noticed, especially in the last few years, an increasing interest in a geometrical construct called the Voronoi diagram. This trend can also be observed in combinatorial geometry and in a considerable number of articles in natural science journals that address the Voronoi diagram under different names specific to the respective area. Given some number of points in the plane, their Voronoi diagram divides the plane according to the nearest-neighbor",poster,cp72
Computer Science,p387,d3,c8bad3f510224e5cb010ca422149bf6ebcaa1d7f,c9,Big Data,Impact of data sources on citation counts and rankings of LIS faculty: Web of science versus scopus and google scholar,"The Institute for Scientific Information's (ISI, now Thomson Scientific, Philadelphia, PA) citation databases have been used for decades as a starting point and often as the only tools for locating citations andsor conducting citation analyses. The ISI databases (or Web of Science [WoS]), however, may no longer be sufficient because new databases and tools that allow citation searching are now available. Using citations to the work of 25 library and information science (LIS) faculty members as a case study, the authors examine the effects of using Scopus and Google Scholar (GS) on the citation counts and rankings of scholars as measured by WoS. Overall, more than 10,000 citing and purportedly citing documents were examined. Results show that Scopus significantly alters the relative ranking of those scholars that appear in the middle of the rankings and that GS stands out in its coverage of conference proceedings as well as international, non-English language journals. The use of Scopus and GS, in addition to WoS, helps reveal a more accurate and comprehensive picture of the scholarly impact of authors. The WoS data took about 100 hours of collecting and processing time, Scopus consumed 200 hours, and GS a grueling 3,000 hours. © 2007 Wiley Periodicals, Inc.",poster,cp9
Computer Science,p391,d3,bf96377353bf9daa8dc0e98eee17335f54cbcc60,j66,Data Science Journal,Data science as an academic discipline,"I recall being a proud young academic about 1970; I had just received a research grant to build and study a scientific database, and I had joined CODATA. I was looking forward to the future in this new exciting discipline when the head of my department, an internationally known professor, advised me that data was “a low level activity” not suitable for an academic. I recall my dismay. What can we do to ensure that this does not happen again and that data science is universally recognized as a worthwhile academic activity? Incidentally, I did not take that advice, or I would not be writing this essay, but moved into computer science. I will use my experience to draw comparisons between the problems computer science had to become academically recognized and those faced by data science.",fullPaper,jv66
Computer Science,p392,d3,116927fbe4c9732fd1e392035a100c33b14e9d59,j16,International Journal of Digital Earth,Big Data and cloud computing: innovation opportunities and challenges,"ABSTRACT Big Data has emerged in the past few years as a new paradigm providing abundant data and opportunities to improve and/or enable research and decision-support applications with unprecedented value for digital earth applications including business, sciences and engineering. At the same time, Big Data presents challenges for digital earth to store, transport, process, mine and serve the data. Cloud computing provides fundamental support to address the challenges with shared computing resources including computing, storage, networking and analytical software; the application of these resources has fostered impressive Big Data advancements. This paper surveys the two frontiers – Big Data and cloud computing – and reviews the advantages and consequences of utilizing cloud computing to tackling Big Data in the digital earth and relevant science domains. From the aspects of a general introduction, sources, challenges, technology status and research opportunities, the following observations are offered: (i) cloud computing and Big Data enable science discoveries and application developments; (ii) cloud computing provides major solutions for Big Data; (iii) Big Data, spatiotemporal thinking and various application domains drive the advancement of cloud computing and relevant technologies with new requirements; (iv) intrinsic spatiotemporal principles of Big Data and geospatial sciences provide the source for finding technical and theoretical solutions to optimize cloud computing and processing Big Data; (v) open availability of Big Data and processing capability pose social challenges of geospatial significance and (vi) a weave of innovations is transforming Big Data into geospatial research, engineering and business values. This review introduces future innovations and a research agenda for cloud computing supporting the transformation of the volume, velocity, variety and veracity into values of Big Data for local to global digital earth science and applications.",fullPaper,jv16
Computer Science,p393,d3,1842a5fb9739149dadba962c94dd7243a5f62242,c53,International Conference on Learning Representations,What is Data Science ? Fundamental Concepts and a Heuristic Example,Abstract,poster,cp53
Computer Science,p402,d3,915cd8e2b39eb02723553913d592b2237d4d9960,j132,Statistical analysis and data mining,Data science: An action plan for expanding the technical areas of the field of statistics,"An action plan to expand the technical areas of statistics focuses on the data analyst. The plan sets out six technical areas of work for a university department and advocates a specific allocation of resources devoted to research in each area and to courses in each area. The value of technical work is judged by the extent to which it benefits the data analyst, either directly or indirectly. The plan is also applicable to government research labs and corporate research organizations.",fullPaper,jv132
Business,p402,d9,915cd8e2b39eb02723553913d592b2237d4d9960,j132,Statistical analysis and data mining,Data science: An action plan for expanding the technical areas of the field of statistics,"An action plan to expand the technical areas of statistics focuses on the data analyst. The plan sets out six technical areas of work for a university department and advocates a specific allocation of resources devoted to research in each area and to courses in each area. The value of technical work is judged by the extent to which it benefits the data analyst, either directly or indirectly. The plan is also applicable to government research labs and corporate research organizations.",fullPaper,jv132
Computer Science,p406,d3,43d75d3a22db904d052d4c435e2d1f22be3887e0,j4,IEEE Transactions on Knowledge and Data Engineering,Outlier Detection for Temporal Data: A Survey,"In the statistics community, outlier detection for time series data has been studied for decades. Recently, with advances in hardware and software technology, there has been a large body of work on temporal outlier detection from a computational perspective within the computer science community. In particular, advances in hardware technology have enabled the availability of various forms of temporal data collection mechanisms, and advances in software technology have enabled a variety of data management mechanisms. This has fueled the growth of different kinds of data sets such as data streams, spatio-temporal data, distributed streams, temporal networks, and time series data, generated by a multitude of applications. There arises a need for an organized and detailed study of the work done in the area of outlier detection with respect to such temporal datasets. In this survey, we provide a comprehensive and structured overview of a large set of interesting outlier definitions for various forms of temporal data, novel techniques, and application scenarios in which specific definitions and techniques have been widely used.",fullPaper,jv4
Computer Science,p407,d3,7ac8f533a18f584387dd412a0a27feb9af1c5c93,j50,ACM Computing Surveys,A Systematic Review on Imbalanced Data Challenges in Machine Learning,"In machine learning, the data imbalance imposes challenges to perform data analytics in almost all areas of real-world research. The raw primary data often suffers from the skewed perspective of data distribution of one class over the other as in the case of computer vision, information security, marketing, and medical science. The goal of this article is to present a comparative analysis of the approaches from the reference of data pre-processing, algorithmic and hybrid paradigms for contemporary imbalance data analysis techniques, and their comparative study in lieu of different data distribution and their application areas.",fullPaper,jv50
Computer Science,p410,d3,023eb29b711014b1a3d2895e19a0fc2aed7a6ab4,c111,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,Data Science and Classification,Abstract,poster,cp111
Mathematics,p410,d6,023eb29b711014b1a3d2895e19a0fc2aed7a6ab4,c111,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,Data Science and Classification,Abstract,poster,cp111
Computer Science,p411,d3,e048f6fdb0a728638af5d8684a32b3dc2ee83259,j134,Big Data Research,Big Data and Science: Myths and Reality,Abstract,fullPaper,jv134
History,p411,d12,e048f6fdb0a728638af5d8684a32b3dc2ee83259,j134,Big Data Research,Big Data and Science: Myths and Reality,Abstract,fullPaper,jv134
Computer Science,p412,d3,de5acd80c5fd8db442a4a5e5ffbc3f3f51161237,c75,International Conference on Predictive Models in Software Engineering,"Data Science, Classification and Related Methods",Abstract,poster,cp75
Computer Science,p413,d3,f03a847c6325d7d5973efd687d2ca86a9c06dd76,c2,International Conference on Software Engineering,Advances in data science and classification,Abstract,poster,cp2
Computer Science,p414,d3,197b30ab1460fe200dba90dc3392ad49a92c2ca4,c50,Conference on Emerging Network Experiment and Technology,Between Data Science and Applied Data Analysis,Abstract,poster,cp50
Computer Science,p416,d3,27245e65a27bde90b5b0bb25d157bb75a0ad8b5a,j136,EURASIP Journal on Advances in Signal Processing,A survey of machine learning for big data processing,Abstract,fullPaper,jv136
Computer Science,p424,d3,d33d879ea94fd36363dc7f015896ac6c0236acac,c113,International Conference on Mobile Data Management,Data Preprocessing in Data Mining,Abstract,poster,cp113
Computer Science,p425,d3,86b05bc7e953e683fa839ad01d6100a8f99558df,c73,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,Concrete mathematics - a foundation for computer science,"From the Publisher: 
This book introduces the mathematics that supports advanced computer programming and the analysis of algorithms. The primary aim of its well-known authors is to provide a solid and relevant base of mathematical skills - the skills needed to solve complex problems, to evaluate horrendous sums, and to discover subtle patterns in data. It is an indispensable text and reference not only for computer scientists - the authors themselves rely heavily on it! - but for serious users of mathematics in virtually every discipline. 
 
Concrete Mathematics is a blending of CONtinuous and disCRETE mathematics. ""More concretely,"" the authors explain, ""it is the controlled manipulation of mathematical formulas, using a collection of techniques for solving problems."" The subject matter is primarily an expansion of the Mathematical Preliminaries section in Knuth's classic Art of Computer Programming, but the style of presentation is more leisurely, and individual topics are covered more deeply. Several new topics have been added, and the most significant ideas have been traced to their historical roots. The book includes more than 500 exercises, divided into six categories. Complete answers are provided for all exercises, except research problems, making the book particularly valuable for self-study. 
 
Major topics include: 
 
Sums 
Recurrences 
Integer functions 
Elementary number theory 
Binomial coefficients 
Generating functions 
Discrete probability 
Asymptotic methods 
 
 
This second edition includes important new material about mechanical summation. In response to the widespread use ofthe first edition as a reference book, the bibliography and index have also been expanded, and additional nontrivial improvements can be found on almost every page. Readers will appreciate the informal style of Concrete Mathematics. Particularly enjoyable are the marginal graffiti contributed by students who have taken courses based on this material. The authors want to convey not only the importance of the techniques presented, but some of the fun in learning and using them.",poster,cp73
Mathematics,p425,d6,86b05bc7e953e683fa839ad01d6100a8f99558df,c73,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,Concrete mathematics - a foundation for computer science,"From the Publisher: 
This book introduces the mathematics that supports advanced computer programming and the analysis of algorithms. The primary aim of its well-known authors is to provide a solid and relevant base of mathematical skills - the skills needed to solve complex problems, to evaluate horrendous sums, and to discover subtle patterns in data. It is an indispensable text and reference not only for computer scientists - the authors themselves rely heavily on it! - but for serious users of mathematics in virtually every discipline. 
 
Concrete Mathematics is a blending of CONtinuous and disCRETE mathematics. ""More concretely,"" the authors explain, ""it is the controlled manipulation of mathematical formulas, using a collection of techniques for solving problems."" The subject matter is primarily an expansion of the Mathematical Preliminaries section in Knuth's classic Art of Computer Programming, but the style of presentation is more leisurely, and individual topics are covered more deeply. Several new topics have been added, and the most significant ideas have been traced to their historical roots. The book includes more than 500 exercises, divided into six categories. Complete answers are provided for all exercises, except research problems, making the book particularly valuable for self-study. 
 
Major topics include: 
 
Sums 
Recurrences 
Integer functions 
Elementary number theory 
Binomial coefficients 
Generating functions 
Discrete probability 
Asymptotic methods 
 
 
This second edition includes important new material about mechanical summation. In response to the widespread use ofthe first edition as a reference book, the bibliography and index have also been expanded, and additional nontrivial improvements can be found on almost every page. Readers will appreciate the informal style of Concrete Mathematics. Particularly enjoyable are the marginal graffiti contributed by students who have taken courses based on this material. The authors want to convey not only the importance of the techniques presented, but some of the fun in learning and using them.",poster,cp73
Computer Science,p426,d3,4b4b63405efd22a96cc45b22c08124d62a475d6f,j7,Journal of Big Data,Big healthcare data: preserving security and privacy,Abstract,fullPaper,jv7
Computer Science,p427,d3,db019eec15d8080086bbc7dc8f5832e431202e0e,c24,International Conference on Data Technologies and Applications,Jupyter: Thinking and Storytelling With Code and Data,"Project Jupyter is an open-source project for interactive computing widely used in data science, machine learning, and scientific computing. We argue that even though Jupyter helps users perform complex, technical work, Jupyter itself solves problems that are fundamentally human in nature. Namely, Jupyter helps humans to think and tell stories with code and data. We illustrate this by describing three dimensions of Jupyter: 1) interactive computing; 2) computational narratives; and 3) the idea that Jupyter is more than software. We illustrate the impact of these dimensions on a community of practice in earth and climate science.",poster,cp24
Computer Science,p428,d3,f4156a05a47fdeda30638e10954d3674cc056ab6,c66,International Conference on Web and Social Media,Discovering Knowledge in Data: An Introduction to Data Mining,"This book is the first volume of a three-volume series on data mining, which introduces the reader to this rapidly growing field. Data mining, which has gained noticeable popularity in the past decade, is essentially an interdisciplinary field bringing together techniques from machine learning, pattern recognition, statistics, databases, and visualization (Cabena et al., 1998) to address the issue of exploring large and complicated databases to identify “interesting” relationships, e.g., high order interactions, or very non-linear relationships that ordinarily would not be detected by standard statistical analyses (Borok, 1997; Szolvits, 1995). This area has been approached by computer scientists and statisticians from slightly different perspectives. The author of the book is a statistician, but has tried to include a computer science theme throughout the book, in which I think he has been successful. As he mentions in the preface, the book is intended to be used either by analysts, managers, and decision makers in industry or as a textbook for an introductory course in data mining for graduate or advanced undergraduate students (in computer science or statistics). Chapter 1 is a short introductory chapter, in which in addition to a brief description of the Cross-Industry Standard Process for Data Mining (CRISP-DM), several real-world case studies are covered to motivate the topics of subsequent chapters. These case studies are also used to describe the first phase of the CRISPDM process, namely business understanding. Chapters 2 and 3 examine the next two phases of the CRISP-DM process, i.e., data understanding and data preparation. Chapter 2 is on data preprocessing, which is divided into the two major tasks of data cleaning and data transformation. In data cleaning, general methods for handling missing data, identifying misclassified records in the data, and also a graphical method for detecting outliers are described. In the data transformation section, min-max normalization and z-score standardization methods are discussed. A numerical method for detecting outliers based on z-score standardization is also covered. Exploratory data analysis is the topic of Chapter 3, which focuses on data understanding. The chapter begins with making a contrast between hypothesis testing and exploratory data analysis, and is followed by the basic ideas of dealing with correlated variables in the data set. Most of this chapter is dedicated to exploring the variables in a real data set, in which by using several diagrams a number of intuitive approaches for obtaining a high level understanding of the data are proposed.",poster,cp66
Computer Science,p430,d3,0578dfb2a28b77abde19b32de777e0365df3020e,j141,Applied Physics Reviews,Data-driven materials research enabled by natural language processing and information extraction,"Given the emergence of data science and machine learning throughout all aspects of society, but particularly in the scientific domain, there is increased importance placed on obtaining data. Data in materials science are particularly heterogeneous, based on the significant range in materials classes that are explored and the variety of materials properties that are of interest. This leads to data that range many orders of magnitude, and these data may manifest as numerical text or image-based information, which requires quantitative interpretation. The ability to automatically consume and codify the scientific literature across domains—enabled by techniques adapted from the field of natural language processing—therefore has immense potential to unlock and generate the rich datasets necessary for data science and machine learning. This review focuses on the progress and practices of natural language processing and text mining of materials science literature and highlights opportunities for extracting additional information beyond text contained in figures and tables in articles. We discuss and provide examples for several reasons for the pursuit of natural language processing for materials, including data compilation, hypothesis development, and understanding the trends within and across fields. Current and emerging natural language processing methods along with their applications to materials science are detailed. We, then, discuss natural language processing and data challenges within the materials science domain where future directions may prove valuable.",fullPaper,jv141
Computer Science,p431,d3,db8335198bd47c8865d0b3408b97e547abfd9ba2,c119,International Conference on Business Process Management,The Fourth Paradigm: Data-Intensive Scientific Discovery,"This presentation will set out the eScience agenda by explaining the current scientific data deluge and the case for a “Fourth Paradigm” for scientific exploration. Examples of data intensive science will be used to illustrate the explosion of data and the associated new challenges for data capture, curation, analysis, and sharing. The role of cloud computing, collaboration services, and research repositories will be discussed.",poster,cp119
Geography,p431,d13,db8335198bd47c8865d0b3408b97e547abfd9ba2,c119,International Conference on Business Process Management,The Fourth Paradigm: Data-Intensive Scientific Discovery,"This presentation will set out the eScience agenda by explaining the current scientific data deluge and the case for a “Fourth Paradigm” for scientific exploration. Examples of data intensive science will be used to illustrate the explosion of data and the associated new challenges for data capture, curation, analysis, and sharing. The role of cloud computing, collaboration services, and research repositories will be discussed.",poster,cp119
Computer Science,p434,d3,67c1dced4e379676c1a6f65226e678d50c9541f1,c2,International Conference on Software Engineering,The Art and Science of Analyzing Software Data,Abstract,fullPaper,cp2
Computer Science,p435,d3,fd40e458a67f9a3854834fd42b66b0d6ed43ab8d,c103,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,Educational Data Mining and Learning Analytics,Abstract,poster,cp103
Computer Science,p436,d3,a4b603ca6aaaa18968e08ac1b0ee093db8a99a6b,c72,Workshop on Research on Enterprise Networking,Topology and data,"An important feature of modern science and engineering is that data of various kinds is being produced at an unprecedented rate. This is so in part because of new experimental methods, and in part because of the increase in the availability of high powered computing technology. It is also clear that the nature of the data we are obtaining is significantly different. For example, it is now often the case that we are given data in the form of very long vectors, where all but a few of the coordinates turn out to be irrelevant to the questions of interest, and further that we don’t necessarily know which coordinates are the interesting ones. A related fact is that the data is often very high-dimensional, which severely restricts our ability to visualize it. The data obtained is also often much noisier than in the past and has more missing information (missing data). This is particularly so in the case of biological data, particularly high throughput data from microarray or other sources. Our ability to analyze this data, both in terms of quantity and the nature of the data, is clearly not keeping pace with the data being produced. In this paper, we will discuss how geometry and topology can be applied to make useful contributions to the analysis of various kinds of data. Geometry and topology are very natural tools to apply in this direction, since geometry can be regarded as the study of distance functions, and what one often works with are distance functions on large finite sets of data. The mathematical formalism which has been developed for incorporating geometric and topological techniques deals with point clouds, i.e. finite sets of points equipped with a distance function. It then adapts tools from the various branches of geometry to the study of point clouds. The point clouds are intended to be thought of as finite samples taken from a geometric object, perhaps with noise. Here are some of the key points which come up when applying these geometric methods to data analysis. • Qualitative information is needed: One important goal of data analysis is to allow the user to obtain knowledge about the data, i.e. to understand how it is organized on a large scale. For example, if we imagine that we are looking at a data set constructed somehow from diabetes patients, it would be important to develop the understanding that there are two types of the disease, namely the juvenile and adult onset forms. Once that is established, one of course wants to develop quantitative methods for distinguishing them, but the first insight about the distinct forms of the disease is key.",poster,cp72
Computer Science,p439,d3,377f1e43c5a48f12b0592b09a142322e74729409,j143,Annals of Data Science,Genetic Algorithms in the Fields of Artificial Intelligence and Data Sciences,Abstract,fullPaper,jv143
Computer Science,p441,d3,fbd9ddc0a3862512ce7a0ba2bb9cb159da0a9d2f,j144,"Marketing science (Providence, R.I.)",Editorial - Marketing Science and Big Data,"This article was downloaded by: [128.97.27.20] On: 25 May 2016, At: 09:44 Publisher: Institute for Operations Research and the Management Sciences (INFORMS) INFORMS is located in Maryland, USA Marketing Science Publication details, including instructions for authors and subscription information: http://pubsonline.informs.org Editorial—Marketing Science and Big Data Pradeep Chintagunta, Dominique M. Hanssens, John R. Hauser To cite this article: Pradeep Chintagunta, Dominique M. Hanssens, John R. Hauser (2016) Editorial—Marketing Science and Big Data. Marketing Science 35(3):341-342. http://dx.doi.org/10.1287/mksc.2016.0996 Full terms and conditions of use: http://pubsonline.informs.org/page/terms-and-conditions This article may be used only for the purposes of research, teaching, and/or private study. Commercial use or systematic downloading (by robots or other automatic processes) is prohibited without explicit Publisher approval, unless otherwise noted. For more information, contact permissions@informs.org. The Publisher does not warrant or guarantee the article’s accuracy, completeness, merchantability, fitness for a particular purpose, or non-infringement. Descriptions of, or references to, products or publications, or inclusion of an advertisement in this article, neither constitutes nor implies a guarantee, endorsement, or support of claims made of that product, publication, or service. Copyright © 2016, INFORMS Please scroll down for article—it is on subsequent pages INFORMS is the largest professional society in the world for professionals in the fields of operations research, management science, and analytics. For more information on INFORMS, its publications, membership, or meetings visit http://www.informs.org",fullPaper,jv144
Computer Science,p442,d3,64ad643e8084486ca7d3312ed491a814d3fe440c,c16,International Conference on Data Science and Advanced Analytics,The Synthetic Data Vault,"The goal of this paper is to build a system that automatically creates synthetic data to enable data science endeavors. To achieve this, we present the Synthetic Data Vault (SDV), a system that builds generative models of relational databases. We are able to sample from the model and create synthetic data, hence the name SDV. When implementing the SDV, we also developed an algorithm that computes statistics at the intersection of related database tables. We then used a state-of-the-art multivariate modeling approach to model this data. The SDV iterates through all possible relations, ultimately creating a model for the entire database. Once this model is computed, the same relational information allows the SDV to synthesize data by sampling from any part of the database. After building the SDV, we used it to generate synthetic data for five different publicly available datasets. We then published these datasets, and asked data scientists to develop predictive models for them as part of a crowdsourced experiment. By analyzing the outcomes, we show that synthetic data can successfully replace original data for data science. Our analysis indicates that there is no significant difference in the work produced by data scientists who used synthetic data as opposed to real data. We conclude that the SDV is a viable solution for synthetic data generation.",fullPaper,cp16
Computer Science,p454,d3,720400bf69c1af50795d7ec1b58e95c682d217aa,j89,bioRxiv,Best Practices in Data Analysis and Sharing in Neuroimaging using MRI,"Neuroimaging enables rich noninvasive measurements of human brain activity, but translating such data into neuroscientific insights and clinical applications requires complex analyses and collaboration among a diverse array of researchers. The open science movement is reshaping scientific culture and addressing the challenges of transparency and reproducibility of research. To advance open science in neuroimaging the Organization for Human Brain Mapping created the Committee on Best Practice in Data Analysis and Sharing (COBIDAS), charged with creating a report that collects best practice recommendations from experts and the entire brain imaging community. The purpose of this work is to elaborate the principles of open and reproducible research for neuroimaging using Magnetic Resonance Imaging (MRI), and then distill these principles to specific research practices. Many elements of a study are so varied that practice cannot be prescribed, but for these areas we detail the information that must be reported to fully understand and potentially replicate a study. For other elements of a study, like statistical modelling where specific poor practices can be identified, and the emerging areas of data sharing and reproducibility, we detail both good practice and reporting standards. For each of seven areas of a study we provide tabular listing of over 100 items to help plan, execute, report and share research in the most transparent fashion. Whether for individual scientists, or for editors and reviewers, we hope these guidelines serve as a benchmark, to raise the standards of practice and reporting in neuroimaging using MRI.",fullPaper,jv89
Biology,p454,d5,720400bf69c1af50795d7ec1b58e95c682d217aa,j89,bioRxiv,Best Practices in Data Analysis and Sharing in Neuroimaging using MRI,"Neuroimaging enables rich noninvasive measurements of human brain activity, but translating such data into neuroscientific insights and clinical applications requires complex analyses and collaboration among a diverse array of researchers. The open science movement is reshaping scientific culture and addressing the challenges of transparency and reproducibility of research. To advance open science in neuroimaging the Organization for Human Brain Mapping created the Committee on Best Practice in Data Analysis and Sharing (COBIDAS), charged with creating a report that collects best practice recommendations from experts and the entire brain imaging community. The purpose of this work is to elaborate the principles of open and reproducible research for neuroimaging using Magnetic Resonance Imaging (MRI), and then distill these principles to specific research practices. Many elements of a study are so varied that practice cannot be prescribed, but for these areas we detail the information that must be reported to fully understand and potentially replicate a study. For other elements of a study, like statistical modelling where specific poor practices can be identified, and the emerging areas of data sharing and reproducibility, we detail both good practice and reporting standards. For each of seven areas of a study we provide tabular listing of over 100 items to help plan, execute, report and share research in the most transparent fashion. Whether for individual scientists, or for editors and reviewers, we hope these guidelines serve as a benchmark, to raise the standards of practice and reporting in neuroimaging using MRI.",fullPaper,jv89
Computer Science,p456,d3,299bab6b327e34c3e4f97cc8d0f9c64c9741fa99,j17,Big Data & Society,Where are human subjects in Big Data research? The emerging ethics divide,"There are growing discontinuities between the research practices of data science and established tools of research ethics regulation. Some of the core commitments of existing research ethics regulations, such as the distinction between research and practice, cannot be cleanly exported from biomedical research to data science research. Such discontinuities have led some data science practitioners and researchers to move toward rejecting ethics regulations outright. These shifts occur at the same time as a proposal for major revisions to the Common Rule—the primary regulation governing human-subjects research in the USA—is under consideration for the first time in decades. We contextualize these revisions in long-running complaints about regulation of social science research and argue data science should be understood as continuous with social sciences in this regard. The proposed regulations are more flexible and scalable to the methods of non-biomedical research, yet problematically largely exclude data science methods from human-subjects regulation, particularly uses of public datasets. The ethical frameworks for Big Data research are highly contested and in flux, and the potential harms of data science research are unpredictable. We examine several contentious cases of research harms in data science, including the 2014 Facebook emotional contagion study and the 2016 use of geographical data techniques to identify the pseudonymous artist Banksy. To address disputes about application of human-subjects research ethics in data science, critical data studies should offer a historically nuanced theory of “data subjectivity” responsive to the epistemic methods, harms and benefits of data science and commerce.",fullPaper,jv17
Sociology,p456,d4,299bab6b327e34c3e4f97cc8d0f9c64c9741fa99,j17,Big Data & Society,Where are human subjects in Big Data research? The emerging ethics divide,"There are growing discontinuities between the research practices of data science and established tools of research ethics regulation. Some of the core commitments of existing research ethics regulations, such as the distinction between research and practice, cannot be cleanly exported from biomedical research to data science research. Such discontinuities have led some data science practitioners and researchers to move toward rejecting ethics regulations outright. These shifts occur at the same time as a proposal for major revisions to the Common Rule—the primary regulation governing human-subjects research in the USA—is under consideration for the first time in decades. We contextualize these revisions in long-running complaints about regulation of social science research and argue data science should be understood as continuous with social sciences in this regard. The proposed regulations are more flexible and scalable to the methods of non-biomedical research, yet problematically largely exclude data science methods from human-subjects regulation, particularly uses of public datasets. The ethical frameworks for Big Data research are highly contested and in flux, and the potential harms of data science research are unpredictable. We examine several contentious cases of research harms in data science, including the 2014 Facebook emotional contagion study and the 2016 use of geographical data techniques to identify the pseudonymous artist Banksy. To address disputes about application of human-subjects research ethics in data science, critical data studies should offer a historically nuanced theory of “data subjectivity” responsive to the epistemic methods, harms and benefits of data science and commerce.",fullPaper,jv17
Computer Science,p457,d3,c8bc2d5edb9307b5c420adc4eee3cf641a781b14,c4,Conference on Innovative Data Systems Research,Online analysis enhances use of NASA Earth science data,"Giovanni, the Goddard Earth Sciences Data and Information Services Center (GES DISC) Interactive Online Visualization and Analysis Infrastructure, has provided researchers with advanced capabilities to perform data exploration and analysis with observational data from NASA Earth observation satellites. In the past 5–10 years, examining geophysical events and processes with remote-sensing data required a multistep process of data discovery, data acquisition, data management, and ultimately data analysis. Giovanni accelerates this process by enabling basic visualization and analysis directly on the World Wide Web. In the last two years, Giovanni has added new data acquisition functions and expanded analysis options to increase its usefulness to the Earth science research community.",poster,cp4
Computer Science,p461,d3,f567f5a4a57509c2288f510d6703212ce8499527,j109,Earth and Space Science,The Ames Stereo Pipeline: NASA's Open Source Software for Deriving and Processing Terrain Data,"The NASA Ames Stereo Pipeline is a suite of free and open source automated geodesy and stereogrammetry tools designed for processing stereo images captured from satellites (around Earth and other planets), robotic rovers, aerial cameras, and historical images, with and without accurate camera pose information. It produces cartographic products, including digital terrain models, ortho‐projected images, 3‐D models, and bundle‐adjusted networks of cameras. Ames Stereo Pipeline's data products are suitable for science analysis, mission planning, and public outreach.",fullPaper,jv109
Computer Science,p462,d3,6d962e9f04c653f732da82073a3446f75a371055,c110,Biometrics and Identity Management,The KDD process for extracting useful knowledge from volumes of data,"AS WE MARCH INTO THE AGE of digital information, the problem of data overload looms ominously ahead. Our ability to analyze and understand massive datasets lags far behind our ability to gather and store the data. A new generation of computational techniques and tools is required to support the extraction of useful knowledge from the rapidly growing volumes of data. These techniques and tools are the subject of the emerging field of knowledge discovery in databases (KDD) and data mining. Large databases of digital information are ubiquitous. Data from the neighborhood store’s checkout register, your bank’s credit card authorization device, records in your doctor’s office, patterns in your telephone calls, and many more applications generate streams of digital records archived in huge databases, sometimes in so-called data warehouses. Current hardware and database technology allow efficient and inexpensive reliable data storage and access. However, whether the context is business, medicine, science, or government, the datasets themselves (in raw form) are of little direct value. What is of value is the knowledge that can be inferred from the data and put to use. For example, the marketing database of a consumer U s a m a F a y y a d ,",poster,cp110
Computer Science,p464,d3,a971f856fcf4a4a7589dbf711dd2544f51c5e9b2,c27,International Conference Geographic Information Science,Linked Data - A Paradigm Shift for Geographic Information Science,Abstract,fullPaper,cp27
Computer Science,p466,d3,b55fda1f58af7fd9ecde8f1dc193ddd6ab6e9d26,c41,IEEE International Conference on Data Engineering,Handbook of theoretical computer science - Part A: Algorithms and complexity; Part B: Formal models and semantics,"""Of all the books I have covered in the Forum to date, this set is the most unique and possibly the most useful to the SIGACT community, in support both of teaching and research.... The books can be used by anyone wanting simply to gain an understanding of one of these areas, or by someone desiring to be in research in a topic, or by instructors wishing to find timely information on a subject they are teaching outside their major areas of expertise."" -- Rocky Ross, ""SIGACT News"" ""This is a reference which has a place in every computer science library."" -- Raymond Lauzzana, ""Languages of Design"" The Handbook of Theoretical Computer Science provides professionals and students with a comprehensive overview of the main results and developments in this rapidly evolving field. Volume A covers models of computation, complexity theory, data structures, and efficient computation in many recognized subdisciplines of theoretical computer science. Volume B takes up the theory of automata and rewriting systems, the foundations of modern programming languages, and logics for program specification and verification, and presents several studies on the theoretic modeling of advanced information processing. The two volumes contain thirty-seven chapters, with extensive chapter references and individual tables of contents for each chapter. There are 5,387 entry subject indexes that include notational symbols, and a list of contributors and affiliations in each volume.",poster,cp41
Computer Science,p468,d3,05859c8d47b16ce84c817c16d29ad6ec9d1d3a33,c76,Group,The Science DMZ: A network design pattern for data-intensive science,"The ever-increasing scale of scientific data has become a significant challenge for researchers that rely on networks to interact with remote computing systems and transfer results to collaborators worldwide. Despite the availability of high-capacity connections, scientists struggle with inadequate cyberinfrastructure that cripples data transfer performance, and impedes scientific progress. The Science DMZ paradigm comprises a proven set of network design patterns that collectively address these problems for scientists. We explain the Science DMZ model, including network architecture, system configuration, cybersecurity, and performance tools, that creates an optimized network environment for science. We describe use cases from universities, supercomputing centers and research laboratories, highlighting the effectiveness of the Science DMZ model in diverse operational settings. In all, the Science DMZ model is a solid platform that supports any science workflow, and flexibly accommodates emerging network technologies. As a result, the Science DMZ vastly improves collaboration, accelerating scientific discovery.",poster,cp76
Computer Science,p469,d3,97156d041b6cae2095dd29d76e24e0017a7ec799,c81,ACM Symposium on Applied Computing,Functional Data Analysis,Abstract,poster,cp81
Mathematics,p469,d6,97156d041b6cae2095dd29d76e24e0017a7ec799,c81,ACM Symposium on Applied Computing,Functional Data Analysis,Abstract,poster,cp81
Computer Science,p472,d3,34ad09cda075101dc4ce3c04006ff804aca3ebf8,c28,International Conference on Contemporary Computing,"Big data: Issues, challenges, tools and Good practices","Big data is defined as large amount of data which requires new technologies and architectures so that it becomes possible to extract value from it by capturing and analysis process. Due to such large size of data it becomes very difficult to perform effective analysis using the existing traditional techniques. Big data due to its various properties like volume, velocity, variety, variability, value and complexity put forward many challenges. Since Big data is a recent upcoming technology in the market which can bring huge benefits to the business organizations, it becomes necessary that various challenges and issues associated in bringing and adapting to this technology are brought into light. This paper introduces the Big data technology along with its importance in the modern world and existing projects which are effective and important in changing the concept of science into big science and society too. The various challenges and issues in adapting and accepting Big data technology, its tools (Hadoop) are also discussed in detail along with the problems Hadoop is facing. The paper concludes with the Good Big data practices to be followed.",fullPaper,cp28
Computer Science,p474,d3,18a940ff6dce8bc140658da52d686291ca965979,c98,Vision,The Analysis of Social Science Data with Missing Values,"Methods for handling missing data in social science data sets are reviewed. Limitations of common practical approaches, including complete-case analysis, available-case analysis and imputation, are illustrated on a simple missing-data problem with one complete and one incomplete variable. Two more principled approaches, namely maximum likelihood under a model for the data and missing-data mechanism and multiple imputation, are applied to the bivariate problem. General properties of these methods are outlined, and applications to more complex missing-data problems are discussed. The EM algorithm, a convenient method for computing maximum likelihood estimates in missing-data problems, is described and applied to two common models, the multivariate normal model for continuous data and the multinomial model for discrete data. Multiple imputation under explicit or implicit models is recommended as a method that retains the advantages of imputation and overcomes its limitations.",poster,cp98
Computer Science,p475,d3,5ae073986408c9931bf6887fafb85e253866f7cc,c20,ACM Conference on Economics and Computation,Fuzzy-Set Social Science,"In this innovative approach to the practice of social science, Charles Ragin explores the use of fuzzy sets to bridge the divide between quantitative and qualitative methods. Paradoxically, the fuzzy set is a powerful tool because it replaces an unwieldy, ""fuzzy"" instrument—the variable, which establishes only the positions of cases relative to each other, with a precise one—degree of membership in a well-defined set. Ragin argues that fuzzy sets allow a far richer dialogue between ideas and evidence in social research than previously possible. They let quantitative researchers abandon ""homogenizing assumptions"" about cases and causes, they extend diversity-oriented research strategies, and they provide a powerful connection between theory and data analysis. Most important, fuzzy sets can be carefully tailored to fit evolving theoretical concepts, sharpening quantitative tools with in-depth knowledge gained through qualitative, case-oriented inquiry. This book will revolutionize research methods not only in sociology, political science, and anthropology but in any field of inquiry dealing with complex patterns of causation.",poster,cp20
Political Science,p475,d15,5ae073986408c9931bf6887fafb85e253866f7cc,c20,ACM Conference on Economics and Computation,Fuzzy-Set Social Science,"In this innovative approach to the practice of social science, Charles Ragin explores the use of fuzzy sets to bridge the divide between quantitative and qualitative methods. Paradoxically, the fuzzy set is a powerful tool because it replaces an unwieldy, ""fuzzy"" instrument—the variable, which establishes only the positions of cases relative to each other, with a precise one—degree of membership in a well-defined set. Ragin argues that fuzzy sets allow a far richer dialogue between ideas and evidence in social research than previously possible. They let quantitative researchers abandon ""homogenizing assumptions"" about cases and causes, they extend diversity-oriented research strategies, and they provide a powerful connection between theory and data analysis. Most important, fuzzy sets can be carefully tailored to fit evolving theoretical concepts, sharpening quantitative tools with in-depth knowledge gained through qualitative, case-oriented inquiry. This book will revolutionize research methods not only in sociology, political science, and anthropology but in any field of inquiry dealing with complex patterns of causation.",poster,cp20
Computer Science,p484,d3,9386590554c429e80402c082e9d6a2398bcc36b3,c29,ACM-SIAM Symposium on Discrete Algorithms,Data streams: algorithms and applications,"Data stream algorithms as an active research agenda emerged only over the past few years, even though the concept of making few passes over the data for performing computations has been around since the early days of Automata Theory. The data stream agenda now pervades many branches of Computer Science including databases, networking, knowledge discovery and data mining, and hardware systems. Industry is in synch too, with Data Stream Management Systems (DSMSs) and special hardware to deal with data speeds. Even beyond Computer Science, data stream concerns are emerging in physics, atmospheric science and statistics. Data Streams: Algorithms and Applications focuses on the algorithmic foundations of data streaming. In the data stream scenario, input arrives very rapidly and there is limited memory to store the input. Algorithms have to work with one or few passes over the data, space less than linear in the input size or time significantly less than the input size. In the past few years, a new theory has emerged for reasoning about algorithms that work within these constraints on space, time and number of passes. Some of the methods rely on metric embeddings, pseudo-random computations, sparse approximation theory and communication complexity. The applications for this scenario include IP network traffic analysis, mining text message streams and processing massive data sets in general. Data Streams: Algorithms and Applications surveys the emerging area of algorithms for processing data streams and associated applications. An extensive bibliography with over 200 entries points the reader to further resources for exploration.",fullPaper,cp29
Computer Science,p485,d3,f7d7f1eb559d8e2f410289fca37bb6cec7a3a907,j17,Big Data & Society,Data politics,"The commentary raises political questions about the ways in which data has been constituted as an object vested with certain powers, influence, and rationalities. We place the emergence and transformation of professional practices such as ‘data science’, ‘data journalism’, ‘data brokerage’, ‘data mining’, ‘data storage’, and ‘data analysis’ as part of the reconfiguration of a series of fields of power and knowledge in the public and private accumulation of data. Data politics asks questions about the ways in which data has become such an object of power and explores how to critically intervene in its deployment as an object of knowledge. It is concerned with the conditions of possibility of data that involve things (infrastructures of servers, devices, and cables), language (code, programming, and algorithms), and people (scientists, entrepreneurs, engineers, information technologists, designers) that together create new worlds. We define ‘data politics’ as both the articulation of political questions about these worlds and the ways in which they provoke subjects to govern themselves and others by making rights claims. We contend that without understanding these conditions of possibility – of worlds, subjects and rights – it would be difficult to intervene in or shape data politics if by that it is meant the transformation of data subjects into data citizens.",fullPaper,jv17
Computer Science,p487,d3,c50dca78e97e335d362d6b991ae0e1448914e9a3,c110,Biometrics and Identity Management,Reducing the Dimensionality of Data with Neural,"http://www.sciencemag.org/cgi/content/full/313/5786/504 version of this article at: including high-resolution figures, can be found in the online Updated information and services, http://www.sciencemag.org/cgi/content/full/313/5786/504/DC1 can be found at: Supporting Online Material found at: can be related to this article A list of selected additional articles on the Science Web sites http://www.sciencemag.org/cgi/content/full/313/5786/504#related-content http://www.sciencemag.org/cgi/content/full/313/5786/504#otherarticles , 6 of which can be accessed for free: cites 8 articles This article 15 article(s) on the ISI Web of Science. cited by This article has been http://www.sciencemag.org/cgi/content/full/313/5786/504#otherarticles 4 articles hosted by HighWire Press; see: cited by This article has been http://www.sciencemag.org/about/permissions.dtl in whole or in part can be found at: this article permission to reproduce of this article or about obtaining reprints Information about obtaining",poster,cp110
Computer Science,p493,d3,8ee4eda834e95124aca1e5ff05a1b8ce7d1487ec,j150,SIAM Journal on Mathematics of Data Science,Why Are Big Data Matrices Approximately Low Rank?,"Matrices of (approximate) low rank are pervasive in data science, appearing in movie preferences, text documents, survey data, medical records, and genomics. While there is a vast literature on how...",fullPaper,jv150
Computer Science,p497,d3,59b2796c176636a3222d7b129c6209fa6e979aa7,j17,Big Data & Society,Data infrastructure literacy,"A recent report from the UN makes the case for “global data literacy” in order to realise the opportunities afforded by the “data revolution”. Here and in many other contexts, data literacy is characterised in terms of a combination of numerical, statistical and technical capacities. In this article, we argue for an expansion of the concept to include not just competencies in reading and working with datasets but also the ability to account for, intervene around and participate in the wider socio-technical infrastructures through which data is created, stored and analysed – which we call “data infrastructure literacy”. We illustrate this notion with examples of “inventive data practice” from previous and ongoing research on open data, online platforms, data journalism and data activism. Drawing on these perspectives, we argue that data literacy initiatives might cultivate sensibilities not only for data science but also for data sociology, data politics as well as wider public engagement with digital data infrastructures. The proposed notion of data infrastructure literacy is intended to make space for collective inquiry, experimentation, imagination and intervention around data in educational programmes and beyond, including how data infrastructures can be challenged, contested, reshaped and repurposed to align with interests and publics other than those originally intended.",fullPaper,jv17
Sociology,p497,d4,59b2796c176636a3222d7b129c6209fa6e979aa7,j17,Big Data & Society,Data infrastructure literacy,"A recent report from the UN makes the case for “global data literacy” in order to realise the opportunities afforded by the “data revolution”. Here and in many other contexts, data literacy is characterised in terms of a combination of numerical, statistical and technical capacities. In this article, we argue for an expansion of the concept to include not just competencies in reading and working with datasets but also the ability to account for, intervene around and participate in the wider socio-technical infrastructures through which data is created, stored and analysed – which we call “data infrastructure literacy”. We illustrate this notion with examples of “inventive data practice” from previous and ongoing research on open data, online platforms, data journalism and data activism. Drawing on these perspectives, we argue that data literacy initiatives might cultivate sensibilities not only for data science but also for data sociology, data politics as well as wider public engagement with digital data infrastructures. The proposed notion of data infrastructure literacy is intended to make space for collective inquiry, experimentation, imagination and intervention around data in educational programmes and beyond, including how data infrastructures can be challenged, contested, reshaped and repurposed to align with interests and publics other than those originally intended.",fullPaper,jv17
Computer Science,p500,d3,6ac1749272c9d2db025a6e26b3166555382db5bc,j152,IEEE Access,Recent Advances in Data Engineering for Networking,"This tutorial paper examines recent advances in data engineering, focusing on aspects of network management and orchestration. We provide a comprehensive analysis of standardization efforts as well as platform development activities related to data engineering driven network design. We then focus on the integration aspects of the data engineering ecosystem and telecommunication networks. The results of our tutorial investigation show that despite various efforts towards standardization and network management and orchestration platforms, there is still a significant gap in applying recent developments in the evolving data engineering world to the telecommunication domain. New advanced functionalities in data engineering as well as clear separations between the building blocks of data engineering pipelines within the proposed standardized architectures have been overlooked or not explored in detail by the standardization or platform development bodies in the telecommunication domain. Therefore, at the end of the paper, we discuss these gaps and research challenges in the context of future development processes for data engineering-driven network design and applications of data engineering concepts in telecommunication networks. We also propose several recommendations for early adoption of these technologies and frameworks in telecommunication infrastructures and platforms.",fullPaper,jv152
Computer Science,p502,d3,18b18969b8688d01c124543f3956d4fd1b5ad5a7,j153,The VLDB journal,Managing bias and unfairness in data for decision support: a survey of machine learning and data engineering approaches to identify and mitigate bias and unfairness within data management and analytics systems,Abstract,fullPaper,jv153
Computer Science,p504,d3,b2ffbd5a4bebb3a67f64e82669b4255f3d1e5bee,c26,Decision Support Systems,Data engineering for fraud detection,Abstract,fullPaper,cp26
Computer Science,p505,d3,fd5532f3da374f1081c15c8913eabb65c2dc582d,j44,Social Science Research Network,Economic Data Engineering,"Economic data engineering deliberately designs novel forms of data to solve fundamental identification problems associated with economic models of choice. I outline three diverse applications: to the economics of information; to life-cycle employment, earnings, and spending; and to public policy analysis. In all three cases one and the same fundamental identification problem is driving data innovation: that of separately identifying appropriately rich preferences and beliefs. In addition to presenting these conceptually linked examples, I provide a general overview of the engineering process, outline important next steps, and highlight larger opportunities.",fullPaper,jv44
Computer Science,p506,d3,ec93a59a495cb1ef4c95a5ecf0e9fa238e88c556,j154,Datenbank-Spektrum,Four Generations in Data Engineering for Data Science,Abstract,fullPaper,jv154
Computer Science,p507,d3,8f8532a193313b9b956a8df402dd7f879bbe1377,c65,International Symposium on Empirical Software Engineering and Measurement,"Data Engineering for Data Analytics: A Classification of the Issues, and Case Studies","Consider the situation where a data analyst wishes to carry out an analysis on a given dataset. It is widely recognized that most of the analyst's time will be taken up with \emph{data engineering} tasks such as acquiring, understanding, cleaning and preparing the data. In this paper we provide a description and classification of such tasks into high-levels groups, namely data organization, data quality and feature engineering. We also make available four datasets and example analyses that exhibit a wide variety of these problems, to help encourage the development of tools and techniques to help reduce this burden and push forward research towards the automation or semi-automation of the data engineering process.",poster,cp65
Computer Science,p508,d3,f288e2238ac8725baa7ca9874bbc3fed1e89a632,c103,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,Data Engineering for Scaling Language Models to 128K Context,"We study the continual pretraining recipe for scaling language models' context lengths to 128K, with a focus on data engineering. We hypothesize that long context modeling, in particular \textit{the ability to utilize information at arbitrary input locations}, is a capability that is mostly already acquired through large-scale pretraining, and that this capability can be readily extended to contexts substantially longer than seen during training~(e.g., 4K to 128K) through lightweight continual pretraining on appropriate data mixture. We investigate the \textit{quantity} and \textit{quality} of the data for continual pretraining: (1) for quantity, we show that 500 million to 5 billion tokens are enough to enable the model to retrieve information anywhere within the 128K context; (2) for quality, our results equally emphasize \textit{domain balance} and \textit{length upsampling}. Concretely, we find that naively upsampling longer data on certain domains like books, a common practice of existing work, gives suboptimal performance, and that a balanced domain mixture is important. We demonstrate that continual pretraining of the full model on 1B-5B tokens of such data is an effective and affordable strategy for scaling the context length of language models to 128K. Our recipe outperforms strong open-source long-context models and closes the gap to frontier models like GPT-4 128K.",poster,cp103
Computer Science,p509,d3,7ce809ac8bb350f5ac7755b4e861710064b38654,c32,International Conference on Smart Data Services,High Performance Data Engineering Everywhere,"The amazing advances being made in the fields of machine and deep learning are a highlight of the Big Data era for both enterprise and research communities. Modern applications require resources beyond a single node's ability to provide. However this is just a small part of the issues facing the overall data processing environment, which must also support a raft of data engineering for pre- and post-data processing, communication, and system integration. An important requirement of data analytics tools is to be able to easily integrate with existing frameworks in a multitude of languages, thereby increasing user productivity and efficiency. All this demands an efficient and highly distributed integrated approach for data processing, yet many of today's popular data analytics tools are unable to satisfy all these requirements at the same time. In this paper we present Cylon, an open-source high performance distributed data processing library that can be seamlessly integrated with existing Big Data and AI/ML frameworks. It is developed with a flexible C++ core on top of a compact data structure and exposes language bindings to C++, Java, and Python. We discuss Cylon's architecture in detail, and reveal how it can be imported as a library to existing applications or operate as a standalone framework. Initial experiments show that Cylon enhances popular tools such as Apache Spark and Dask with major performance improvements for key operations and better component linkages. Finally, we show how its design enables Cylon to be used cross-platform with minimum overhead, which includes popular AI tools such as PyTorch, Tensorflow, and Jupyter notebooks.",fullPaper,cp32
Computer Science,p512,d3,b931d52d7d72cac8cfc104f1945fb513f60ef3e3,c33,Workshop on Python for High-Performance and Scientific Computing,Data Engineering for HPC with Python,"Data engineering is becoming an increasingly important part of scientific discoveries with the adoption of deep learning and machine learning. Data engineering deals with a variety of data formats, storage, data extraction, transformation, and data movements. One goal of data engineering is to transform data from original data to vector/matrix/tensor formats accepted by deep learning and machine learning applications. There are many structures such as tables, graphs, and trees to represent data in these data engineering phases. Among them, tables are a versatile and commonly used format to load and process data. In this paper, we present a distributed Python API based on table abstraction for representing and processing data. Unlike existing state-of-the-art data engineering tools written purely in Python, our solution adopts high performance compute kernels in C++, with an in-memory table representation with Cython-based Python bindings. In the core system, we use MPI for distributed memory computations with a data-parallel approach for processing large datasets in HPC clusters.",fullPaper,cp33
Computer Science,p513,d3,15d8143cccd70a82d43c342e92a8808f6232dea3,c34,International Conference on Data Warehousing and Knowledge Discovery,Data Engineering for Data Science: Two Sides of the Same Coin,Abstract,fullPaper,cp34
Computer Science,p514,d3,6ae876357f168a852028d97531c207321e10fce6,c35,"International Conference on Internet of Things, Big Data and Security",Challenging Big Data Engineering: Positioning of Current and Future Development,"This contribution examines the terms of big data and big data engineering, considering the specific characteristics and challenges. Deduced by those, it concludes the need for new ways to support the creation of corresponding systems to help big data in reaching its full potential. In the following, the state of the art is analysed and subdomains in the engineering of big data solutions are presented. In the end, a possible concept for filling the identified gap is proposed and future perspectives are highlighted.",fullPaper,cp35
Computer Science,p515,d3,090cfa03354d8c367d31f52cea7582fde6ecd572,j4,IEEE Transactions on Knowledge and Data Engineering,Special Section on the International Conference on Data Engineering 2016,"The papers in this special section were presented at the 32nd International Conference on Data Engineering that was held in Helsinki, Finland, May 16- May 20, 2016.",fullPaper,jv4
Computer Science,p516,d3,aeca9ab416adb5a514e57fa41705a71918c48488,c36,International Conference on Information Technology Based Higher Education and Training,Learning data engineering: Creating IoT apps using the node-RED and the RPI technologies,"This paper demonstrates the suitability and the practicality of using the advanced open source tools such as the Raspberry Pi and the Node-RED for teaching and learning in the Internet of Things (IOT) subject within a newly created major of Data Engineering in the Faculty of Engineering and IT at University of Technology, Sydney. Understanding and practicing of the Internet of Things largely depend on the high availability of tools, their low cost, and ease of use that can accelerate learning processes. This paper demonstrates relatively uncomplicated practical lab exercises involving the Raspberry Pi hardware, firmware and the Node-RED programming environment that students can execute to stimulate their learning, understanding of the Internet of Things technology and acquire fundamental data engineering skills.",fullPaper,cp36
Computer Science,p517,d3,401a4dc0c07aec4b3ed624eeb19c7cbea7d602e7,j4,IEEE Transactions on Knowledge and Data Engineering,Special Section on the International Conference on Data Engineering 2015,"The papers in this special section were presented at the 31st International Conference on Data Engineering that was held in Seoul, Korea, on April 13-17, 2015. 17, 2015.",fullPaper,jv4
Computer Science,p519,d3,c48e015debf92bd6a02354a8b3b271c440ec56dc,c50,Conference on Emerging Network Experiment and Technology,Some key problems of data management in army data engineering based on big data,"This paper analyzed the challenges of data management in army data engineering, such as big data volume, data heterogeneous, high rate of data generation and update, high time requirement of data processing, and widely separated data sources. We discussed the disadvantages of traditional data management technologies to deal with these problems. We also highlighted the key problems of data management in army data engineering including data integration, data analysis, representation of data analysis results, and evaluation of data quality.",poster,cp50
Computer Science,p520,d3,cbb9cdf9f7e233948420a479831e9d2d19f72677,c56,International Conference on Automated Software Engineering,"Enterprise Big Data Engineering, Analytics, and Management","The significance of big data can be observed in any decision-making process as it is often used for forecasting and predictive analytics. Additionally, big data can be used to build a holistic view of an enterprise through a collection and analysis of large data sets retrospectively. As the data deluge deepens, new methods for analyzing, comprehending, and making use of big data become necessary.Enterprise Big Data Engineering, Analytics, and Management presents novel methodologies and practical approaches to engineering, managing, and analyzing large-scale data sets with a focus on enterprise applications and implementation. Featuring essential big data concepts including data mining, artificial intelligence, and information extraction, this publication provides a platform for retargeting the current research available in the field. Data analysts, IT professionals, researchers, and graduate-level students will find the timely research presented in this publication essential to furthering their knowledge in the field.",poster,cp56
Computer Science,p521,d3,3d7e34ca30e1aacefc27c7bcd16753791f622f06,c37,International Workshop on the Semantic Web,Enabling Combined Software and Data Engineering at Web-Scale: The ALIGNED Suite of Ontologies,Abstract,fullPaper,cp37
Computer Science,p522,d3,59b43d261fddb909ef9b79e09a557da8f4b925a7,j156,Lecture Notes in Computer Science,Intelligence Science and Big Data Engineering. Image and Video Data Engineering,Abstract,fullPaper,jv156
Computer Science,p525,d3,f3eda875e14bf933759f3b777131a4a9973537b4,c112,British Machine Vision Conference,"Data-driven science and engineering: machine learning, dynamical systems, and control",Abstract,poster,cp112
Computer Science,p527,d3,30c9e3fcb1ead2a827f91ff5cd203aa0d8058bff,c44,Italian National Conference on Sensors,Data-Driven Science and Engineering,Abstract,poster,cp44
Computer Science,p528,d3,d553d008f643622e87e3ac061226865cad3b2928,c38,IEEE Global Engineering Education Conference,Engineering Education in the Era of ChatGPT: Promise and Pitfalls of Generative AI for Education,"Engineering education is constantly evolving to keep up with the latest technological developments and meet the changing needs of the engineering industry. One promising development in this field is the use of generative artificial intelligence technology, such as the ChatGPT conversational agent. ChatGPT has the potential to offer personalized and effective learning experiences by providing students with customized feedback and explanations, as well as creating realistic virtual simulations for hands-on learning. However, it is important to also consider the limitations of this technology. ChatGPT and other generative AI systems are only as good as their training data and may perpetuate biases or even generate and spread misinformation. Additionally, the use of generative AI in education raises ethical concerns such as the potential for unethical or dishonest use by students and the potential unemployment of humans who are made redundant by technology. While the current state of generative AI technology represented by ChatGPT is impressive but flawed, it is only a preview of what is to come. It is important for engineering educators to understand the implications of this technology and study how to adapt the engineering education ecosystem to ensure that the next generation of engineers can take advantage of the benefits offered by generative AI while minimizing any negative consequences.",fullPaper,cp38
Computer Science,p534,d3,9f2b2111cd65cc33c0c440f4f8e548b58d8dd851,j27,Engineering,The State of the Art of Data Science and Engineering in Structural Health Monitoring,Abstract,fullPaper,jv27
Computer Science,p538,d3,e9c9b4549adca2437ed49d01220db9b02b57af19,j156,Lecture Notes in Computer Science,Intelligent Data Engineering and Automated Learning - IDEAL 2012,Abstract,fullPaper,jv156
Computer Science,p542,d3,59eccbeeb7bcd72925247559513be157fd68a802,c78,ACM/IEEE International Conference on Model Driven Engineering Languages and Systems,Requirements Engineering for Machine Learning: Perspectives from Data Scientists,"Machine learning (ML) is used increasingly in real-world applications. In this paper, we describe our ongoing endeavor to define characteristics and challenges unique to Requirements Engineering (RE) for ML-based systems. As a first step, we interviewed four data scientists to understand how ML experts approach elicitation, specification, and assurance of requirements and expectations. The results show that changes in the development paradigm, i.e., from coding to training, also demands changes in RE. We conclude that development of ML systems demands requirements engineers to: (1) understand ML performance measures to state good functional requirements, (2) be aware of new quality requirements such as explainability, freedom from discrimination, or specific legal requirements, and (3) integrate ML specifics in the RE process. Our study provides a first contribution towards an RE methodology for ML systems.",poster,cp78
Computer Science,p543,d3,b4498f4cb6d6fa41d4cd0da31e0d1019756e63b6,c39,Online World Conference on Soft Computing in Industrial Applications,Using the levels of conceptual interoperability model and model-based data engineering to develop a modular interoperability framework,This paper describes how to use the Levels of Conceptual Interoperability (LCIM) as the theoretical backbone for developing and implementing an interoperability framework that supports the exchange of XML-based languages used by M&S systems across the web. The principles of Model-based Data Engineering (MBDE) are integrated within the framework to support the interactions between systems across the layers of the LCIM. We present a use case that shows how the framework supports the interoperability of heterogeneous military systems.,fullPaper,cp39
Computer Science,p544,d3,d13fd4d5d057e0ad8d8aded828f971fa51c97654,j159,Computers and Chemical Engineering,Combining machine learning and process engineering physics towards enhanced accuracy and explainability of data-driven models,Abstract,fullPaper,jv159
Computer Science,p545,d3,fad6242bdd15cc563c7d44fa1eaf89c8d08cec52,j156,Lecture Notes in Computer Science,Intelligent Science and Intelligent Data Engineering,Abstract,fullPaper,jv156
Computer Science,p546,d3,652c77a90d84df639622efdc9cd7475e96a248c9,j99,Comptes rendus. Mecanique,Data-driven modeling and learning in science and engineering,Abstract,fullPaper,jv99
Computer Science,p547,d3,a78c4df65ee23e4b36207163c5408d61da812f4b,j156,Lecture Notes in Computer Science,Intelligent Data Engineering and Automated Learning - IDEAL 2011,Abstract,fullPaper,jv156
Computer Science,p548,d3,d442ab43731a9c9dc93cd2303b86d1e4bbe5c06e,j160,Archives of Computational Methods in Engineering,"Virtual, Digital and Hybrid Twins: A New Paradigm in Data-Based Engineering and Engineered Data",Abstract,fullPaper,jv160
Computer Science,p549,d3,e4d66b15fce00531b96af6330238301ebbb76291,c40,European Conference on Computer Vision,StyleGAN-Human: A Data-Centric Odyssey of Human Generation,"Unconditional human image generation is an important task in vision and graphics, which enables various applications in the creative industry. Existing studies in this field mainly focus on""network engineering""such as designing new components and objective functions. This work takes a data-centric perspective and investigates multiple critical aspects in""data engineering"", which we believe would complement the current practice. To facilitate a comprehensive study, we collect and annotate a large-scale human image dataset with over 230K samples capturing diverse poses and textures. Equipped with this large dataset, we rigorously investigate three essential factors in data engineering for StyleGAN-based human generation, namely data size, data distribution, and data alignment. Extensive experiments reveal several valuable observations w.r.t. these aspects: 1) Large-scale data, more than 40K images, are needed to train a high-fidelity unconditional human generation model with vanilla StyleGAN. 2) A balanced training set helps improve the generation quality with rare face poses compared to the long-tailed counterpart, whereas simply balancing the clothing texture distribution does not effectively bring an improvement. 3) Human GAN models with body centers for alignment outperform models trained using face centers or pelvis points as alignment anchors. In addition, a model zoo and human editing applications are demonstrated to facilitate future research in the community.",fullPaper,cp40
Computer Science,p551,d3,106274c0cb6b8551fc36b75632658f382474a890,j152,IEEE Access,Big Data Software Engineering: Analysis of Knowledge Domains and Skill Sets Using LDA-Based Topic Modeling,"Software engineering is a data-driven discipline and an integral part of data science. The introduction of big data systems has led to a great transformation in the architecture, methodologies, knowledge domains, and skills related to software engineering. Accordingly, education programs are now required to adapt themselves to up-to-date developments by first identifying the competencies concerning big data software engineering to meet the industrial needs and follow the latest trends. This paper aims to reveal the knowledge domains and skill sets required for big data software engineering and develop a taxonomy by mapping these competencies. A semi-automatic methodology is proposed for the semantic analysis of the textual contents of online job advertisements related to big data software engineering. This methodology uses the latent Dirichlet allocation (LDA), a probabilistic topic-modeling technique to discover the hidden semantic structures from a given textual corpus. The output of this paper is a systematic competency map comprising the essential knowledge domains, skills, and tools for big data software engineering. The findings of this paper are expected to help evaluate and improve IT professionals’ vocational knowledge and skills, identify professional roles and competencies in personnel recruitment processes of companies, and meet the skill requirements of the industry through software engineering education programs. Additionally, the proposed model can be extended to blogs, social networks, forums, and other online communities to allow automatic identification of emerging trends and generate contextual tags.",fullPaper,jv152
Computer Science,p552,d3,061a3b79f51007fc12b133761683b03687d73c74,c57,IEEE International Geoscience and Remote Sensing Symposium,Feature Engineering for Machine Learning and Data Analytics,Abstract,poster,cp57
Computer Science,p553,d3,90d88a4bb7b133be9ea2cfb5159b46b58f49fa13,j4,IEEE Transactions on Knowledge and Data Engineering,Guest Editor's Introduction to the Special Section on the IEEE International Conference on Data Engineering,"The eight papers in this special section were selected from the 93 long papers presented at the 25th IEEE International Conference on Data Engineering (ICDE 2009), held in Shanghai, China, on 29 March-2 April 2009.",fullPaper,jv4
Computer Science,p554,d3,87f7c170aecf8f3465b26a11b9a384fef934337b,c92,International Symposium on Computer Architecture,Measurement and Data Analysis for Engineering and Science,"Fundamentals of Experimentation Introduction Experiments Chapter Overview Experimental Approach Role of Experiments The Experiment Classification of Experiments Plan for Successful Experimentation Hypothesis Testing* Design of Experiments* Factorial Design* Problems Bibliography Fundamental Electronics Chapter Overview Concepts and Definitions Circuit Elements RLC Combinations Elementary DC Circuit Analysis Elementary AC Circuit Analysis Equivalent Circuits* Meters* Impedance Matching and Loading Error* Electrical Noise* Problems Bibliography Measurement Systems: Sensors and Transducers Chapter Overview Measurement System Overview Sensor Domains Sensor Characteristics Physical Principles of Sensors Electric Piezoelectric Fluid Mechanic Optic Photoelastic Thermoelectric Electrochemical Sensor Scaling* Problems Bibliography Measurement Systems: Other Components Chapter Overview Signal Conditioning, Processing, and Recording Amplifiers Filters Analog-to-Digital Converters Smart Measurement Systems Other Example Measurement Systems Problems Bibliography Measurement Systems: Calibration and Response Chapter Overview Static Response Characterization by Calibration Dynamic Response Characterization Zero-Order System Dynamic Response First-Order System Dynamic Response Second-Order System Dynamic Response Measurement System Dynamic Response Problems Bibliography Measurement Systems: Design-Stage Uncertainty Chapter Overview Design-Stage Uncertainty Analysis Design-Stage Uncertainty Estimate of a Measurand Design-Stage Uncertainty Estimate of a Result Problems Bibliography Signal Characteristics Chapter Overview Signal Classification Signal Variables Signal Statistical Parameters Problems Bibliography The Fourier Transform Chapter Overview Fourier Series of a Periodic Signal Complex Numbers and Waves Exponential Fourier Series Spectral Representations Continuous Fourier Transform Continuous Fourier Transform Properties* Discrete Fourier Transform Fast Fourier Transform Problems Bibliography Digital Signal Analysis Chapter Overview Digital Sampling Digital Sampling Errors Windowing* Determining a Sample Period Problems Bibliography Probability Chapter Overview Relation to Measurements Basic Probability Concepts Sample versus Population Plotting Statistical Information Probability Density Function Various Probability Density Functions Central Moments Probability Distribution Function Problems Bibliography Statistics Chapter Overview Normal Distribution Normalized Variables Student's t Distribution Rejection of Data Standard Deviation of the Means Chi-Square Distribution Pooling Samples* Problems Bibliography Uncertainty Analysis Chapter Overview Modeling and Experimental Uncertainties Probabilistic Basis of Uncertainty Identifying Sources of Error Systematic and Random Errors Quantifying Systematic and Random Errors Measurement Uncertainty Analysis Uncertainty Analysis of a Multiple-Measurement Result Uncertainty Analyses for Other Measurement Situations Uncertainty Analysis Summary Finite-Difference Uncertainties* Uncertainty Based upon Interval Statistics* Problems Bibliography Regression and Correlation Chapter Overview Least-Squares Approach Least-Squares Regression Analysis Linear Analysis Higher-Order Analysis* Multi-Variable Linear Analysis* Determining the Appropriate Fit Regression Confidence Intervals Regression Parameters Linear Correlation Analysis Signal Correlations in Time* Problems Bibliography Units and Significant Figures Chapter Overview English and Metric Systems Systems of Units SI Standards Technical English and SI Conversion Factors Prefixes Significant Figures Problems Bibliography Technical Communication Chapter Overview Guidelines for Writing Technical Memo Technical Report Oral Technical Presentation Problems Bibliography A Glossary B Symbols C Review Problem Answers Index",poster,cp92
Computer Science,p555,d3,2a375b0d8c643aa98df0555355a40526de773ed5,j162,IEEE Software,Toward Data-Driven Requirements Engineering,"Nowadays, users can easily submit feedback about software products in app stores, social media, or user groups. Moreover, software vendors are collecting massive amounts of implicit feedback in the form of usage data, error logs, and sensor data. These trends suggest a shift toward data-driven user-centered identification, prioritization, and management of software requirements. Developers should be able to adopt the requirements of masses of users when deciding what to develop and when to release. They could systematically use explicit and implicit user data in an aggregated form to support requirements decisions. The goal is data-driven requirements engineering by the masses and for the masses.",fullPaper,jv162
Computer Science,p558,d3,f70b2f20be241f445a61f33c4b8e76e554760340,c77,Visualization for Computer Security,Software Engineering for Machine Learning: A Case Study,"Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workflow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workflow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components - models may be ""entangled"" in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations.",poster,cp77
Computer Science,p559,d3,6a97303b92477d95d1e6acf7b443ebe19a6beb60,j4,IEEE Transactions on Knowledge and Data Engineering,Learning from Imbalanced Data,"With the continuous expansion of data availability in many large-scale, complex, and networked systems, such as surveillance, security, Internet, and finance, it becomes critical to advance the fundamental understanding of knowledge discovery and analysis from raw data to support decision-making processes. Although existing knowledge discovery and data engineering techniques have shown great success in many real-world applications, the problem of learning from imbalanced data (the imbalanced learning problem) is a relatively new challenge that has attracted growing attention from both academia and industry. The imbalanced learning problem is concerned with the performance of learning algorithms in the presence of underrepresented data and severe class distribution skews. Due to the inherent complex characteristics of imbalanced data sets, learning from such data requires new understandings, principles, algorithms, and tools to transform vast amounts of raw data efficiently into information and knowledge representation. In this paper, we provide a comprehensive review of the development of research in learning from imbalanced data. Our focus is to provide a critical review of the nature of the problem, the state-of-the-art technologies, and the current assessment metrics used to evaluate learning performance under the imbalanced learning scenario. Furthermore, in order to stimulate future research in this field, we also highlight the major opportunities and challenges, as well as potential important research directions for learning from imbalanced data.",fullPaper,jv4
Computer Science,p560,d3,bfdab5fb2049cbb78a2ef36d17d7cef3779b4010,j163,Nature Reviews Materials,The stiffness of living tissues and its implications for tissue engineering,Abstract,fullPaper,jv163
Computer Science,p564,d3,391a5f286f814d852dddcab1b2b68e5c1af6c79e,j4,IEEE Transactions on Knowledge and Data Engineering,Data mining with big data,"Big Data concern large-volume, complex, growing data sets with multiple, autonomous sources. With the fast development of networking, data storage, and the data collection capacity, Big Data are now rapidly expanding in all science and engineering domains, including physical, biological and biomedical sciences. This paper presents a HACE theorem that characterizes the features of the Big Data revolution, and proposes a Big Data processing model, from the data mining perspective. This data-driven model involves demand-driven aggregation of information sources, mining and analysis, user interest modeling, and security and privacy considerations. We analyze the challenging issues in the data-driven model and also in the Big Data revolution.",fullPaper,jv4
Computer Science,p565,d3,c1137ba91f24c31af959e21af90aaa88605e618c,c45,IEEE Symposium on Security and Privacy,XBRL for Interactive Data: Engineering the Information Value Chain.,"The article reviews the book ""XBRL for Interactive Data: Engineering the Information Value Chain,"" by R. Debreceny, C. Felden, B. Ochocki, M. Piechocki, and M. Piechocki.",poster,cp45
Computer Science,p568,d3,16b86d4d590433a76520ed0feed5af11adf1ee63,j4,IEEE Transactions on Knowledge and Data Engineering,Guest Editors' Introduction: Knowledge and Data Engineering for E-Learning,"The 13 papers in this special issue focus on knowledge and data engineering for e-learning. Some of these papers were recommended submissions from the best ranked papers presented at the Sixth International Conference on Web-Based Learning (ICWL '07), held in August 2007 in Edinburgh, United Kingdom.",fullPaper,jv4
Computer Science,p569,d3,dabb54c8241fb1b9eb681f722613814a74e9e6fb,c54,ACM SIGCOMM Symposium on Software Defined Networking Research,SFPE handbook of fire protection engineering,Abstract,poster,cp54
Computer Science,p571,d3,032832f71799eba6565e2444778dcffccbd76280,c42,IEEE Working Conference on Mining Software Repositories,Microsoft CloudMine: Data Mining for the Executive Order on Improving the Nation's Cybersecurity,"As any other US software maker, Microsoft is bound by the “Executive Order on Improving the Nation's Cybersecurity” [2] which dictates a clear mandate to “enhance the software supply chain security” and to generally improve the cyber security practices. However, this is much easier written down than enforced. The executive order imposes new rules and requirements that will impact engineering practices and evidence collection for most projects and engineering teams in a relatively short period of time. Part of the response is the requirement to build up comprehensive inventories of software artifacts contributing to US government systems, which is a massive task when done manually would be tedious and fragile as software eco-systems change rapidly. Required is a system that will constantly monitor and update the inventory of software artifacts and contributors so that at any given point of time, the scope and involved teams for any software security incident can be notified and response plans activated. The front line of this security battle includes data mining platforms providing the security and compliance teams with engineering artifacts and insights into artifact dependencies and engineering practices of the corresponding engineering teams. The data provided does not only allow Microsoft to build an accurate engineering artifact inventory, but also enables Microsoft�s teams to initiate so called “get-clean” initiatives to start issue remediation before proper policy tools and pipelines (“stay-clean”) can be developed, tested, and deployed. In this talk we will present CloudMine1, one of Microsoft's main data mining platforms serving data sets and dependency graphs of more than 270 different engineering artifacts (e.g., builds, releases, commits, pull requests, etc.) gathered on an hourly basis. During the talk we will provide some insights into CloudMine, its engineering team and operational costs-which is significant. We will then highlight the benefits and opportunities a data mining framework like CloudMine provides the company including insights into how inventory and automation bots use CloudMine data to impact thousands of Microsoft engineers daily, saving the company significant costs and response times to security incidents: the ability to scan more than 100,000 code repositories across the enterprise within hours; building up an artifact engineering inventory enabling us to flag any known security vulnerability in any of the software components within hours; or spotting non-compliant build and release pipelines across Microsoft's 500,000 pipelines. In addition, we will also present open challenges the CloudMine engineering team is facing during operating and growing CloudMine as a platform, which will hopefully provide motivation and inspiration for researcher and other companies to start a dialog with us and other companies about these challenges and latest research results that may help us solve these issues. From the talk it should become clear that running enterprise scale systems is not cheap but worth the effort as it enables Microsoft and its engineering teams to respond to current cyber security threads even before we can build and test best in class built-in defense systems.",fullPaper,cp42
Computer Science,p574,d3,ddc30215f89e48fa59c0d78c1746d5a974fd0ec6,c71,International Joint Conference on Artificial Intelligence,On the feature engineering of building energy data mining,Abstract,poster,cp71
Computer Science,p575,d3,bbbe0b59e24b2355eab71914c2387f504c9e39aa,j165,Bulletin of Earthquake Engineering,The pan-European Engineering Strong Motion (ESM) flatfile: compilation criteria and data statistics,Abstract,fullPaper,jv165
Computer Science,p576,d3,0e76c4da3f5c0cd6948128dcfe984e7d158bfc15,j166,Journal of Chemical and Engineering Data,Semantic interoperability and characterization of data provenance in computational molecular engineering,"By introducing a common representational system for metadata that describe the employed simulation workflows, diverse sources of data and platforms in computational molecular engineering, such as workflow management systems, can become interoperable at the semantic level. To achieve semantic interoperability, the present work introduces two ontologies that provide a formal specification of the entities occurring in a simulation workflow and the relations between them: The software ontology VISO is developed to represent software packages and their features, and OSMO, an ontology for simulation, modelling, and optimization, is introduced on the basis of MODA, a previously developed semi-intuitive graph notation for workflows in materials modelling. As a proof of concept, OSMO is employed to describe a leading-edge application scenario of the TaLPas workflow management system.",fullPaper,jv166
Chemistry,p576,d8,0e76c4da3f5c0cd6948128dcfe984e7d158bfc15,j166,Journal of Chemical and Engineering Data,Semantic interoperability and characterization of data provenance in computational molecular engineering,"By introducing a common representational system for metadata that describe the employed simulation workflows, diverse sources of data and platforms in computational molecular engineering, such as workflow management systems, can become interoperable at the semantic level. To achieve semantic interoperability, the present work introduces two ontologies that provide a formal specification of the entities occurring in a simulation workflow and the relations between them: The software ontology VISO is developed to represent software packages and their features, and OSMO, an ontology for simulation, modelling, and optimization, is introduced on the basis of MODA, a previously developed semi-intuitive graph notation for workflows in materials modelling. As a proof of concept, OSMO is employed to describe a leading-edge application scenario of the TaLPas workflow management system.",fullPaper,jv166
Computer Science,p577,d3,04e2f184505a6b67c611bc57c05864385c024418,j167,First Monday,"Engineering the public: Big data, surveillance and computational politics","Digital technologies have given rise to a new combination of big data and computational practices which allow for massive, latent data collection and sophisticated computational modeling, increasing the capacity of those with resources and access to use these tools to carry out highly effective, opaque and unaccountable campaigns of persuasion and social engineering in political, civic and commercial spheres. I examine six intertwined dynamics that pertain to the rise of computational politics: the rise of big data, the shift away from demographics to individualized targeting, the opacity and power of computational modeling, the use of persuasive behavioral science, digital media enabling dynamic real-time experimentation, and the growth of new power brokers who own the data or social media environments. I then examine the consequences of these new mechanisms on the public sphere and political campaigns.",fullPaper,jv167
Computer Science,p579,d3,3f0c29566f996933ba6fc556735f1ecff5ce3c1d,c65,International Symposium on Empirical Software Engineering and Measurement,Privacy and Data Protection by Design - from policy to engineering,"Privacy and data protection constitute core values of individuals and of democratic societies. There have been decades of debate on how those values -and legal obligations- can be embedded into systems, preferably from the very beginning of the design process. One important element in this endeavour are technical mechanisms, known as privacy-enhancing technologies (PETs). Their effectiveness has been demonstrated by researchers and in pilot implementations. However, apart from a few exceptions, e.g., encryption became widely used, PETs have not become a standard and widely used component in system design. Furthermore, for unfolding their full benefit for privacy and data protection, PETs need to be rooted in a data governance strategy to be applied in practice. This report contributes to bridging the gap between the legal framework and the available technological implementation measures by providing an inventory of existing approaches, privacy design strategies, and technical building blocks of various degrees of maturity from research and development. Starting from the privacy principles of the legislation, important elements are presented as a first step towards a design process for privacy-friendly systems and services. The report sketches a method to map legal obligations to design strategies, which allow the system designer to select appropriate techniques for implementing the identified privacy requirements. Furthermore, the report reflects limitations of the approach. It concludes with recommendations on how to overcome and mitigate these limits.",poster,cp65
Computer Science,p581,d3,0a5128188245516f716918faef7dae1e6ee4cb1f,c58,Extreme Science and Engineering Discovery Environment,Leveraging Digital Twin Technology in Model-Based Systems Engineering,"Digital twin, a concept introduced in 2002, is becoming increasingly relevant to systems engineering and, more specifically, to model-based system engineering (MBSE). A digital twin, like a virtual prototype, is a dynamic digital representation of a physical system. However, unlike a virtual prototype, a digital twin is a virtual instance of a physical system (twin) that is continually updated with the latter’s performance, maintenance, and health status data throughout the physical system’s life cycle. This paper presents an overall vision and rationale for incorporating digital twin technology into MBSE. The paper discusses the benefits of integrating digital twins with system simulation and Internet of Things (IoT) in support of MBSE and provides specific examples of the use and benefits of digital twin technology in different industries. It concludes with a recommendation to make digital twin technology an integral part of MBSE methodology and experimentation testbeds.",poster,cp58
Computer Science,p582,d3,5692e63b4a2947f777cc1c3eb2a0369e774eac99,j170,IEEE Transactions on Software Engineering,Bayesian Data Analysis in Empirical Software Engineering Research,"Statistics comes in two main flavors: frequentist and Bayesian. For historical and technical reasons, frequentist statistics have traditionally dominated empirical data analysis, and certainly remain prevalent in empirical software engineering. This situation is unfortunate because frequentist statistics suffer from a number of shortcomings—such as lack of flexibility and results that are unintuitive and hard to interpret—that curtail their effectiveness when dealing with the heterogeneous data that is increasingly available for empirical analysis of software engineering practice. In this paper, we pinpoint these shortcomings, and present Bayesian data analysis techniques that provide tangible benefits—as they can provide clearer results that are simultaneously robust and nuanced. After a short, high-level introduction to the basic tools of Bayesian statistics, we present the reanalysis of two empirical studies on the effectiveness of automatically generated tests and the performance of programming languages. By contrasting the original frequentist analyses with our new Bayesian analyses, we demonstrate the concrete advantages of the latter. To conclude we advocate a more prominent role for Bayesian statistical techniques in empirical software engineering research and practice.",fullPaper,jv170
Mathematics,p582,d6,5692e63b4a2947f777cc1c3eb2a0369e774eac99,j170,IEEE Transactions on Software Engineering,Bayesian Data Analysis in Empirical Software Engineering Research,"Statistics comes in two main flavors: frequentist and Bayesian. For historical and technical reasons, frequentist statistics have traditionally dominated empirical data analysis, and certainly remain prevalent in empirical software engineering. This situation is unfortunate because frequentist statistics suffer from a number of shortcomings—such as lack of flexibility and results that are unintuitive and hard to interpret—that curtail their effectiveness when dealing with the heterogeneous data that is increasingly available for empirical analysis of software engineering practice. In this paper, we pinpoint these shortcomings, and present Bayesian data analysis techniques that provide tangible benefits—as they can provide clearer results that are simultaneously robust and nuanced. After a short, high-level introduction to the basic tools of Bayesian statistics, we present the reanalysis of two empirical studies on the effectiveness of automatically generated tests and the performance of programming languages. By contrasting the original frequentist analyses with our new Bayesian analyses, we demonstrate the concrete advantages of the latter. To conclude we advocate a more prominent role for Bayesian statistical techniques in empirical software engineering research and practice.",fullPaper,jv170
Computer Science,p583,d3,ed163f11318db2f29e0036277868159c61bcf47b,c49,ACM/SIGCOMM Internet Measurement Conference,Knowledge and Data Engineering for e-Learning Special Issue of IEEE Transactions on Knowledge and Data Engineering,"With the advent of the Internet, we are seeing more sophisticated techniques being developed to support e-Iearning. The rapid developme nt of Web-based learning and new concepts like virtual classrooms, virtual laboratories and virtual universities introduces many new issues to be addressed. On the technical side, we need to develop effective e-technologies for supporting distance education. On the learning and management side, we need to consider issues such as new style of learning and different system set-u p requirements. Finally, the issue of standardization of e-Iearning systems should also be considered. In this special issue, our focus will be on the technical side, although other issues related to knowledge and data engineering for e-Iearning may also be considered. Topics: In this special issue, we call for original papers describing novel knowledge and data engineering techniques that support e-Iearning. Preference will be given to papers that include an evaluation of users' experience in using the proposed methods. Areas of interests include, but are not limited to: • Semantic Web technology for e-Iearning • Data modeling (eg., XML) for efficient management of course materials • Searching and indexing techniques to suppo rt effective course notes retrieval • User-centric e-Iearning systems and user interaction management • Profiling techniques to support grading and learning recommendation • Data and knowledge base suppo rt for pervasive e-Iearning • Course material analysis and understanding • Automatic generation of questions and answers • Collaborative communities for e-Iearning",poster,cp49
Computer Science,p584,d3,2e461882b8f55b11efca5edf74c1a1b40e404075,c24,International Conference on Data Technologies and Applications,Methods and Tools for GDPR Compliance Through Privacy and Data Protection Engineering,"In this position paper we posit that, for Privacy by Design to be viable, engineers must be effectively involved and endowed with methodological and technological tools closer to their mindset, and which integrate within software and systems engineering methods and tools, realizing in fact the definition of Privacy Engineering. This position will be applied in the soon-to-start PDP4E project, where privacy will be introduced into existent general-purpose software engineering tools and methods, dealing with (risk management, requirements engineering, model-driven design, and software/systems assurance).",poster,cp24
Computer Science,p586,d3,97bfa89addc6e5d76361e4c1e296949cad887b86,j171,Transactions of the Association for Computational Linguistics,Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science,"In this paper, we propose data statements as a design solution and professional practice for natural language processing technologists, in both research and development. Through the adoption and widespread use of data statements, the field can begin to address critical scientific and ethical issues that result from the use of data from certain populations in the development of technology for other populations. We present a form that data statements can take and explore the implications of adopting them as part of regular practice. We argue that data statements will help alleviate issues related to exclusion and bias in language technology, lead to better precision in claims about how natural language processing research can generalize and thus better engineering results, protect companies from public embarrassment, and ultimately lead to language technology that meets its users in their own preferred linguistic style and furthermore does not misrepresent them to others.",fullPaper,jv171
Computer Science,p588,d3,766ed11ce5ffcd27879716d817f2f5fd697bbb27,c55,Design Automation Conference,Modeling & Simulation-Based Data Engineering: Introducing Pragmatics into Ontologies for Net-Centric Information Exchange,"Data Engineering has become a necessary and critical activity for business, engineering, and scientific organizations as the move to service oriented architecture and web services moves into full swing. Notably, the US Department of Defense is mandating that all of its agencies and contractors assume a defining presence on the Net-centric Global Information Grid. This book provides the first practical approach to data engineering and modeling, which supports interoperabililty with consumers of the data in a service- oriented architectures (SOAs). Although XML (eXtensible Modeling Language) is the lingua franca for such interoperability, it is not sufficient on its own. The approach in this book addresses critical objectives such as creating a single representation for multiple applications, designing models capable of supporting dynamic processes, and harmonizing legacy data models for web-based co-existence. The approach is based on the System Entity Structure (SES) which is a well-defined structure, methodology, and practical tool with all of the functionality of UML (Unified Modeling Language) and few of the drawbacks. The SES originated in the formal representation of hierarchical simulation models. So it provides an axiomatic formalism that enables automating the development of XML dtds and schemas, composition and decomposition of large data models, and analysis of commonality among structures.Zeigler and Hammond include a range of features to benefit their readers. Natural language, graphical and XML forms of SES specification are employed to allow mapping of legacy meta-data. Real world examples and case studies provide insight into data engineering and test evaluation in various application domains. Comparative information is provided on concepts of ontologies, modeling and simulation, introductory linguistic background, and support options enable programmers to work with advanced tools in the area. The website of the Arizona Center for Integrative Modeling and Simulation, co-founded by Zeigler in 2001, provides links to downloadable software to accompany the book. * The only practical guide to integrating XML and web services in data engineering* Introduces linguistic levels of interoperability for effective information exchange* Covers the interoperability standards mandated by national and international agencies * Complements Zeigler's classic THEORY OF MODELING AND SIMULATION",poster,cp55
Computer Science,p589,d3,4cd3e13ffe85fa9b378e48cbd4288e5e569e9693,j156,Lecture Notes in Computer Science,Intelligent Data Engineering and Automated Learning – IDEAL 2008,Abstract,fullPaper,jv156
Computer Science,p593,d3,cd0488f6270559806f96f03cf8597b62239211b1,j4,IEEE Transactions on Knowledge and Data Engineering,Knowledge and Data Engineering,"The authors provide an overview of the current research and development directions in knowledge and data engineering. They classify research problems and approaches in this area and discuss future trends. Research on knowledge and data engineering is examined with respect to programmability and representation, design tradeoffs, algorithms and control, and emerging technologies. Future challenges are considered with respect to software and hardware architecture and system design. The paper serves as an introduction to this first issue of a new quarter. >",fullPaper,jv4
Computer Science,p594,d3,1a6fc05a37ae1b2fc7c6193cb6dc8282767c2cb6,j172,ACM Transactions on Software Engineering and Methodology,Software Engineering for AI-Based Systems: A Survey,"AI-based systems are software systems with functionalities enabled by at least one AI component (e.g., for image-, speech-recognition, and autonomous driving). AI-based systems are becoming pervasive in society due to advances in AI. However, there is limited synthesized knowledge on Software Engineering (SE) approaches for building, operating, and maintaining AI-based systems. To collect and analyze state-of-the-art knowledge about SE for AI-based systems, we conducted a systematic mapping study. We considered 248 studies published between January 2010 and March 2020. SE for AI-based systems is an emerging research area, where more than 2/3 of the studies have been published since 2018. The most studied properties of AI-based systems are dependability and safety. We identified multiple SE approaches for AI-based systems, which we classified according to the SWEBOK areas. Studies related to software testing and software quality are very prevalent, while areas like software maintenance seem neglected. Data-related issues are the most recurrent challenges. Our results are valuable for: researchers, to quickly understand the state-of-the-art and learn which topics need more research; practitioners, to learn about the approaches and challenges that SE entails for AI-based systems; and, educators, to bridge the gap among SE and AI in their curricula.",fullPaper,jv172
Computer Science,p596,d3,35aab48e1045740b1a8b3992e541f51f624130bc,j174,Acta Numerica,Solving inverse problems using data-driven models,"Recent research in inverse problems seeks to develop a mathematically coherent foundation for combining data-driven models, and in particular those based on deep learning, with domain-specific knowledge contained in physical–analytical models. The focus is on solving ill-posed inverse problems that are at the core of many challenging applications in the natural sciences, medicine and life sciences, as well as in engineering and industrial applications. This survey paper aims to give an account of some of the main contributions in data-driven inverse problems.",fullPaper,jv174
Computer Science,p597,d3,3a83d8595e6727269c876fcebd23ee9ddd524b76,j4,IEEE Transactions on Knowledge and Data Engineering,A Survey on Data Collection for Machine Learning: A Big Data - AI Integration Perspective,"Data collection is a major bottleneck in machine learning and an active research topic in multiple communities. There are largely two reasons data collection has recently become a critical issue. First, as machine learning is becoming more widely-used, we are seeing new applications that do not necessarily have enough labeled data. Second, unlike traditional machine learning, deep learning techniques automatically generate features, which saves feature engineering costs, but in return may require larger amounts of labeled data. Interestingly, recent research in data collection comes not only from the machine learning, natural language, and computer vision communities, but also from the data management community due to the importance of handling large amounts of data. In this survey, we perform a comprehensive study of data collection from a data management point of view. Data collection largely consists of data acquisition, data labeling, and improvement of existing data or models. We provide a research landscape of these operations, provide guidelines on which technique to use when, and identify interesting research challenges. The integration of machine learning and data management for data collection is part of a larger trend of Big data and Artificial Intelligence (AI) integration and opens many opportunities for new research.",fullPaper,jv4
Mathematics,p597,d6,3a83d8595e6727269c876fcebd23ee9ddd524b76,j4,IEEE Transactions on Knowledge and Data Engineering,A Survey on Data Collection for Machine Learning: A Big Data - AI Integration Perspective,"Data collection is a major bottleneck in machine learning and an active research topic in multiple communities. There are largely two reasons data collection has recently become a critical issue. First, as machine learning is becoming more widely-used, we are seeing new applications that do not necessarily have enough labeled data. Second, unlike traditional machine learning, deep learning techniques automatically generate features, which saves feature engineering costs, but in return may require larger amounts of labeled data. Interestingly, recent research in data collection comes not only from the machine learning, natural language, and computer vision communities, but also from the data management community due to the importance of handling large amounts of data. In this survey, we perform a comprehensive study of data collection from a data management point of view. Data collection largely consists of data acquisition, data labeling, and improvement of existing data or models. We provide a research landscape of these operations, provide guidelines on which technique to use when, and identify interesting research challenges. The integration of machine learning and data management for data collection is part of a larger trend of Big data and Artificial Intelligence (AI) integration and opens many opportunities for new research.",fullPaper,jv4
Computer Science,p598,d3,e1cbcffa6875920d3c8df398d16caaa4d4a25269,j175,IEEE Internet Computing,Model-Based Data Engineering for Web Services,"Although XML offers heterogeneous IT systems a new level of interoperability, it doesn't ensure that the various systems correctly interpret the data they receive. To address this, data engineering supports clear definitions for exchanged data elements. With model-based data engineering, organizations use a common reference model, which offers further clarity and performance improvements. Organizations can use the resulting data to configure mediation services, translating dialects of new or legacy services into a common language for use in a service-oriented architecture.",fullPaper,jv175
Computer Science,p599,d3,9a1f352ef21044700c180882038c28c3b2361914,j153,The VLDB journal,Data collection and quality challenges in deep learning: a data-centric AI perspective,Abstract,fullPaper,jv153
Computer Science,p600,d3,f14a544b15f8646cdaa7a1f8ef5db3c0ef083750,c5,Technical Symposium on Computer Science Education,Data Engineering education with real-world projects,"This paper presents an experience report on teaching Data Engineering using a real-world project domain. Our course introduces databases within the context of Systems and Information Engineering, supplementing relational database theory with requirements engineering, design, and analysis. The primary deliverable of the course was a semester-long project to implement an information system in a real-world application domain, interacting with an external customer with uncertain requirements. We believe that real-world projects motivate students to apply good Software Engineering principles in the classroom and encourage those principles to be adopted into industrial practice.",poster,cp5
Computer Science,p601,d3,02791ee57137fee9915aba8eac54886f772a54f0,c43,European Conference on Machine Learning,Predicate Invention in Inductive Data Engineering,Abstract,fullPaper,cp43
Computer Science,p603,d3,70956547fcaf4ae6c7fc6e53142b9ed30e1c14fd,c110,Biometrics and Identity Management,Out-of-the-box data engineering events in heterogeneous data environments,"Data has changed significantly over the last few decades. Computing systems that initially dealt with data and computation rapidly moved to information and communication. The next step on the evolutionary scale is insight and experience. Applications are demanding the use of live, spatio-temporal, heterogeneous data. Data engineering must keep pace by designing experiential environments that let users apply their senses to observe data and information about an event and to interact with aspects of the event that are of particular interest. We call this out-of-the-box data engineering because it means we must think beyond many of our time-worn perspectives and technical approaches.",poster,cp110
Computer Science,p604,d3,116927fbe4c9732fd1e392035a100c33b14e9d59,j16,International Journal of Digital Earth,Big Data and cloud computing: innovation opportunities and challenges,"ABSTRACT Big Data has emerged in the past few years as a new paradigm providing abundant data and opportunities to improve and/or enable research and decision-support applications with unprecedented value for digital earth applications including business, sciences and engineering. At the same time, Big Data presents challenges for digital earth to store, transport, process, mine and serve the data. Cloud computing provides fundamental support to address the challenges with shared computing resources including computing, storage, networking and analytical software; the application of these resources has fostered impressive Big Data advancements. This paper surveys the two frontiers – Big Data and cloud computing – and reviews the advantages and consequences of utilizing cloud computing to tackling Big Data in the digital earth and relevant science domains. From the aspects of a general introduction, sources, challenges, technology status and research opportunities, the following observations are offered: (i) cloud computing and Big Data enable science discoveries and application developments; (ii) cloud computing provides major solutions for Big Data; (iii) Big Data, spatiotemporal thinking and various application domains drive the advancement of cloud computing and relevant technologies with new requirements; (iv) intrinsic spatiotemporal principles of Big Data and geospatial sciences provide the source for finding technical and theoretical solutions to optimize cloud computing and processing Big Data; (v) open availability of Big Data and processing capability pose social challenges of geospatial significance and (vi) a weave of innovations is transforming Big Data into geospatial research, engineering and business values. This review introduces future innovations and a research agenda for cloud computing supporting the transformation of the volume, velocity, variety and veracity into values of Big Data for local to global digital earth science and applications.",fullPaper,jv16
Computer Science,p605,d3,250eb86ff0fc3694904e25006e7a4416b9fe08c2,c23,International Conference on Open and Big Data,Data-Driven Requirements Engineering - An Update,"Nowadays, users can easily submit feedback about software products in app stores, social media, or user groups. Moreover, software vendors are collecting massive amounts of implicit feedback in the form of usage data, error logs, and sensor data. These trends suggest a shift toward data-driven user-centered identification, prioritization, and management of software requirements. Developers should be able to adopt the requirements of masses of users when deciding what to develop and when to release. They could systematically use explicit and implicit user data in an aggregated form to support requirements decisions. In this talk we will present and discuss most recent achievements in this direction since the paper's original publication. We will also show to mine data sets mobile apps, give a few success/failure stories and a few practical advises.",poster,cp23
Computer Science,p606,d3,f50389864d9da0266ab9f21acfb913f0f7f4c8c3,c81,ACM Symposium on Applied Computing,Data Engineering for the Analysis of Semiconductor Manufacturing Data,"We have analyzed manufacturing data from several different semiconductor manufacturing plants, using decision tree induction software called Q-YIELD. The software generates rules for predicting when a given product should be rejected. The rules are intended to help the process engineers improve the yield of the product, by helping them to discover the causes of rejection. Experience with Q-YIELD has taught us the importance of data engineering — preprocessing the data to enable or facilitate decision tree induction. This paper discusses some of the data engineering problems we have encountered with semiconductor manufacturing data. The paper deals with two broad classes of problems: engineering the features in a feature vector representation and engineering the definition of the target concept (the classes). Manufacturing process data present special problems for feature engineering, since the data have multiple levels of granularity (detail, resolution). Engineering the target concept is important, due to our focus on understanding the past, as opposed to the more common focus in machine learning on predicting the future.",poster,cp81
Computer Science,p609,d3,d5e0f5cfb95f9da8b8a9f7a004695dc3eeef0a7e,c45,IEEE Symposium on Security and Privacy,RIDL: Rogue In-Flight Data Load,"We present Rogue In-flight Data Load (RIDL), a new class of speculative unprivileged and constrained attacks to leak arbitrary data across address spaces and privilege boundaries (e.g., process, kernel, SGX, and even CPU-internal operations). Our reverse engineering efforts show such vulnerabilities originate from a variety of micro-optimizations pervasive in commodity (Intel) processors, which cause the CPU to speculatively serve loads using extraneous CPU-internal in-flight data (e.g., in the line fill buffers). Contrary to other state-of-the-art speculative execution attacks, such as Spectre, Meltdown and Foreshadow, RIDL can leak this arbitrary in-flight data with no assumptions on the state of the caches or translation data structures controlled by privileged software. The implications are worrisome. First, RIDL attacks can be implemented even from linear execution with no invalid page faults, eliminating the need for exception suppression mechanisms and enabling system-wide attacks from arbitrary unprivileged code (including JavaScript in the browser). To exemplify such attacks, we build a number of practical exploits that leak sensitive information from victim processes, virtual machines, kernel, SGX and CPU-internal components. Second, and perhaps more importantly, RIDL bypasses all existing “spot” mitigations in software (e.g., KPTI, PTE inversion) and hardware (e.g., speculative store bypass disable) and cannot easily be mitigated even by more heavyweight defenses (e.g., L1D flushing or disabling SMT). RIDL questions the sustainability of a per-variant, spot mitigation strategy and suggests more fundamental mitigations are needed to contain ever-emerging speculative execution attacks.",fullPaper,cp45
Computer Science,p610,d3,ae9368b469a8fa0922fd5985e70af2501f7b25e4,c46,Ideal,"Intelligent Data Engineering and Automated Learning - IDEAL 2006, 7th International Conference, Burgos, Spain, September 20-23, 2006, Proceedings",Abstract,fullPaper,cp46
Computer Science,p611,d3,03514ab95768a2f2c2eb7aaf441707e5759c2425,c24,International Conference on Data Technologies and Applications,Rules of thumb in data engineering,"This paper reexamines the rules of thumb for the design of data storage systems. Briefly, it looks at storage, processing, and networking costs, ratios, and trends with a particular focus on performance and price/performance. Amdahl's ratio laws for system design need only slight revision after 35 years-the major change being the increased use of RAM. An analysis also indicates storage should be used to cache both database and Web data to save disk bandwidth, network bandwidth, and people's time. Surprisingly, the 5-minute rule for disk caching becomes a cache-everything rule for Web caching.",poster,cp24
Computer Science,p613,d3,38688e84d2009d885a39a6490f5e78fc4c13c6a8,c54,ACM SIGCOMM Symposium on Software Defined Networking Research,Proceedings of the Eighth International Conference on Data Engineering,Abstract,poster,cp54
Computer Science,p616,d3,5d381a18c2ddca3871a52b27eedf8cb76a5a1e29,c115,International Conference on Information Integration and Web-based Applications & Services,Data Engineering: Fuzzy Mathematics in Systems Theory and Data Analysis,"From the Publisher: 
There are many situations in science and engineering where complex output data from a given system is used to formulate a model of how that system operates, or to simulate its response to different inputs. Applications include control, decision theory, and the emerging fields of bioinformatics. A key advance in this general area is the use of fuzzy mathematics in the development of models.",poster,cp115
Computer Science,p617,d3,3be4b774d0112ce8d76c56215dab2d80f2a00db3,c41,IEEE International Conference on Data Engineering,"Proceedings of the 22nd International Conference on Data Engineering Workshops, ICDE 2006, 3-7 April 2006, Atlanta, GA, USA",Abstract,fullPaper,cp41
Computer Science,p619,d3,70eb87613183a9141a8c3ea8212384446e5120a5,c71,International Joint Conference on Artificial Intelligence,Intelligent Data Engineering and Automated Learning,Abstract,poster,cp71
Computer Science,p620,d3,b37baff7cfaa30988496a59d89024918ee6630e9,c48,"Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing","Intelligent data engineering and automated learning - IDEAL 2004 : 5th International Conference, Exeter, UK, August 25-27, 2004 : proceedings","Bioinformatics.- Modelling and Clustering of Gene Expressions Using RBFs and a Shape Similarity Metric.- A Novel Hybrid GA/SVM System for Protein Sequences Classification.- Building Genetic Networks for Gene Expression Patterns.- SVM-Based Classification of Distant Proteins Using Hierarchical Motifs.- Knowledge Discovery in Lymphoma Cancer from Gene-Expression.- A Method of Filtering Protein Surface Motifs Based on Similarity Among Local Surfaces.- Qualified Predictions for Proteomics Pattern Diagnostics with Confidence Machines.- An Assessment of Feature Relevance in Predicting Protein Function from Sequence.- A New Artificial Immune System Algorithm for Clustering.- The Categorisation of Similar Non-rigid Biological Objects by Clustering Local Appearance Patches.- Unsupervised Dense Regions Discovery in DNA Microarray Data.- Visualisation of Distributions and Clusters Using ViSOMs on Gene Expression Data.- Prediction of Implicit Protein-Protein Interaction by Optimal Associative Feature Mining.- Exploring Dependencies Between Yeast Stress Genes and Their Regulators.- Poly-transformation.- Prediction of Natively Disordered Regions in Proteins Using a Bio-basis Function Neural Network.- The Effect of Image Compression on Classification and Storage Requirements in a High-Throughput Crystallization System.- PromSearch: A Hybrid Approach to Human Core-Promoter Prediction.- Data Mining and Knowledge Engineering.- Synergy of Logistic Regression and Support Vector Machine in Multiple-Class Classification.- Deterministic Propagation of Blood Pressure Waveform from Human Wrists to Fingertips.- Pre-pruning Decision Trees by Local Association Rules.- A New Approach for Selecting Attributes Based on Rough Set Theory.- A Framework for Mining Association Rules in Data Warehouses.- Intelligent Web Service Discovery in Large Distributed System.- The Application of K-Medoids and PAM to the Clustering of Rules.- A Comparison of Texture Teatures for the Classification of Rock Images.- A Mixture of Experts Image Enhancement Scheme for CCTV Images.- Integration of Projected Clusters and Principal Axis Trees for High-Dimensional Data Indexing and Query.- Unsupervised Segmentation on Image with JSEG Using Soft Class Map.- DESCRY: A Density Based Clustering Algorithm for Very Large Data Sets.- A Fuzzy Set Based Trust and Reputation Model in P2P Networks.- Video Based Human Behavior Identification Using Frequency Domain Analysis.- Mobile Data Mining by Location Dependencies.- An Algorithm for Artificial Intelligence-Based Model Adaptation to Dynamic Data Distribution.- On a Detection of Korean Prosody Phrases Boundaries.- A Collision Avoidance System for Autonomous Ship Using Fuzzy Relational Products and COLREGs.- Engineering Knowledge Discovery in Network Intrusion Detection.- False Alarm Classification Model for Network-Based Intrusion Detection System.- Exploiting Safety Constraints in Fuzzy Self-organising Maps for Safety Critical Applications.- Surface Spatial Index Structure of High-Dimensional Space.- Generating and Applying Rules for Interval Valued Fuzzy Observations.- Automatic Video Shot Boundary Detection Using Machine Learning.- On Building XML Data Warehouses.- A Novel Method for Mining Frequent Subtrees from XML Data.- Mining Association Rules Using Relative Confidence.- Multiple Classifiers Fusion System Based on the Radial Basis Probabilistic Neural Networks.- An Effective Distributed Privacy-Preserving Data Mining Algorithm.- Dimensionality Reduction with Image Data.- Implicit Fitness Sharing Speciation and Emergent Diversity in Tree Classifier Ensembles.- Improving Decision Tree Performance Through Induction- and Cluster-Based Stratified Sampling.- Learning to Classify Biomedical Terms Through Literature Mining and Genetic Algorithms.- PRICES: An Efficient Algorithm for Mining Association Rules.- Combination of SVM Knowledge for Microcalcification Detection in Digital Mammograms.- Char: An Automatic Way to Describe Characteristics of Data.- Two Methods for Automatic 3D Reconstruction from Long Un-calibrated Sequences.- Wrapper for Ranking Feature Selection.- Simultaneous Feature Selection and Weighting for Nearest Neighbor Using Tabu Search.- Fast Filtering of Structural Similarity Search Using Discovery of Topological Patterns.- Detecting Worm Propagation Using Traffic Concentration Analysis and Inductive Learning.- Comparing Study for Detecting Microcalcifications in Digital Mammogram Using Wavelets.- A Hybrid Multi-layered Speaker Independent Arabic Phoneme Identification System.- Feature Selection for Natural Disaster Texts Classification Using Testors.- Mining Large Engineering Data Sets on the Grid Using AURA.- Self-tuning Based Fuzzy PID Controllers: Application to Control of Nonlinear HVAC Systems.- Ontology-Based Web Navigation Assistant.- A Hybrid Fuzzy-Neuro Model for Preference-Based Decision Analysis.- Combining Rules for Text Categorization Using Dempster's Rule of Combination.- Genetic Program Based Data Mining for Fuzzy Decision Trees.- Automating Co-evolutionary Data Mining.- Topological Tree for Web Organisation, Discovery and Exploration.- New Medical Diagnostic Method for Oriental Medicine Using BYY Harmony Learning.- An Intelligent Topic-Specific Crawler Using Degree of Relevance.- Development of a Global and Integral Model of Business Management Using an Unsupervised Model.- Spam Mail Detection Using Artificial Neural Network and Bayesian Filter.- An Integrated Approach to Automatic Indoor Outdoor Scene Classification in Digital Images.- Using Fuzzy Sets in Contextual Word Similarity.- Summarizing Time Series: Learning Patterns in 'Volatile' Series.- Cosine Transform Priors for Enhanced Decoding of Compressed Images.- Partial Discharge Classification Through Wavelet Packets of Their Modulated Ultrasonic Emission.- A Hybrid Optimization Method of Multi-objective Genetic Algorithm (MOGA) and K-Nearest Neighbor (KNN) Classifier for Hydrological Model Calibration.- Cluster-Based Visualisation of Marketing Data.- Dynamic Symbolization of Streaming Time Series.- A Clustering Model for Mining Evolving Web User Patterns in Data Stream Environment.- An Improved Constructive Neural Network Ensemble Approach to Medical Diagnoses.- Spam Classification Using Nearest Neighbour Techniques.- Learning Algorithms and Systems.- Kernel Density Construction Using Orthogonal Forward Regression.- Orthogonal Least Square with Boosting for Regression.- New Applications for Object Recognition and Affine Motion Estimation by Independent Component Analysis.- Personalized News Reading via Hybrid Learning.- Mercer Kernel, Fuzzy C-Means Algorithm, and Prototypes of Clusters.- DIVACE: Diverse and Accurate Ensemble Learning Algorithm.- Parallel Processing for Movement Detection in Neural Networks with Nonlinear Functions.- Combining Multiple k-Nearest Neighbor Classifiers Using Different Distance Functions.- Finding Minimal Addition Chains Using Ant Colony.- Combining Local and Global Models to Capture Fast and Slow Dynamics in Time Series Data.- A Variable Metric Probabilistic k-Nearest-Neighbours Classifier.- Feature Word Tracking in Time Series Documents.- Combining Gaussian Mixture Models.- Global Convergence of Steepest Descent for Quadratic Functions.- Boosting Orthogonal Least Squares Regression.- Local Separation Property of the Two-Source ICA Problem with the One-Bit-Matching Condition.- Two Further Gradient BYY Learning Rules for Gaussian Mixture with Automated Model Selection.- Improving Support Vector Solutions by Selecting a Sequence of Training Subsets.- Machine Learning for Matching Astronomy Catalogues.- Boosting the Tree Augmented Naive Bayes Classifier.- Clustering Model Selection for Reduced Support Vector Machines.- Generating the Reduced Set by Systematic Sampling.- Experimental Comparison of Classification Uncertainty for Randomised and Bayesian Decision Tree Ensembles.- Policy Gradient Method for Team Markov Games.- An Information Theoretic Optimal Classifier for Semi-supervised Learning.- Improving Evolutionary Algorithms by a New Smoothing Technique.- In-Situ Learning in Multi-net Systems.- Multi-objective Genetic Algorithm Based Method for Mining Optimized Fuzzy Association Rules.- Co-training from an Incremental EM Perspective.- Locally Tuned General Regression for Learning Mixture Models Using Small Incomplete Data Sets with Outliers and Overlapping Classes.- Financial Engineering.- Credit Risks of Interest Rate Swaps: A Comparative Study of CIR and Monte Carlo Simulation Approach.- Cardinality Constrained Portfolio Optimisation.- Stock Trading by Modelling Price Trend with Dynamic Bayesian Networks.- Detecting Credit Card Fraud by Using Questionnaire-Responded Transaction Model Based on Support Vector Machines.- Volatility Forecasts in Financial Time Series with HMM-GARCH Models.- Agent Technologies.- User Adaptive Answers Generation for Conversational Agent Using Genetic Programming.- Comparing Measures of Agreement for Agent Attitudes.- Hierarchical Agglomerative Clustering for Agent-Based Dynamic Collaborative Filtering.- Learning Users' Interests in a Market-Based Recommender System.- Visualisation of Multi-agent System Organisations Using a Self-organising Map of Pareto Solutions.",poster,cp48
Computer Science,p622,d3,1bce1b410cbde7851e902291aad14616fbc318f7,j156,Lecture Notes in Computer Science,Intelligent Data Engineering and Automated Learning,Abstract,fullPaper,jv156
Computer Science,p623,d3,582640c0b232acdb487cb17ebab4dabff3b1d264,c6,Annual Conference on Genetic and Evolutionary Computation,Editorial: Two Named to Editorial Board of IEEE Transactions on Knowledge and Data Engineering,Abstract,poster,cp6
Computer Science,p624,d3,839f883c01af2d6d2287de9904a4860a8e6fb7cf,c77,Visualization for Computer Security,Knowledge and Data Engineering: An Outlook,Abstract,poster,cp77
Computer Science,p625,d3,eb9272596b20ca3e6b4483c639280671b39065c9,c6,Annual Conference on Genetic and Evolutionary Computation,Mining Software Engineering Data from GitHub,"GitHub is the largest collaborative source code hosting site built on top of the Git version control system. The availability of a comprehensive API has made GitHub a target for many software engineering and online collaboration research efforts. In our work, we have discovered that a) obtaining data from GitHub is not trivial, b) the data may not be suitable for all types of research, and c) improper use can lead to biased results. In this tutorial, we analyze how data from GitHub can be used for large-scale, quantitative research, while avoiding common pitfalls. We use the GHTorrent dataset, a queryable offline mirror of the GitHub API data, to draw examples from and present pitfall avoidance strategies.",poster,cp6
Computer Science,p626,d3,70a86a524917db7b6122b6f5e2f45661dca1b8c9,j156,Lecture Notes in Computer Science,Intelligent Data Engineering and Automated Learning — IDEAL 2002,Abstract,fullPaper,jv156
Business,p626,d9,70a86a524917db7b6122b6f5e2f45661dca1b8c9,j156,Lecture Notes in Computer Science,Intelligent Data Engineering and Automated Learning — IDEAL 2002,Abstract,fullPaper,jv156
Computer Science,p627,d3,3cfea68a088b443b94052c3c4b53bde136cbaac9,j4,IEEE Transactions on Knowledge and Data Engineering,Towards Benchmarks for Knowledge Systems and Their Implications for Data Engineering,"The author suggests a new focus on benchmarks for knowledge systems, following the lines of similar benchmarks in other computing fields. It is noted that knowledge systems differ from conventional systems in a key way, namely their ability to interpret and apply knowledge. This gives rise to a distinction between intrinsic measures concerned with engineering qualities and extrinsic measures relating to task productivity, and both warrant improved measurement techniques. Primary concerns within the extrinsic realm include advice quality, reasoning correctness, robustness, and solution efficiency. Intrinsic concerns, on the other hand, center on elegance of knowledge base design, modularity, and architecture. The author suggests criteria for good measures and benchmarks, and ways to satisfy these through the design of knowledge and key knowledge engineering costs and performance parameters. It is suggest that the focus on measuring knowledge systems should help clarify the technical relationships between knowledge engineering and data engineering. >",fullPaper,jv4
Computer Science,p628,d3,7caf67fec572c532056f941d7b03811e5eebcb1a,c21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,Proceedings of the Eleventh International Conference on Data Engineering,Abstract,poster,cp21
Computer Science,p630,d3,3a97bd1115686e4959e84a60eeb518dfd2de052d,c121,International Conference on Interaction Sciences,Engineering Design via Surrogate Modelling - A Practical Guide,"Preface. About the Authors. Foreword. Prologue. Part I: Fundamentals. 1. Sampling Plans. 1.1 The 'Curse of Dimensionality' and How to Avoid It. 1.2 Physical versus Computational Experiments. 1.3 Designing Preliminary Experiments (Screening). 1.3.1 Estimating the Distribution of Elementary Effects. 1.4 Designing a Sampling Plan. 1.4.1 Stratification. 1.4.2 Latin Squares and Random Latin Hypercubes. 1.4.3 Space-filling Latin Hypercubes. 1.4.4 Space-filling Subsets. 1.5 A Note on Harmonic Responses. 1.6 Some Pointers for Further Reading. References. 2. Constructing a Surrogate. 2.1 The Modelling Process. 2.1.1 Stage One: Preparing the Data and Choosing a Modelling Approach. 2.1.2 Stage Two: Parameter Estimation and Training. 2.1.3 Stage Three: Model Testing. 2.2 Polynomial Models. 2.2.1 Example One: Aerofoil Drag. 2.2.2 Example Two: a Multimodal Testcase. 2.2.3 What About the k -variable Case? 2.3 Radial Basis Function Models. 2.3.1 Fitting Noise-Free Data. 2.3.2 Radial Basis Function Models of Noisy Data. 2.4 Kriging. 2.4.1 Building the Kriging Model. 2.4.2 Kriging Prediction. 2.5 Support Vector Regression. 2.5.1 The Support Vector Predictor. 2.5.2 The Kernel Trick. 2.5.3 Finding the Support Vectors. 2.5.4 Finding . 2.5.5 Choosing C and epsilon. 2.5.6 Computing epsilon : v -SVR 71. 2.6 The Big(ger) Picture. References. 3. Exploring and Exploiting a Surrogate. 3.1 Searching the Surrogate. 3.2 Infill Criteria. 3.2.1 Prediction Based Exploitation. 3.2.2 Error Based Exploration. 3.2.3 Balanced Exploitation and Exploration. 3.2.4 Conditional Likelihood Approaches. 3.2.5 Other Methods. 3.3 Managing a Surrogate Based Optimization Process. 3.3.1 Which Surrogate for What Use? 3.3.2 How Many Sample Plan and Infill Points? 3.3.3 Convergence Criteria. 3.3.4 Search of the Vibration Isolator Geometry Feasibility Using Kriging Goal Seeking. References. Part II: Advanced Concepts. 4. Visualization. 4.1 Matrices of Contour Plots. 4.2 Nested Dimensions. Reference. 5. Constraints. 5.1 Satisfaction of Constraints by Construction. 5.2 Penalty Functions. 5.3 Example Constrained Problem. 5.3.1 Using a Kriging Model of the Constraint Function. 5.3.2 Using a Kriging Model of the Objective Function. 5.4 Expected Improvement Based Approaches. 5.4.1 Expected Improvement With Simple Penalty Function. 5.4.2 Constrained Expected Improvement. 5.5 Missing Data. 5.5.1 Imputing Data for Infeasible Designs. 5.6 Design of a Helical Compression Spring Using Constrained Expected Improvement. 5.7 Summary. References. 6. Infill Criteria With Noisy Data. 6.1 Regressing Kriging. 6.2 Searching the Regression Model. 6.2.1 Re-Interpolation. 6.2.2 Re-Interpolation With Conditional Likelihood Approaches. 6.3 A Note on Matrix Ill-Conditioning. 6.4 Summary. References. 7. Exploiting Gradient Information. 7.1 Obtaining Gradients. 7.1.1 Finite Differencing. 7.1.2 Complex Step Approximation. 7.1.3 Adjoint Methods and Algorithmic Differentiation. 7.2 Gradient-enhanced Modelling. 7.3 Hessian-enhanced Modelling. 7.4 Summary. References. 8. Multi-fidelity Analysis. 8.1 Co-Kriging. 8.2 One-variable Demonstration. 8.3 Choosing X c and X e . 8.4 Summary. References. 9. Multiple Design Objectives. 9.1 Pareto Optimization. 9.2 Multi-objective Expected Improvement. 9.3 Design of the Nowacki Cantilever Beam Using Multi-objective, Constrained Expected Improvement. 9.4 Design of a Helical Compression Spring Using Multi-objective, Constrained Expected Improvement. 9.5 Summary. References. Appendix: Example Problems. A.1 One-Variable Test Function. A.2 Branin Test Function. A.3 Aerofoil Design. A.4 The Nowacki Beam. A.5 Multi-objective, Constrained Optimal Design of a Helical Compression Spring. A.6 Novel Passive Vibration Isolator Feasibility. References. Index.",poster,cp121
Mathematics,p630,d6,3a97bd1115686e4959e84a60eeb518dfd2de052d,c121,International Conference on Interaction Sciences,Engineering Design via Surrogate Modelling - A Practical Guide,"Preface. About the Authors. Foreword. Prologue. Part I: Fundamentals. 1. Sampling Plans. 1.1 The 'Curse of Dimensionality' and How to Avoid It. 1.2 Physical versus Computational Experiments. 1.3 Designing Preliminary Experiments (Screening). 1.3.1 Estimating the Distribution of Elementary Effects. 1.4 Designing a Sampling Plan. 1.4.1 Stratification. 1.4.2 Latin Squares and Random Latin Hypercubes. 1.4.3 Space-filling Latin Hypercubes. 1.4.4 Space-filling Subsets. 1.5 A Note on Harmonic Responses. 1.6 Some Pointers for Further Reading. References. 2. Constructing a Surrogate. 2.1 The Modelling Process. 2.1.1 Stage One: Preparing the Data and Choosing a Modelling Approach. 2.1.2 Stage Two: Parameter Estimation and Training. 2.1.3 Stage Three: Model Testing. 2.2 Polynomial Models. 2.2.1 Example One: Aerofoil Drag. 2.2.2 Example Two: a Multimodal Testcase. 2.2.3 What About the k -variable Case? 2.3 Radial Basis Function Models. 2.3.1 Fitting Noise-Free Data. 2.3.2 Radial Basis Function Models of Noisy Data. 2.4 Kriging. 2.4.1 Building the Kriging Model. 2.4.2 Kriging Prediction. 2.5 Support Vector Regression. 2.5.1 The Support Vector Predictor. 2.5.2 The Kernel Trick. 2.5.3 Finding the Support Vectors. 2.5.4 Finding . 2.5.5 Choosing C and epsilon. 2.5.6 Computing epsilon : v -SVR 71. 2.6 The Big(ger) Picture. References. 3. Exploring and Exploiting a Surrogate. 3.1 Searching the Surrogate. 3.2 Infill Criteria. 3.2.1 Prediction Based Exploitation. 3.2.2 Error Based Exploration. 3.2.3 Balanced Exploitation and Exploration. 3.2.4 Conditional Likelihood Approaches. 3.2.5 Other Methods. 3.3 Managing a Surrogate Based Optimization Process. 3.3.1 Which Surrogate for What Use? 3.3.2 How Many Sample Plan and Infill Points? 3.3.3 Convergence Criteria. 3.3.4 Search of the Vibration Isolator Geometry Feasibility Using Kriging Goal Seeking. References. Part II: Advanced Concepts. 4. Visualization. 4.1 Matrices of Contour Plots. 4.2 Nested Dimensions. Reference. 5. Constraints. 5.1 Satisfaction of Constraints by Construction. 5.2 Penalty Functions. 5.3 Example Constrained Problem. 5.3.1 Using a Kriging Model of the Constraint Function. 5.3.2 Using a Kriging Model of the Objective Function. 5.4 Expected Improvement Based Approaches. 5.4.1 Expected Improvement With Simple Penalty Function. 5.4.2 Constrained Expected Improvement. 5.5 Missing Data. 5.5.1 Imputing Data for Infeasible Designs. 5.6 Design of a Helical Compression Spring Using Constrained Expected Improvement. 5.7 Summary. References. 6. Infill Criteria With Noisy Data. 6.1 Regressing Kriging. 6.2 Searching the Regression Model. 6.2.1 Re-Interpolation. 6.2.2 Re-Interpolation With Conditional Likelihood Approaches. 6.3 A Note on Matrix Ill-Conditioning. 6.4 Summary. References. 7. Exploiting Gradient Information. 7.1 Obtaining Gradients. 7.1.1 Finite Differencing. 7.1.2 Complex Step Approximation. 7.1.3 Adjoint Methods and Algorithmic Differentiation. 7.2 Gradient-enhanced Modelling. 7.3 Hessian-enhanced Modelling. 7.4 Summary. References. 8. Multi-fidelity Analysis. 8.1 Co-Kriging. 8.2 One-variable Demonstration. 8.3 Choosing X c and X e . 8.4 Summary. References. 9. Multiple Design Objectives. 9.1 Pareto Optimization. 9.2 Multi-objective Expected Improvement. 9.3 Design of the Nowacki Cantilever Beam Using Multi-objective, Constrained Expected Improvement. 9.4 Design of a Helical Compression Spring Using Multi-objective, Constrained Expected Improvement. 9.5 Summary. References. Appendix: Example Problems. A.1 One-Variable Test Function. A.2 Branin Test Function. A.3 Aerofoil Design. A.4 The Nowacki Beam. A.5 Multi-objective, Constrained Optimal Design of a Helical Compression Spring. A.6 Novel Passive Vibration Isolator Feasibility. References. Index.",poster,cp121
Computer Science,p631,d3,887646bf8f0c9437e89c69aec05d2eba67bb5f36,j177,Computers & industrial engineering,Applying Kansei Engineering and data mining to design door-to-door delivery service,Abstract,fullPaper,jv177
Computer Science,p633,d3,27245e65a27bde90b5b0bb25d157bb75a0ad8b5a,j136,EURASIP Journal on Advances in Signal Processing,A survey of machine learning for big data processing,Abstract,fullPaper,jv136
Computer Science,p634,d3,542c23f026a1801d30153fcf718b5c9b4bdd20e1,j72,IEEE Intelligent Systems,Research Directions for Engineering Big Data Analytics Software,"Many software startups and research and development efforts are actively trying to harness the power of big data and create software with the potential to improve almost every aspect of human life. As these efforts continue to increase, full consideration needs to be given to the engineering aspects of big data software. Since these systems exist to make predictions on complex and continuous massive datasets, they pose unique problems during specification, design, and verification of software that needs to be delivered on time and within budget. But, given the nature of big data software, can this be done? Does big data software engineering really work? This article explores the details of big data software, discusses the main problems encountered when engineering big data software, and proposes avenues for future research.",fullPaper,jv72
Computer Science,p635,d3,c31665ce7ec70a876d2993761018c3e93912a1a3,c47,EUROMICRO Conference on Software Engineering and Advanced Applications,A Collection of Software Engineering Challenges for Big Data System Development,"In recent years, the development of systems for processing and analyzing large amounts of data (so-called Big Data) has become an important sub-discipline of software engineering. However, to date there exits no comprehensive summary of the specific idiosyncrasies and challenges that the development of Big Data systems imposes on software engineers. With this paper, we aim to provide a first step towards filling this gap based on our collective experience from industry and academic projects as well as from consulting and initial literature reviews. The main contribution of our work is a concise summary of 26 challenges in engineering Big Data systems, collected and consolidated by means of a systematic identification process. The aim is to make practitioners more aware of common challenges and to offer researchers a solid baseline for identifying novel software engineering research directions.",fullPaper,cp47
Computer Science,p637,d3,d996bd45107881735664a08113034fc016dde41a,c48,"Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing",Exploring topic models in software engineering data analysis: A survey,"Topic models are shown to be effective to mine unstructured software engineering (SE) data. In this paper, we give a simple survey of exploring topic models to support various SE tasks between 2003 and 2015. The survey results show that there is an increasing concern in this area. Among the SE tasks, source code comprehension and software history comprehension are the mostly studied, followed by software defects prediction. However, there is still only a few studies on other SE tasks, such as feature location and regression testing.",fullPaper,cp48
Computer Science,p638,d3,ef51193388ebcc0ab19a6200e469fb6666e42d3c,j170,IEEE Transactions on Software Engineering,A Methodology for Collecting Valid Software Engineering Data,"An effective data collection method for evaluating software development methodologies and for studying the software development process is described. The method uses goal-directed data collection to evaluate methodologies with respect to the claims made for them. Such claims are used as a basis for defining the goals of the data collection, establishing a list of questions of interest to be answered by data analysis, defining a set of data categorization schemes, and designing a data collection form. The data to be collected are based on the changes made to the software during development, and are obtained when the changes are made. To ensure accuracy of the data, validation is performed concurrently with software development and data collection. Validation is based on interviews with those people supplying the data. Results from using the methodology show that data validation is a necessary part of change data collection. Without it, as much as 50 percent of the data may be erroneous. Feasibility of the data collection methodology was demonstrated by applying it to five different projects in two different environments. The application showed that the methodology was both feasible and useful.",fullPaper,jv170
Computer Science,p639,d3,49bf4507f5670dacbf8299af83ab1390438d79f7,j178,Data & Knowledge Engineering,Knowledge Engineering: Principles and Methods,Abstract,fullPaper,jv178
Computer Science,p641,d3,921d1869f4cb4b0aa19199a1dabe3a00e0dce023,j72,IEEE Intelligent Systems,Knowledge Engineering with Big Data,"In the era of big data, knowledge engineering faces fundamental challenges induced by fragmented knowledge from heterogeneous, autonomous sources with complex and evolving relationships. The knowledge representation, acquisition, and inference techniques developed in the 1970s and 1980s, driven by research and development of expert systems, must be updated to cope with both fragmented knowledge from multiple sources in the big data revolution and in-depth knowledge from domain experts. This article presents BigKE, a knowledge engineering framework that handles fragmented knowledge modeling and online learning from multiple information sources, nonlinear fusion on fragmented knowledge, and automated demand-driven knowledge navigation.",fullPaper,jv72
Computer Science,p642,d3,8ece479b5dfed4727d2d9b9763f777bb9a94096e,c74,International Conference on Computational Linguistics,Human-AI Collaboration in Data Science,"The rapid advancement of artificial intelligence (AI) is changing our lives in many ways. One application domain is data science. New techniques in automating the creation of AI, known as AutoAI or AutoML, aim to automate the work practices of data scientists. AutoAI systems are capable of autonomously ingesting and pre-processing data, engineering new features, and creating and scoring models based on a target objectives (e.g. accuracy or run-time efficiency). Though not yet widely adopted, we are interested in understanding how AutoAI will impact the practice of data science. We conducted interviews with 20 data scientists who work at a large, multinational technology company and practice data science in various business settings. Our goal is to understand their current work practices and how these practices might change with AutoAI. Reactions were mixed: while informants expressed concerns about the trend of automating their jobs, they also strongly felt it was inevitable. Despite these concerns, they remained optimistic about their future job security due to a view that the future of data science work will be a collaboration between humans and AI systems, in which both automation and human expertise are indispensable.",poster,cp74
Psychology,p642,d10,8ece479b5dfed4727d2d9b9763f777bb9a94096e,c74,International Conference on Computational Linguistics,Human-AI Collaboration in Data Science,"The rapid advancement of artificial intelligence (AI) is changing our lives in many ways. One application domain is data science. New techniques in automating the creation of AI, known as AutoAI or AutoML, aim to automate the work practices of data scientists. AutoAI systems are capable of autonomously ingesting and pre-processing data, engineering new features, and creating and scoring models based on a target objectives (e.g. accuracy or run-time efficiency). Though not yet widely adopted, we are interested in understanding how AutoAI will impact the practice of data science. We conducted interviews with 20 data scientists who work at a large, multinational technology company and practice data science in various business settings. Our goal is to understand their current work practices and how these practices might change with AutoAI. Reactions were mixed: while informants expressed concerns about the trend of automating their jobs, they also strongly felt it was inevitable. Despite these concerns, they remained optimistic about their future job security due to a view that the future of data science work will be a collaboration between humans and AI systems, in which both automation and human expertise are indispensable.",poster,cp74
Computer Science,p644,d3,7a0a373a80c4f65950958d6a2253d8da90493bc5,j179,Bulletin of Engineering Geology and the Environment,The use of unmanned aerial vehicles (UAVs) for engineering geology applications,Abstract,fullPaper,jv179
Computer Science,p645,d3,8220aa685354d53868899a08ca97c74c669f7c4a,j180,Future Internet,Social Engineering Attacks: A Survey,"The advancements in digital communication technology have made communication between humans more accessible and instant. However, personal and sensitive information may be available online through social networks and online services that lack the security measures to protect this information. Communication systems are vulnerable and can easily be penetrated by malicious users through social engineering attacks. These attacks aim at tricking individuals or enterprises into accomplishing actions that benefit attackers or providing them with sensitive data such as social security number, health records, and passwords. Social engineering is one of the biggest challenges facing network security because it exploits the natural human tendency to trust. This paper provides an in-depth survey about the social engineering attacks, their classifications, detection strategies, and prevention procedures.",fullPaper,jv180
Computer Science,p646,d3,65d61afd9c35b0a75d9de77c2a4a2428af0f7f7b,j181,IEEE Signal Processing Magazine,Big Data Analysis with Signal Processing on Graphs: Representation and processing of massive data sets with irregular structure,"Analysis and processing of very large data sets, or big data, poses a significant challenge. Massive data sets are collected and studied in numerous domains, from engineering sciences to social networks, biomolecular research, commerce, and security. Extracting valuable information from big data requires innovative approaches that efficiently process large amounts of data as well as handle and, moreover, utilize their structure. This article discusses a paradigm for large-scale data analysis based on the discrete signal processing (DSP) on graphs (DSPG). DSPG extends signal processing concepts and methodologies from the classical signal processing theory to data indexed by general graphs. Big data analysis presents several challenges to DSPG, in particular, in filtering and frequency analysis of very large data sets. We review fundamental concepts of DSPG, including graph signals and graph filters, graph Fourier transform, graph frequency, and spectrum ordering, and compare them with their counterparts from the classical signal processing theory. We then consider product graphs as a graph model that helps extend the application of DSPG methods to large data sets through efficient implementation based on parallelization and vectorization. We relate the presented framework to existing methods for large-scale data processing and illustrate it with an application to data compression.",fullPaper,jv181
Computer Science,p647,d3,698c1bd50f476899c9959d6830434d4a455941e2,c12,The Compass,Big Picture of Big Data Software Engineering: With Example Research Challenges,"In the rapidly growing field of Big Data, we note that a disproportionately larger amount of effort is being invested in infrastructure development and data analytics in comparison to applications software development -- approximately a 80:20 ratio. This prompted us to create a context model of Big Data Software Engineering (BDSE) containing various elements -- such as development practice, Big Data systems, corporate decision-making, and research -- and their relationships. The model puts into perspective where various types of stakeholders fit in. From the research perspective, we describe example challenges in BDSE, specifically requirements, architectures, and testing and maintenance.",poster,cp12
Computer Science,p650,d3,a787499299fa75a7d8597aa4793255e9587a3c7f,c46,Ideal,Sentiment Analysis of Twitter Data,"We examine sentiment analysis on Twitter data. The contributions of this paper are: (1) We introduce POS-specific prior polarity features. (2) We explore the use of a tree kernel to obviate the need for tedious feature engineering. The new features (in conjunction with previously proposed features) and the tree kernel perform approximately at the same level, both outperforming the state-of-the-art baseline.",poster,cp46
Computer Science,p651,d3,988653d9cc361ce41dbe0930eaf2ecea27c85053,c50,Conference on Emerging Network Experiment and Technology,MicroTE: fine grained traffic engineering for data centers,"The effects of data center traffic characteristics on data center traffic engineering is not well understood. In particular, it is unclear how existing traffic engineering techniques perform under various traffic patterns, namely how do the computed routes differ from the optimal routes. Our study reveals that existing traffic engineering techniques perform 15% to 20% worse than the optimal solution. We find that these techniques suffer mainly due to their inability to utilize global knowledge about flow characteristics and make coordinated decision for scheduling flows.
 To this end, we have developed MicroTE, a system that adapts to traffic variations by leveraging the short term and partial predictability of the traffic matrix. We implement MicroTE within the OpenFlow framework and with minor modification to the end hosts. In our evaluations, we show that our system performs close to the optimal solution and imposes minimal overhead on the network making it appropriate for current and future data centers.",fullPaper,cp50
Computer Science,p652,d3,813c3085647a7147ae69df7f2eaa619f85f4da77,c51,International Conference on Engineering Education,Educational data mining for prediction and classification of engineering students achievement,"This paper highlights the importance of using student data to drive improvement in education planning. It then presents techniques of how to obtain knowledge from databases such as large arrays of student data from academic Institution databases. Further, it describes the development of a tool that will enable faculty members to identify, predict and classify students based on academic performance measured using Cumulative Grade point average (CGPA) grades. The need for prediction of a student's performance is to enable the university to intervene and provide assistance to low achievers as early as possible. Included in the paper is a brief overview of the most commonly used classifiers techniques in educational data mining and an outline of the use of Neuro-Fuzzy classification in a case study research to predict and classify students' academic achievement in an Electrical Engineering faculty of a Malaysian public university.",fullPaper,cp51
Computer Science,p653,d3,25ac496bc07c2961860542c83abbc14b26cf42f4,c52,Workshop on Applied Computational Geometry,Triangle: Engineering a 2D Quality Mesh Generator and Delaunay Triangulator,Abstract,fullPaper,cp52
Computer Science,p654,d3,775a9c722262c7b656876a5fef20f4577afd8981,c2,International Conference on Software Engineering,Multilingual training for Software Engineering,"Well-trained machine-learning models, which leverage large amounts of open-source software data, have now become an interesting approach to automating many software engineering tasks. Several SE tasks have all been subject to this approach, with performance gradually improving over the past several years with better models and training methods. More, and more diverse, clean, labeled data is better for training; but constructing good-quality datasets is time-consuming and challenging. Ways of augmenting the volume and diversity of clean, labeled data generally have wide applicability. For some languages (e.g., Ruby) labeled data is less abundant; in others (e.g., JavaScript) the available data maybe more focused on some application domains, and thus less diverse. As a way around such data bottlenecks, we present evidence suggesting that human-written code in different languages (which performs the same function), is rather similar, and particularly preserving of identifier naming patterns; we further present evidence suggesting that identifiers are a very important element of training data for software engineering tasks. We leverage this rather fortuitous phenomenon to find evidence that available multilingual training data (across different languages) can be used to amplify performance. We study this for 3 different tasks: code summarization, code retrieval, and function naming. We note that this data-augmenting approach is broadly compatible with different tasks, languages, and machine-learning models.",fullPaper,cp2
Computer Science,p656,d3,a4b603ca6aaaa18968e08ac1b0ee093db8a99a6b,c107,Annual Haifa Experimental Systems Conference,Topology and data,"An important feature of modern science and engineering is that data of various kinds is being produced at an unprecedented rate. This is so in part because of new experimental methods, and in part because of the increase in the availability of high powered computing technology. It is also clear that the nature of the data we are obtaining is significantly different. For example, it is now often the case that we are given data in the form of very long vectors, where all but a few of the coordinates turn out to be irrelevant to the questions of interest, and further that we don’t necessarily know which coordinates are the interesting ones. A related fact is that the data is often very high-dimensional, which severely restricts our ability to visualize it. The data obtained is also often much noisier than in the past and has more missing information (missing data). This is particularly so in the case of biological data, particularly high throughput data from microarray or other sources. Our ability to analyze this data, both in terms of quantity and the nature of the data, is clearly not keeping pace with the data being produced. In this paper, we will discuss how geometry and topology can be applied to make useful contributions to the analysis of various kinds of data. Geometry and topology are very natural tools to apply in this direction, since geometry can be regarded as the study of distance functions, and what one often works with are distance functions on large finite sets of data. The mathematical formalism which has been developed for incorporating geometric and topological techniques deals with point clouds, i.e. finite sets of points equipped with a distance function. It then adapts tools from the various branches of geometry to the study of point clouds. The point clouds are intended to be thought of as finite samples taken from a geometric object, perhaps with noise. Here are some of the key points which come up when applying these geometric methods to data analysis. • Qualitative information is needed: One important goal of data analysis is to allow the user to obtain knowledge about the data, i.e. to understand how it is organized on a large scale. For example, if we imagine that we are looking at a data set constructed somehow from diabetes patients, it would be important to develop the understanding that there are two types of the disease, namely the juvenile and adult onset forms. Once that is established, one of course wants to develop quantitative methods for distinguishing them, but the first insight about the distinct forms of the disease is key.",poster,cp107
Computer Science,p657,d3,682b105746238d3c39bd4f6cd0baa375dc0c2534,c23,International Conference on Open and Big Data,Perspectives on Data Science for Software Engineering,Abstract,poster,cp23
Computer Science,p658,d3,fb3082412509fb50cb5a8b014b9eddbe6e5002cb,c38,IEEE Global Engineering Education Conference,Methodologies of knowledge discovery from data and data mining methods in mechanical engineering,Abstract,poster,cp38
Computer Science,p659,d3,365aed570f5ee0a2e0a5590feaa48a2b563b4344,j182,Concurrent Engineering - Research and Applications,Conceptual data model: A foundation for successful concurrent engineering,"Today, phase A studies of future space systems are often conducted in special design facilities such as the Concurrent Engineering Facility at the German Aerospace Center (DLR). Within these facilities, the studies are performed following a defined process making use of a data model for information exchange. Quite often it remains unclear what exactly such a data model is and how it is implemented and applied. Nowadays, such a data model is usually a software using a formal specification describing its capabilities within a so-called meta-model. This meta-model, often referred as conceptual data model, is finally used and instantiated as system model during these concurrent engineering studies. Such software also provides a user interface for instantiating and sharing the system model within the design team and it provides capabilities to analyze the system model on the fly. This is possible due to the semantics of the underlying conceptual data model creating a common language used to exchange and process design information. This article explains the implementation of the data model at DLR and shows information how it is applied in the concurrent engineering process of the Concurrent Engineering Facility. It highlights important aspects concerning the modeling capabilities during a study and discusses how they can be implemented into a corresponding conceptual data model. Accordingly, the article presents important aspects such as rights management and data consistency and the implications of them to the software’s underlying technology. A special use case of the data model is depicted and shows the flexibility of the implementation proven by a study of a multi-module space station.",fullPaper,jv182
Computer Science,p665,d3,11907f691e9b7fc32a492e1de676a4b788add155,j170,IEEE Transactions on Software Engineering,On the Validity of Pre-Trained Transformers for Natural Language Processing in the Software Engineering Domain,"Transformers are the current state-of-the-art of natural language processing in many domains and are using traction within software engineering research as well. Such models are pre-trained on large amounts of data, usually from the general domain. However, we only have a limited understanding regarding the validity of transformers within the software engineering domain, i.e., how good such models are at understanding words and sentences within a software engineering context and how this improves the state-of-the-art. Within this article, we shed light on this complex, but crucial issue. We compare BERT transformer models trained with software engineering data with transformers based on general domain data in multiple dimensions: their vocabulary, their ability to understand which words are missing, and their performance in classification tasks. Our results show that for tasks that require understanding of the software engineering context, pre-training with software engineering data is valuable, while general domain models are sufficient for general language understanding, also within the software engineering domain.",fullPaper,jv170
Computer Science,p666,d3,48febeeed061d93e5bb17ee7dba03ff8ab37353e,c5,Technical Symposium on Computer Science Education,Joint virtual machine assignment and traffic engineering for green data center networks,"The popularization of cloud computing brings emergency concern to the energy consumption in big data centers. Besides the servers, the energy consumed by the network in a data center is also considerable. Existing works for improving the network energy efficiency are mainly focused on traffic engineering, i.e., consolidating flows and switching off unnecessary devices, which fails to comprehensively consider the unique features in data centers. In this paper, we advocate a joint optimization for achieving energy efficiency of data center networks by proposing a unified optimization framework. In this framework, we consider to take advantage of the application characteristics and topology features, and to integrate virtual machine assignment and traffic engineering. Under this framework, we then devise two efficient algorithms, TE VMA and TER, for assigning virtual machines and routing traffic flows respectively. Knowing the ncommunication patterns of the applications, the TE VMA algorithm is purposeful and can generate desirable traffic conditions for the next-step routing optimization. The TER algorithm makes full use of the hierarchical feature of the topology and is conducted on the multipath routing protocol. The performance of the overall framework is confirmed by both theoretical analysis and simulation results, where up to 50% total energy savings can be achieved, 20% more compared with traffic engineering only approaches.",poster,cp5
Computer Science,p669,d3,2de0a40e9a5d4f1feb07d61af5a5d87a069653f0,j4,IEEE Transactions on Knowledge and Data Engineering,Data Mining: An Overview from a Database Perspective,"Mining information and knowledge from large databases has been recognized by many researchers as a key research topic in database systems and machine learning, and by many industrial companies as an important area with an opportunity of major revenues. Researchers in many different fields have shown great interest in data mining. Several emerging applications in information-providing services, such as data warehousing and online services over the Internet, also call for various data mining techniques to better understand user behavior, to improve the service provided and to increase business opportunities. In response to such a demand, this article provides a survey, from a database researcher's point of view, on the data mining techniques developed recently. A classification of the available data mining techniques is provided and a comparative study of such techniques is presented.",fullPaper,jv4
Computer Science,p670,d3,e8b9fa6f9e0b606ff335a0557a838dea2696b084,c53,International Conference on Learning Representations,Towards Reverse-Engineering Black-Box Neural Networks,Abstract,fullPaper,cp53
Mathematics,p670,d6,e8b9fa6f9e0b606ff335a0557a838dea2696b084,c53,International Conference on Learning Representations,Towards Reverse-Engineering Black-Box Neural Networks,Abstract,fullPaper,cp53
Computer Science,p671,d3,377f1e43c5a48f12b0592b09a142322e74729409,j143,Annals of Data Science,Genetic Algorithms in the Fields of Artificial Intelligence and Data Sciences,Abstract,fullPaper,jv143
Computer Science,p673,d3,f85879782b6a586f992d3bb99f9907ecce61924a,c97,International Conference on Computational Logic,Embrace the Challenges: Software Engineering in a Big Data World,"The design and development of data-intensive software systems -- systems that generate, collect, store, process, analyze, query, and visualize large sets of data -- is fraught with significant challenges both technical and social. Project EPIC has been designing and developing data-intensive systems in support of crisis informatics research since Fall 2009. Our experience working on Project EPIC has provided insight into these challenges. In this paper, we share our experience working in this design space and describe the choices we made in tackling these challenges and their attendant trade-offs. We highlight the lack of developer support tools for data-intensive systems, the importance of multidisciplinary teams, the use of highly-iterative life cycles, the need for deep understanding of the frameworks and technologies used in data intensive systems, how simple operations transform into significant challenges at scale, and the paramount significance of data modeling in producing systems that are scalable, robust, and efficient.",poster,cp97
Computer Science,p674,d3,6aca07154c111f1c8738347d7112cad6b0bf974a,j7,Journal of Big Data,Customer churn prediction in telecom using machine learning in big data platform,Abstract,fullPaper,jv7
Computer Science,p675,d3,0405150d045bc39ddf5ba8a1c71df8f2d63c661e,c54,ACM SIGCOMM Symposium on Software Defined Networking Research,Engineering Applications of Correlation and Spectral Analysis,Probability Functions and Amplitude Measures. Correlation and Spectral Density Functions. Single-Input/Single-Output Relationships. System Identification and Response. Propagation-Path Identification. Single-Input/Multiple-Output Problems. Multiple-Input/Multiple-Output Relationships. Energy-Source Identification. Procedures to Solve Multiple-Input/Multiple-Output Problems. Statistical Errors in Estimates. Nonstationary Data Analysis Techniques. Nonlinear System Analysis Techniques. References. List of Figures. List of Tables. Index. Glossary of Symbols.,poster,cp54
Computer Science,p676,d3,0c471c3aa5aef737ee00cfe9c58e9097328a954e,j4,IEEE Transactions on Knowledge and Data Engineering,A Survey of Data-Driven and Knowledge-Aware eXplainable AI,"We are witnessing a fast development of Artificial Intelligence (AI), but it becomes dramatically challenging to explain AI models in the past decade. “Explanation” has a flexible philosophical concept of “satisfying the subjective curiosity for causal information”, driving a wide spectrum of methods being invented and/or adapted from many aspects and communities, including machine learning, visual analytics, human-computer interaction and so on. Nevertheless, from the view-point of data and knowledge engineering (DKE), a best explaining practice that is cost-effective in terms of extra intelligence acquisition should exploit the causal information and explaining scenarios which is hidden richly in the data itself. In the past several years, there are plenty of works contributing in this line but there is a lack of a clear taxonomy and systematic review of the current effort. To this end, we propose this survey, reviewing and taxonomizing existing efforts from the view-point of DKE, summarizing their contribution, technical essence and comparative characteristics. Specifically, we categorize methods into data-driven methods where explanation comes from the task-related data, and knowledge-aware methods where extraneous knowledge is incorporated. Furthermore, in the light of practice, we provide survey of state-of-art evaluation metrics and deployed explanation applications in industrial practice.",fullPaper,jv4
Computer Science,p680,d3,40ed38db181aff0882689d939c3c0dc69a33d34b,c97,International Conference on Computational Logic,Analyzing Dependent Data with Vine Copulas,Abstract,poster,cp97
Mathematics,p680,d6,40ed38db181aff0882689d939c3c0dc69a33d34b,c97,International Conference on Computational Logic,Analyzing Dependent Data with Vine Copulas,Abstract,poster,cp97
Computer Science,p681,d3,c93b938cc6942497a0b683c54a4b606727b2417a,j185,Proceedings of the IEEE,Fuzzy logic systems for engineering: a tutorial,"A fuzzy logic system (FLS) is unique in that it is able to simultaneously handle numerical data and linguistic knowledge. It is a nonlinear mapping of an input data (feature) vector into a scalar output, i.e., it maps numbers into numbers. Fuzzy set theory and fuzzy logic establish the specifics of the nonlinear mapping. This tutorial paper provides a guided tour through those aspects of fuzzy sets and fuzzy logic that are necessary to synthesize an FLS. It does this by starting with crisp set theory and dual logic and demonstrating how both can be extended to their fuzzy counterparts. Because engineering systems are, for the most part, causal, we impose causality as a constraint on the development of the FLS. After synthesizing a FLS, we demonstrate that it can be expressed mathematically as a linear combination of fuzzy basis functions, and is a nonlinear universal function approximator, a property that it shares with feedforward neural networks. The fuzzy basis function expansion is very powerful because its basis functions can be derived from either numerical data or linguistic knowledge, both of which can be cast into the forms of IF-THEN rules. >",fullPaper,jv185
Mathematics,p681,d6,c93b938cc6942497a0b683c54a4b606727b2417a,j185,Proceedings of the IEEE,Fuzzy logic systems for engineering: a tutorial,"A fuzzy logic system (FLS) is unique in that it is able to simultaneously handle numerical data and linguistic knowledge. It is a nonlinear mapping of an input data (feature) vector into a scalar output, i.e., it maps numbers into numbers. Fuzzy set theory and fuzzy logic establish the specifics of the nonlinear mapping. This tutorial paper provides a guided tour through those aspects of fuzzy sets and fuzzy logic that are necessary to synthesize an FLS. It does this by starting with crisp set theory and dual logic and demonstrating how both can be extended to their fuzzy counterparts. Because engineering systems are, for the most part, causal, we impose causality as a constraint on the development of the FLS. After synthesizing a FLS, we demonstrate that it can be expressed mathematically as a linear combination of fuzzy basis functions, and is a nonlinear universal function approximator, a property that it shares with feedforward neural networks. The fuzzy basis function expansion is very powerful because its basis functions can be derived from either numerical data or linguistic knowledge, both of which can be cast into the forms of IF-THEN rules. >",fullPaper,jv185
Computer Science,p682,d3,532b2033ed22b81333cdc5a60c4a545849388ca9,c24,International Conference on Data Technologies and Applications,Joint VM placement and routing for data center traffic engineering,"Today's data centers need efficient traffic management to improve resource utilization in their networks. In this work, we study a joint tenant (e.g., server or virtual machine) placement and routing problem to minimize traffic costs. These two complementary degrees of freedom-placement and routing-are mutually-dependent, however, are often optimized separately in today's data centers. Leveraging and expanding the technique of Markov approximation, we propose an efficient online algorithm in a dynamic environment under changing traffic loads. The algorithm requires a very small number of virtual machine migrations and is easy to implement in practice. Performance evaluation that employs the real data center traffic traces under a spectrum of elephant and mice flows, demonstrates a consistent and significant improvement over the benchmark achieved by common heuristics.",poster,cp24
Computer Science,p683,d3,cd7b4fdd5d945cb3be828cc544f3239e060b802a,c54,ACM SIGCOMM Symposium on Software Defined Networking Research,Heavy-Hitter Detection Entirely in the Data Plane,"Identifying the ""heavy hitter"" flows or flows with large traffic volumes in the data plane is important for several applications e.g., flow-size aware routing, DoS detection, and traffic engineering. However, measurement in the data plane is constrained by the need for line-rate processing (at 10-100Gb/s) and limited memory in switching hardware. We propose HashPipe, a heavy hitter detection algorithm using emerging programmable data planes. HashPipe implements a pipeline of hash tables which retain counters for heavy flows while evicting lighter flows over time. We prototype HashPipe in P4 and evaluate it with packet traces from an ISP backbone link and a data center. On the ISP trace (which contains over 400,000 flows), we find that HashPipe identifies 95% of the 300 heaviest flows with less than 80KB of memory.",fullPaper,cp54
Computer Science,p684,d3,f463018624b6f4b8dd576732b6cce36e31bac978,c62,International Conference on Advanced Data and Information Engineering,Software Engineering of Self-adaptive Systems,Abstract,poster,cp62
Computer Science,p685,d3,b5839c523c9647b89b0453e2efce12f47380d2f3,c20,ACM Conference on Economics and Computation,"Data Assimilation: Methods, Algorithms, and Applications","Data assimilation is an approach that combines observations and model output, with the objective of improving the latter. This book places data assimilation into the broader context of inverse problems and the theory, methods, and algorithms that are used for their solution. It provides a framework for, and insight into, the inverse problem nature of data assimilation, emphasizing “why” and not just “how.” Methods and diagnostics are emphasized, enabling readers to readily apply them to their own field of study. 
 
Readers will find a comprehensive guide that is accessible to nonexperts; numerous examples and diverse applications from a broad range of domains, including geophysics and geophysical flows, environmental acoustics, medical imaging, mechanical and biomedical engineering, economics and finance, and traffic control and urban planning; and the latest methods for advanced data assimilation, combining variational and statistical approaches.",poster,cp20
Computer Science,p686,d3,4d111edca5bf3b878d9b7e1df43b757da1af86da,c21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,Engineering big data solutions,"Structured and unstructured data in operational support tools have long been prevalent in software engineering. Similar data is now becoming widely available in other domains. Software systems that utilize such operational data (OD) to help with software design and maintenance activities are increasingly being built despite the difficulties of drawing valid conclusions from disparate and low-quality data and the continuing evolution of operational support tools. This paper proposes systematizing approaches to the engineering of OD-based systems. To prioritize and structure research areas we consider historic developments, such as big data hype; synthesize defining features of OD, such as confounded measures and unobserved context; and discuss emerging new applications, such as diverse and large OD collections and extremely short development intervals. To sustain the credibility of OD-based systems more research will be needed to investigate effective existing approaches and to synthesize novel, OD-specific engineering principles.",poster,cp21
Computer Science,p687,d3,93ef4425014a22f9e9466809834dd70573d28750,c51,International Conference on Engineering Education,2019 Novel Coronavirus COVID-19 (2019-nCoV) Data Repository,"This review provides an overview of 2019 Novel Coronavirus COVID-19 (2019-nCoV) Data Repository compiled by the Johns Hopkins University Center for Systems Science and Engineering. It provides a background of how the repository was compiled, the data included and how the repo is being made use of in a Canadian academic library context.",poster,cp51
Computer Science,p688,d3,12d89245440d8c2a57c0741d10177189adf230d3,j186,Open Engineering,"“Zhores” — Petaflops supercomputer for data-driven modeling, machine learning and artificial intelligence installed in Skolkovo Institute of Science and Technology","Abstract The Petaflops supercomputer “Zhores” recently launched in the “Center for Computational and Data-Intensive Science and Engineering” (CDISE) of Skolkovo Institute of Science and Technology (Skoltech) opens up new exciting opportunities for scientific discoveries in the institute especially in the areas of data-driven modeling, machine learning and artificial intelligence. This supercomputer utilizes the latest generation of Intel and NVidia processors to provide resources for the most compute intensive tasks of the Skoltech scientists working in digital pharma, predictive analytics, photonics, material science, image processing, plasma physics and many more. Currently it places 7th in the Russian and CIS TOP-50 (2019) supercomputer list. In this article we summarize the cluster properties and discuss the measured performance and usage modes of this new scientific instrument in Skoltech.",fullPaper,jv186
Computer Science,p689,d3,d094f0faff376af9a0ee79a742b350531b3c89bf,j12,Communications of the ACM,Exascale computing and big data,Scientific discovery and engineering innovation requires unifying traditionally separated high-performance computing and big data analytics.,fullPaper,jv12
Computer Science,p690,d3,0caa61c0ffa59fd74e11368b77e267bc27bf5564,c103,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,Engineering AI Systems: A Research Agenda,"Artificial intelligence (AI) and machine learning (ML) are increasingly broadly adopted in industry. However, based on well over a dozen case studies, we have learned that deploying industry-strength, production quality ML models in systems proves to be challenging. Companies experience challenges related to data quality, design methods and processes, performance of models as well as deployment and compliance. We learned that a new, structured engineering approach is required to construct and evolve systems that contain ML/DL components. In this chapter, the authors provide a conceptualization of the typical evolution patterns that companies experience when employing ML as well as an overview of the key problems experienced by the companies that they have studied. The main contribution of the chapter is a research agenda for AI engineering that provides an overview of the key engineering challenges surrounding ML solutions and an overview of open items that need to be addressed by the research community at large.",poster,cp103
Computer Science,p693,d3,4644afca80b98dc39ee8ad4cf1ce1e940214c366,j187,Empirical Software Engineering,A practical guide on conducting eye tracking studies in software engineering,Abstract,fullPaper,jv187
Computer Science,p694,d3,49d3816939a2ccf0916919695cf023e4b1d7726f,c59,Australian Software Engineering Conference,Requirements Engineering: Processes and Techniques,"Requirements Engineering Processes and Techniques Why this book was written The value of introducing requirements engineering to trainee software engineers is to equip them for the real world of software and systems development. What is involved in Requirements Engineering? As a discipline, newly emerging from software engineering, there are a range of views on where requirements engineering starts and finishes and what it should encompass. This book offers the most comprehensive coverage of the requirements engineering process to date - from initial requirements elicitation through to requirements validation. How and Which methods and techniques should you use? As there is no one catch-all technique applicable to all types of system, requirements engineers need to know about a range of different techniques. Tried and tested techniques such as data-flow and object-oriented models are covered as well as some promising new ones. They are all based on real systems descriptions to demonstrate the applicability of the approach. Who should read it? Principally written for senior undergraduate and graduate students studying computer science, software engineering or systems engineering, this text will also be helpful for those in industry new to requirements engineering. Accompanying Website: http: //www.comp.lancs.ac.uk/computing/resources/re Visit our Website: http://www.wiley.com/college/wws",poster,cp59
Computer Science,p695,d3,5935e5460b8aa62a41e2e29a6b92dce95b441fb1,c120,SIGSAND-Europe Symposium,Big data challenges in railway engineering,"As Big Data becomes part of railroad data analysis, there are many challenges which need to be addressed by the railway industry. This extended abstract highlights some of the challenges from specific examples in railway engineering. This work does not present the challenges of dealing with Big Data in general which is beyond the scope of this paper. The examples provided in this extended abstract cover both the engineering and the management of railroad applications.",poster,cp120
Computer Science,p697,d3,13aff185e35503491bf40d7e2c6219552f462031,c42,IEEE Working Conference on Mining Software Repositories,Data Mining: A Prediction for Performance Improvement of Engineering Students using Classification,"Now-a-days the amount of data stored in educational database increasing rapidly. These databases contain hidden information for improvement of students' performance. Educational data mining is used to study the data available in the educational field and bring out the hidden knowledge from it. Classification methods like decision trees, Bayesian network etc can be applied on the educational data for predicting the student's performance in examination. This prediction will help to identify the weak students and help them to score better marks. The C4.5, ID3 and CART decision tree algorithms are applied on engineering student's data to predict their performance in the final exam. The outcome of the decision tree predicted the number of students who are likely to pass, fail or promoted to next year. The results provide steps to improve the performance of the students who were predicted to fail or promoted. After the declaration of the results in the final examination the marks obtained by the students are fed into the system and the results were analyzed for the next session. The comparative analysis of the results states that the prediction has helped the weaker students to improve and brought out betterment in the result.",poster,cp42
Computer Science,p698,d3,f74d664f0c3451bd4c31cf973a549a6dc00897c6,c112,British Machine Vision Conference,\{PROMISE\} Repository of empirical software engineering data,Abstract,poster,cp112
Computer Science,p701,d3,92b5986906dc286d7008323bd49833eaa2dd5e5f,c4,Conference on Innovative Data Systems Research,Sharing Data and Models in Software Engineering,Abstract,poster,cp4
Computer Science,p702,d3,6704d01bb7788b67bec3194962a57693766bd417,c22,Grid Computing Environments,A Survey of Available Corpora for Building Data-Driven Dialogue Systems,"During the past decade, several areas of speech and language understanding have witnessed substantial breakthroughs from the use of data-driven models. In the area of dialogue systems, the trend is less obvious, and most practical systems are still built through significant engineering and expert knowledge. Nevertheless, several recent results suggest that data-driven approaches are feasible and quite promising. To facilitate research in this area, we have carried out a wide survey of publicly available datasets suitable for data-driven learning of dialogue systems. We discuss important characteristics of these datasets and how they can be used to learn diverse dialogue strategies. We also describe other potential uses of these datasets, such as methods for transfer learning between datasets and the use of external knowledge, and discuss appropriate choice of evaluation metrics for the learning objective.",poster,cp22
Mathematics,p702,d6,6704d01bb7788b67bec3194962a57693766bd417,c22,Grid Computing Environments,A Survey of Available Corpora for Building Data-Driven Dialogue Systems,"During the past decade, several areas of speech and language understanding have witnessed substantial breakthroughs from the use of data-driven models. In the area of dialogue systems, the trend is less obvious, and most practical systems are still built through significant engineering and expert knowledge. Nevertheless, several recent results suggest that data-driven approaches are feasible and quite promising. To facilitate research in this area, we have carried out a wide survey of publicly available datasets suitable for data-driven learning of dialogue systems. We discuss important characteristics of these datasets and how they can be used to learn diverse dialogue strategies. We also describe other potential uses of these datasets, such as methods for transfer learning between datasets and the use of external knowledge, and discuss appropriate choice of evaluation metrics for the learning objective.",poster,cp22
Computer Science,p703,d3,bcc7041e0fb7717a7d67a9c00e08b7fb81384cbf,j187,Empirical Software Engineering,Robust Statistical Methods for Empirical Software Engineering,Abstract,fullPaper,jv187
Computer Science,p705,d3,92ca9f57ebb0cfdbfa07e5d5956e11509f902f0b,c2,International Conference on Software Engineering,A practical guide for using statistical tests to assess randomized algorithms in software engineering,"Randomized algorithms have been used to successfully address many different types of software engineering problems. This type of algorithms employ a degree of randomness as part of their logic. Randomized algorithms are useful for difficult problems where a precise solution cannot be derived in a deterministic way within reasonable time. However, randomized algorithms produce different results on every run when applied to the same problem instance. It is hence important to assess the effectiveness of randomized algorithms by collecting data from a large enough number of runs. The use of rigorous statistical tests is then essential to provide support to the conclusions derived by analyzing such data. In this paper, we provide a systematic review of the use of randomized algorithms in selected software engineering venues in 2009. Its goal is not to perform a complete survey but to get a representative snapshot of current practice in software engineering research. We show that randomized algorithms are used in a significant percentage of papers but that, in most cases, randomness is not properly accounted for. This casts doubts on the validity of most empirical results assessing randomized algorithms. There are numerous statistical tests, based on different assumptions, and it is not always clear when and how to use these tests. We hence provide practical guidelines to support empirical research on randomized algorithms in software engineering",fullPaper,cp2
Computer Science,p706,d3,78706624cf201635f30bf93b10ae23806ca3c204,c112,British Machine Vision Conference,Semantic Matching of Engineering Data Structures,Abstract,poster,cp112
Computer Science,p708,d3,a6e695ddd07aad719001c0fc1129328452385949,c74,International Conference on Computational Linguistics,The New Data and New Challenges in Multimedia Research,"We present the Yahoo Flickr Creative Commons 100 Million Dataset (YFCC100M), the largest public multimedia collection that has ever been released. The dataset contains a total of 100 million media objects, of which approximately 99.2 million are photos and 0.8 million are videos, all of which carry a Creative Commons license. Each media object in the dataset is represented by several pieces of metadata, e.g. Flickr identifier, owner name, camera, title, tags, geo, media source. The collection provides a comprehensive snapshot of how photos and videos were taken, described, and shared over the years, from the inception of Flickr in 2004 until early 2014. In this article we explain the rationale behind its creation, as well as the implications the dataset has for science, research, engineering, and development. We further present several new challenges in multimedia research that can now be expanded upon with our dataset.",poster,cp74
Computer Science,p709,d3,92f160c2ccf58c8098a062abae5f3c99c6951aa0,j4,IEEE Transactions on Knowledge and Data Engineering,Benchmarking Attribute Selection Techniques for Discrete Class Data Mining,"Data engineering is generally considered to be a central issue in the development of data mining applications. The success of many learning schemes, in their attempts to construct models of data, hinges on the reliable identification of a small set of highly predictive attributes. The inclusion of irrelevant, redundant, and noisy attributes in the model building process phase can result in poor predictive performance and increased computation. Attribute selection generally involves a combination of search and attribute utility estimation plus evaluation with respect to specific learning schemes. This leads to a large number of possible permutations and has led to a situation where very few benchmark studies have been conducted. This paper presents a benchmark comparison of several attribute selection methods for supervised classification. All the methods produce an attribute ranking, a useful devise for isolating the individual merit of an attribute. Attribute selection is achieved by cross-validating the attribute rankings with respect to a classification learner to find the best attributes. Results are reported for a selection of standard data sets and two diverse learning schemes C4.5 and naive Bayes.",fullPaper,jv4
Computer Science,p710,d3,37a0fcb9ce00b16b47c7030a50075d149a614762,j156,Lecture Notes in Computer Science,Fundamentals of Software Engineering,Abstract,fullPaper,jv156
Computer Science,p711,d3,918b10db517a3fde10e2b5291186998a33530397,j189,Frontiers of Engineering Management,Data analytics and optimization for smart industry,Abstract,fullPaper,jv189
Computer Science,p712,d3,f6e0aeaaaae8736007657b627c714261f7638c96,c4,Conference on Innovative Data Systems Research,Brainwash: A Data System for Feature Engineering,"A new generation of data processing systems, including web search, Google’s Knowledge Graph, IBM’s Watson, and several different recommendation systems, combine rich databases with software driven by machine learning. The spectacular successes of these trained systems have been among the most notable in all of computing and have generated excitement in health care, finance, energy, and general business. But building them can be challenging, even for computer scientists with PhD-level training. If these systems are to have a truly broad impact, building them must become easier. We explore one crucial pain point in the construction of trained systems: feature engineering. Given the sheer size of modern datasets, feature developers must (1) write code with few effective clues about how their code will interact with the data and (2) repeatedly endure long system waits even though their code typically changes little from run to run. We propose brainwash, a vision for a feature engineering data system that could dramatically ease the ExploreExtract-Evaluate interaction loop that characterizes many trained system projects.",fullPaper,cp4
Computer Science,p713,d3,f77331fc287e4bc76c4b3c464121ec6453fd448b,j190,IEEE Network,When big data meets software-defined networking: SDN for big data and big data for SDN,"Both big data and software-defined networking (SDN) have attracted great interests from both academia and industry. These two important areas have traditionally been addressed separately in the most of previous works. However, on the one hand, the good features of SDN can greatly facilitate big data acquisition, transmission, storage, and processing. On the other hand, big data will have profound impacts on the design and operation of SDN. In this paper, we present the good features of SDN in solving several issues prevailing with big data applications, including big data processing in cloud data centers, data delivery, joint optimization, scientific big data architectures and scheduling issues. We show that SDN can manage the network efficiently for improving the performance of big data applications. In addition, we show that big data can benefit SDN as well, including traffic engineering, cross-layer design, defeating security attacks, and SDN-based intra and inter data center networks. Moreover, we discuss a number of open issues that need to be addressed to jointly consider big data and SDN in future research.",fullPaper,jv190
Computer Science,p715,d3,b10829b87072a6c3faf971bd94c1f0dc71053194,j191,Advanced Engineering Informatics,"A cloud approach to unified lifecycle data management in architecture, engineering, construction and facilities management: Integrating BIMs and SNS",Abstract,fullPaper,jv191
Computer Science,p718,d3,a7797aed6d23d7b599e71ad129211c2834925c0d,c90,International Conference on Collaboration Technologies and Systems,EMG Pattern Recognition in the Era of Big Data and Deep Learning,"The increasing amount of data in electromyographic (EMG) signal research has greatly increased the importance of developing advanced data analysis and machine learning techniques which are better able to handle “big data”. Consequently, more advanced applications of EMG pattern recognition have been developed. This paper begins with a brief introduction to the main factors that expand EMG data resources into the era of big data, followed by the recent progress of existing shared EMG data sets. Next, we provide a review of recent research and development in EMG pattern recognition methods that can be applied to big data analytics. These modern EMG signal analysis methods can be divided into two main categories: (1) methods based on feature engineering involving a promising big data exploration tool called topological data analysis; and (2) methods based on feature learning with a special emphasis on “deep learning”. Finally, directions for future research in EMG pattern recognition are outlined and discussed.",poster,cp90
Computer Science,p719,d3,845afdf05ac75fedb65532487aadd0538bc4c6da,c53,International Conference on Learning Representations,Qualitative Methods in Empirical Studies of Software Engineering,"While empirical studies in software engineering are beginning to gain recognition in the research community, this subarea is also entering a new level of maturity by beginning to address the human aspects of software development. This added focus has added a new layer of complexity to an already challenging area of research. Along with new research questions, new research methods are needed to study nontechnical aspects of software engineering. In many other disciplines, qualitative research methods have been developed and are commonly used to handle the complexity of issues involving human behaviour. The paper presents several qualitative methods for data collection and analysis and describes them in terms of how they might be incorporated into empirical studies of software engineering, in particular how they might be combined with quantitative methods. To illustrate this use of qualitative methods, examples from real software engineering studies are used throughout.",poster,cp53
Computer Science,p722,d3,678731fe120f4faf05de4ee6b5def8305c8155cb,c65,International Symposium on Empirical Software Engineering and Measurement,Data Processing of Point Clouds for Object Detection for Structural Engineering Applications,"This research investigates the use of high‐resolution three‐dimensional terrestrial laser scanners as tools to capture geometric range data of complex scenes for structural engineering applications. Laser scanning technology is continuously improving, with commonly available scanners now able to capture over 1,000,000 points per second with an accuracy of ∼0.1 mm. This research focuses on developing the foundation toward the use of laser scanning to structural engineering applications, including structural health monitoring, collapse assessment, and post‐hazard response assessment. One of the keys to this work is to establish a process for extracting important information from raw laser‐scanned data sets such as the location, orientation, and size of objects in a scene, and location of damaged regions on a structure. A methodology for processing range data to identify objects in the scene is presented. Previous work in this area has created an initial foundation of basic data processing steps. Existing algorithms, including sharp feature detection and segmentation are implemented and extended in this work. Additional steps to remove extraneous and outlying points are added. Object detection based on a predefined library is developed allowing generic description of objects. The algorithms are demonstrated on synthetic scenes as well as validated on range data collected from an experimental test specimen and a collapsed bridge. The accuracy of the object detection is presented, demonstrating the applicability of the methodology. These additional steps and modifications to existing algorithms are presented to advance the performance of data processing on laser scan range data sets for future application in structural engineering applications such as robust determination of damage location and finite element modeling.",poster,cp65
Computer Science,p723,d3,acdc89274e34d2f905a055aad41e7f815aef52eb,c82,Symposium on Networked Systems Design and Implementation,Selecting Empirical Methods for Software Engineering Research,Abstract,poster,cp82
Computer Science,p726,d3,ee25620eea19f793d598dbbf6241cf7520a60ccd,c56,International Conference on Automated Software Engineering,Patching as Translation: the Data and the Metaphor,"Machine Learning models from other fields, like Computational Linguistics, have been transplanted to Software Engineering tasks, often quite successfully. Yet a transplanted model's initial success at a given task does not necessarily mean it is well-suited for the task. In this work, we examine a common example of this phenomenon: the conceit that software patching is like language translation. We demonstrate empirically that there are subtle, but critical distinctions between sequence-to-sequence models and translation model: while program repair benefits greatly from the former, general modeling architecture, it actually suffers from design decisions built into the latter, both in terms of translation accuracy and diversity. Given these findings, we demonstrate how a more principled approach to model design, based on our empirical findings and general knowledge of software development, can lead to better solutions. Our findings also lend strong support to the recent trend towards synthesizing edits of code conditional on the buggy context, to repair bugs. We implement such models ourselves as “proof-of-concept” tools and empirically confirm that they behave in a fundamentally different, more effective way than the studied translation-based architectures. Overall, our results demonstrate the merit of studying the intricacies of machine learned models in software engineering: not only can this help elucidate potential issues that may be overshadowed by increases in accuracy; it can also help innovate on these models to raise the state-of-the-art further. We will publicly release our replication data and materials at https://github.com/ARiSE-Lab/Patch-as-translation.",fullPaper,cp56
Computer Science,p727,d3,f759d01c4cc9633a3732c353ed0ce8f3009f52af,j187,Empirical Software Engineering,Naming the pain in requirements engineering,Abstract,fullPaper,jv187
Computer Science,p728,d3,4f9b916ae1c38b991f39b6ebd6e0a92f88ece698,c57,IEEE International Geoscience and Remote Sensing Symposium,Building detection in very high resolution multispectral data with deep learning features,"The automated man-made object detection and building extraction from single satellite images is, still, one of the most challenging tasks for various urban planning and monitoring engineering applications. To this end, in this paper we propose an automated building detection framework from very high resolution remote sensing data based on deep convolutional neural networks. The core of the developed method is based on a supervised classification procedure employing a very large training dataset. An MRF model is then responsible for obtaining the optimal labels regarding the detection of scene buildings. The experimental results and the performed quantitative validation indicate the quite promising potentials of the developed approach.",fullPaper,cp57
Computer Science,p730,d3,a4a582c6739c8f6d88f3ad01671b5b6733eb464c,c58,Extreme Science and Engineering Discovery Environment,Bridges: a uniquely flexible HPC resource for new communities and data analytics,"In this paper, we describe Bridges, a new HPC resource that will integrate advanced memory technologies with a uniquely flexible, user-focused, data-centric environment to empower new research communities, bring desktop convenience to HPC, connect to campuses, and drive complex workflows. Bridges will differ from traditional HPC systems and support new communities through extensive interactivity, gateways (convenient web interfaces that hide complex functionality and ease access to HPC resources) and tools for gateway building, persistent databases and web servers, high-productivity programming languages, and virtualization. Bridges will feature three tiers of processing nodes having 128GB, 3TB, and 12TB of hardware-enabled coherent shared memory per node to support memory-intensive applications and ease of use, together with persistent database and web nodes and nodes for logins, data transfer, and system management. State-of-the-art Intel® Xeon® CPUs and NVIDIA Tesla GPUs will power Bridges' compute nodes. Multiple filesystems will provide optimal handling for different data needs: a high-performance, parallel, shared filesystem, node-local filesystems, and memory filesystems. Bridges' nodes and parallel filesystem will be interconnected by the Intel Omni-Path Fabric, configured in a topology developed by PSC to be optimal for the anticipated data-centric workload. Bridges will be a resource on XSEDE, the NSF Extreme Science and Engineering Discovery Environment, and will interoperate with other advanced cyberinfrastructure resources. Through a pilot project with Temple University, Bridges will develop infrastructure and processes for campus bridging, consisting of offloading jobs at periods of unusually high load to the other site and facilitating cross-site data management. Education, training, and outreach activities will raise awareness of Bridges and data-intensive science across K-12 and university communities, industry, and the general public.",fullPaper,cp58
Computer Science,p731,d3,d1cc35e2a547ba79f1b07fdd81ee0da264c0d6b6,c2,International Conference on Software Engineering,Belief & Evidence in Empirical Software Engineering,"Empirical software engineering has produced a steady stream of evidence-based results concerning the factors that affect important outcomes such as cost, quality, and interval. However, programmers often also have strongly-held a priori opinions about these issues. These opinions are important, since developers are highlytrained professionals whose beliefs would doubtless affect their practice. As in evidence-based medicine, disseminating empirical findings to developers is a key step in ensuring that the findings impact practice. In this paper, we describe a case study, on the prior beliefs of developers at Microsoft, and the relationship of these beliefs to actual empirical data on the projects in which these developers work. Our findings are that a) programmers do indeed have very strong beliefs on certain topics b) their beliefs are primarily formed based on personal experience, rather than on findings in empirical research and c) beliefs can vary with each project, but do not necessarily correspond with actual evidence in that project. Our findings suggest that more effort should be taken to disseminate empirical findings to developers and that more in-depth study the interplay of belief and evidence in software practice is needed.",fullPaper,cp2
Computer Science,p733,d3,c575eb25feb0f06d2702fcf7751f6b4f61b892ee,c2,International Conference on Software Engineering,The Emerging Role of Data Scientists on Software Development Teams,"Creating and running software produces large amounts of raw data about the development process and the customer usage, which can be turned into actionable insight with the help of skilled data scientists. Unfortunately, data scientists with the analytical and software engineering skills to analyze these large data sets have been hard to come by; only recently have software companies started to develop competencies in software-oriented data analytics. To understand this emerging role, we interviewed data scientists across several product groups at Microsoft. In this paper, we describe their education and training background, their missions in software engineering contexts, and the type of problems on which they work. We identify five distinct working styles of data scientists: (1) Insight Providers, who work with engineers to collect the data needed to inform decisions that managers make; (2) Modeling Specialists, who use their machine learning expertise to build predictive models; (3) Platform Builders, who create data platforms, balancing both engineering and data analysis concerns; (4) Polymaths, who do all data science activities themselves; and (5) Team Leaders, who run teams of data scientists and spread best practices. We further describe a set of strategies that they employ to increase the impact and actionability of their work.",fullPaper,cp2
Computer Science,p735,d3,050d2be0b031878282e2bc626ffe31e064679188,c50,Conference on Emerging Network Experiment and Technology,Big Social Data Analytics in Journalism and Mass Communication,"This article presents an empirical study that investigated and compared two “big data” text analysis methods: dictionary-based analysis, perhaps the most popular automated analysis approach in social science research, and unsupervised topic modeling (i.e., Latent Dirichlet Allocation [LDA] analysis), one of the most widely used algorithms in the field of computer science and engineering. By applying two “big data” methods to make sense of the same dataset—77 million tweets about the 2012 U.S. presidential election—the study provides a starting point for scholars to evaluate the efficacy and validity of different computer-assisted methods for conducting journalism and mass communication research, especially in the area of political communication.",poster,cp50
Computer Science,p737,d3,1f2a86ab0d88b9bb31a7cafd1c1c3fcb4e193547,c2,International Conference on Software Engineering,[Journal First] Data Scientists in Software Teams: State of the Art and Challenges,"The demand for analyzing large scale telemetry, machine, and quality data is rapidly increasing in software industry. Data scientists are becoming popular within software teams. For example, Facebook, LinkedIn and Microsoft are creating a new career path for data scientists. In this paper, we present a large-scale survey with 793 professional data scientists at Microsoft to understand their educational background, problem topics that they work on, tool usages, and activities. We cluster these data scientists based on the time spent for various activities and identify 9 distinct clusters of data scientists and their corresponding characteristics. We also discuss the challenges that they face and the best practices they share with other data scientists. Our study finds several trends about data scientists in the software engineering context at Microsoft, and should inform managers on how to leverage data science capability effectively within their teams.",fullPaper,cp2
Computer Science,p740,d3,267fd0014fb61d46f764dc5d0c720507a3a0a583,j195,IEEE Transactions on Learning Technologies,Mining Social Media Data for Understanding Students’ Learning Experiences,"Students' informal conversations on social media (e.g., Twitter, Facebook) shed light into their educational experiences-opinions, feelings, and concerns about the learning process. Data from such uninstrumented environments can provide valuable knowledge to inform student learning. Analyzing such data, however, can be challenging. The complexity of students' experiences reflected from social media content requires human interpretation. However, the growing scale of data demands automatic data analysis techniques. In this paper, we developed a workflow to integrate both qualitative analysis and large-scale data mining techniques. We focused on engineering students' Twitter posts to understand issues and problems in their educational experiences. We first conducted a qualitative analysis on samples taken from about 25,000 tweets related to engineering students' college life. We found engineering students encounter problems such as heavy study load, lack of social engagement, and sleep deprivation. Based on these results, we implemented a multi-label classification algorithm to classify tweets reflecting students' problems. We then used the algorithm to train a detector of student problems from about 35,000 tweets streamed at the geo-location of Purdue University. This work, for the first time, presents a methodology and results that show how informal social media data can provide insights into students' experiences.",fullPaper,jv195
Computer Science,p742,d3,ba69dc1fe7da073efcadcde60ba22f085f440b61,j4,IEEE Transactions on Knowledge and Data Engineering,A Study on Big Knowledge and Its Engineering Issues,"After entering the big data era, a new term of ‘big knowledge’ has been coined to deal with challenges in mining a mass of knowledge from big data. While researchers used to explore the basic characteristics of big data, we have not seen any studies on the general and essential properties of big knowledge. To fill this gap, this paper studies the concepts of big knowledge, big-knowledge system, and big-knowledge engineering. Ten massiveness characteristics for big knowledge and big-knowledge systems, including massive concepts, connectedness, clean data resources, cases, confidence, capabilities, cumulativeness, concerns, consistency, and completeness, are defined and explored. Based on these characteristics, a comprehensive investigation is conducted on some large-scale knowledge engineering projects, including the Fifth Comprehensive Traffic Survey in Shanghai, the China's Xia-Shang-Zhou Chronology Project, the Troy and Trojan War Project, and the International Human Genome Project, as well as the online free encyclopedia Wikipedia. We also investigate the recent research efforts on knowledge graphs, where they are analyzed to determine which ones can be considered as big knowledge and big-knowledge systems. Further, a definition of big-knowledge engineering and its life cycle paradigm is presented. All of these projects are accordingly checked to determine whether they belong to big-knowledge engineering projects. Finally, the perspectives of big knowledge research are discussed.",fullPaper,jv4
Computer Science,p743,d3,402fac84b796104a63b7f917307347040fa616bf,c30,PS,A robust data mining approach for formulation of geotechnical engineering systems,"Purpose – The complexity of analysis of geotechnical behavior is due to multivariable dependencies of soil and rock responses. In order to cope with this complex behavior, traditional forms of engineering design solutions are reasonably simplified. Incorporating simplifying assumptions into the development of the traditional models may lead to very large errors. The purpose of this paper is to illustrate capabilities of promising variants of genetic programming (GP), namely linear genetic programming (LGP), gene expression programming (GEP), and multi‐expression programming (MEP) by applying them to the formulation of several complex geotechnical engineering problems.Design/methodology/approach – LGP, GEP, and MEP are new variants of GP that make a clear distinction between the genotype and the phenotype of an individual. Compared with the traditional GP, the LGP, GEP, and MEP techniques are more compatible with computer architectures. This results in a significant speedup in their execution. These method...",poster,cp30
Computer Science,p745,d3,49f9a1caabcf5a24ee0dbdc829562b81b7867b72,j196,"Software, Practice & Experience",The anatomy of big data computing,"Advances in information technology and its widespread growth in several areas of business, engineering, medical, and scientific studies are resulting in information/data explosion. Knowledge discovery and decision‐making from such rapidly growing voluminous data are a challenging task in terms of data organization and processing, which is an emerging trend known as big data computing, a new paradigm that combines large‐scale compute, new data‐intensive techniques, and mathematical models to build data analytics. Big data computing demands a huge storage and computing for data curation and processing that could be delivered from on‐premise or clouds infrastructures. This paper discusses the evolution of big data computing, differences between traditional data warehousing and big data, taxonomy of big data computing and underpinning technologies, integrated platform of big data and clouds known as big data clouds, layered architecture and components of big data cloud, and finally open‐technical challenges and future directions. Copyright © 2015 John Wiley & Sons, Ltd.",fullPaper,jv196
Computer Science,p746,d3,6ff5544640e0ac1e41b98c98a8ef2cd0ba62e9b5,c54,ACM SIGCOMM Symposium on Software Defined Networking Research,Cloud Paradigms and Practices for Computational and Data-Enabled Science and Engineering,"Clouds are rapidly joining high-performance computing (HPC) systems, clusters, and grids as viable platforms for scientific exploration and discovery. As a result, understanding application formulations and usage modes that are meaningful in such a hybrid infrastructure, and how application workflows can effectively utilize it, is critical. Here, three hybrid HPC/grid and cloud cyber infrastructure usage modes are explored: HPC in the Cloud, HPC plus Cloud, and HPC as a Service, presenting illustrative scenarios in each case and outlining benefits, limitations, and research challenges.",poster,cp54
Computer Science,p747,d3,0442b04b4e8741900b65de0721f0c3e152e044ef,c92,International Symposium on Computer Architecture,Materials Data Science: Current Status and Future Outlook,"The field of materials science and engineering is on the cusp of a digital data revolution. After reviewing the nature of data science and Big Data, we discuss the features of materials data that distinguish them from data in other fields. We introduce the concept of process-structure-property (PSP) linkages and illustrate how the determination of PSPs is one of the main objectives of materials data science. Then we review a selection of materials databases, as well as important aspects of materials data management, such as storage hardware, archiving strategies, and data access strategies. We introduce the emerging field of materials data analytics, which focuses on data-driven approaches to extract and curate materials knowledge from available data sets. The critical need for materials e-collaboration platforms is highlighted, and we conclude the article with a number of suggestions regarding the near-term future of the materials data science field.",poster,cp92
Computer Science,p748,d3,64cc4ef5def3919049bdd3a645af198922d626c2,c2,International Conference on Software Engineering,An Empirical Study of Practitioners' Perspectives on Green Software Engineering,"The energy consumption of software is an increasing concern as the use of mobile applications, embedded systems, and data center-based services expands. While research in green software engineering is correspondingly increasing, little is known about the current practices and perspectives of software engineers in the field. This paper describes the first empirical study of how practitioners think about energy when they write requirements, design, construct, test, and maintain their software. We report findings from a quantitative,targeted survey of 464 practitioners from ABB, Google, IBM, and Microsoft, which was motivated by and supported with qualitative data from 18 in-depth interviews with Microsoft employees. The major findings and implications from the collected data contextualize existing green software engineering research and suggest directions for researchers aiming to develop strategies and tools to help practitioners improve the energy usage of their applications.",fullPaper,cp2
Computer Science,p749,d3,a4a84f5b19aa8c1eb8b31db67d2534ad3565ccab,c81,ACM Symposium on Applied Computing,Visualization of Time-Oriented Data,Abstract,poster,cp81
Computer Science,p750,d3,9ada0c69d4d8cb6fefb8f2dd3370d32df3b627c5,j187,Empirical Software Engineering,Software engineering in start-up companies: An analysis of 88 experience reports,Abstract,fullPaper,jv187
Computer Science,p752,d3,efc91095d187abafeab8785eb57bbdef66c82d1c,c39,Online World Conference on Soft Computing in Industrial Applications,Data and Computer Communications,"Data and Computer Communications, 9e, is a two-time winner of the best Computer Science and Engineering textbook of the year award from the Textbook and Academic Authors Association. It is ideal for one/two-semester courses in Computer Networks, Data Communications, and Communications Networks in CS, CIS, and Electrical Engineering departments. With a focus on the most current technology and a convenient modular format, this best-selling text offers a clear and comprehensive survey of the entire data and computer communications field. Emphasizing both the fundamental principles as well as the critical role of performance in driving protocol and network design, it explores in detail all the critical technical areas in data communications, wide-area networking, local area networking, and protocol design.",poster,cp39
Computer Science,p753,d3,8de121442c5df6ebd1d93c132086c80ae7613b06,c92,International Symposium on Computer Architecture,Handbook of software reliability engineering,Technical foundations introduction software reliability and system reliability the operational profile software reliability modelling survey model evaluation and recalibration techniques practices and experiences best current practice of SRE software reliability measurement experience measurement-based analysis of software reliability software fault and failure classification techniques trend analysis in validation and maintenance software reliability and field data analysis software reliability process assessment emerging techniques software reliability prediction metrics software reliability and testing fault-tolerant SRE software reliability using fault trees software reliability process simulation neural networks and software reliability. Appendices: software reliability tools software failure data set repository.,poster,cp92
Computer Science,p754,d3,fd51bef5d4c12bb1fa49457e5a44fef0b8bc1295,c4,Conference on Innovative Data Systems Research,Ground: A Data Context Service,"Ground is an open-source data context service , a system to manage all the information that informs the use of data. Data usage has changed both philosophically and practically in the last decade, creating an opportunity for new data context services to foster further innovation. In this paper we frame the challenges of managing data context with basic ABCs: Applications , Behavior , and Change . We provide motivation and design guidelines, present our initial design of a common metamodel and API, and explore the current state of the storage solutions that could serve the needs of a data context service. Along the way we highlight opportunities for new research and engineering solutions.",fullPaper,cp4
Computer Science,p755,d3,301311f883cb3df1b1c00077ddf5f2fc0ed2f4f8,c37,International Workshop on the Semantic Web,Traffic engineering in software defined networks,"Software Defined Networking is a new networking paradigm that separates the network control plane from the packet forwarding plane and provides applications with an abstracted centralized view of the distributed network state. A logically centralized controller that has a global network view is responsible for all the control decisions and it communicates with the network-wide distributed forwarding elements via standardized interfaces. Google recently announced [5] that it is using a Software Defined Network (SDN) to interconnect its data centers due to the ease, efficiency and flexibility in performing traffic engineering functions. It expects the SDN architecture to result in better network capacity utilization and improved delay and loss performance. The contribution of this paper is on the effective use of SDNs for traffic engineering especially when SDNs are incrementally introduced into an existing network. In particular, we show how to leverage the centralized controller to get significant improvements in network utilization as well as to reduce packet losses and delays. We show that these improvements are possible even in cases where there is only a partial deployment of SDN capability in a network. We formulate the SDN controller's optimization problem for traffic engineering with partial deployment and develop fast Fully Polynomial Time Approximation Schemes (FPTAS) for solving these problems. We show, by both analysis and ns-2 simulations, the performance gains that are achievable using these algorithms even with an incrementally deployed SDN.",poster,cp37
Computer Science,p758,d3,0e528eb8167c68930c2e1ab20ab5c14f98446927,j170,IEEE Transactions on Software Engineering,Investigating Country Differences in Mobile App User Behavior and Challenges for Software Engineering,"Mobile applications (apps) are software developed for use on mobile devices and made available through app stores. App stores are highly competitive markets where developers need to cater to a large number of users spanning multiple countries. This work hypothesizes that there exist country differences in mobile app user behavior and conducts one of the largest surveys to date of app users across the world, in order to identify the precise nature of those differences. The survey investigated user adoption of the app store concept, app needs, and rationale for selecting or abandoning an app. We collected data from more than 15 countries, including USA, China, Japan, Germany, France, Brazil, United Kingdom, Italy, Russia, India, Canada, Spain, Australia, Mexico, and South Korea. Analysis of data provided by 4,824 participants showed significant differences in app user behaviors across countries, for example users from USA are more likely to download medical apps, users from the United Kingdom and Canada are more likely to be influenced by price, users from Japan and Australia are less likely to rate apps. Analysis of the results revealed new challenges to market-driven software engineering related to packaging requirements, feature space, quality expectations, app store dependency, price sensitivity, and ecosystem effect.",fullPaper,jv170
Computer Science,p759,d3,43f3a76901595b91832e12e0b5f27a6ef4e9fb92,j187,Empirical Software Engineering,On the reproducibility of empirical software engineering studies based on data retrieved from development repositories,Abstract,fullPaper,jv187
Computer Science,p760,d3,eebd35be4d21e1485733c406ff6eebf7746e84ac,c59,Australian Software Engineering Conference,A Taxonomy of Data Quality Challenges in Empirical Software Engineering,"Reliable empirical models such as those used in software effort estimation or defect prediction are inherently dependent on the data from which they are built. As demands for process and product improvement continue to grow, the quality of the data used in measurement and prediction systems warrants increasingly close scrutiny. In this paper we propose a taxonomy of data quality challenges in empirical software engineering, based on an extensive review of prior research. We consider current assessment techniques for each quality issue and proposed mechanisms to address these issues, where available. Our taxonomy classifies data quality issues into three broad areas: first, characteristics of data that mean they are not fit for modeling, second, data set characteristics that lead to concerns about the suitability of applying a given model to another data set, and third, factors that prevent or limit data accessibility and trust. We identify this latter area as of particular need in terms of further research.",fullPaper,cp59
Computer Science,p761,d3,660fa773b70629aa8c36f4e11812b2b07fcfa04d,c4,Conference on Innovative Data Systems Research,Starfish: A Self-tuning System for Big Data Analytics,"Timely and cost-effective analytics over “Big Data” is now a key ingredient for success in many businesses, scientific and engineering disciplines, and government endeavors. The Hadoop software stack—which consists of an extensible MapReduce execution engine, pluggable distributed storage engines, and a range of procedural to declarative interfaces—is a popular choice for big data analytics. Most practitioners of big data analytics—like computational scientists, systems researchers, and business analysts—lack the expertise to tune the system to get good performance. Unfortunately, Hadoop’s performance out of the box leaves much to be desired, leading to suboptimal use of resources, time, and money (in payas-you-go clouds). We introduce Starfish, a self-tuning system for big data analytics. Starfish builds on Hadoop while adapting to user needs and system workloads to provide good performance automatically, without any need for users to understand and manipulate the many tuning knobs in Hadoop. While Starfish’s system architecture is guided by work on self-tuning database systems, we discuss how new analysis practices over big data pose new challenges; leading us to different design choices in Starfish.",fullPaper,cp4
Computer Science,p762,d3,30dd12a894eff2a488c83f565bc287b4dd03c0cc,c60,Network and Distributed System Security Symposium,Howard: A Dynamic Excavator for Reverse Engineering Data Structures,"Even the most advanced reverse engineering techniques and products are weak in recovering data structures in stripped binaries—binaries without symbol tables. Unfortunately, forensics and reverse engineering without data structures is exceedingly hard. We present a new solution, known as Howard, to extract data structures from C binaries without any need for symbol tables. Our results are significantly more accurate than those of previous methods — sufficiently so to allow us to generate our own (partial) symbol tables without access to source code. Thus, debugging such binaries becomes feasible and reverse engineering becomes simpler. Also, we show that we can protect existing binaries from popular memory corruption attacks, without access to source code. Unlike most existing tools, our system uses dynamic analysis (on a QEMU-based emulator) and detects data structures by tracking how a program uses memory.",fullPaper,cp60
Computer Science,p767,d3,e7d9c306138b3a583c48d9a3d46a5597221deaae,c114,Chinese Conference on Biometric Recognition,Semantic data mining: A survey of ontology-based approaches,"Semantic Data Mining refers to the data mining tasks that systematically incorporate domain knowledge, especially formal semantics, into the process. In the past, many research efforts have attested the benefits of incorporating domain knowledge in data mining. At the same time, the proliferation of knowledge engineering has enriched the family of domain knowledge, especially formal semantics and Semantic Web ontologies. Ontology is an explicit specification of conceptualization and a formal way to define the semantics of knowledge and data. The formal structure of ontology makes it a nature way to encode domain knowledge for the data mining use. In this survey paper, we introduce general concepts of semantic data mining. We investigate why ontology has the potential to help semantic data mining and how formal semantics in ontologies can be incorporated into the data mining process. We provide detail discussions for the advances and state of art of ontology-based approaches and an introduction of approaches that are based on other form of knowledge representations.",poster,cp114
Computer Science,p769,d3,bc4949c27b55a5644e6d68c8e5970f8a6a1d5436,c12,The Compass,Support vector machines in engineering: an overview,"This paper provides an overview of the support vector machine (SVM) methodology and its applicability to real‐world engineering problems. Specifically, the aim of this study is to review the current state of the SVM technique, and to show some of its latest successful results in real‐world problems present in different engineering fields. The paper starts by reviewing the main basic concepts of SVMs and kernel methods. Kernel theory, SVMs, support vector regression (SVR), and SVM in signal processing and hybridization of SVMs with meta‐heuristics are fully described in the first part of this paper. The adoption of SVMs in engineering is nowadays a fact. As we illustrate in this paper, SVMs can handle high‐dimensional, heterogeneous and scarcely labeled datasets very efficiently, and it can be also successfully tailored to particular applications. The second part of this review is devoted to different case studies in engineering problems, where the application of the SVM methodology has led to excellent results. First, we discuss the application of SVR algorithms in two renewable energy problems: the wind speed prediction from measurements in neighbor stations and the wind speed reconstruction using synoptic‐pressure data. The application of SVMs in noninvasive cardiac indices estimation is described next, and results obtained there are presented. The application of SVMs in problems of functional magnetic resonance imaging (fMRI) data processing is further discussed in the paper: brain decoding and mental disorder characterization. The following application deals with antenna array processing, namely SVMs for spatial nonlinear beamforming, and the SVM application in a problem of arrival angle detection. Finally, the application of SVMs to remote sensing image classification and target detection problems closes this review. WIREs Data Mining Knowl Discov 2014, 4:234–267. doi: 10.1002/widm.1125",poster,cp12
Computer Science,p770,d3,5f06b4590a179a529182cd35f65773bb261f9b98,j62,Business & Information Systems Engineering,Recombinant Service Systems Engineering,Abstract,fullPaper,jv62
Computer Science,p774,d3,9d0cf900881c995d07f3967d7c81d2950d2541ea,c62,International Conference on Advanced Data and Information Engineering,"Proceedings of the First International Conference on Advanced Data and Information Engineering, DaEng 2013, Kuala Lumpur, Malaysia, December 16-18, 2013",Abstract,fullPaper,cp62
Computer Science,p775,d3,878bcf59cb8efc0bb8341dff78cdb80d7ff076da,c63,International Conference on Evaluation & Assessment in Software Engineering,Data quality in empirical software engineering: a targeted review,"Context: The utility of prediction models in empirical software engineering (ESE) is heavily reliant on the quality of the data used in building those models. Several data quality challenges such as noise, incompleteness, outliers and duplicate data points may be relevant in this regard. Objective: We investigate the reporting of three potentially influential elements of data quality in ESE studies: data collection, data pre-processing, and the identification of data quality issues. This enables us to establish how researchers view the topic of data quality and the mechanisms that are being used to address it. Greater awareness of data quality should inform both the sound conduct of ESE research and the robust practice of ESE data collection and processing. Method: We performed a targeted literature review of empirical software engineering studies covering the period January 2007 to September 2012. A total of 221 relevant studies met our inclusion criteria and were characterized in terms of their consideration and treatment of data quality. Results: We obtained useful insights as to how the ESE community considers these three elements of data quality. Only 23 of these 221 studies reported on all three elements of data quality considered in this paper. Conclusion: The reporting of data collection procedures is not documented consistently in ESE studies. It will be useful if data collection challenges are reported in order to improve our understanding of why there are problems with software engineering data sets and the models developed from them. More generally, data quality should be given far greater attention by the community. The improvement of data sets through enhanced data collection, pre-processing and quality assessment should lead to more reliable prediction models, thus improving the practice of software engineering.",fullPaper,cp63
Computer Science,p776,d3,b3e44942580a6254706804ef95c22fe9ac13f92e,c64,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",Traffic engineering with forward fault correction,"Faults such as link failures and high switch configuration delays can cause heavy congestion and packet loss. Because it takes time to detect and react to faults, these conditions can last long---even tens of seconds. We propose forward fault correction (FFC), a proactive approach to handling faults. FFC spreads network traffic such that freedom from congestion is guaranteed under arbitrary combinations of up to k faults. We show how FFC can be practically realized by compactly encoding the constraints that arise from this large number of possible faults and solving them efficiently using sorting networks. Experiments with data from real networks show that, with negligible loss in overall network throughput, FFC can reduce data loss by a factor of 7--130 in well-provisioned networks, and reduce the loss of high-priority traffic to almost zero in well-utilized networks.",fullPaper,cp64
Computer Science,p777,d3,2ad27a03e7c120c3d1fe31368a7a897960afd8cb,j170,IEEE Transactions on Software Engineering,Data Quality: Some Comments on the NASA Software Defect Datasets,"Background--Self-evidently empirical analyses rely upon the quality of their data. Likewise, replications rely upon accurate reporting and using the same rather than similar versions of datasets. In recent years, there has been much interest in using machine learners to classify software modules into defect-prone and not defect-prone categories. The publicly available NASA datasets have been extensively used as part of this research. Objective--This short note investigates the extent to which published analyses based on the NASA defect datasets are meaningful and comparable. Method--We analyze the five studies published in the IEEE Transactions on Software Engineering since 2007 that have utilized these datasets and compare the two versions of the datasets currently in use. Results--We find important differences between the two versions of the datasets, implausible values in one dataset and generally insufficient detail documented on dataset preprocessing. Conclusions--It is recommended that researchers 1) indicate the provenance of the datasets they use, 2) report any preprocessing in sufficient detail to enable meaningful replication, and 3) invest effort in understanding the data prior to applying machine learners.",fullPaper,jv170
Computer Science,p778,d3,7599dfed1de67c726f9e4fd372cc9ef03d2cf3e9,c106,International Conference on Biometrics,Feature engineering in Context-Dependent Deep Neural Networks for conversational speech transcription,"We investigate the potential of Context-Dependent Deep-Neural-Network HMMs, or CD-DNN-HMMs, from a feature-engineering perspective. Recently, we had shown that for speaker-independent transcription of phone calls (NIST RT03S Fisher data), CD-DNN-HMMs reduced the word error rate by as much as one third—from 27.4%, obtained by discriminatively trained Gaussian-mixture HMMs with HLDA features, to 18.5%—using 300+ hours of training data (Switchboard), 9000+ tied triphone states, and up to 9 hidden network layers.",poster,cp106
Computer Science,p779,d3,6ef57ad9b72635e53f07dad114f8e231d02155cf,c116,International Society for Music Information Retrieval Conference,Managing Big Data for Scientific Visualization,"Many areas of endeavor have problems with big data. Some classical business applications have faced big data for some time (e.g. airline reservation systems), and newer business applications to exploit big data are under construction (e.g. data warehouses, federations of databases). While engineering and scientific visualization have also faced the problem for some time, solutions are less well developed, and common techniques are less well understood. In this section we offer some structure to understand what has been done to manage big data for engineering and scientific visualization, and to understand and go forward in areas that may prove fruitful. With this structure as backdrop, we discuss the work that has been done in management of big data, as well as our own work on demand-paged segments for fluid flow visualization.",poster,cp116
Computer Science,p780,d3,ab2ec1a1b6fdb710b2782554086a4782d50fc2c8,c65,International Symposium on Empirical Software Engineering and Measurement,Recommended Steps for Thematic Synthesis in Software Engineering,"Thematic analysis is an approach that is often used for identifying, analyzing, and reporting patterns (themes) within data in primary qualitative research. 'Thematic synthesis' draws on the principles of thematic analysis and identifies the recurring themes or issues from multiple studies, interprets and explains these themes, and draws conclusions in systematic reviews. This paper conceptualizes the thematic synthesis approach in software engineering as a scientific inquiry involving five steps that parallel those of primary research. The process and outcome associated with each step are described and illustrated with examples from systematic reviews in software engineering.",fullPaper,cp65
Computer Science,p781,d3,2c763b13fa9c51d150940f9b7e5bd8e376cccf7f,j170,IEEE Transactions on Software Engineering,Data Scientists in Software Teams: State of the Art and Challenges,"The demand for analyzing large scale telemetry, machine, and quality data is rapidly increasing in software industry. Data scientists are becoming popular within software teams, e.g., Facebook, LinkedIn and Microsoft are creating a new career path for data scientists. In this paper, we present a large-scale survey with 793 professional data scientists at Microsoft to understand their educational background, problem topics that they work on, tool usages, and activities. We cluster these data scientists based on the time spent for various activities and identify 9 distinct clusters of data scientists, and their corresponding characteristics. We also discuss the challenges that they face and the best practices they share with other data scientists. Our study finds several trends about data scientists in the software engineering context at Microsoft, and should inform managers on how to leverage data science capability effectively within their teams.",fullPaper,jv170
Computer Science,p782,d3,e1946e597b27cd4526ee71800f6454b8dcb5d4d1,c66,International Conference on Web and Social Media,Fuzzy mathematical models in engineering and management science,"Theoretical Concepts. Fuzzy Set Theory and System Modelling. Theory of Fuzzy Sets. Theory of Fuzzy Numbers. Linear Ordering of Fuzzy Numbers. Evaluation of Imprecision in Fuzzy Numbers. Triangular Approximation of Various Functions of Triangular Fuzzy Numbers. Deconvolution of the Fuzzy Equations A(+)B=C, and A( . )B=C in R. T-Norms and T-Conorms. Fuzzy Numbers in [0,1]. Fuzzy Numbers in [0,1] with Higher-Order Intervals of Confidence in [0,1]. Models in Engineering and Management Science. Modelling Issues in Engineering and Management Science. Fuzzy Zero-Base Budgeting (F.Z.B.B.). Fuzzy Delphi Method (F.D.M.) in Forecasting and Decision Making. Discounting Problem using Fuzzy Numbers. Smoothing (Filtering) of Fuzzy Data. Reliability Modelling and Evaluation with Fuzzy Data. Ordering of Fuzzy Quotients. Critical Path Method (C.P.M.) with Fuzzy Data. Investment Problem with Fuzzy Data. Transportation Optimization with Fuzzy Data: (Fuzzy Stepping Stone Method). A General View about the Fuzzification of Models in Engineering and Management Science. Appendices.",poster,cp66
Mathematics,p782,d6,e1946e597b27cd4526ee71800f6454b8dcb5d4d1,c66,International Conference on Web and Social Media,Fuzzy mathematical models in engineering and management science,"Theoretical Concepts. Fuzzy Set Theory and System Modelling. Theory of Fuzzy Sets. Theory of Fuzzy Numbers. Linear Ordering of Fuzzy Numbers. Evaluation of Imprecision in Fuzzy Numbers. Triangular Approximation of Various Functions of Triangular Fuzzy Numbers. Deconvolution of the Fuzzy Equations A(+)B=C, and A( . )B=C in R. T-Norms and T-Conorms. Fuzzy Numbers in [0,1]. Fuzzy Numbers in [0,1] with Higher-Order Intervals of Confidence in [0,1]. Models in Engineering and Management Science. Modelling Issues in Engineering and Management Science. Fuzzy Zero-Base Budgeting (F.Z.B.B.). Fuzzy Delphi Method (F.D.M.) in Forecasting and Decision Making. Discounting Problem using Fuzzy Numbers. Smoothing (Filtering) of Fuzzy Data. Reliability Modelling and Evaluation with Fuzzy Data. Ordering of Fuzzy Quotients. Critical Path Method (C.P.M.) with Fuzzy Data. Investment Problem with Fuzzy Data. Transportation Optimization with Fuzzy Data: (Fuzzy Stepping Stone Method). A General View about the Fuzzification of Models in Engineering and Management Science. Appendices.",poster,cp66
Computer Science,p785,d3,2e3a9a4dbf4862757fee5a074592929910e7034d,c65,International Symposium on Empirical Software Engineering and Measurement,Survey Guidelines in Software Engineering: An Annotated Review,"Background: Survey is a method of research aiming to gather data from a large population of interest. Despite being extensively used in software engineering, survey-based research faces several challenges, such as selecting a representative population sample and designing the data collection instruments. Objective: This article aims to summarize the existing guidelines, supporting instruments and recommendations on how to conduct and evaluate survey-based research. Methods: A systematic search using manual search and snowballing techniques were used to identify primary studies supporting survey research in software engineering. We used an annotated review to present the findings, describing the references of interest in the research topic. Results: The summary provides a description of 15 available articles addressing the survey methodology, based upon which we derived a set of recommendations on how to conduct survey research, and their impact in the community. Conclusion: Survey-based research in software engineering has its particular challenges, as illustrated by several articles in this review. The annotated review can contribute by raising awareness of such challenges and present the proper recommendations to overcome them.",fullPaper,cp65
Computer Science,p786,d3,b58725d3935d36685e4002b6a522fdfbf9d8e2a1,c77,Visualization for Computer Security,Radar Data Processing With Applications,"• Presents both classical theory and development methods of radar data processing • Provides state-of-the-art research results, including data processing for modern style radars, and tracking performance evaluation theory • Includes coverage of performance evaluation, registration algorithm for Radar network, data processing of passive radar, pulse Doppler radar, and phased array radar • Has applications for those engaged in information engineering, radar engineering, electronic countermeasures, infrared techniques, sonar techniques, and military command",poster,cp77
Computer Science,p788,d3,a21921eb0c5600562c8dad8e2bc40fff1ec8906b,c60,Network and Distributed System Security Symposium,Automatic Reverse Engineering of Data Structures from Binary Execution,"With only the binary executable of a program, it is useful to discover the program's data structures and infer their syntactic and semantic definitions. Such knowledge is highly valuable in a variety of security and forensic applications. Although there exist efforts in program data structure inference, the existing solutions are not suitable for our targeted application scenarios. In this paper, we propose a reverse engineering technique to automatically reveal program data structures from binaries. Our technique, called REWARDS, is based on dynamic analysis. More specifically, each memory location accessed by the program is tagged with a timestamped type attribute. Following the program's runtime data flow, this attribute is propagated to other memory locations and registers that share the same type. During the propagation, a variable's type gets resolved if it is involved in a type-revealing execution point or type sink. More importantly, besides the forward type propagation, REWARDS involves a backward type resolution procedure where the types of some previously accessed variables get recursively resolved starting from a type sink. This procedure is constrained by the timestamps of relevant memory locations to disambiguate variables re-using the same memory location. In addition, REWARDS is able to reconstruct in-memory data structure layout based on the type information derived. We demonstrate that REWARDS provides unique benefits to two applications: memory image forensics and binary fuzzing for vulnerability discovery.",fullPaper,cp60
Computer Science,p790,d3,b419355d277d2dbf3d66b1c54b6db60a4251df03,c68,Symposium on Advances in Databases and Information Systems,Probability Concepts in Engineering: Emphasis on Applications to Civil and Environmental Engineering,Chapter 1 - Role of Probability and Statistics in Engineering Chapter 2 -- Fundamentals of Probability Models Chapter 3 -- Analytical Models of Random Phenomena Chapter 4 -- Functions of Random Variables Chapter 5 - Computer-Based Numerical and Simulation Methods in Probability Chapter 6 -- Statistical Inferences from Observational Data Chapter 7 -- Determination of Probability Distribution Models Chapter 8 -- Regression and Correlation Analyses Chapter 9 -- The Bayesian Approach Chapter 10 - Elements of Quality Assurance and Acceptance Sampling (Available only online at the Wiley web site) Appendices: Table A.1 -- Standard Normal Probabilities Table A.2 - CDF of the Binomial Distribution Table A.3 - Critical Values of t Distribution at Confidence Level (1- a)=p Table A.4 - Critical Values of the c2 Distribution at Confidence Level (1-a)=pTable A.5 - Critical Values of Dna at Significance Level a in the K-S Test Table A.6 - Critical Values of the Anderson-Darling Goodness-of-fit Test (for 4 specific distributions),poster,cp68
Computer Science,p791,d3,eb181f71960c675d6bf586b974bc77342b639fee,j54,Data Science,The knowledge graph as the default data model for learning on heterogeneous knowledge,"In modern machine learning, raw data is the pre-ferred input for our models. Where a decade ago data scien-tists were still engineering features, manually picking out the details they thought salient, they now prefer the data in their raw form. As long as we can assume that all relevant and ir-relevant information is present in the input data, we can de-sign deep models that build up intermediate representations to sift out relevant features. However, these models are often domain specific and tailored to the task at hand, and therefore unsuited for learning on heterogeneous knowledge: informa-tion of different types and from different domains. If we can develop methods that operate on this form of knowledge, we can dispense with a great deal of ad-hoc feature engineering and train deep models end-to-end in many more domains. To accomplish this, we first need a data model capable of ex-pressing heterogeneous knowledge naturally in various do-mains, in as usable a form as possible, and satisfying as many use cases as possible. In this position paper, we argue that the knowledge graph is a suitable candidate for this data model. This paper describes current research and discusses some of the promises and challenges of this approach.",fullPaper,jv54
Computer Science,p793,d3,1a6f4495474f75ae1e8bbf407f70d9a874e5b4d6,c66,International Conference on Web and Social Media,The Pushshift Reddit Dataset,"Social media data has become crucial to the advancement of scientific understanding. However, even though it has become ubiquitous, just collecting large-scale social media data involves a high degree of engineering skill set and computational resources. In fact, research is often times gated by data engineering problems that must be overcome before analysis can proceed. This has resulted recognition of datasets as meaningful research contributions in and of themselves.Reddit, the so called “front page of the Internet,” in particular has been the subject of numerous scientific studies. Although Reddit is relatively open to data acquisition compared to social media platforms like Facebook and Twitter, the technical barriers to acquisition still remain. Thus, Reddit's millions of subreddits, hundreds of millions of users, and billions of comments are at the same time relatively accessible, but time consuming to collect and analyze systematically.In this paper, we present the Pushshift Reddit dataset. Pushshift is a social media data collection, analysis, and archiving platform that since 2015 has collected Reddit data and made it available to researchers. Pushshift's Reddit dataset is updated in real-time, and includes historical data back to Reddit's inception. In addition to monthly dumps, Pushshift provides computational tools to aid in searching, aggregating, and performing exploratory analysis on the entirety of the dataset. The Pushshift Reddit dataset makes it possible for social media researchers to reduce time spent in the data collection, cleaning, and storage phases of their projects.",fullPaper,cp66
Computer Science,p795,d3,49149bd7c772b77937f459f56e5bf01c1ac4675f,j110,Scientometrics,"The advantages of an Ontology-Based Data Management approach: openness, interoperability and data quality",Abstract,fullPaper,jv110
Computer Science,p797,d3,922e773e471430f637d07af4f17b72461ede8b7c,c20,ACM Conference on Economics and Computation,Reusing Scientific Data: How Earthquake Engineering Researchers Assess the Reusability of Colleagues’ Data,Abstract,poster,cp20
Computer Science,p798,d3,9759ed3befc96caa5035b7176671efea99cd3493,j66,Data Science Journal,A Brief Review on Leading Big Data Models,"Today, science is passing through an era of transformation, where the inundation of data, dubbed data deluge is influencing the decision making process. The science is driven by the data and is being termed as data science. In this internet age, the volume of the data has grown up to petabytes, and this large, complex, structured or unstructured, and heterogeneous data in the form of “Big Data” has gained significant attention. The rapid pace of data growth through various disparate sources, especially social media such as Facebook, has seriously challenged the data analytic capabilities of traditional relational databases. The velocity of the expansion of the amount of data gives rise to a complete paradigm shift in how new age data is processed. Confidence in the data engineering of the existing data processing systems is gradually fading whereas the capabilities of the new techniques for capturing, storing, visualizing, and analyzing data are evolving. In this review paper, we discuss some of the modern Big Data models that are leading contributors in the NoSQL era and claim to address Big Data challenges in reliable and efficient ways. Also, we take the potential of Big Data into consideration and try to reshape the original operationaloriented definition of “Big Science” (Furner, 2003) into a new data-driven definition and rephrase it as “The science that deals with Big Data is Big Science.”",fullPaper,jv66
Computer Science,p799,d3,328251a1caef158f32773e0f77ce484849baf4b9,c86,International Conference on Big Data and Education,"Probability, Reliability and Statistical Methods in Engineering Design (Haldar, Mahadevan)",Basic Concept of Reliability. Mathematics of Probability. Modeling of Uncertainty. Commonly Used Probability Distributions. Determination of Distributions and Parameters from Observed Data. Randomness in Response Variables. Fundamentals of Reliability Analysis. Advanced Topics on Reliability Analysis. Simulation Techniques. Appendices. Conversion Factors. References. Index.,poster,cp86
Computer Science,p800,d3,328251a1caef158f32773e0f77ce484849baf4b9,c49,ACM/SIGCOMM Internet Measurement Conference,"Probability, Reliability and Statistical Methods in Engineering Design (Haldar, Mahadevan)",Basic Concept of Reliability. Mathematics of Probability. Modeling of Uncertainty. Commonly Used Probability Distributions. Determination of Distributions and Parameters from Observed Data. Randomness in Response Variables. Fundamentals of Reliability Analysis. Advanced Topics on Reliability Analysis. Simulation Techniques. Appendices. Conversion Factors. References. Index.,poster,cp49
Computer Science,p801,d3,de7a529d144f0a78dd75864dd6012f969521f7f8,c67,The Sea,From Theory to Practice: Plug and Play with Succinct Data Structures,Abstract,fullPaper,cp67
Computer Science,p803,d3,66f48bc391b0fd135c8b4915618aebf663d56316,j153,The VLDB journal,Query reverse engineering,Abstract,fullPaper,jv153
Computer Science,p804,d3,bc1022b031dc6c7019696492e8116598097a8c12,j201,Journal of machine learning research,Natural Language Processing (Almost) from Scratch,"We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.",fullPaper,jv201
Computer Science,p806,d3,31ebee998db7125228d6119200594dfe2a6dbc3c,c68,Symposium on Advances in Databases and Information Systems,On the Convergence of Data and Process Engineering,Abstract,fullPaper,cp68
Computer Science,p808,d3,5a30fd3718835c500d1833492d6cd833c959d155,c12,The Compass,Non-Functional Requirements in Software Engineering,Abstract,poster,cp12
Computer Science,p810,d3,c2fb0ded7b21a23cd0931558b52ddbc98fc4f934,c81,ACM Symposium on Applied Computing,Doing Data Science: Straight Talk from the Frontline,"Now that people are aware that data can make the difference in an election or a business model, data science as an occupation is gaining ground. But how can you get started working in a wide-ranging, interdisciplinary field thats so clouded in hype? This insightful book, based on Columbia Universitys Introduction to Data Science class, tells you what you need to know. In many of these chapter-long lectures, data scientists from companies such as Google, Microsoft, and eBay share new algorithms, methods, and models by presenting case studies and the code they use. If youre familiar with linear algebra, probability, and statistics, and have programming experience, this book is an ideal introduction to data science. Topics include:Statistical inference, exploratory data analysis, and the data science process Algorithms Spam filters, Naive Bayes, and data wrangling Logistic regression Financial modeling Recommendation engines and causality Data visualization Social networks and data journalism Data engineering, MapReduce, Pregel, and Hadoop Doing Data Science is collaboration between course instructor Rachel Schutt, Senior VP of Data Science at News Corp, and data science consultant Cathy ONeil, a senior data scientist at Johnson Research Labs, who attended and blogged about the course.",poster,cp81
Computer Science,p812,d3,e28d1aebca9fcd36d0a23b494b9dfa2ef8795277,c97,International Conference on Computational Logic,System Identification: A Frequency Domain Approach,"System identification is a general term used to describe mathematical tools and algorithms that build dynamical models from measured data. Used for prediction, control, physical interpretation, and the designing of any electrical systems, they are vital in the fields of electrical, mechanical, civil, and chemical engineering. Focusing mainly on frequency domain techniques, System Identification: A Frequency Domain Approach, Second Edition also studies in detail the similarities and differences with the classical time domain approach. It high??lights many of the important steps in the identification process, points out the possible pitfalls to the reader, and illustrates the powerful tools that are available.",poster,cp97
Computer Science,p813,d3,07993804501ae4df9fe43cf8afb009ae38fe902e,c60,Network and Distributed System Security Symposium,A Mathematical Introduction to Compressive Sensing,Abstract,poster,cp60
Computer Science,p815,d3,dca45bd363820bce269a176a1ecde7e1885e2ea6,c64,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",B4: experience with a globally-deployed software defined wan,"We present the design, implementation, and evaluation of B4, a private WAN connecting Google's data centers across the planet. B4 has a number of unique characteristics: i) massive bandwidth requirements deployed to a modest number of sites, ii) elastic traffic demand that seeks to maximize average bandwidth, and iii) full control over the edge servers and network, which enables rate limiting and demand measurement at the edge. These characteristics led to a Software Defined Networking architecture using OpenFlow to control relatively simple switches built from merchant silicon. B4's centralized traffic engineering service drives links to near 100% utilization, while splitting application flows among multiple paths to balance capacity against application priority/demands. We describe experience with three years of B4 production deployment, lessons learned, and areas for future work.",fullPaper,cp64
Computer Science,p818,d3,fa9f622a1182400067d911b0900733695ec1b358,j187,Empirical Software Engineering,Parameter tuning or default values? An empirical investigation in search-based software engineering,Abstract,fullPaper,jv187
Computer Science,p819,d3,221da285b72f6272c2d7332a547d1e034a60e154,c51,International Conference on Engineering Education,Data modelling versus ontology engineering,"Ontologies in current computer science parlance are computer based resources that represent agreed domain semantics. Unlike data models, the fundamental asset of ontologies is their relative independence of particular applications, i.e. an ontology consists of relatively generic knowledge that can be reused by different kinds of applications/tasks. The first part of this paper concerns some aspects that help to understand the differences and similarities between ontologies and data models. In the second part we present an ontology engineering framework that supports and favours the genericity of an ontology. We introduce the DOGMA ontology engineering approach that separates ""atomic"" conceptual relations from ""predicative"" domain rules. A DOGMA ontology consists of an ontology base that holds sets of intuitive context-specific conceptual relations and a layer of ""relatively generic"" ontological commitments that hold the domain rules. This constitutes what we shall call the double articulation of a DOGMA ontology 1.",poster,cp51
Computer Science,p822,d3,497e4b08279d69513e4d2313a7fd9a55dfb73273,c69,Neural Information Processing Systems,LightGBM: A Highly Efficient Gradient Boosting Decision Tree,"Gradient Boosting Decision Tree (GBDT) is a popular machine learning algorithm, and has quite a few effective implementations such as XGBoost and pGBRT. Although many engineering optimizations have been adopted in these implementations, the efficiency and scalability are still unsatisfactory when the feature dimension is high and data size is large. A major reason is that for each feature, they need to scan all the data instances to estimate the information gain of all possible split points, which is very time consuming. To tackle this problem, we propose two novel techniques: \emph{Gradient-based One-Side Sampling} (GOSS) and \emph{Exclusive Feature Bundling} (EFB). With GOSS, we exclude a significant proportion of data instances with small gradients, and only use the rest to estimate the information gain. We prove that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size. With EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features. We prove that finding the optimal bundling of exclusive features is NP-hard, but a greedy algorithm can achieve quite good approximation ratio (and thus can effectively reduce the number of features without hurting the accuracy of split point determination by much). We call our new GBDT implementation with GOSS and EFB \emph{LightGBM}. Our experiments on multiple public datasets show that, LightGBM speeds up the training process of conventional GBDT by up to over 20 times while achieving almost the same accuracy.",fullPaper,cp69
Mathematics,p822,d6,497e4b08279d69513e4d2313a7fd9a55dfb73273,c69,Neural Information Processing Systems,LightGBM: A Highly Efficient Gradient Boosting Decision Tree,"Gradient Boosting Decision Tree (GBDT) is a popular machine learning algorithm, and has quite a few effective implementations such as XGBoost and pGBRT. Although many engineering optimizations have been adopted in these implementations, the efficiency and scalability are still unsatisfactory when the feature dimension is high and data size is large. A major reason is that for each feature, they need to scan all the data instances to estimate the information gain of all possible split points, which is very time consuming. To tackle this problem, we propose two novel techniques: \emph{Gradient-based One-Side Sampling} (GOSS) and \emph{Exclusive Feature Bundling} (EFB). With GOSS, we exclude a significant proportion of data instances with small gradients, and only use the rest to estimate the information gain. We prove that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size. With EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features. We prove that finding the optimal bundling of exclusive features is NP-hard, but a greedy algorithm can achieve quite good approximation ratio (and thus can effectively reduce the number of features without hurting the accuracy of split point determination by much). We call our new GBDT implementation with GOSS and EFB \emph{LightGBM}. Our experiments on multiple public datasets show that, LightGBM speeds up the training process of conventional GBDT by up to over 20 times while achieving almost the same accuracy.",fullPaper,cp69
Computer Science,p825,d3,47f84928dd6e40797255fa1e1bbb3c12b2659a7c,c41,IEEE International Conference on Data Engineering,Input selection for fast feature engineering,"The application of machine learning to large datasets has become a vital component of many important and sophisticated software systems built today. Such trained systems are often based on supervised learning tasks that require features, signals extracted from the data that distill complicated raw data objects into a small number of salient values. A trained system's success depends substantially on the quality of its features. Unfortunately, feature engineering-the process of writing code that takes raw data objects as input and outputs feature vectors suitable for a machine learning algorithm-is a tedious, time-consuming experience. Because “big data” inputs are so diverse, feature engineering is often a trial-and-error process requiring many small, iterative code changes. Because the inputs are so large, each code change can involve a time-consuming data processing task (over each page in a Web crawl, for example). We introduce Zombie, a data-centric system that accelerates feature engineering through intelligent input selection, optimizing the “inner loop” of the feature engineering process. Our system yields feature evaluation speedups of up to 8× in some cases and reduces engineer wait times from 8 to 5 hours in others.",fullPaper,cp41
Computer Science,p826,d3,68d54f9dacbb5416c1aafb3399c072497c320021,c45,IEEE Symposium on Security and Privacy,"Network Flows: Theory, Algorithms, and Applications","A comprehensive introduction to network flows that brings together the classic and the contemporary aspects of the field, and provides an integrative view of theory, algorithms, and applications. presents in-depth, self-contained treatments of shortest path, maximum flow, and minimum cost flow problems, including descriptions of polynomial-time algorithms for these core models. emphasizes powerful algorithmic strategies and analysis tools such as data scaling, geometric improvement arguments, and potential function arguments. provides an easy-to-understand descriptions of several important data structures, including d-heaps, Fibonacci heaps, and dynamic trees. devotes a special chapter to conducting empirical testing of algorithms. features over 150 applications of network flows to a variety of engineering, management, and scientific domains. contains extensive reference notes and illustrations.",poster,cp45
Computer Science,p827,d3,69cfb48c45b59243d60342b796dbac35e9efd6bc,c8,Frontiers in Education Conference,Toward principles for the design of ontologies used for knowledge sharing?,"Recent work in Artificial Intelligence is exploring the use of formal ontologies as a way of specifying content-specific agreements for the sharing and reuse of knowledge among software entities. We take an engineering perspective on the development of such ontologies. Formal ontologies are viewed as designed artifacts, formulated for specific purposes and evaluated against objective design criteria. We describe the role of ontologies in supporting knowledge sharing activities, and then present a set of criteria to guide the development of ontologies for these purposes. We show how these criteria are applied in case studies from the design of ontologies for engineering mathematics and bibliographic data. Selected design decisions are discussed, and alternative representation choices and evaluated against the design criteria.",poster,cp8
Computer Science,p828,d3,daa63f57c3fbe994c4356f8d986a22e696e776d2,j203,Journal of Global Optimization,Efficient Global Optimization of Expensive Black-Box Functions,Abstract,fullPaper,jv203
Mathematics,p828,d6,daa63f57c3fbe994c4356f8d986a22e696e776d2,j203,Journal of Global Optimization,Efficient Global Optimization of Expensive Black-Box Functions,Abstract,fullPaper,jv203
Computer Science,p832,d3,8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4,c70,Annual Meeting of the Association for Computational Linguistics,End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF,"State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both the two data --- 97.55\% accuracy for POS tagging and 91.21\% F1 for NER.",fullPaper,cp70
Mathematics,p832,d6,8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4,c70,Annual Meeting of the Association for Computational Linguistics,End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF,"State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both the two data --- 97.55\% accuracy for POS tagging and 91.21\% F1 for NER.",fullPaper,cp70
Computer Science,p833,d3,1d122a074c936fcfd95faf44608e377a9d1799c8,c71,International Joint Conference on Artificial Intelligence,DeepFM: A Factorization-Machine based Neural Network for CTR Prediction,"Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide & Deep model from Google, DeepFM has a shared input to its ""wide"" and ""deep"" parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data.",fullPaper,cp71
Computer Science,p834,d3,617e2d3a42c23a758daf8dd51ab82f89d171ef54,c110,Biometrics and Identity Management,The Mythical Man-Month,"The book, The Mythical Man-Month, Addison-Wesley, 1975 (excerpted in Datamation, December 1974), gathers some of the published data about software engineering and mixes it with the assertion of a lot of personal opinions. In this presentation, the author will list some of the assertions and invite dispute or support from the audience. This is intended as a public discussion of the published book, not a regular paper.",poster,cp110
Computer Science,p836,d3,be652e2b5986d15eb5b3c9d5fc5c8fbe108ed2ba,c71,International Joint Conference on Artificial Intelligence,Business Value of Information Technology: A Study of Electronic Data Interchange,"A great deal of controversy exists about the impact of information technology on firm performance. While some authors have reported positive impacts, others have found negative or no impacts. This study focuses on Electronic Data Interchange (EDI) technology. Many of the problems in this line of research are over-come in this study by conducting a careful analysis of the performance data of the past decade gathered from the assembly centers of Chrysler Corporation. This study estimates the dollar benefits of improved information exchanges between Chrysler and its suppliers that result from using EDI. After controlling for variations in operational complexity arising from mix, volume, parts complexity, model, and engineering changes, the savings per vehicle that result from improved information exchanges are estimated to be about $60. Including the additional savings from electronic document preparation and transmission, the total benefits of EDI per vehicle amount to over $100. System wide, this translates to annual savings of $220 million for the company.",poster,cp71
Economics,p836,d11,be652e2b5986d15eb5b3c9d5fc5c8fbe108ed2ba,c71,International Joint Conference on Artificial Intelligence,Business Value of Information Technology: A Study of Electronic Data Interchange,"A great deal of controversy exists about the impact of information technology on firm performance. While some authors have reported positive impacts, others have found negative or no impacts. This study focuses on Electronic Data Interchange (EDI) technology. Many of the problems in this line of research are over-come in this study by conducting a careful analysis of the performance data of the past decade gathered from the assembly centers of Chrysler Corporation. This study estimates the dollar benefits of improved information exchanges between Chrysler and its suppliers that result from using EDI. After controlling for variations in operational complexity arising from mix, volume, parts complexity, model, and engineering changes, the savings per vehicle that result from improved information exchanges are estimated to be about $60. Including the additional savings from electronic document preparation and transmission, the total benefits of EDI per vehicle amount to over $100. System wide, this translates to annual savings of $220 million for the company.",poster,cp71
Computer Science,p838,d3,f5079168f914014e9cfe888bdfd58b7bac2b1fe9,c3,Knowledge Discovery and Data Mining,Implementing Smart Factory of Industrie 4.0: An Outlook,"With the application of Internet of Things and services to manufacturing, the fourth stage of industrialization, referred to as Industrie 4.0, is believed to be approaching. For Industrie 4.0 to come true, it is essential to implement the horizontal integration of inter-corporation value network, the end-to-end integration of engineering value chain, and the vertical integration of factory inside. In this paper, we focus on the vertical integration to implement flexible and reconfigurable smart factory. We first propose a brief framework that incorporates industrial wireless networks, cloud, and fixed or mobile terminals with smart artifacts such as machines, products, and conveyors. Then, we elaborate the operational mechanism from the perspective of control engineering, that is, the smart artifacts form a self-organized system which is assisted with the feedback and coordination blocks that are implemented on the cloud and based on the big data analytics. In addition, we outline the main technical features and beneficial outcomes and present a detailed design scheme. We conclude that the smart factory of Industrie 4.0 is achievable by extensively applying the existing enabling technologies while actively coping with the technical challenges.",poster,cp3
Computer Science,p839,d3,875c77b0daf65f2db77b48e784cb68fb312edea3,c9,Big Data,Sequential Monte Carlo Methods in Practice,Abstract,poster,cp9
Computer Science,p844,d3,498d27f4d3a616beb058333e410811808f7fe46d,c4,Conference on Innovative Data Systems Research,Meshfree Approximation Methods with Matlab,"Meshfree approximation methods are a relatively new area of research, and there are only a few books covering it at present. Whereas other works focus almost entirely on theoretical aspects or applications in the engineering field, this book provides the salient theoretical results needed for a basic understanding of meshfree approximation methods. The emphasis here is on a hands-on approach that includes MATLAB routines for all basic operations. Meshfree approximation methods, such as radial basis function and moving least squares method, are discussed from a scattered data approximation and partial differential equations point of view. A good balance is supplied between the necessary theory and implementation in terms of many MATLAB programs, with examples and applications to illustrate key points. Used as class notes for graduate courses at Northwestern University, Illinois Institute of Technology, and Vanderbilt University, this book will appeal to both mathematics and engineering graduate students.",poster,cp4
Mathematics,p844,d6,498d27f4d3a616beb058333e410811808f7fe46d,c4,Conference on Innovative Data Systems Research,Meshfree Approximation Methods with Matlab,"Meshfree approximation methods are a relatively new area of research, and there are only a few books covering it at present. Whereas other works focus almost entirely on theoretical aspects or applications in the engineering field, this book provides the salient theoretical results needed for a basic understanding of meshfree approximation methods. The emphasis here is on a hands-on approach that includes MATLAB routines for all basic operations. Meshfree approximation methods, such as radial basis function and moving least squares method, are discussed from a scattered data approximation and partial differential equations point of view. A good balance is supplied between the necessary theory and implementation in terms of many MATLAB programs, with examples and applications to illustrate key points. Used as class notes for graduate courses at Northwestern University, Illinois Institute of Technology, and Vanderbilt University, this book will appeal to both mathematics and engineering graduate students.",poster,cp4
Computer Science,p845,d3,b1119811137978ea7eed9241d696d2e139dd10f9,c2,International Conference on Software Engineering,"Exploring Data in Engineering, the Sciences, and Medicine","1. The Art of Analyzing Data 2. Data: Types, Uncertainty and Quality 3. Characterizing Categorical Variables 4. Uncertainty in Real Variables 5. Fitting Straight Lines 6. A Brief Introduction to Estimation Theory 7. Outliers: Distributional Monsters That Lurk in Data 8. Characterizing a Dataset 9. Confidence Intervals and Hypothesis Testing 10. Associations between Variables 11. Regression Models I: Real Data 12. Re-expression: Data Transformations 13. Regression Models II: Mixed Data Types 14. Characterizing Analysis Results 15. Regression Models III: Diagnostics and Refinements 16. Dynamic Data Characterization 17. Linear Data Filters 18. Nonparametric Spectrum Estimation 19. Irregularities in Dynamic Analysis 20. Dealing with Missing Data",poster,cp2
Computer Science,p846,d3,05dd3536ba7a8e017cf7b7cb56975a4cd6a53392,c83,"International Convention on Information and Communication Technology, Electronics and Microelectronics",DICE: Quality-Driven Development of Data-Intensive Cloud Applications,"Model-driven engineering (MDE) often features quality assurance (QA) techniques to help developers creating software that meets reliability, efficiency, and safety requirements. In this paper, we consider the question of how quality-aware MDE should support data-intensive software systems. This is a difficult challenge, since existing models and QA techniques largely ignore properties of data such as volumes, velocities, or data location. Furthermore, QA requires the ability to characterize the behavior of technologies such as Hadoop/MapReduce, NoSQL, and stream-based processing, which are poorly understood from a modeling standpoint. To foster a community response to these challenges, we present the research agenda of DICE, a quality-aware MDE methodology for data-intensive cloud applications. DICE aims at developing a quality engineering tool chain offering simulation, verification, and architectural optimization for Big Data applications. We overview some key challenges involved in developing these tools and the underpinning models.",poster,cp83
Computer Science,p847,d3,a1b51ff5cfb974fdef34386bed5c5844ba7a8dcf,c107,Annual Haifa Experimental Systems Conference,Managing the software process,Foreword. Preface. I. SOFTWARE PROCESS MATURITY. A Software Maturity Framework. The Principles of Software Process Change. Software Process Assessment. The Initial Process. II. THE REPEATABLE PROCESS. Managing Software Organizations. The Project Plan. Software Configuration Management-Part 1: Software Quality Assurance. III. THE DEFINED PROCESS. Software Standards. Software Inspections. Software Testing. Software Configuration Management (Continued). Defining the Software Process. The Software Engineering Process Group IV. THE MANAGED PROCESS. Data Gathering and Analysis. Managing Software Quality. V. THE OPTIMIZING PROCESS. Defect Prevention. Automating The Software Process. Contracting for Software. Conclusion. Appendices. Index. 0201180952T04062001,poster,cp107
Computer Science,p850,d3,7c4a9643c701c0c91ea50fd587038f79187a0a5e,c43,European Conference on Machine Learning,Artificial Intelligence: A Guide to Intelligent Systems,"From the Publisher: 
Virtually all the literature on artificial intelligence is expressed in the jargon of commuter science, crowded with complex matrix algebra and differential equations. Unlike many other books on computer intelligence, this one demonstrates that most ideas behind intelligent systems are simple and straightforward. The book has evolved from lectures given to students with little knowledge of calculus, and the reader needs no prerequisites associated with knowledge of any programming language. The methods used in the book have been extensively tested through several courses given by the author. 
 
The book provides an introduction to the field of computer intelligence, covering 
 
rule-based expert systems, 
fuzzy expert systems, 
frame-based expert systems, 
artificail neural networks, 
evolutionary computation, 
hybrid intelligent systems, 
knowledge engineering, 
data mining. 
 
 
In a university setting the book can be used as an introductory course within computer science, information systems or engineering departments. The book is also suitable as a self-study guide for non-computer science professionals, giving access to the state of the art in knowledge-based systems and computational intelligence. Everyone who faces challenging problems and cannot solve them using traditional approaches can benefit",poster,cp43
Computer Science,p851,d3,59a79116f9f7a20560da0aeb7519c33f6ec84a17,c106,International Conference on Biometrics,"Urban Computing: Concepts, Methodologies, and Applications","Urbanization's rapid progress has modernized many people's lives but also engendered big issues, such as traffic congestion, energy consumption, and pollution. Urban computing aims to tackle these issues by using the data that has been generated in cities (e.g., traffic flow, human mobility, and geographical data). Urban computing connects urban sensing, data management, data analytics, and service providing into a recurrent process for an unobtrusive and continuous improvement of people's lives, city operation systems, and the environment. Urban computing is an interdisciplinary field where computer sciences meet conventional city-related fields, like transportation, civil engineering, environment, economy, ecology, and sociology in the context of urban spaces. This article first introduces the concept of urban computing, discussing its general framework and key challenges from the perspective of computer sciences. Second, we classify the applications of urban computing into seven categories, consisting of urban planning, transportation, the environment, energy, social, economy, and public safety and security, presenting representative scenarios in each category. Third, we summarize the typical technologies that are needed in urban computing into four folds, which are about urban sensing, urban data management, knowledge fusion across heterogeneous data, and urban data visualization. Finally, we give an outlook on the future of urban computing, suggesting a few research topics that are somehow missing in the community.",poster,cp106
Computer Science,p853,d3,022a0317d5bf2b38847b03f7c9bc3bfa35950199,c72,Workshop on Research on Enterprise Networking,Understanding data center traffic characteristics,"As data centers become more and more central in Internet communications, both research and operations communities have begun to explore how to better design and manage them. In this paper, we present a preliminary empirical study of end-to-end traffic patterns in data center networks that can inform and help evaluate research and operational approaches. We analyze SNMP logs collected at 19 data centers to examine temporal and spatial variations in link loads and losses. We find that while links in the core are heavily utilized the ones closer to the edge observe a greater degree of loss. We then study packet traces collected at a small number of switches in one data center and find evidence of ON-OFF traffic behavior. Finally, we develop a framework that derives ON-OFF traffic parameters for data center traffic sources that best explain the SNMP data collected for the data center. We show that the framework can be used to evaluate data center traffic engineering approaches. We are also applying the framework to design network-level traffic generators for data centers.",fullPaper,cp72
Computer Science,p854,d3,a8edd60a3be9a6e09bd4c6c586a109e4f3f7b116,c22,Grid Computing Environments,Introduction to artificial neural systems,"Jacek M. Zurada received his MS and Ph.D. degrees (with distinction) in electrical engineering from the Technical University of Gdansk, Poland. Since 1989 he has been a Professor with the Electrical and Computer Engineering Department at the University of Louisville, Kentucky. He was Department Chair from 2004 to 2006. He has published over 350 journal and conference papers in the areas of neural networks, computational intelligence, data mining, image processing and VLSI circuits. INTRODUCTION TO ARTIFICIAL NEURAL SYSTEMS",poster,cp22
Computer Science,p856,d3,1eb131a34fbb508a9dd8b646950c65901d6f1a5b,c69,Neural Information Processing Systems,Hidden Technical Debt in Machine Learning Systems,"Machine learning offers a fantastically powerful toolkit for building useful complex prediction systems quickly. This paper argues it is dangerous to think of these quick wins as coming for free. Using the software engineering framework of technical debt, we find it is common to incur massive ongoing maintenance costs in real-world ML systems. We explore several ML-specific risk factors to account for in system design. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, configuration issues, changes in the external world, and a variety of system-level anti-patterns.",fullPaper,cp69
Computer Science,p862,d3,48e752c719d33ff55b3b3bec3538727f8ce69399,j72,IEEE Intelligent Systems,Ontology Learning for the Semantic Web,"The Semantic Web relies heavily on formal ontologies to structure data for comprehensive and transportable machine understanding. Thus, the proliferation of ontologies factors largely in the Semantic Web's success. The authors present an ontology learning framework that extends typical ontology engineering environments by using semiautomatic ontology construction tools. The framework encompasses ontology import, extraction, pruning, refinement and evaluation.",fullPaper,jv72
Computer Science,p863,d3,5f0a22399569a458b45961577582a83ab8cca2a7,c60,Network and Distributed System Security Symposium,Engineering data compendium : human perception and performance,Abstract,poster,cp60
Computer Science,p869,d3,1e62a8afbe6018540c60d9dcce1ff6bd98f2e404,c2,International Conference on Software Engineering,Automatic query reformulations for text retrieval in software engineering,"There are more than twenty distinct software engineering tasks addressed with text retrieval (TR) techniques, such as, traceability link recovery, feature location, refactoring, reuse, etc. A common issue with all TR applications is that the results of the retrieval depend largely on the quality of the query. When a query performs poorly, it has to be reformulated and this is a difficult task for someone who had trouble writing a good query in the first place. We propose a recommender (called Refoqus) based on machine learning, which is trained with a sample of queries and relevant results. Then, for a given query, it automatically recommends a reformulation strategy that should improve its performance, based on the properties of the query. We evaluated Refoqus empirically against four baseline approaches that are used in natural language document retrieval. The data used for the evaluation corresponds to changes from five open source systems in Java and C++ and it is used in the context of TR-based concept location in source code. Refoqus outperformed the baselines and its recommendations lead to query performance improvement or preservation in 84% of the cases (in average).",fullPaper,cp2
Computer Science,p870,d3,90b8cfa993357cccd94d05a4317342892516731b,j206,Computer,Data Mining for Software Engineering,"To improve software productivity and quality, software engineers are increasingly applying data mining algorithms to various software engineering tasks. However, mining SE data poses several challenges. The authors present various algorithms to effectively mine sequences, graphs, and text from such data.",fullPaper,jv206
Computer Science,p872,d3,fb2704b2eeb306bf9dab852c45e909f928645be3,j207,Management Sciences,Corporate Social Responsibility and Firm Risk: Theory and Empirical Evidence,"This paper presents an industry equilibrium model where firms can choose to engage in corporate social responsibility (CSR) activities. We model CSR activities as an investment in customer loyalty and show that CSR decreases systematic risk and increases firm value. These effects are stronger for firms producing differentiated goods and when consumers' expenditure share on CSR goods is small. We find supporting evidence for our predictions. In our empirical tests, we address a potential endogeneity problem by instrumenting CSR using data on the political affiliation of the firm's home state, and data on environmental and engineering disasters and product recalls.",fullPaper,jv207
Business,p872,d9,fb2704b2eeb306bf9dab852c45e909f928645be3,j207,Management Sciences,Corporate Social Responsibility and Firm Risk: Theory and Empirical Evidence,"This paper presents an industry equilibrium model where firms can choose to engage in corporate social responsibility (CSR) activities. We model CSR activities as an investment in customer loyalty and show that CSR decreases systematic risk and increases firm value. These effects are stronger for firms producing differentiated goods and when consumers' expenditure share on CSR goods is small. We find supporting evidence for our predictions. In our empirical tests, we address a potential endogeneity problem by instrumenting CSR using data on the political affiliation of the firm's home state, and data on environmental and engineering disasters and product recalls.",fullPaper,jv207
Computer Science,p875,d3,b22de434b462558a127f327f29e2b0c673c0d7ab,j208,Multimedia tools and applications,"Latent Dirichlet allocation (LDA) and topic modeling: models, applications, a survey",Abstract,fullPaper,jv208
Computer Science,p877,d3,497abc0ddace6a7772a5f5a3edb3d7b751476755,c71,International Joint Conference on Artificial Intelligence,Methodology for the Design and Evaluation of Ontologies,"As information systems play a more active role in the management and operations of an enterprise, the demands on these systems have also increased. Departing from their traditional role as simple repositories of data, information systems must now provide more sophisticated support to manual and automated decision making; they must not only answer queries with what is explicitly represented in their Enterprise Model, but must be able to answer queries with what is implied by the model. The goal of the TOVE (TOronto Virtual Enterprise) Enterprise Modelling project is to create the next generation Enterprise Model, a Common Sense Enterprise Model. By common sense we mean that an Enterprise Model has the ability to deduce answers to queries that require relatively shallow knowledge of the domain. We are taking what can be viewed as a `second generation knowledge engineering' approach to constructing our Common Sense Enterprise Model. Rather than extracting rules from experts, we are `engineering ontologies.' An ontology is a formal description of entities and their properties, relationships, constraints, behaviours. Through interaction with our industrial partners, we encounter problems that arise in their particular enterprises. Our approach to engineering ontologies begins with using these problems to de ne an ontology's requirements in the form of questions that an ontology must be able to answer. We call this the competency of the ontology. The second step is to de ne the terminology of the ontology its objects, attributes, and relations. In this way the ontology provides the language that will be used to express the de nitions in the terminology and the constraints required by the application. The third step is to specify the de nitions and constraints on the terminology, where possible. The speci cations are represented in First Order Logic and implemented in Prolog. Lastly, we test the competency of the ontology by proving completeness theorems with respect to the competency questions. Our initial e orts have focused on ontologies to support reasoning in industrial environments. The tasks that we have targeted to support are in `supply chain management' which extends MRP (Manufacturing Requirements Planning) to include logistics/distribution [Fox",fullPaper,cp71
Computer Science,p879,d3,2e1b489e9f5964eee1f09c08cf36f5377c2688c6,c115,International Conference on Information Integration and Web-based Applications & Services,Introduction to Database Systems,Abstract,poster,cp115
Computer Science,p880,d3,76aad5b272219c6745c76b7874129797e97e6041,c73,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,Paxos made live: an engineering perspective,"We describe our experience in building a fault-tolerant data-base using the Paxos consensus algorithm. Despite the existing literature in the field, building such a database proved to be non-trivial. We describe selected algorithmic and engineering problems encountered, and the solutions we found for them. Our measurements indicate that we have built a competitive system.",fullPaper,cp73
Computer Science,p882,d3,d873ca587904ee6c1823a78995f77cee849e4c9f,c34,International Conference on Data Warehousing and Knowledge Discovery,Modern Software Engineering Methodologies Meet Data Warehouse Design: 4WD,Abstract,fullPaper,cp34
Computer Science,p885,d3,74b90167eb80e959e84497c51e4e9da37a8de0ea,j210,IEEE Communications Magazine,"Visible light communication: opportunities, challenges and the path to market","Visible light communication is a potentially disruptive form of wireless communication that can supplement radio frequency communication and also uniquely enable novel mobile wireless device use cases. High data rate downlink communication in homes and offices and high accuracy indoor positioning in retail stores are two of the most compelling use cases of this promising new technology. Large-scale commercialization of visible light communication devices will depend on both the development of robust and efficient engineering solutions, and the execution of incremental commercialization strategies.",fullPaper,jv210
Computer Science,p886,d3,86c35680190dcb1a040243927e98e4ca08f8d80e,c78,ACM/IEEE International Conference on Model Driven Engineering Languages and Systems,Generalized linear models. 2nd ed.,"Addresses a class of statistical models that generalizes classical linear models-extending them to include many other models useful in statistical analysis. Incorporates numerous exercises, both theoretical and data-analytic Discusses quasi-likelihood functions and estimating equations, models for dispersion effect, components of dispersion, and conditional likelihoods Holds particular interest for statisticians in medicine, biology, agriculture, social science, and engineering",poster,cp78
Computer Science,p888,d3,9fc1062987cac66390a77f31d5758bdbb6f3ec50,c69,Neural Information Processing Systems,Towards a Model-Driven Design Tool for Big Data Architectures,"Big Data technologies are rapidly becoming a key enabler for modern industries. However, the entry costs inherent to ``going Big"" are considerable, ranging from learning curve, renting/buying infrastructure, etc. A key component of these costs is the time spent on learning about and designing with the many big data frameworks (e.g., Spark, Storm, HadoopMR, etc.) on the market. To reduce said costs while decreasing time-to-market we advocate the usage of Model-Driven Engineering (MDE), i.e., software engineering by means of models and their automated manipulation. This paper outlines a tool architecture to support MDE for big data applications, illustrating with a case-study.",poster,cp69
Computer Science,p891,d3,78379eeba6d8de526dfbc983818401baa689f0b5,j212,Data mining and knowledge discovery,Ontology of core data mining entities,Abstract,fullPaper,jv212
Computer Science,p892,d3,e9e5e46ca827493feb8b6c85ebd9c8ead8a70dae,j213,IEEE transactions on engineering management,The design structure system: A method for managing the design of complex systems,"Systems design involves the determination of interdependent variables. Thus the precedence ordering for the tasks of determining these variables involves circuits. Circuits require planning decisions about how to iterate and where to use estimates. Conventional planning techniques, such as critical path, do not deal with these problems. Techniques are shown which acknowledge these circuits in the design of systems. These techniques can be used to develop an effective engineering plan, showing where estimates are to be used, how design iterations and reviews are handled, and how information flows during the design work. This information flow can be used to determine the consequences of a change in any variable on the rest of the variables in the system, and thus which engineers must be informed and which documents must be changed. From this, a critical path schedule can be developed for implementing the change. This method is ideally suited to an automated design office where data, computer input and output, and communications are all handled through the use of computer terminals and data bases. However, these same techniques can also be effectively used in classical engineering environments.",fullPaper,jv213
Computer Science,p893,d3,7321ae1b6dc03f4575b9f1acee6c69b1072e45f8,c74,International Conference on Computational Linguistics,GATE-a General Architecture for Text Engineering,"Much progress has been made in the provision of reusable data resources for Natural Language Engineering, such as grammars, lexicons, thesauruses. Although a number of projects have addressed the provision of reusable algorithmic resources (or 'tools'), takeup of these resources has been relatively slow. This paper describes GATE, a General Architecture for Text Engineering, which is a freely-available system designed to help alleviate the problem. 1 Resource Reuse and Natura l Language Engineering Car designers don't reinvent the wheel each time they plan a new model, but software engineers often find themselves repetitively producing roughly the same piece of software in slightly ditfenmt R)rm. The reasons for this inefficency have been extensively studied, and a number of solutions are now available (Prieto-Diaz and t~h'eeman, 1987; Prieto-Diaz, 1993). Similarly, the Natural Language Engineering (NLE l) community has identified the potential benefits of reducing repetition, and work has been flmded to promote reuse. This work concerns either reusable resources which are primarily data or those which are primarily algorithmic (i.e. processing 'tools', or programs, or code libraries). Successflfl examples of reuse of data resources include: the WordNet thesaurus (Miller el; al., 1993); the Penn Tree Bank (Marcus et al., 1993); the Longmans Dictionary of Contemporary English (Summers, 1995). A large number of papers report results relative to these and other resources, and these successes have spawned a num1See (Boguraev et al., 1995) or (Cunningham et al., 1995) for discussion of the significance of this label. ber of projects with similar directions, one of the latest examples of which being ELRA, tile EuroI)ean Language Resources Association. The reuse of algorithmic resources remains more limited (Gunninghaln et al., 1994). There are a number of reasons for this, including: 1. cultural resistance to reuse, e.g. mistrust of 'foreign' code; 2. integration overheads. In some respects these probleIns are insoluble without general changes in the way NLE research is done researchers will always be reluctant to use poorly-documented or unreliable systems as part of their work, for exmnple. In other respects, solutions are possible. They include: 1. increasing the granularity of the units of reuse, i.e. providing sets of small buildingblocks instead of large, Inonolithic systems; 2. increasing the confidence of researchers in available algorithmic resources by increasing their reuse and the amount of testing and evaluation they are subjected to; 3. separating out, the integration problems that are independent of the type of information being processed and reducing the overhead caused by these problems by providing a software architecture for NLE systems. Our view is that succesful algorithmic reuse in NLE will require the provision of support software for NLE in the form of a general architecture and development environment which is specifically designed for text processing systems. Under EPSRC 2 grant GR/K25267 the NLP group at, the University of Sheffield are developing a system that aims to implement this new approach. The system is called GATE the General Architecture for Text Engineering. ~The Engineering and Physical Science Research Council, UK funding body.",fullPaper,cp74
Computer Science,p898,d3,8213bbf377efd4f6bae2ec4950aeaa25a1728646,j216,Technometrics,Taguchi's Quality Engineering Handbook,"points, decisions are made based on results for the study endpoints. In clinical trials, the decisions are usually whether to stop the trial because the efficacy and safety of the treatment can be confirmed already, the safety risks are too great, or the treatment is very unlikely to achieve its therapeutic goal (called stopping for futility). Rules for stopping the trial are made prior to collecting any data. Such rules, called stopping rules, are typically formally defined in a protocol that is completed and approved prior to the start of the trial. Adaptive procedures add the following features to the possible decisions at the interim analyses: (1) the addition or deletion of trial arms in a multiple armed clinical trial, (2) an increase or decrease in the total sample at the end of the study (based on interim estimates of variability and/or other assumed parameters, e.g., effect size), and (3) other changes to the design (such as changes to the inclusion/exclusion criteria for the study subjects). Statisticians in the pharmaceutical and the medical device industries as well as at the National Institutes of Health (NIH) and other medical research institutes will find this book invaluable. Given that most readers of Technometrics are statisticians and practitioners in either the physical, chemical, or engineering sciences, they may not find it as immediately applicable as a biostatistician would. Prior to the development of group sequential procedures there were sequential procedures. Sequential procedures are just like group sequential procedures except that the interim analyses occur after each newly observed data point. These sequential methods were developed (both theory and applications) by Abraham Wald in the United States and George Barnard in the United Kingdom in the 1940s as part of the war effort during World War II. The motivating application was reliability testing of military products such as ammunition. There was a desire to determine that the ammunition was safe and reliable without wasting a lot of ammunition in testing. The same reasoning could apply to any product that requires destructive testing to determine its reliability and is expensive or time consuming to produce. After the war, the practical application was hindered by the need to make and the difficulty of making real-time decisions after every sample. Group sequential methods made the whole idea of sequential testing or monitoring much more useful. The reliability applications may be of interest to the general Technometrics reader, but this book and the text by Jennison and Turnbull (2000) include only clinical trial applications. The authors of the text under review are among the top researchers in the field, and this text by Proschan, Lan, and Wittes very well written and provides thorough and nearly complete coverage of the latest developments in group sequential methods. It also contains a chapter on adaptive sample size methods (Chap. 11). These methods are a subset of the adaptive procedures, and include Stein’s method and others for constructing two-stage designs to deal with nuisance parameters. Among other sample size adjustment methods, the authors include adjusting the sample size based on an interim assessment of the effect size. The few topics in group sequential methods that are not covered in detail are outlined in Chapter 12 (titled “Topics Not Covered”). The text by Jennison and Turnbull (2000) was the first major text on group sequential methods. It came out in 2000 and is considered by many to be a classic on the subject. Both, Jennison and Turnbull are well-known statisticians and they have published widely in the statistics and biostatistics literature. These two texts cover mostly the same topics, are both very current, and both give examples in clinical trials. So a reader, like me, who already owns a copy of Jennison and Turnbull might ask what would be the added value of purchasing Proschan, Lan, and Wittes? I would give the following reasons:",fullPaper,jv216
Mathematics,p898,d6,8213bbf377efd4f6bae2ec4950aeaa25a1728646,j216,Technometrics,Taguchi's Quality Engineering Handbook,"points, decisions are made based on results for the study endpoints. In clinical trials, the decisions are usually whether to stop the trial because the efficacy and safety of the treatment can be confirmed already, the safety risks are too great, or the treatment is very unlikely to achieve its therapeutic goal (called stopping for futility). Rules for stopping the trial are made prior to collecting any data. Such rules, called stopping rules, are typically formally defined in a protocol that is completed and approved prior to the start of the trial. Adaptive procedures add the following features to the possible decisions at the interim analyses: (1) the addition or deletion of trial arms in a multiple armed clinical trial, (2) an increase or decrease in the total sample at the end of the study (based on interim estimates of variability and/or other assumed parameters, e.g., effect size), and (3) other changes to the design (such as changes to the inclusion/exclusion criteria for the study subjects). Statisticians in the pharmaceutical and the medical device industries as well as at the National Institutes of Health (NIH) and other medical research institutes will find this book invaluable. Given that most readers of Technometrics are statisticians and practitioners in either the physical, chemical, or engineering sciences, they may not find it as immediately applicable as a biostatistician would. Prior to the development of group sequential procedures there were sequential procedures. Sequential procedures are just like group sequential procedures except that the interim analyses occur after each newly observed data point. These sequential methods were developed (both theory and applications) by Abraham Wald in the United States and George Barnard in the United Kingdom in the 1940s as part of the war effort during World War II. The motivating application was reliability testing of military products such as ammunition. There was a desire to determine that the ammunition was safe and reliable without wasting a lot of ammunition in testing. The same reasoning could apply to any product that requires destructive testing to determine its reliability and is expensive or time consuming to produce. After the war, the practical application was hindered by the need to make and the difficulty of making real-time decisions after every sample. Group sequential methods made the whole idea of sequential testing or monitoring much more useful. The reliability applications may be of interest to the general Technometrics reader, but this book and the text by Jennison and Turnbull (2000) include only clinical trial applications. The authors of the text under review are among the top researchers in the field, and this text by Proschan, Lan, and Wittes very well written and provides thorough and nearly complete coverage of the latest developments in group sequential methods. It also contains a chapter on adaptive sample size methods (Chap. 11). These methods are a subset of the adaptive procedures, and include Stein’s method and others for constructing two-stage designs to deal with nuisance parameters. Among other sample size adjustment methods, the authors include adjusting the sample size based on an interim assessment of the effect size. The few topics in group sequential methods that are not covered in detail are outlined in Chapter 12 (titled “Topics Not Covered”). The text by Jennison and Turnbull (2000) was the first major text on group sequential methods. It came out in 2000 and is considered by many to be a classic on the subject. Both, Jennison and Turnbull are well-known statisticians and they have published widely in the statistics and biostatistics literature. These two texts cover mostly the same topics, are both very current, and both give examples in clinical trials. So a reader, like me, who already owns a copy of Jennison and Turnbull might ask what would be the added value of purchasing Proschan, Lan, and Wittes? I would give the following reasons:",fullPaper,jv216
Computer Science,p899,d3,294df07a2eaaa9eb39d3404c145ef429f84b56ae,c75,International Conference on Predictive Models in Software Engineering,Data sets and data quality in software engineering,"OBJECTIVE - to assess the extent and types of techniques used to manage quality within software engineering data sets. We consider this a particularly interesting question in the context of initiatives to promote sharing and secondary analysis of data sets. METHOD - we perform a systematic review of available empirical software engineering studies. RESULTS - only 23 out of the many hundreds of studies assessed, explicitly considered data quality. CONCLUSIONS - first, the community needs to consider the quality and appropriateness of the data set being utilised; not all data sets are equal. Second, we need more research into means of identifying, and ideally repairing, noisy cases. Third, it should become routine to use sensitivity analysis to assess conclusion stability with respect to the assumptions that must be made concerning noise levels.",fullPaper,cp75
Computer Science,p904,d3,de760552279b241976676f4723a159060a433198,c42,IEEE Working Conference on Mining Software Repositories,GHTorrent: Github's data from a firehose,"A common requirement of many empirical software engineering studies is the acquisition and curation of data from software repositories. During the last few years, GitHub has emerged as a popular project hosting, mirroring and collaboration platform. GitHub provides an extensive REST API, which enables researchers to retrieve both the commits to the projects' repositories and events generated through user actions on project resources. GHTorrent aims to create a scalable off line mirror of GitHub's event streams and persistent data, and offer it to the research community as a service. In this paper, we present the project's design and initial implementation and demonstrate how the provided datasets can be queried and processed.",fullPaper,cp42
Computer Science,p907,d3,49a94b4a091924eb287eceb62f5dd9d656a3ec74,c16,International Conference on Data Science and Advanced Analytics,A Review of data fusion models and architectures: towards engineering guidelines,Abstract,poster,cp16
Computer Science,p911,d3,d93d4b8a821a6761341fa9ace16b0e3a2ab232c3,j219,International Journal of Production Research,Smart manufacturing,"Manufacturing has evolved and become more automated, computerised and complex. In this paper, the origin, current status and the future developments in manufacturing are disused. Smart manufacturing is an emerging form of production integrating manufacturing assets of today and tomorrow with sensors, computing platforms, communication technology, control, simulation, data intensive modelling and predictive engineering. It utilises the concepts of cyber-physical systems spearheaded by the internet of things, cloud computing, service-oriented computing, artificial intelligence and data science. Once implemented, these concepts and technologies would make smart manufacturing the hallmark of the next industrial revolution. The essence of smart manufacturing is captured in six pillars, manufacturing technology and processes, materials, data, predictive engineering, sustainability and resource sharing and networking. Material handling and supply chains have been an integral part of manufacturing. The anticipated developments in material handling and transportation and their integration with manufacturing driven by sustainability, shared services and service quality and are outlined. The future trends in smart manufacturing are captured in ten conjectures ranging from manufacturing digitisation and material-product-process phenomenon to enterprise dichotomy and standardisation.",fullPaper,jv219
Computer Science,p913,d3,f5705a2c23603579e25c31d3390ddc083f5bfefa,c7,International Symposium on Intelligent Data Analysis,Data Mining for Scientific and Engineering Applications,Abstract,poster,cp7
Computer Science,p914,d3,3de62b3ce8b5de79ccc6e381eedbd49a328a520f,c87,International Conference on Big Data Research,Combination of Evidence in Dempster-Shafer Theory,"Dempster-Shafer theory offers an alternative to traditional probabilistic theory for the mathematical representation of uncertainty. The significant innovation of this framework is that it allows for the allocation of a probability mass to sets or intervals. Dempster-Shafer theory does not require an assumption regarding the probability of the individual constituents of the set or interval. This is a potentially valuable tool for the evaluation of risk and reliability in engineering applications when it is not possible to obtain a precise measurement from experiments, or when knowledge is obtained from expert elicitation. An important aspect of this theory is the combination of evidence obtained from multiple sources and the modeling of conflict between them. This report surveys a number of possible combination rules for Dempster-Shafer structures and provides examples of the implementation of these rules for discrete and interval-valued data.",poster,cp87
Computer Science,p915,d3,09e71e98a1f9b04b97165b0e56935e1054239556,j220,Journal of Computing and Information Science in Engineering,Knowledge Representation and Ontology Mapping Methods for Product Data in Engineering Applications,"This work seeks to create a semantic approach that uses ontologies for sharing knowledge related to product data in CAD/CAE applications and for integrating the design evaluation information that these applications individually provide. Our overall approach is coined OADE, Ontology-based Adaptive Design Evaluation. This paper reports on a piece of our ongoing work in this area. The key contributions of this paper include methods for the design of knowledge representation in product design and analysis, population of product data semantics, creation of ontology mapping methods and mapping representations, and mapping of product data semantics to the target application. The mapping method finds matching concepts between different ontologies based on three basic concept relation types: composition, inheritance, and attribute. A prototype implementation is being created using technologies such as OWL (representation tool), Jena (ontology builder), and Protege (ontology editor) to demonstrate the approach for integrating a parametric CAD system, custom virtual assembly application, and an ergonomics engineering application. An example is given in this paper to illustrate how this approach can help integration between a product design application and an assembly simulation analysis application. The significance of this work is that it will provide the capability to create, share, and exchange knowledge for solving design evaluation challenges involving multiple applications and multiple viewpoints. A design decision can thus be described using the common concepts across the diverse entities.Copyright © 2008 by ASME",fullPaper,jv220
Computer Science,p916,d3,0a4c6ab97208b9dd5d47a60ac7e55aeefdabe6aa,c98,Vision,The Measurement of Power Spectra from the Point of View of Communications Engineering,"The measurement of power spectra is a problem of steadily increasing importance which appears to some to be primarily a problem in statistical estimation. Others may see it as a problem of instrumentation, recording and analysis which vitally involves the ideas of transmission theory. Actually, ideas and techniques from both fields are needed. When they are combined, they provide a basis for developing the insight necessary (i) to plan both the acquisition of adequate data and sound procedures for its reduction to meaningful estimates and (ii) to interpret these estimates correctly and usefully. This account attempts to provide and relate the necessary ideas and techniques in reasonable detail — Part I of this article appeared in the January, 1958 issue of THE BELL SYSTEM TECHNICAL JOURNAL.",poster,cp98
Computer Science,p917,d3,0476ddfada4efc840f38a71588afb6a88874e5dc,c76,Group,Data at work: supporting sharing in science and engineering,"Data are a fundamental component of science and engineering work, and the ability to share data is critical to the validation and progress of science. Data sharing and reuse in some fields, however, has proven to be a difficult problem. This paper argues that the development of effective CSCW systems to support data sharing in work groups requires a better understanding of the use of data in practice. Drawing on our work with three scientific disciplines, we show that data play two general roles in scientific communities: 1) they serve as evidence to support scientific inquiry, and 2) they make a social contribution to the establishment and maintenance of communities of practice. A clearer consideration and understanding of these roles can contribute to the design of more effective data sharing systems. We suggest that this can be achieved through supporting social interaction around data abstractions, reaching beyond current metadata models, and supporting the social roles of data.",fullPaper,cp76
Computer Science,p918,d3,30e741a0330cdcaf6a6466eaca2f09c8bd604b57,j170,IEEE Transactions on Software Engineering,A survey of controlled experiments in software engineering,"The classical method for identifying cause-effect relationships is to conduct controlled experiments. This paper reports upon the present state of how controlled experiments in software engineering are conducted and the extent to which relevant information is reported. Among the 5,453 scientific articles published in 12 leading software engineering journals and conferences in the decade from 1993 to 2002, 103 articles (1.9 percent) reported controlled experiments in which individuals or teams performed one or more software engineering tasks. This survey quantitatively characterizes the topics of the experiments and their subjects (number of subjects, students versus professionals, recruitment, and rewards for participation), tasks (type of task, duration, and type and size of application) and environments (location, development tools). Furthermore, the survey reports on how internal and external validity is addressed and the extent to which experiments are replicated. The gathered data reflects the relevance of software engineering experiments to industrial practice and the scientific maturity of software engineering research.",fullPaper,jv170
Computer Science,p919,d3,4e5f14131db1dada5eab8c5d4bf1317e611bb107,c98,Vision,A Pattern Recognition Approach for Software Engineering Data Analysis,"In order to plan, control, and evaluate the software development process, one needs to collect and analyze data in a meaningful way. Classical techniques for such analysis are not always well suited to software engineering data. A pattern recognition approach for analyzing software engineering data, called optimized set reduction (OSR), that addresses many of the problems associated with the usual approaches is described. Methods are discussed for using the technique for prediction, risk management, and quality evaluation. Experimental results are provided to demonstrate the effectiveness of the technique for the particular application of software cost estimation. >",poster,cp98
Computer Science,p921,d3,53e0cea00e0653faba15d91f9b5673576af65967,c15,Pacific Symposium on Biocomputing,Software Engineering Data Collection for Field Studies,Abstract,poster,cp15
Computer Science,p925,d3,fef2e8c362418adee1a1464e2912ff4e5b7f6817,j178,Data & Knowledge Engineering,The thematic and citation landscape of Data and Knowledge Engineering,Abstract,fullPaper,jv178
Computer Science,p926,d3,250b4f05982b491ad80ba8b986d958eedb69a6be,j216,Technometrics,ROBPCA: A New Approach to Robust Principal Component Analysis,"We introduce a new method for robust principal component analysis (PCA). Classical PCA is based on the empirical covariance matrix of the data and hence is highly sensitive to outlying observations. Two robust approaches have been developed to date. The first approach is based on the eigenvectors of a robust scatter matrix such as the minimum covariance determinant or an S-estimator and is limited to relatively low-dimensional data. The second approach is based on projection pursuit and can handle high-dimensional data. Here we propose the ROBPCA approach, which combines projection pursuit ideas with robust scatter matrix estimation. ROBPCA yields more accurate estimates at noncontaminated datasets and more robust estimates at contaminated data. ROBPCA can be computed rapidly, and is able to detect exact-fit situations. As a by-product, ROBPCA produces a diagnostic plot that displays and classifies the outliers. We apply the algorithm to several datasets from chemometrics and engineering.",fullPaper,jv216
Mathematics,p926,d6,250b4f05982b491ad80ba8b986d958eedb69a6be,j216,Technometrics,ROBPCA: A New Approach to Robust Principal Component Analysis,"We introduce a new method for robust principal component analysis (PCA). Classical PCA is based on the empirical covariance matrix of the data and hence is highly sensitive to outlying observations. Two robust approaches have been developed to date. The first approach is based on the eigenvectors of a robust scatter matrix such as the minimum covariance determinant or an S-estimator and is limited to relatively low-dimensional data. The second approach is based on projection pursuit and can handle high-dimensional data. Here we propose the ROBPCA approach, which combines projection pursuit ideas with robust scatter matrix estimation. ROBPCA yields more accurate estimates at noncontaminated datasets and more robust estimates at contaminated data. ROBPCA can be computed rapidly, and is able to detect exact-fit situations. As a by-product, ROBPCA produces a diagnostic plot that displays and classifies the outliers. We apply the algorithm to several datasets from chemometrics and engineering.",fullPaper,jv216
Computer Science,p928,d3,f18c8e5970e47d3ea5079412c6e05bcdfc7f84da,c84,EUROCON Conference,Computational Thermodynamics: The Calphad Method,"Phase diagrams are used in materials research and engineering to understand the interrelationship between composition, microstructure and process conditions. In complex systems, computational methods such as CALPHAD are employed to model thermodynamic properties for each phase and simulate multicomponent phase behavior. Written by recognized experts in the field, this is the first introductory guide to the CALPHAD method, providing a theoretical and practical approach. Building on core thermodynamic principles, this book applies crystallography, first principles methods and experimental data to computational phase behavior modeling using the CALPHAD method. With a chapter dedicated to creating thermodynamic databases, the reader will be confident in assessing, optimizing and validating complex thermodynamic systems alongside database construction and manipulation. Several case studies put the methods into a practical context, making this suitable for use on advanced materials design and engineering courses and an invaluable reference to those using thermodynamic data in their research or simulations.",poster,cp84
Computer Science,p930,d3,a996987a529f6c436f1c348af2ff4c1983b18b23,j222,IEEE Transactions on Parallel and Distributed Systems,EPPA: An Efficient and Privacy-Preserving Aggregation Scheme for Secure Smart Grid Communications,"The concept of smart grid has emerged as a convergence of traditional power system engineering and information and communication technology. It is vital to the success of next generation of power grid, which is expected to be featuring reliable, efficient, flexible, clean, friendly, and secure characteristics. In this paper, we propose an efficient and privacy-preserving aggregation scheme, named EPPA, for smart grid communications. EPPA uses a superincreasing sequence to structure multidimensional data and encrypt the structured data by the homomorphic Paillier cryptosystem technique. For data communications from user to smart grid operation center, data aggregation is performed directly on ciphertext at local gateways without decryption, and the aggregation result of the original data can be obtained at the operation center. EPPA also adopts the batch verification technique to reduce authentication cost. Through extensive analysis, we demonstrate that EPPA resists various security threats and preserve user privacy, and has significantly less computation and communication overhead than existing competing approaches.",fullPaper,jv222
Computer Science,p932,d3,911d20ba5972fb5660cdd6c4a8842d635366a577,c94,International Conferences on Contemporary Computing and Informatics,An evolutionary‐based data mining technique for assessment of civil engineering systems,"Purpose – Analysis of many civil engineering phenomena is a complex problem due to the participation of a large number of factors involved. Traditional methods usually suffer from a lack of physical understanding. Furthermore, the simplifying assumptions that are usually made in the development of the traditional methods may, in some cases, lead to very large errors. The purpose of this paper is to present a new method, based on evolutionary polynomial regression (EPR) for capturing nonlinear interaction between various parameters of civil engineering systems.Design/methodology/approach – EPR is a data‐driven method based on evolutionary computing, aimed to search for polynomial structures representing a system. In this technique, a combination of the genetic algorithm and the least‐squares method is used to find feasible structures and the appropriate constants for those structures.Findings – Capabilities of the EPR methodology are illustrated by application to two complex practical civil engineering pro...",poster,cp94
Computer Science,p935,d3,8cc8af9a9cf9d77f72894a4580db0da7b4efcdef,j223,Requirements Engineering,An approach to engineering the requirements of data warehouses,Abstract,fullPaper,jv223
Computer Science,p936,d3,c017f9ac893ebf4dc48c97415f0f095b18d7c43f,c23,International Conference on Open and Big Data,"Data-based mechanistic modelling of environmental, ecological, economic and engineering systems.",Abstract,poster,cp23
Computer Science,p937,d3,87f8ad665dd8dae1a65f8c5bfbd91da1c1e70e2d,j224,Journal of Systems Science and Systems Engineering,Big Data: Unleashing information,Abstract,fullPaper,jv224
Computer Science,p938,d3,982de3bd9da5b6793be3f1221e896621242cbcbf,j185,Proceedings of the IEEE,Engineering applications of the self-organizing map,"The self-organizing map (SOM) method is a new, powerful software tool for the visualization of high-dimensional data. It converts complex, nonlinear statistical relationships between high-dimensional data into simple geometric relationships on a low-dimensional display. As it thereby compresses information while preserving the most important topological and metric relationships of the primary data elements on the display, it may also be thought to produce some kind of abstractions. The term self-organizing map signifies a class of mappings defined by error-theoretic considerations. In practice they result in certain unsupervised, competitive learning processes, computed by simple-looking SOM algorithms. Many industries have found the SOM-based software tools useful. The most important property of the SOM, orderliness of the input-output mapping, can be utilized for many tasks: reduction of the amount of training data, speeding up learning nonlinear interpolation and extrapolation, generalization, and effective compression of information for its transmission.",fullPaper,jv185
Computer Science,p939,d3,9ed09a07aa1e4a2010a16e687b56f12f4e7df58f,c79,IEEE Symposium on Computational Intelligence for Engineering Solutions,A Model-Driven Goal-Oriented Requirement Engineering Approach for Data Warehouses,Abstract,poster,cp79
Computer Science,p941,d3,535489ffb7a3c9a226a593e52f9a27466fa5f9af,c53,International Conference on Learning Representations,Numerical Modeling of Wind Turbine Wakes,"An aerodynamical model for studying three-dimensional flow fields about wind turbine rotors is presented. The developed algorithm combines a three-dimensional Navier-Stokes solver with a so-called actuator line technique in which the loading is distributed along lines representing the blade forces. The loading is determined iteratively using a bladeelement approach and tabulated airfoil data. Computations are carried out for a 500 kW Nordtank wind turbine equipped with three LM19.1 blades. The computations give detailed information about basic features of wind turbine wakes, including distributions of interference factors and vortex structures. The model serves in particular to analyze and verify the validity of the basic assumptions employed in the simple engineering models",poster,cp53
Computer Science,p942,d3,854055131cb3f029ccf2ea2f7ce8aa675d5d8f6e,c77,Visualization for Computer Security,Visual Reverse Engineering of Binary and Data Files,Abstract,fullPaper,cp77
Computer Science,p944,d3,49d1a61d240300c2485fc08ec2c2bb01d1dff11e,j226,Information Systems,Toward data mining engineering: A software engineering approach,Abstract,fullPaper,jv226
Computer Science,p947,d3,fa26a55ec872f03388343c7bbde36f19d6abc429,c70,Annual Meeting of the Association for Computational Linguistics,Transcriptional Regulatory Networks in Saccharomyces cerevisiae,"One of the main goals of systems biology is reverse engineering network structures from experimental data. Using quantitative networks measures, biological processes can be characterized and understood on a systems level. This paper reviews the scientific literature on the transcriptional regulatory networks of Saccharomyces cerevisiae to discuss what kind of structures can be identified and what this means biologically. This discussion comprises network characteristics, network dynamics in biological processes, and robustness; an inherent emergent property of networks.",poster,cp70
Computer Science,p948,d3,4ba01f5f01a3e6cc9f7f08529a49b024148c8843,j227,International Journal of Information Quality,Developing a data quality framework for asset management in engineering organisations,"Data Quality (DQ) is seen as critical to effective business decision-making. However, maintaining DQ is often acknowledged as problematic. Asset data is the key enabler in gaining control of assets. The quality asset data provides the foundation for effective Asset Management (AM). Researches have indicated that achieving AM DQ is the key challenge engineering organisations face today. This paper investigates the DQ issues emerging from the unique nature of engineering AM data. It presents an exploratory research of a large scale national-wide DQ survey on how Australian engineering organisations address DQ issues, and proposes an AM specific DQ framework.",fullPaper,jv227
Business,p948,d9,4ba01f5f01a3e6cc9f7f08529a49b024148c8843,j227,International Journal of Information Quality,Developing a data quality framework for asset management in engineering organisations,"Data Quality (DQ) is seen as critical to effective business decision-making. However, maintaining DQ is often acknowledged as problematic. Asset data is the key enabler in gaining control of assets. The quality asset data provides the foundation for effective Asset Management (AM). Researches have indicated that achieving AM DQ is the key challenge engineering organisations face today. This paper investigates the DQ issues emerging from the unique nature of engineering AM data. It presents an exploratory research of a large scale national-wide DQ survey on how Australian engineering organisations address DQ issues, and proposes an AM specific DQ framework.",fullPaper,jv227
Computer Science,p949,d3,4b3ecddd09912cb7de31ea2ab77411416a3f33f3,c78,ACM/IEEE International Conference on Model Driven Engineering Languages and Systems,Industrial Adoption of Model-Driven Engineering: Are the Tools Really the Problem?,Abstract,fullPaper,cp78
Computer Science,p950,d3,0ffae70637e5ceef913c56ad290d22585bd27d49,j228,Computational Geosciences,Engineering new paths to water data,Abstract,fullPaper,jv228
Computer Science,p952,d3,a0af28958edf2bc84285152bc0bed734dfc250a9,j229,Journal on Data Semantics,On Using Conceptual Data Modeling for Ontology Engineering,Abstract,fullPaper,jv229
Computer Science,p953,d3,b0d04d05adf63cb635215f142e22f83d48cbb81b,j230,Intelligent Data Analysis,Data mining in software engineering,"The increased availability of data created as part of the software development process allows us to apply novel analysis techniques on the data and use the results to guide the process's optimization. In this paper we describe various data sources and discuss the principles and techniques of data mining as applied on software engineering data. Data that can be mined is generated by most parts of the development process: requirements elicitation, development analysis, testing, debugging, and maintenance. Based on this classification we survey the mining approaches that have been used and categorize them according to the corresponding parts of the development process and the task they assist. Thus the survey provides researchers with a concise overview of data mining techniques applied to software engineering data, and aids practitioners on the selection of appropriate data mining techniques for their work.",fullPaper,jv230
Computer Science,p955,d3,7bbc0a66e843e2ce8c01ea4ed531bd81b37cf3eb,c79,IEEE Symposium on Computational Intelligence for Engineering Solutions,Why Ricker wavelets are successful in processing seismic data: Towards a theoretical explanation,"In many engineering applications ranging from engineering seismology to petroleum engineering and civil engineering, it is important to process seismic data. In processing seismic data, it turns out to be very efficient to describe the signal's spectrum as a linear combination of Ricker wavelet spectra. In this paper, we provide a possible theoretical explanation for this empirical efficiency. Specifically, signal propagation through several layers is discussed, and it is shown that the Ricker wavelet is the simplest non-trivial solution for the corresponding data processing problem, under the condition that the described properties of the approximation family are satisfied.",fullPaper,cp79
Mathematics,p955,d6,7bbc0a66e843e2ce8c01ea4ed531bd81b37cf3eb,c79,IEEE Symposium on Computational Intelligence for Engineering Solutions,Why Ricker wavelets are successful in processing seismic data: Towards a theoretical explanation,"In many engineering applications ranging from engineering seismology to petroleum engineering and civil engineering, it is important to process seismic data. In processing seismic data, it turns out to be very efficient to describe the signal's spectrum as a linear combination of Ricker wavelet spectra. In this paper, we provide a possible theoretical explanation for this empirical efficiency. Specifically, signal propagation through several layers is discussed, and it is shown that the Ricker wavelet is the simplest non-trivial solution for the corresponding data processing problem, under the condition that the described properties of the approximation family are satisfied.",fullPaper,cp79
Computer Science,p956,d3,c2841ad5e94943d4fde4e7b059ffb51783922fca,c89,Conference on Uncertainty in Artificial Intelligence,The meaningful use of big data: four perspectives -- four challenges,"Twenty-five Semantic Web and Database researchers met at the 2011 STI Semantic Summit in Riga, Latvia July 6-8, 2011[1] to discuss the opportunities and challenges posed by Big Data for the Semantic Web, Semantic Technologies, and Database communities. The unanimous conclusion was that the greatest shared challenge was not only engineering Big Data, but also doing so meaningfully. The following are four expressions of that challenge from different perspectives.",poster,cp89
Economics,p956,d11,c2841ad5e94943d4fde4e7b059ffb51783922fca,c89,Conference on Uncertainty in Artificial Intelligence,The meaningful use of big data: four perspectives -- four challenges,"Twenty-five Semantic Web and Database researchers met at the 2011 STI Semantic Summit in Riga, Latvia July 6-8, 2011[1] to discuss the opportunities and challenges posed by Big Data for the Semantic Web, Semantic Technologies, and Database communities. The unanimous conclusion was that the greatest shared challenge was not only engineering Big Data, but also doing so meaningfully. The following are four expressions of that challenge from different perspectives.",poster,cp89
Computer Science,p957,d3,c9c73f5a1668b8bf12aae2efb6ac5a5a2c34002a,j206,Computer,"CAP twelve years later: How the ""rules"" have changed","The CAP theorem asserts that any networked shared-data system can have only two of three desirable properties. However, by explicitly handling partitions, designers can optimize consistency and availability, thereby achieving some trade-off of all three. The featured Web extra is a podcast from Software Engineering Radio, in which the host interviews Dwight Merriman about the emerging NoSQL movement, the three types of nonrelational data stores, Brewer's CAP theorem, and much more.",fullPaper,jv206
Computer Science,p958,d3,fdd8cc8a0152d09d1e32da6e45196b569abe146a,c80,International Symposium on Software Engineering for Adaptive and Self-Managing Systems,Data-Driven Continuous Evolution of Smart Systems,"As Marc Andreessen said in his Wall Street Journal OpEd, software is eating the world. The systems that we are building today and in the near future will exhibit levels of autonomy that will put new demands on the engineering of such systems. Although promising examples of autonomous systems exist, there is no established methodology for systematically building autonomous systems that employ modern software engineering technology such as continuous deployment and data-driven engineering. The contribution of this paper is twofold. First, it identifies and presents the challenge of continuous evolution of autonomous systems as a well-defined problem that needs to be addressed by software engineering research. Second, it presents a conceptual solution to this problem that integrates the development of new software for autonomous systems by R&D teams with systematic experimentation by autonomous systems.",fullPaper,cp80
Computer Science,p959,d3,f9e74ce54eaa219b08333f9c129a736dd0619863,c48,"Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing","A case study of multiservice, multipriority traffic engineering design for data networks","We present techniques, some of which are novel, for traffic engineering in QoS-supported data networks, and also illustrate the application of these techniques in a case study. In the interest of scalability all these techniques use multicommodity flow (MCF) solution techniques as primitives. The techniques address the design of topology and size of explicit routes in MPLS-supported IP networks and VPNs. The techniques are for network-wide optimization subject to constraints on routing imposed by end-to-end QoS and other considerations. The notion of admissible route sets specific to service class and source-destination pair is used to differentiate QoS constraints on real-time services, such as Internet telephony and video, and relatively delay insensitive services, such as premium data. Contrasting optimization techniques are given for services, such as best effort, for which no restriction on routes are imposed. Another important traffic engineering requirement addressed is priorities, for which an efficient and accurate design technique is given.",poster,cp48
Computer Science,p960,d3,ec9196b9edd46c5c59124ca776ca71538655fbc6,c74,International Conference on Computational Linguistics,Empirical Data Modeling in Software Engineering Using Radical Basis Functions,"Many empirical studies in software engineering involve relationships between various process and product characteristics derived via linear regression analysis. We propose an alternative modeling approach using radial basis functions (RBFs) which provide a flexible way to generalize linear regression function. Further, RBF models possess strong mathematical properties of universal and best approximation. We present an objective modeling methodology for determining model parameters using our recent SG algorithm, followed by a model selection procedure based on generalization ability. Finally, we describe a detailed RBF modeling study for software effort estimation using a well-known NASA dataset.",poster,cp74
Computer Science,p961,d3,60cef66236eee4cc7c124abfa04d27cbc379362f,c58,Extreme Science and Engineering Discovery Environment,Missing Data in Software Engineering,Abstract,poster,cp58
Computer Science,p963,d3,726cb75a8a10b915f463cb45a6dbaf266eabf7b4,j231,International Journal of Data Analysis Techniques and Strategies,Applications of data mining in software engineering,"Software engineering processes are complex, and the related activities often produce a large number and variety of artefacts, making them well-suited to data mining. Recent years have seen an increase in the use of data mining techniques on such artefacts with the goal of analysing and improving software processes for a given organisation or project. After a brief survey of current uses, we offer insight into how data mining can make a significant contribution to the success of current software engineering efforts.",fullPaper,jv231
Computer Science,p965,d3,d6cc07dcf7d76a216c2dbe50b716e29d450141ee,c42,IEEE Working Conference on Mining Software Repositories,Design Data for Engineering Ceramics: A Review of the Flexure Test,"The uniaxial strength of engineering ceramics is often measured by the well-known flexure strength test method there is a risk that flexure data are not representative of the properties of fabricated components. Reliability estimates for components based upon statistical extrapolation techniques from flexure data may not be valid. This paper reviews the problem and judges the usefulness of flexure data for design purposes. It is shown that some of the limitations of flexure data apply; to other modes of testing, including direct tension testing",poster,cp42
Chemistry,p965,d8,d6cc07dcf7d76a216c2dbe50b716e29d450141ee,c42,IEEE Working Conference on Mining Software Repositories,Design Data for Engineering Ceramics: A Review of the Flexure Test,"The uniaxial strength of engineering ceramics is often measured by the well-known flexure strength test method there is a risk that flexure data are not representative of the properties of fabricated components. Reliability estimates for components based upon statistical extrapolation techniques from flexure data may not be valid. This paper reviews the problem and judges the usefulness of flexure data for design purposes. It is shown that some of the limitations of flexure data apply; to other modes of testing, including direct tension testing",poster,cp42
Computer Science,p968,d3,388d8d15920fd7994bfe40b9247a8213e06e7a16,c23,International Conference on Open and Big Data,Engineering Privacy by Design,"The design and implementation of privacy requirements in systems is a difficult problem and requires the translation of complex social, legal and ethical concerns into systems requirements. The concept of “privacy by design” has been proposed to serve as a guideline on how to address these concerns. “Privacy by design” consists of a number of principles that can be applied from the onset of systems development to mitigate privacy concerns and achieve data protection compliance. However, these principles remain vague and leave many open questions about their application when engineering systems. In this paper we show how starting from data minimization is a necessary and foundational first step to engineer systems in line with the principles of privacy by design. We first discuss what data minimization can mean from a security engineering perspective. We then present a summary of two case studies in which privacy is achieved by minimizing different types of data, according to the purpose of each application. First, we present a privacypreserving ePetition system, in which user’s privacy is guaranteed by hiding their identity from the provider while revealing their votes. Secondly, we study a road tolling system, in which users have to be identified for billing reasons and data minimization is applied to protect further sensitive information (in this case location information). The case studies make evident that the application of data minimization does not necessarily imply anonymity, but may also be achieved by means of concealing information related to identifiable individuals. In fact, different kinds of data minimization are possible, and each system requires careful crafting of data minimization best suited for its purpose. Most importantly, the two case studies underline that the interpretation of privacy by design principles requires specific engineering expertise, contextual analysis, and a balancing of multilateral security and privacy interests. They show that privacy by design is a productive space in which there is no one way of solving the problems. Based on our analysis of the two case studies, we argue that engineering systems according to the privacy by design principles requires the development of generalizable methodologies that build upon the principle of data minimization. However, the complexity of this engineering task demands caution against reducing such methodologies to “privacy by design check lists” that can easily be ticked away for compliance reasons while not mitigating some of the risks that privacy by design is meant to address.",poster,cp23
Computer Science,p971,d3,24f6cb1a80792138c856cc8f759672d29eaa4fb7,j233,IEEE Journal on Selected Areas in Communications,GreenDCN: A General Framework for Achieving Energy Efficiency in Data Center Networks,"The popularization of cloud computing has raised concerns over the energy consumption that takes place in data centers. In addition to the energy consumed by servers, the energy consumed by large numbers of network devices emerges as a significant problem. Existing work on energy-efficient data center networking primarily focuses on traffic engineering, which is usually adapted from traditional networks. We propose a new framework to embrace the new opportunities brought by combining some special features of data centers with traffic engineering. Based on this framework, we characterize the problem of achieving energy efficiency with a time-aware model, and we prove its NP-hardness with a solution that has two steps. First, we solve the problem of assigning virtual machines (VM) to servers to reduce the amount of traffic and to generate favorable conditions for traffic engineering. The solution reached for this problem is based on three essential principles that we propose. Second, we reduce the number of active switches and balance traffic flows, depending on the relation between power consumption and routing, to achieve energy conservation. Experimental results confirm that, by using this framework, we can achieve up to 50 percent energy savings. We also provide a comprehensive discussion on the scalability and practicability of the framework.",fullPaper,jv233
Computer Science,p978,d3,bae9848263949626961936b90565d4b5b407c00c,c62,International Conference on Advanced Data and Information Engineering,Recent Techniques of Clustering of Time Series Data: A Survey,"Time-Series clustering is one of the important concepts of data mining that is used to gain insight into the mechanism that generate the time-series and predicting the future values of the given time-series. Time-series data are frequently very large and elements of these kinds of data have temporal ordering. The clustering of time series is organized into three groups depending upon whether they work directly on raw data either in frequency or time domain, indirectly with the features extracted from the raw data or with model built from raw data. In this paper, we have shown the survey and summarization of previous work that investigated the clustering of time series in various application domains ranging from science, engineering, business, finance, economic, health care, to government.",poster,cp62
Computer Science,p980,d3,b5fd36b9492a4682a4dd5f7454425c5c42b7e322,j236,Journal of Aerospace Computing Information and Communication,Engineering Complex Embedded Systems with State Analysis and the Mission Data System,"It has become clear that spacecraft system complexity is reaching a threshold where customary methods of control are no longer affordable or sufficiently reliable. At the heart of this problem are the conventional approaches to systems and software engineering based on subsystem-level functional decomposition, which fail to scale in the tangled web of interactions typically encountered in complex spacecraft designs. Furthermore, there is a fundamental gap between the requirements on software specified by systems engineers and the implementation of these requirements by software engineers. Software engineers must perform the translation of requirements into software code, hoping to accurately capture the systems engineer's understanding of the system behavior, which is not always explicitly specified. This gap opens up the possibility for misinterpretation of the systems engineer s intent, potentially leading to software errors. This problem is addressed by a systems engineering methodology called State Analysis, which provides a process for capturing system and software requirements in the form of explicit models. This paper describes how requirements for complex aerospace systems can be developed using State Analysis and how these requirements inform the design of the system software, using representative spacecraft examples.",fullPaper,jv236
Computer Science,p981,d3,e4b9a5ed3f838da72a8f3169a01be3268c4d3c2c,c90,International Conference on Collaboration Technologies and Systems,NGA Project Strong-Motion Database,"A key component of the NGA research project was the development of a strong-motion database with improved quality and content that could be used for ground-motion research as well as for engineering practice. Development of the NGA database was executed through the Lifelines program of the PEER Center with contributions from several research organizations and many individuals in the engineering and seismological communities. Currently, the data set consists of 3551 publicly available multi-component records from 173 shallow crustal earthquakes, ranging in magnitude from 4.2 to 7.9. Each acceleration time series has been corrected and filtered, and pseudo absolute spectral acceleration at multiple damping levels has been computed for each of the 3 components of the acceleration time series. The lowest limit of usable spectral frequency was determined based on the type of filter and the filter corner frequency. For NGA model development, the two horizontal acceleration components were further rotated to form the orientation-independent measure of horizontal ground motion (GMRotI50). In addition to the ground-motion parameters, a large and comprehensive list of metadata characterizing the recording conditions of each record was also developed. NGA data have been systematically checked and reviewed by experts and NGA developers.",poster,cp90
Computer Science,p982,d3,2114c0768c341b82fd22ce3df5a0aab1cf37134f,c81,ACM Symposium on Applied Computing,Information requirements engineering for data warehouse systems,"Information requirements analysis for data warehouse systems differs significantly from requirements analysis for conventional information systems. Based on interviews with project managers and information systems managers, requirements for a methodological support of information requirements analysis for data warehouse systems are derived. Existing approaches are reviewed with regard to these requirements. Using the method engineering approach, a comprehensive methodology that supports the entire process of determining information requirements of data warehouse users, matching information requirements with actual information supply, evaluating and homogenizing resulting information requirements, establishing priorities for unsatisfied information requirements, and formally specifying the results as a basis for subsequent phases of the data warehouse development (sub)project has been proposed. The most important sources for methodology components were four in-depth case studies of information requirements analysis practices observed in data warehousing development projects of large organizations. In this paper, these case studies are presented and the resulting consolidated methodology is summarized. While an application of the proposed methodology in its entirety is still outstanding, its components have been successfully applied in actual data warehouse development projects.",fullPaper,cp81
Computer Science,p985,d3,03c7355211f80f7e415fdeb544520776a3d4b0d1,c85,IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering,Data warehousing and analytics infrastructure at facebook,"Scalable analysis on large data sets has been core to the functions of a number of teams at Facebook - both engineering and non-engineering. Apart from ad hoc analysis of data and creation of business intelligence dashboards by analysts across the company, a number of Facebook's site features are also based on analyzing large data sets. These features range from simple reporting applications like Insights for the Facebook Advertisers, to more advanced kinds such as friend recommendations. In order to support this diversity of use cases on the ever increasing amount of data, a flexible infrastructure that scales up in a cost effective manner, is critical. We have leveraged, authored and contributed to a number of open source technologies in order to address these requirements at Facebook. These include Scribe, Hadoop and Hive which together form the cornerstones of the log collection, storage and analytics infrastructure at Facebook. In this paper we will present how these systems have come together and enabled us to implement a data warehouse that stores more than 15PB of data (2.5PB after compression) and loads more than 60TB of new data (10TB after compression) every day. We discuss the motivations behind our design choices, the capabilities of this solution, the challenges that we face in day today operations and future capabilities and improvements that we are working on.",poster,cp85
Computer Science,p987,d3,f35e1e920e7d85dd4b54aadc1b4dc57dadf15778,j238,Journal of Open Source Software,Methods and Algorithms for Correlation Analysis in R,"Correlations tests are arguably one of the most commonly used statistical procedures, and are used as a basis in many applications such as exploratory data analysis, structural modelling, data engineering etc. In this context, we present correlation, a toolbox for the R language (R Core Team, 2019) and part of the easystats collection, focused on correlation analysis. Its goal is to be lightweight, easy to use, and allows for the computation of many different kinds of correlations, such as:",fullPaper,jv238
Psychology,p987,d10,f35e1e920e7d85dd4b54aadc1b4dc57dadf15778,j238,Journal of Open Source Software,Methods and Algorithms for Correlation Analysis in R,"Correlations tests are arguably one of the most commonly used statistical procedures, and are used as a basis in many applications such as exploratory data analysis, structural modelling, data engineering etc. In this context, we present correlation, a toolbox for the R language (R Core Team, 2019) and part of the easystats collection, focused on correlation analysis. Its goal is to be lightweight, easy to use, and allows for the computation of many different kinds of correlations, such as:",fullPaper,jv238
Computer Science,p988,d3,163475ddb7bb20393c4065f68d467fdf856b0a0e,c9,Big Data,Summary and Discussion,Abstract,poster,cp9
Computer Science,p992,d3,7feb6a3936b175373fe1f9b20718caa43542a400,c42,IEEE Working Conference on Mining Software Repositories,An extensive comparison of bug prediction approaches,"Reliably predicting software defects is one of software engineering's holy grails. Researchers have devised and implemented a plethora of bug prediction approaches varying in terms of accuracy, complexity and the input data they require. However, the absence of an established benchmark makes it hard, if not impossible, to compare approaches.",fullPaper,cp42
Computer Science,p995,d3,88697a0b7f2614a15095ab3a379a7496952357cd,c11,European Conference on Modelling and Simulation,Parameter mapping and data transformation for engineering application integration,Abstract,poster,cp11
Computer Science,p998,d3,e9931ea8ae9b8db38b519ef9ae32ec41a06d8445,c19,International Conference on Conceptual Structures,Doing Data Science,"Now that people are aware that data can make the difference in an election or a business model, data science as an occupation is gaining ground. But how can you get started working in a wide-ranging, interdisciplinary field that's so clouded in hype? This insightful book, based on Columbia University's Introduction to Data Science class, tells you what you need to know. In many of these chapter-long lectures, data scientists from companies such as Google, Microsoft, and eBay share new algorithms, methods, and models by presenting case studies and the code they use. If you're familiar with linear algebra, probability, and statistics, and have programming experience, this book is an ideal introduction to data science. Topics include: Statistical inference, exploratory data analysis, and the data science process Algorithms Spam filters, Naive Bayes, and data wrangling Logistic regression Financial modeling Recommendation engines and causality Data visualization Social networks and data journalism Data engineering, MapReduce, Pregel, and Hadoop Doing Data Science is collaboration between course instructor Rachel Schutt, Senior VP of Data Science at News Corp, and data science consultant Cathy O'Neil, a senior data scientist at Johnson Research Labs, who attended and blogged about the course.",poster,cp19
Computer Science,p1005,d3,38f5b53b49be555430f33b8363910191a3df1d14,j242,International Journal for Research in Applied Science and Engineering Technology,"A Survey on Big Data Analytics: Challenges, Open Research Issues and Tools","Abstract: A huge repository of terabytes of data is generated each day from modern information systems and digital technologies such as Internet of Things and cloud computing. Analysis of these massive data requires a lot of efforts at multiple levels to extract knowledge for decision making. Therefore, big data analysis is a current area of research and development. The basic objective of this paper is to explore the potential impact of big data challenges, open research issues, and various tools associated with it. As a result, this article provides a platform to explore big data at numerous stages. Additionally, it opens a new horizon for researchers to develop the solution, based on the challenges and open research issues.",fullPaper,jv242
Computer Science,p1006,d3,cc017a62c605a0749e35a1264a46d62e78fb68b7,j156,Lecture Notes in Computer Science,Big Data Analytics,Abstract,fullPaper,jv156
Computer Science,p1007,d3,4b06c7e29280b1c6bc05c9df39023b48fef02c93,c67,The Sea,Escaping the Big Data Paradigm with Compact Transformers,"With the rise of Transformers as the standard for language processing, and their advancements in computer vision, there has been a corresponding growth in parameter size and amounts of training data. Many have come to believe that because of this, transformers are not suitable for small sets of data. This trend leads to concerns such as: limited availability of data in certain scientific domains and the exclusion of those with limited resource from research in the field. In this paper, we aim to present an approach for small-scale learning by introducing Compact Transformers. We show for the first time that with the right size, convolutional tokenization, transformers can avoid overfitting and outperform state-of-the-art CNNs on small datasets. Our models are flexible in terms of model size, and can have as little as 0.28M parameters while achieving competitive results. Our best model can reach 98% accuracy when training from scratch on CIFAR-10 with only 3.7M parameters, which is a significant improvement in data-efficiency over previous Transformer based models being over 10x smaller than other transformers and is 15% the size of ResNet50 while achieving similar performance. CCT also outperforms many modern CNN based approaches, and even some recent NAS-based approaches. Additionally, we obtain a new SOTA result on Flowers-102 with 99.76% top-1 accuracy, and improve upon the existing baseline on ImageNet (82.71% accuracy with 29% as many parameters as ViT), as well as NLP tasks. Our simple and compact design for transformers makes them more feasible to study for those with limited computing resources and/or dealing with small datasets, while extending existing research efforts in data efficient transformers. Our code and pre-trained models are publicly available at https://github.com/SHI-Labs/Compact-Transformers.",poster,cp67
Computer Science,p1008,d3,1bc34cb22131554ba18f6ba9e6ede5beb42939f1,j65,International Journal of Information Management,"Beyond the hype: Big data concepts, methods, and analytics",Abstract,fullPaper,jv65
Computer Science,p1013,d3,3a74bed911ccf213d9595b2b02a5b1c4ac4dcaf8,j243,The Information Society,Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy,Abstract,fullPaper,jv243
Computer Science,p1015,d3,cbad0923db89f23febcbd6192ff4149289ff2ad9,j7,Journal of Big Data,A survey on data‐efficient algorithms in big data era,Abstract,fullPaper,jv7
Computer Science,p1020,d3,b6b7fea1846e85ac1e3c7e3adda6e65b127d0368,j246,IEEE Internet of Things Journal,"IoT, Big Data, and Artificial Intelligence in Agriculture and Food Industry","Internet of Things (IoT) results in a massive amount of streaming data, often referred to as “big data,” which brings new opportunities to monitor agricultural and food processes. Besides sensors, big data from social media is also becoming important for the food industry. In this review, we present an overview of IoT, big data, and artificial intelligence (AI), and their disruptive role in shaping the future of agri-food systems. Following an introduction to the fields of IoT, big data, and AI, we discuss the role of IoT and big data analysis in agriculture (including greenhouse monitoring, intelligent farm machines, and drone-based crop imaging), supply chain modernization, social media (for open innovation and sentiment analysis) in food industry, food quality assessment (using spectral methods and sensor fusion), and finally, food safety (using gene sequencing and blockchain-based digital traceability). A special emphasis is laid on the commercial status of applications and translational research outcomes.",fullPaper,jv246
Computer Science,p1021,d3,17fca92ffd527c78c5dc6c7953e96671743807fa,j247,British Journal of Management,Big Data Analytics Capabilities and Innovation: The Mediating Role of Dynamic Capabilities and Moderating Effect of the Environment,"With big data analytics growing rapidly in popularity, academics and practitioners have been considering the means through which they can incorporate the shifts these technologies bring into their competitive strategies. Drawing on the resource&#8208;based view, the dynamic capabilities view, and on recent literature on big data analytics, this study examines the indirect relationship between a big data analytics capability (BDAC) and two types of innovation capabilities: incremental and radical. The study extends existing research by proposing that BDACs enable firms to generate insight that can help strengthen their dynamic capabilities, which in turn positively impact incremental and radical innovation capabilities. To test their proposed research model, the authors used survey data from 175 chief information officers and IT managers working in Greek firms. By means of partial least squares structural equation modelling, the results confirm the authors&#8217; assumptions regarding the indirect effect that BDACs have on innovation capabilities. Specifically, they find that dynamic capabilities fully mediate the effect on both incremental and radical innovation capabilities. In addition, under conditions of high environmental heterogeneity, the impact of BDACs on dynamic capabilities and, in sequence, incremental innovation capability is enhanced, while under conditions of high environmental dynamism the effect of dynamic capabilities on incremental innovation capabilities is amplified.",fullPaper,jv247
Computer Science,p1022,d3,456c011594ecacdd24298a161787389ccbe4b88b,c15,Pacific Symposium on Biocomputing,Big Data Service Architecture: A Survey,"As one of the main development directions in the information field, big data technology can be applied for data mining, data analysis and data sharing in the massive data, and it created huge economic benefits by using the potential value of data. Meanwhile, it can provide decision-making strategies for social and economic development. Big data service architecture is a new service economic model that takes data as a resource, and it loads and extracts the data collected from different data sources. This service architecture provides various customized data processing methods, data analysis and visualization services for service consumers. This paper first briefly introduces the general big data service architecture and the technical processing framework, which covered data collection and storage. Next, we discuss big data processing and analysis according to different service requirements, which can present valuable data for service consumers. Then, we introduce the detailed cloud computing service system based on big data, which provides high performance solutions for large-scale data storage, processing and analysis. Finally, we summarize some big data application scenarios over various fields.",poster,cp15
Computer Science,p1023,d3,9d6cbac04c498b424dfaa5ce82ac201a180c1502,j248,Big Data Mining and Analytics,Big data with cloud computing: Discussions and challenges,"Big Data and cloud computing integration has become a formidable strategy for businesses to unlock the potential of enormous and complicated data sets. With the scalability, flexibility, and cost-effectiveness that this combination provides, businesses are able to handle and analyse massive amounts of data in a distributed, as-needed way. But there are also issues and restrictions that need to be resolved with this integration. This overview of the literature focuses on the issues, difficulties, and potential applications of big data and cloud computing. It offers information on the advantages of this integration, such as improved data processing capabilities, increased scalability, and cost reduction. The difficulties with data migration, security, privacy, data governance, talent needs, vendor lock-in, and compliance are all discussed. Future research areas are also highlighted, such as enhanced analytics methods, edge computing integration, privacy-preserving data analysis, hybrid cloud architectures, data governance, real-time decision-making, and applications specialised to particular industries. Organisations can maximise the potential of Big Data with cloud computing and generate priceless insights to fuel innovation and informed decision-making by comprehending, tackling, and investigating these difficulties.",fullPaper,jv248
Computer Science,p1026,d3,4e13a8e8ba8d33e15ed037bfca7c651047533990,j249,Enterprise Information Systems,Big data for cyber physical systems in industry 4.0: a survey,"ABSTRACT With the technology development in cyber physical systems and big data, there are huge potential to apply them to achieve personalization and improve resource efficiency in Industry 4.0. As Industry 4.0 is the relatively new concept originated from an advanced manufacturing vision supported by the German government in 2011, there are only several existing surveys on either cyber physical systems or big data in Industry 4.0. In addition, there are much less surveys related to the intersection between cyber physical systems and big data in Industry 4.0. However, cyber physical systems are closely related to big data in nature. For example, cyber physical systems will continuously generate a large amount of data which requires the big data techniques to process and help to improve system scalability, security, and efficiency. Therefore, we conduct this survey to bring more attention to this critical intersection and highlight the future research direction to achieve the fully autonomy in Industry 4.0.",fullPaper,jv249
Computer Science,p1027,d3,10d89b13a6309a531c35701d37d3bd76a27a3942,c79,IEEE Symposium on Computational Intelligence for Engineering Solutions,Big Data Storage,Abstract,poster,cp79
Computer Science,p1028,d3,b52db9e41e15f76bdcfbe674abe0314af545c430,c103,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,The Rise of “Big Data” on Cloud Computing,Abstract,poster,cp103
Computer Science,p1029,d3,b34fc78de28be598e21118d7cb9d84d63374addc,j152,IEEE Access,Analysis of Dimensionality Reduction Techniques on Big Data,"Due to digitization, a huge volume of data is being generated across several sectors such as healthcare, production, sales, IoT devices, Web, organizations. Machine learning algorithms are used to uncover patterns among the attributes of this data. Hence, they can be used to make predictions that can be used by medical practitioners and people at managerial level to make executive decisions. Not all the attributes in the datasets generated are important for training the machine learning algorithms. Some attributes might be irrelevant and some might not affect the outcome of the prediction. Ignoring or removing these irrelevant or less important attributes reduces the burden on machine learning algorithms. In this work two of the prominent dimensionality reduction techniques, Linear Discriminant Analysis (LDA) and Principal Component Analysis (PCA) are investigated on four popular Machine Learning (ML) algorithms, Decision Tree Induction, Support Vector Machine (SVM), Naive Bayes Classifier and Random Forest Classifier using publicly available Cardiotocography (CTG) dataset from University of California and Irvine Machine Learning Repository. The experimentation results prove that PCA outperforms LDA in all the measures. Also, the performance of the classifiers, Decision Tree, Random Forest examined is not affected much by using PCA and LDA.To further analyze the performance of PCA and LDA the eperimentation is carried out on Diabetic Retinopathy (DR) and Intrusion Detection System (IDS) datasets. Experimentation results prove that ML algorithms with PCA produce better results when dimensionality of the datasets is high. When dimensionality of datasets is low it is observed that the ML algorithms without dimensionality reduction yields better results.",fullPaper,jv152
Computer Science,p1031,d3,02a88547d6f6022bebc9aba6723a310cdf132f3f,c42,IEEE Working Conference on Mining Software Repositories,Big Data,Abstract,poster,cp42
Computer Science,p1033,d3,2660dcf5bd16d14862a7bbb241fa4d85ae34327f,j226,Information Systems,"The rise of ""big data"" on cloud computing: Review and open research issues",Abstract,fullPaper,jv226
Computer Science,p1034,d3,bc6dbcaf4d2c76e618ae3f1043fd7276cbdf7f9b,j7,Journal of Big Data,"Big data in healthcare: management, analysis and future prospects",Abstract,fullPaper,jv7
Computer Science,p1035,d3,dbabab9bf5955558f73a37644f4bb626106a6d73,c0,International Conference on Machine Learning,Big Data Analytics in Intelligent Transportation Systems: A Survey,"Big data is becoming a research focus in intelligent transportation systems (ITS), which can be seen in many projects around the world. Intelligent transportation systems will produce a large amount of data. The produced big data will have profound impacts on the design and application of intelligent transportation systems, which makes ITS safer, more efficient, and profitable. Studying big data analytics in ITS is a flourishing field. This paper first reviews the history and characteristics of big data and intelligent transportation systems. The framework of conducting big data analytics in ITS is discussed next, where the data source and collection methods, data analytics methods and platforms, and big data analytics application categories are summarized. Several case studies of big data analytics applications in intelligent transportation systems, including road traffic accidents analysis, road traffic flow prediction, public transportation service plan, personal travel route plan, rail transportation management and control, and assets maintenance are introduced. Finally, this paper discusses some open challenges of using big data analytics in ITS.",poster,cp0
Computer Science,p1037,d3,247dec05283a1a521f99253a6cca6a5858cac0d2,j247,British Journal of Management,"Big Data and Predictive Analytics and Manufacturing Performance: Integrating Institutional Theory, Resource‐Based View and Big Data Culture","The importance of big data and predictive analytics has been at the forefront of research for operations and manufacturing management. The literature has reported the influence of big data and predictive analytics for improved supply chain and operational performance, but there has been a paucity of literature regarding the role of external institutional pressures on the resources of the organization to build big data capability. To address this gap, this paper draws on the resource‐based view of the firm, institutional theory and organizational culture to develop and test a model that describes the importance of resources for building capabilities, skills and big data culture and subsequently improving cost and operational performance. We test our research hypotheses using 195 surveys, gathered using a pre‐tested questionnaire. Our contribution lies in providing insights regarding the role of external pressures on the selection of resources under the moderating effect of big data culture and their utilization for capability building, and how this capability affects cost and operational performance.",fullPaper,jv247
Computer Science,p1039,d3,391a5f286f814d852dddcab1b2b68e5c1af6c79e,j4,IEEE Transactions on Knowledge and Data Engineering,Data mining with big data,"Big Data concern large-volume, complex, growing data sets with multiple, autonomous sources. With the fast development of networking, data storage, and the data collection capacity, Big Data are now rapidly expanding in all science and engineering domains, including physical, biological and biomedical sciences. This paper presents a HACE theorem that characterizes the features of the Big Data revolution, and proposes a Big Data processing model, from the data mining perspective. This data-driven model involves demand-driven aggregation of information sources, mining and analysis, user interest modeling, and security and privacy considerations. We analyze the challenging issues in the data-driven model and also in the Big Data revolution.",fullPaper,jv4
Computer Science,p1041,d3,b080d072cfde697180db3234da08903c092e72c3,j251,Management Decision,Big data analytics capabilities and knowledge management: impact on firm performance,"
Purpose
Big data analytics (BDA) guarantees that data may be analysed and categorised into useful information for businesses and transformed into big data related-knowledge and efficient decision-making processes, thereby improving performance. However, the management of the knowledge generated from the BDA as well as its integration and combination with firm knowledge have scarcely been investigated, despite an emergent need of a structured and integrated approach. The paper aims to discuss these issues.


Design/methodology/approach
Through an empirical analysis based on structural equation modelling with data collected from 88 Italian SMEs, the authors tested if BDA capabilities have a positive impact on firm performances, as well as the mediator effect of knowledge management (KM) on this relationship.


Findings
The findings of this paper show that firms that developed more BDA capabilities than others, both technological and managerial, increased their performances and that KM orientation plays a significant role in amplifying the effect of BDA capabilities.


Originality/value
BDA has the potential to change the way firms compete through better understanding, processing, and exploiting of huge amounts of data coming from different internal and external sources and processes. Some managerial and theoretical implications are proposed and discussed in light of the emergence of this new phenomenon.
",fullPaper,jv251
Computer Science,p1042,d3,046eb47d56beb8069b0098e3d01608f81ebb6849,j7,Journal of Big Data,"Uncertainty in big data analytics: survey, opportunities, and challenges",Abstract,fullPaper,jv7
Computer Science,p1045,d3,aca6d5f3866372a4506cf15773ae298f18c3f453,j7,Journal of Big Data,A comprehensive survey of anomaly detection techniques for high dimensional big data,Abstract,fullPaper,jv7
Computer Science,p1046,d3,752604994a7ca548ff2954114fc61a501d857b1c,c119,International Conference on Business Process Management,Big data analytics and firm performance: Effects of dynamic capabilities,Abstract,poster,cp119
Computer Science,p1048,d3,3789eb72c32ecf5e33442570358dd786dd67c8a2,c96,Human Language Technology - The Baltic Perspectiv,Text Mining in Big Data Analytics,"Text mining in big data analytics is emerging as a powerful tool for harnessing the power of unstructured textual data by analyzing it to extract new knowledge and to identify significant patterns and correlations hidden in the data. This study seeks to determine the state of text mining research by examining the developments within published literature over past years and provide valuable insights for practitioners and researchers on the predominant trends, methods, and applications of text mining research. In accordance with this, more than 200 academic journal articles on the subject are included and discussed in this review; the state-of-the-art text mining approaches and techniques used for analyzing transcripts and speeches, meeting transcripts, and academic journal articles, as well as websites, emails, blogs, and social media platforms, across a broad range of application areas are also investigated. Additionally, the benefits and challenges related to text mining are also briefly outlined.",poster,cp96
Computer Science,p1051,d3,3b217403302f9cb9d9685404c7646de7bc0db428,j254,Information Sciences,"Data-intensive applications, challenges, techniques and technologies: A survey on Big Data",Abstract,fullPaper,jv254
Computer Science,p1053,d3,c24d47ff95cd4bda073c75ec24ececaa3b10c995,j248,Big Data Mining and Analytics,A survey of data partitioning and sampling methods to support big data analysis,"Computer clusters with the shared-nothing architecture are the major computing platforms for big data processing and analysis. In cluster computing, data partitioning and sampling are two fundamental strategies to speed up the computation of big data and increase scalability. In this paper, we present a comprehensive survey of the methods and techniques of data partitioning and sampling with respect to big data processing and analysis. We start with an overview of the mainstream big data frameworks on Hadoop clusters. The basic methods of data partitioning are then discussed including three classical horizontal partitioning schemes: range, hash, and random partitioning. Data partitioning on Hadoop clusters is also discussed with a summary of new strategies for big data partitioning, including the new Random Sample Partition (RSP) distributed model. The classical methods of data sampling are then investigated, including simple random sampling, stratified sampling, and reservoir sampling. Two common methods of big data sampling on computing clusters are also discussed: record-level sampling and block-level sampling. Record-level sampling is not as efficient as block-level sampling on big distributed data. On the other hand, block-level sampling on data blocks generated with the classical data partitioning methods does not necessarily produce good representative samples for approximate computing of big data. In this survey, we also summarize the prevailing strategies and related work on sampling-based approximation on Hadoop clusters. We believe that data partitioning and sampling should be considered together to build approximate cluster computing frameworks that are reliable in both the computational and statistical respects.",fullPaper,jv248
Computer Science,p1057,d3,18d87bff073687c025f9bd23ab2dfb20d5f72a66,j258,IEEE Transactions on Industrial Informatics,BIM Big Data Storage in WebVRGIS,"In the context of big data and the Internet of Things, with the advancement of geospatial data acquisition and retrieval, the volume of available geospatial data is increasing every minute. Thus, new data-management architecture is needed. We proposed a building information model (BIM) big data-storage-management solution with hybrid storage architecture based on web virtual reality geographical information system (WebVRGIS). BIM is associated with the integration of spatial and semantic information on the various stages of urban building. In this paper, based on the spatial distribution characteristics of BIM geospatial big data, a data storage and management model is proposed for BIM geospatial big data management. The architecture primarily includes Not only Structured Query Language (NoSQL) database and distributed peer-to-peer storage. The evaluation of the proposed storage method is conducted on the same software platform as our previous research about WebVR. The experimental results show that the hybrid storage architecture proposed in this research has a lower response time compared to the traditional relational database in geospatial big data searches. The integration and fusion of BIM big data in WebVRGIS realizes a revolutionary transformation of city information management during a full lifecycle. The system also has great promise for the storage of other geospatial big data, such as traffic data.",fullPaper,jv258
Computer Science,p1058,d3,80dd97954ddf3edd22d4cb21f0ac31b7ffed6bbf,j152,IEEE Access,Digital Twin and Big Data Towards Smart Manufacturing and Industry 4.0: 360 Degree Comparison,"With the advances in new-generation information technologies, especially big data and digital twin, smart manufacturing is becoming the focus of global manufacturing transformation and upgrading. Intelligence comes from data. Integrated analysis for the manufacturing big data is beneficial to all aspects of manufacturing. Besides, the digital twin paves a way for the cyber-physical integration of manufacturing, which is an important bottleneck to achieve smart manufacturing. In this paper, the big data and digital twin in manufacturing are reviewed, including their concept as well as their applications in product design, production planning, manufacturing, and predictive maintenance. On this basis, the similarities and differences between big data and digital twin are compared from the general and data perspectives. Since the big data and digital twin can be complementary, how they can be integrated to promote smart manufacturing are discussed.",fullPaper,jv152
Computer Science,p1060,d3,1e4709c0b8fe3bf759cd64dc1ede695d6e5316f0,j7,Journal of Big Data,Deep learning applications and challenges in big data analytics,Abstract,fullPaper,jv7
Computer Science,p1061,d3,c48e0bd0f36c25ab83befbc7b7da369b75fd25f5,c66,International Conference on Web and Social Media,Big Data-Survey,"Big data is the term for any gathering of information sets, so expensive and complex, that it gets to be hard to process for utilizing customary information handling applications. The difficulties incorporate investigation, catch, duration, inquiry, sharing, stockpiling, Exchange, perception, and protection infringement. To reduce spot business patterns, anticipate diseases, conflict etc., we require bigger data sets when compared with the smaller data sets. Enormous information is hard to work with utilizing most social database administration frameworks and desktop measurements and perception bundles, needing rather enormously parallel programming running on tens, hundreds, or even a large number of servers. In this paper there was an observation on Hadoop architecture, different tools used for big data and its security issues.",poster,cp66
Computer Science,p1062,d3,ba9b6f805feb62c978d384211f910790643a023e,j7,Journal of Big Data,Big data monetization throughout Big Data Value Chain: a comprehensive review,Abstract,fullPaper,jv7
Computer Science,p1063,d3,a60a4e5f7f872b9825ddff5d379857c2091ca52b,j7,Journal of Big Data,Current landscape and influence of big data on finance,Abstract,fullPaper,jv7
Computer Science,p1065,d3,0026ea8a0fd31bdc959a4b9ed4d449f3015be8d1,c54,ACM SIGCOMM Symposium on Software Defined Networking Research,Big Data and Its Applications in Smart Real Estate and the Disaster Management Life Cycle: A Systematic Analysis,"Big data is the concept of enormous amounts of data being generated daily in different fields due to the increased use of technology and internet sources. Despite the various advancements and the hopes of better understanding, big data management and analysis remain a challenge, calling for more rigorous and detailed research, as well as the identifications of methods and ways in which big data could be tackled and put to good use. The existing research lacks in discussing and evaluating the pertinent tools and technologies to analyze big data in an efficient manner which calls for a comprehensive and holistic analysis of the published articles to summarize the concept of big data and see field-specific applications. To address this gap and keep a recent focus, research articles published in last decade, belonging to top-tier and high-impact journals, were retrieved using the search engines of Google Scholar, Scopus, and Web of Science that were narrowed down to a set of 139 relevant research articles. Different analyses were conducted on the retrieved papers including bibliometric analysis, keywords analysis, big data search trends, and authors’ names, countries, and affiliated institutes contributing the most to the field of big data. The comparative analyses show that, conceptually, big data lies at the intersection of the storage, statistics, technology, and research fields and emerged as an amalgam of these four fields with interlinked aspects such as data hosting and computing, data management, data refining, data patterns, and machine learning. The results further show that major characteristics of big data can be summarized using the seven Vs, which include variety, volume, variability, value, visualization, veracity, and velocity. Furthermore, the existing methods for big data analysis, their shortcomings, and the possible directions were also explored that could be taken for harnessing technology to ensure data analysis tools could be upgraded to be fast and efficient. The major challenges in handling big data include efficient storage, retrieval, analysis, and visualization of the large heterogeneous data, which can be tackled through authentication such as Kerberos and encrypted files, logging of attacks, secure communication through Secure Sockets Layer (SSL) and Transport Layer Security (TLS), data imputation, building learning models, dividing computations into sub-tasks, checkpoint applications for recursive tasks, and using Solid State Drives (SDD) and Phase Change Material (PCM) for storage. In terms of frameworks for big data management, two frameworks exist including Hadoop and Apache Spark, which must be used simultaneously to capture the holistic essence of the data and make the analyses meaningful, swift, and speedy. Further field-specific applications of big data in two promising and integrated fields, i.e., smart real estate and disaster management, were investigated, and a framework for field-specific applications, as well as a merger of the two areas through big data, was highlighted. The proposed frameworks show that big data can tackle the ever-present issues of customer regrets related to poor quality of information or lack of information in smart real estate to increase the customer satisfaction using an intermediate organization that can process and keep a check on the data being provided to the customers by the sellers and real estate managers. Similarly, for disaster and its risk management, data from social media, drones, multimedia, and search engines can be used to tackle natural disasters such as floods, bushfires, and earthquakes, as well as plan emergency responses. In addition, a merger framework for smart real estate and disaster risk management show that big data generated from the smart real estate in the form of occupant data, facilities management, and building integration and maintenance can be shared with the disaster risk management and emergency response teams to help prevent, prepare, respond to, or recover from the disasters.",poster,cp54
Computer Science,p1068,d3,6aca07154c111f1c8738347d7112cad6b0bf974a,j7,Journal of Big Data,Customer churn prediction in telecom using machine learning in big data platform,Abstract,fullPaper,jv7
Computer Science,p1070,d3,1f8a23697562b001082b147779b5eaefd3513d0a,j13,International Journal of Data Science and Analysis,Human migration: the big data perspective,Abstract,fullPaper,jv13
Computer Science,p1071,d3,7f5cd5b1340ac06ea38bd05373c30136a6f4c1ca,j17,Big Data & Society,The value of Big Data in government: The case of ‘smart cities’,"The emergence of Big Data has added a new aspect to conceptualizing the use of digital technologies in the delivery of public services and for realizing digital governance. This article explores, via the ‘value-chain’ approach, the evolution of digital governance research, and aligns it with current developments associated with data analytics, often referred to as ‘Big Data’. In many ways, the current discourse around Big Data reiterates and repeats established commentaries within the eGovernment research community. This body of knowledge provides an opportunity to reflect on the ‘promise’ of Big Data, both in relation to service delivery and policy formulation. This includes, issues associated with the quality and reliability of data, from mixing public and private sector data, issues associated with the ownership of raw and manipulated data, and ethical issues concerning surveillance and privacy. These insights and the issues raised help assess the value of Big Data in government and smart city environments.",fullPaper,jv17
Business,p1071,d9,7f5cd5b1340ac06ea38bd05373c30136a6f4c1ca,j17,Big Data & Society,The value of Big Data in government: The case of ‘smart cities’,"The emergence of Big Data has added a new aspect to conceptualizing the use of digital technologies in the delivery of public services and for realizing digital governance. This article explores, via the ‘value-chain’ approach, the evolution of digital governance research, and aligns it with current developments associated with data analytics, often referred to as ‘Big Data’. In many ways, the current discourse around Big Data reiterates and repeats established commentaries within the eGovernment research community. This body of knowledge provides an opportunity to reflect on the ‘promise’ of Big Data, both in relation to service delivery and policy formulation. This includes, issues associated with the quality and reliability of data, from mixing public and private sector data, issues associated with the ownership of raw and manipulated data, and ethical issues concerning surveillance and privacy. These insights and the issues raised help assess the value of Big Data in government and smart city environments.",fullPaper,jv17
Computer Science,p1073,d3,3bc9bb1f2218dcbd15c3b7cdfcb43077a3f30779,j249,Enterprise Information Systems,"Big data analytics for manufacturing internet of things: opportunities, challenges and enabling technologies","ABSTRACT Data analytics in massive manufacturing data can extract huge business values while can also result in research challenges due to the heterogeneous data types, enormous volume and real-time velocity of manufacturing data. This paper provides an overview on big data analytics in manufacturing Internet of Things (MIoT). This paper first starts with a discussion on necessities and challenges of big data analytics in manufacturing data of MIoT. Then, the enabling technologies of big data analytics of manufacturing data are surveyed and discussed. Moreover, this paper also outlines the future directions in this promising area.",fullPaper,jv249
Computer Science,p1076,d3,d63b884d5ebc739f6e1bdf861fa9276260781404,j265,IEEE Communications Surveys and Tutorials,Deep Learning for IoT Big Data and Streaming Analytics: A Survey,"In the era of the Internet of Things (IoT), an enormous amount of sensing devices collect and/or generate various sensory data over time for a wide range of fields and applications. Based on the nature of the application, these devices will result in big or fast/real-time data streams. Applying analytics over such data streams to discover new information, predict future insights, and make control decisions is a crucial process that makes IoT a worthy paradigm for businesses and a quality-of-life improving technology. In this paper, we provide a thorough overview on using a class of advanced machine learning techniques, namely deep learning (DL), to facilitate the analytics and learning in the IoT domain. We start by articulating IoT data characteristics and identifying two major treatments for IoT data from a machine learning perspective, namely IoT big data analytics and IoT streaming data analytics. We also discuss why DL is a promising approach to achieve the desired analytics in these types of data and applications. The potential of using emerging DL techniques for IoT data analytics are then discussed, and its promises and challenges are introduced. We present a comprehensive background on different DL architectures and algorithms. We also analyze and summarize major reported research attempts that leveraged DL in the IoT domain. The smart IoT devices that have incorporated DL in their intelligence background are also discussed. DL implementation approaches on the fog and cloud centers in support of IoT applications are also surveyed. Finally, we shed light on some challenges and potential directions for future research. At the end of each section, we highlight the lessons learned based on our experiments and review of the recent literature.",fullPaper,jv265
Computer Science,p1079,d3,ca0e479ba2327f71e842d033b6b48b082962cc6a,j267,International Journal of Big Data Management,Big data and analytics: a data management perspective in public administration,"In recent years, data analytics has enabled the policy makers to improve the accuracy levels of results while framing policies and strategies. This research field still has great potential waiting to be tapped, which would help to mitigate the challenges of public administration system. The present article introduces the concept of big data and provides a comprehensive overview to readers about the 'big data application framework' in public administration via data driven e-governance (DDeG). The conceptual framework here identifies the inherent possibilities of big data from the perspective of individual citizen as well as the administration. The overall finding of the study has broadened the scope of e-governance by exploring the technological aspects like network of internet (IoT), and artificial intelligence (AI). The author has concluded by pointing, the role of big data processes and its corresponding improved characteristics in public administration.",fullPaper,jv267
Computer Science,p1081,d3,e7c8fcbc24c73a576339e5f34f9f23f5ea732b3b,j268,Journal of Management Information Systems,Creating Strategic Business Value from Big Data Analytics: A Research Framework,"Abstract Despite the publicity regarding big data and analytics (BDA), the success rate of these projects and strategic value created from them are unclear. Most literature on BDA focuses on how it can be used to enhance tactical organizational capabilities, but very few studies examine its impact on organizational value. Further, we see limited framing of how BDA can create strategic value for the organization. After all, the ultimate success of any BDA project lies in realizing strategic business value, which gives firms a competitive advantage. In this study, we describe the value proposition of BDA by delineating its components. We offer a framing of BDA value by extending existing frameworks of information technology value, then illustrate the framework through BDA applications in practice. The framework is then discussed in terms of its ability to study constructs and relationships that focus on BDA value creation and realization. We also present a problem-oriented view of the framework—where problems in BDA components can give rise to targeted research questions and areas for future study. The framing in this study could help develop a significant research agenda for BDA that can better target research and practice based on effective use of data resources.",fullPaper,jv268
Business,p1081,d9,e7c8fcbc24c73a576339e5f34f9f23f5ea732b3b,j268,Journal of Management Information Systems,Creating Strategic Business Value from Big Data Analytics: A Research Framework,"Abstract Despite the publicity regarding big data and analytics (BDA), the success rate of these projects and strategic value created from them are unclear. Most literature on BDA focuses on how it can be used to enhance tactical organizational capabilities, but very few studies examine its impact on organizational value. Further, we see limited framing of how BDA can create strategic value for the organization. After all, the ultimate success of any BDA project lies in realizing strategic business value, which gives firms a competitive advantage. In this study, we describe the value proposition of BDA by delineating its components. We offer a framing of BDA value by extending existing frameworks of information technology value, then illustrate the framework through BDA applications in practice. The framework is then discussed in terms of its ability to study constructs and relationships that focus on BDA value creation and realization. We also present a problem-oriented view of the framework—where problems in BDA components can give rise to targeted research questions and areas for future study. The framing in this study could help develop a significant research agenda for BDA that can better target research and practice based on effective use of data resources.",fullPaper,jv268
Computer Science,p1083,d3,6570ee5bd1656fddbdb030410a9d996739586067,c65,International Symposium on Empirical Software Engineering and Measurement,Big Data and Big Data Analytics,"Big data is emerging, and the latest developments in technology have spawned enormous amounts of data. The traditional databases lack the capabilities to handle this diverse data and thus has led to the employment of new technologies, methods, and tools. This research discusses big data, the available big data analytical tools, the need to use big data analytics with its benefits and challenges. Through a research drawing on survey questionnaires, observation of the business processes, interviews and secondary research methods, the organizations, and companies in a small island state are identified to survey which of them use analytical tools to handle big data and the benefits it proposes to these businesses. Organizations and companies that do not use these tools were also surveyed and reasons were outlined as to why these organizations hesitate to utilize such tools.",poster,cp65
Computer Science,p1084,d3,6eefbb79f98e902e9d2efa057bfea174843bf3dc,j78,Future generations computer systems,Analysis of healthcare big data,Abstract,fullPaper,jv78
Computer Science,p1086,d3,57dcff5c3c15f6c5e5935ef7164c8f467a53848b,j219,International Journal of Production Research,Big data in lean six sigma: a review and further research directions,"Manufacturing and service organisations improve their processes on a continuous basis to have better operational performance. They use lean six sigma (LSS) projects for process improvement. Therefore, this study aims to investigate the existing literature in LSS and the application of big data analytics (BDA) to have more confident and predictable decisions in each phase of LSS. Fifty-two articles have been identified after a careful and vigilant screening of closely related themes. Future research directions in the big data and LSS have been highlighted on the basis of organisational theories. Review presents an investigation framework consisting of BDA techniques applicable to each phase of LSS in all the dimensions such as volume, variety, velocity and veracity of big data. Review highlights the concerns of big data in LSS such as system design and integration, system performance, security and reliability of data, sustaining the control and conducting the experiments, distributed material and information flow. The review unveils the application of 8 modern organisational theories to big data in LSS with 21 key aspects of related theories and 19 distinct research gaps as opportunities for future research.",fullPaper,jv219
Computer Science,p1088,d3,65499384f0947e223ec83f582366203e7aa64f1f,j272,Artificial Intelligence Review,The state of the art and taxonomy of big data analytics: view from new big data framework,Abstract,fullPaper,jv272
Computer Science,p1089,d3,16575f23ff879e6353a55bbfbbcc54e27606bfc5,c18,International Conference on Exploring Services Science,Big data analytics: Understanding its capabilities and potential benefits for healthcare organizations,Abstract,poster,cp18
Computer Science,p1092,d3,0a537eaf4444f0bd59f62a550430edab9d0a820d,j7,Journal of Big Data,A survey on addressing high-class imbalance in big data,Abstract,fullPaper,jv7
Computer Science,p1094,d3,243d553f1fc5b7ee8cfb6f49629f3a0a32b2c5c5,c81,ACM Symposium on Applied Computing,Big Data Analytics in Operations Management,"Big data analytics is critical in modern operations management (OM). In this study, we first explore the existing big data‐related analytics techniques, and identify their strengths, weaknesses as well as major functionalities. We then discuss various big data analytics strategies to overcome the respective computational and data challenges. After that, we examine the literature and reveal how different types of big data methods (techniques, strategies, and architectures) can be applied to different OM topical areas, namely forecasting, inventory management, revenue management and marketing, transportation management, supply chain management, and risk analysis. We also investigate via case studies the real‐world applications of big data analytics in top branded enterprises. Finally, we conclude the study with a discussion of future research.",poster,cp81
Computer Science,p1095,d3,3527f43e8156d1bf5a0998405047c98036d172f0,j273,IEEE/CAA Journal of Automatica Sinica,Internet of vehicles in big data era,"As the rapid development of automotive telematics, modern vehicles are expected to be connected through heterogeneous radio access technologies and are able to exchange massive information with their surrounding environment. By significantly expanding the network scale and conducting both real time and long term information processing, the traditional Vehicular Ad- Hoc Networks U+0028 VANETs U+0029 are evolving to the Internet of Vehicles U+0028 IoV U+0029, which promises efficient and intelligent prospect for the future transportation system. On the other hand, vehicles are not only consuming but also generating a huge amount and enormous types of data, which are referred to as Big Data. In this article, we first investigate the relationship between IoV and big data in vehicular environment, mainly on how IoV supports the transmission, storage, computing of the big data, and in return how IoV benefits from big data in terms of IoV characterization, performance evaluation and big data assisted communication protocol design. We then investigate the application of IoV big data for autonomous vehicles. Finally the emerging issues of the big data enabled IoV are discussed.",fullPaper,jv273
Computer Science,p1096,d3,1fe201d9ec4d072c7af233d265927388f32d3ab3,j274,Entropy,Optimization of Big Data Scheduling in Social Networks,"In social network big data scheduling, it is easy for target data to conflict in the same data node. Of the different kinds of entropy measures, this paper focuses on the optimization of target entropy. Therefore, this paper presents an optimized method for the scheduling of big data in social networks and also takes into account each task’s amount of data communication during target data transmission to construct a big data scheduling model. Firstly, the task scheduling model is constructed to solve the problem of conflicting target data in the same data node. Next, the necessary conditions for the scheduling of tasks are analyzed. Then, the a periodic task distribution function is calculated. Finally, tasks are scheduled based on the minimum product of the corresponding resource level and the minimum execution time of each task is calculated. Experimental results show that our optimized scheduling model quickly optimizes the scheduling of social network data and solves the problem of strong data collision.",fullPaper,jv274
Mathematics,p1096,d6,1fe201d9ec4d072c7af233d265927388f32d3ab3,j274,Entropy,Optimization of Big Data Scheduling in Social Networks,"In social network big data scheduling, it is easy for target data to conflict in the same data node. Of the different kinds of entropy measures, this paper focuses on the optimization of target entropy. Therefore, this paper presents an optimized method for the scheduling of big data in social networks and also takes into account each task’s amount of data communication during target data transmission to construct a big data scheduling model. Firstly, the task scheduling model is constructed to solve the problem of conflicting target data in the same data node. Next, the necessary conditions for the scheduling of tasks are analyzed. Then, the a periodic task distribution function is calculated. Finally, tasks are scheduled based on the minimum product of the corresponding resource level and the minimum execution time of each task is calculated. Experimental results show that our optimized scheduling model quickly optimizes the scheduling of social network data and solves the problem of strong data collision.",fullPaper,jv274
Computer Science,p1097,d3,a5400a6f415d65aac1312edd2ca6b7361c241f85,c7,International Symposium on Intelligent Data Analysis,Challenges and Opportunities with Big Data,"- Data is exploding at a rapid rate. Big data is the term for data sets so large and complicated that it becomes difficult to process using traditional data management tools or processing applications. Heterogeneity, scale, timeliness, complexity, and privacy problems with Big Data impede progress at all phases of the pipeline that can create value from data. The problems start right away during data acquisition, when the data tsunami requires us to make decisions, currently in an ad hoc manner, about what data to keep and what to discard, and how to store what we keep reliably with the right metadata. Much data today is not natively in structured format, tweets and blogs are weakly structured pieces of text, while images and video are structured for storage and display, but not for semantic content and search. Transforming such content into a structured format for later analysis is a major challenge. The value of data explodes when it can be linked with other data, thus data integration is a major creator of value. Since most data is directly generated in digital format today, we have the opportunity and the challenge both to influence the creation to facilitate later linkage and to automatically link previously created data. Data analysis, organization, retrieval, and modeling are other foundational challenges. Data analysis is a clear bottleneck in many applications, both due to lack of scalability of the underlying algorithms and due to the complexity of the data that needs to be analyzed. Finally, presentation of the results and its interpretation by non-technical domain experts is crucial to extracting actionable knowledge.",poster,cp7
Computer Science,p1098,d3,0608fc7e1825c4ac1f61aaa95b67f155d93b0ea4,j7,Journal of Big Data,On the sustainability of smart and smarter cities in the era of big data: an interdisciplinary and transdisciplinary literature review,Abstract,fullPaper,jv7
Computer Science,p1099,d3,adf914b04914a9b856e26c575c789ba19f9f118d,c82,Symposium on Networked Systems Design and Implementation,CherryPick: Adaptively Unearthing the Best Cloud Configurations for Big Data Analytics,"Picking the right cloud configuration for recurring big data analytics jobs running in clouds is hard, because there can be tens of possible VM instance types and even more cluster sizes to pick from. Choosing poorly can significantly degrade performance and increase the cost to run a job by 2-3x on average, and as much as 12x in the worst-case. However, it is challenging to automatically identify the best configuration for a broad spectrum of applications and cloud configurations with low search cost. CherryPick is a system that leverages Bayesian Optimization to build performance models for various applications, and the models are just accurate enough to distinguish the best or close-to-the-best configuration from the rest with only a few test runs. Our experiments on five analytic applications in AWS EC2 show that CherryPick has a 45-90% chance to find optimal configurations, otherwise near-optimal, saving up to 75% search cost compared to existing solutions.",fullPaper,cp82
Computer Science,p1100,d3,da619a6c524f5ab800b44c8728db3cef3d3b25d9,j17,Big Data & Society,"Big Data, new epistemologies and paradigm shifts","This article examines how the availability of Big Data, coupled with new data analytics, challenges established epistemologies across the sciences, social sciences and humanities, and assesses the extent to which they are engendering paradigm shifts across multiple disciplines. In particular, it critically explores new forms of empiricism that declare ‘the end of theory’, the creation of data-driven rather than knowledge-driven science, and the development of digital humanities and computational social sciences that propose radically different ways to make sense of culture, history, economy and society. It is argued that: (1) Big Data and new data analytics are disruptive innovations which are reconfiguring in many instances how research is conducted; and (2) there is an urgent need for wider critical reflection within the academy on the epistemological implications of the unfolding data revolution, a task that has barely begun to be tackled despite the rapid changes in research practices presently taking place. After critically reviewing emerging epistemological positions, it is contended that a potentially fruitful approach would be the development of a situated, reflexive and contextually nuanced epistemology.",fullPaper,jv17
Sociology,p1100,d4,da619a6c524f5ab800b44c8728db3cef3d3b25d9,j17,Big Data & Society,"Big Data, new epistemologies and paradigm shifts","This article examines how the availability of Big Data, coupled with new data analytics, challenges established epistemologies across the sciences, social sciences and humanities, and assesses the extent to which they are engendering paradigm shifts across multiple disciplines. In particular, it critically explores new forms of empiricism that declare ‘the end of theory’, the creation of data-driven rather than knowledge-driven science, and the development of digital humanities and computational social sciences that propose radically different ways to make sense of culture, history, economy and society. It is argued that: (1) Big Data and new data analytics are disruptive innovations which are reconfiguring in many instances how research is conducted; and (2) there is an urgent need for wider critical reflection within the academy on the epistemological implications of the unfolding data revolution, a task that has barely begun to be tackled despite the rapid changes in research practices presently taking place. After critically reviewing emerging epistemological positions, it is contended that a potentially fruitful approach would be the development of a situated, reflexive and contextually nuanced epistemology.",fullPaper,jv17
Computer Science,p1101,d3,6eb5744bd933dcd29ff4d79e340cf890272ef099,c77,Visualization for Computer Security,Big Data for Health,Abstract,poster,cp77
Business,p1101,d9,6eb5744bd933dcd29ff4d79e340cf890272ef099,c77,Visualization for Computer Security,Big Data for Health,Abstract,poster,cp77
Computer Science,p1103,d3,4068c3303340ed9796463a2064a74f2dc6ea5795,j7,Journal of Big Data,Big data stream analysis: a systematic literature review,Abstract,fullPaper,jv7
Computer Science,p1104,d3,0c7ae2c9e8c91edd6801f5578176b28e0a0414c6,c7,International Symposium on Intelligent Data Analysis,"Big Data and Business Analytics: Trends, Platforms, Success Factors and Applications","Big data and business analytics are trends that are positively impacting the business world. Past researches show that data generated in the modern world is huge and growing exponentially. These include structured and unstructured data that flood organizations daily. Unstructured data constitute the majority of the world’s digital data and these include text files, web, and social media posts, emails, images, audio, movies, etc. The unstructured data cannot be managed in the traditional relational database management system (RDBMS). Therefore, data proliferation requires a rethinking of techniques for capturing, storing, and processing the data. This is the role big data has come to play. This paper, therefore, is aimed at increasing the attention of organizations and researchers to various applications and benefits of big data technology. The paper reviews and discusses, the recent trends, opportunities and pitfalls of big data and how it has enabled organizations to create successful business strategies and remain competitive, based on available literature. Furthermore, the review presents the various applications of big data and business analytics, data sources generated in these applications and their key characteristics. Finally, the review not only outlines the challenges for successful implementation of big data projects but also highlights the current open research directions of big data analytics that require further consideration. The reviewed areas of big data suggest that good management and manipulation of the large data sets using the techniques and tools of big data can deliver actionable insights that create business values.",poster,cp7
Computer Science,p1105,d3,606b5be1f747387e908674d89df152e85295f0b4,j7,Journal of Big Data,Investigating the adoption of big data analytics in healthcare: the moderating role of resistance to change,Abstract,fullPaper,jv7
Computer Science,p1106,d3,e799d31e1c2d80a971c1f956d62b98c0a9f27031,j36,British Journal of Educational Technology,Big Data and data science: A critical review of issues for educational research,"Big Data refers to large and disparate volumes of data generated by people, applications and machines. It is gaining increasing attention from a variety of domains, including education. What are the challenges of engaging with Big Data research in education? This paper identifies a wide range of critical issues that researchers need to consider when working with Big Data in education. The issues identified include diversity in the conception and meaning of Big Data in education, ontological, epistemological disparity, technical challenges, ethics and privacy, digital divide and digital dividend, lack of expertise and academic development opportunities to prepare educational researchers to leverage opportunities afforded by Big Data. The goal of this paper is to raise awareness on these issues and initiate a dialogue. The paper was inspired partly by insights drawn from the literature but mostly informed by experience researching into Big Data in education. [ABSTRACT FROM AUTHOR]",fullPaper,jv36
Sociology,p1106,d4,e799d31e1c2d80a971c1f956d62b98c0a9f27031,j36,British Journal of Educational Technology,Big Data and data science: A critical review of issues for educational research,"Big Data refers to large and disparate volumes of data generated by people, applications and machines. It is gaining increasing attention from a variety of domains, including education. What are the challenges of engaging with Big Data research in education? This paper identifies a wide range of critical issues that researchers need to consider when working with Big Data in education. The issues identified include diversity in the conception and meaning of Big Data in education, ontological, epistemological disparity, technical challenges, ethics and privacy, digital divide and digital dividend, lack of expertise and academic development opportunities to prepare educational researchers to leverage opportunities afforded by Big Data. The goal of this paper is to raise awareness on these issues and initiate a dialogue. The paper was inspired partly by insights drawn from the literature but mostly informed by experience researching into Big Data in education. [ABSTRACT FROM AUTHOR]",fullPaper,jv36
Computer Science,p1107,d3,064eec8458596ee45e9c1787086afdc9352a5ef7,j248,Big Data Mining and Analytics,"Big data analytics for healthcare industry: impact, applications, and tools","In recent years, huge amounts of structured, unstructured, and semi-structured data have been generated by various institutions around the world and, collectively, this heterogeneous data is referred to as big data. The health industry sector has been confronted by the need to manage the big data being produced by various sources, which are well known for producing high volumes of heterogeneous data. Various big-data analytics tools and techniques have been developed for handling these massive amounts of data, in the healthcare sector. In this paper, we discuss the impact of big data in healthcare, and various tools available in the Hadoop ecosystem for handling it. We also explore the conceptual architecture of big data analytics for healthcare which involves the data gathering history of different branches, the genome database, electronic health records, text/imagery, and clinical decisions support system.",fullPaper,jv248
Computer Science,p1108,d3,8cc8f762a52936f0d6baddb4f1bd5c06f1062605,j276,Computers in Human Behavior,Social media big data analytics: A survey,Abstract,fullPaper,jv276
Computer Science,p1109,d3,0eb35b498e3e9d5b51e5d7f9435206b99ce22505,j7,Journal of Big Data,An analytical study of information extraction from unstructured and multidimensional big data,Abstract,fullPaper,jv7
Computer Science,p1110,d3,1d1645c3bbfcd1e45677aabbeffe1f0df2397bf6,j251,Management Decision,A bibliometric analysis of research on Big Data analytics for business and management,"
Purpose
The purpose of this paper is to scrutinize and classify the literature linking Big Data analytics and management phenomena.


Design/methodology/approach
An objective bibliometric analysis is conducted, supported by subjective assessments based on the studies focused on the intertwining of Big Data analytics and management fields. Specifically, deeper descriptive statistics and document co-citation analysis are provided.


Findings
From the document co-citation analysis and its evaluation, four clusters depicting literature linking Big Data analytics and management phenomena are revealed: theoretical development of Big Data analytics; management transition to Big Data analytics; Big Data analytics and firm resources, capabilities and performance; and Big Data analytics for supply chain management.


Originality/value
To the best of the authors’ knowledge, this is one of the first attempts to comprehend the research streams which, over time, have paved the way to the intersection between Big Data analytics and management fields.
",fullPaper,jv251
Computer Science,p1111,d3,7893b84886938300dad02f6dc6154e026113bc12,c83,"International Convention on Information and Communication Technology, Electronics and Microelectronics",An overview and comparison of free Python libraries for data mining and big data analysis,"The popularity of Python is growing, especially in the field of data science. Consequently, there is an increasing number of free libraries available for usage. The aim of this review paper is to describe and compare the characteristics of different data mining and big data analysis libraries in Python. There is currently no paper dealing with the subject and describing pros and cons of all these libraries. Here we consider more than 20 libraries and separate them into six groups: core libraries, data preparation, data visualization, machine learning, deep learning and big data. Beside functionalities of a certain library, important factors for comparison are the number of contributors developing and maintaining the library and the size of the community. Bigger communities mean larger chances for easily finding solution to a certain problem. We currently recommend: pandas for data preparation; Matplotlib, seaborn or Plotly for data visualization; scikit-learn for machine leraning; TensorFlow, Keras and PyTorch for deep learning; and Hadoop Streaming and PySpark for big data.",fullPaper,cp83
Computer Science,p1112,d3,2cb4eefa839eb9faea2e0cb5fe5882b6c1a7dd78,j78,Future generations computer systems,A novel big data analytics framework for smart cities,Abstract,fullPaper,jv78
Computer Science,p1113,d3,ac1eeb98c388fa6be4579817721a75ab17b4de59,j78,Future generations computer systems,The Role of Big Data Analytics in Industrial Internet of Things,Abstract,fullPaper,jv78
Computer Science,p1114,d3,e1ff974613f471f91a15a7746fa0cbaf12eda26d,j7,Journal of Big Data,"Big Data and discrimination: perils, promises and solutions. A systematic review",Abstract,fullPaper,jv7
Computer Science,p1115,d3,cf6225095886b6b203d61539f917956748c4b8cf,j260,IEEE Transactions on Big Data,Robust Big Data Analytics for Electricity Price Forecasting in the Smart Grid,"Electricity price forecasting is a significant part of smart grid because it makes smart grid cost efficient. Nevertheless, existing methods for price forecasting may be difficult to handle with huge price data in the grid, since the redundancy from feature selection cannot be averted and an integrated infrastructure is also lacked for coordinating the procedures in electricity price forecasting. To solve such a problem, a novel electricity price forecasting model is developed. Specifically, three modules are integrated in the proposed model. First, by merging of Random Forest (RF) and Relief-F algorithm, we propose a hybrid feature selector based on Grey Correlation Analysis (GCA) to eliminate the feature redundancy. Second, an integration of Kernel function and Principle Component Analysis (KPCA) is used in feature extraction process to realize the dimensionality reduction. Finally, to forecast price classification, we put forward a differential evolution (DE) based Support Vector Machine (SVM) classifier. Our proposed electricity price forecasting model is realized via these three parts. Numerical results show that our proposal has superior performance than other methods.",fullPaper,jv260
Computer Science,p1116,d3,3edc44af4020554422f4775d4b7ee6ac9188c84e,j260,IEEE Transactions on Big Data,Big Data for Cybersecurity: Vulnerability Disclosure Trends and Dependencies,"Complex Big Data systems in modern organisations are progressively becoming attack targets by existing and emerging threat agents. Elaborate and specialised attacks will increasingly be crafted to exploit vulnerabilities and weaknesses. With the ever-increasing trend of cybercrime and incidents due to these vulnerabilities, effective vulnerability management is imperative for modern organisations regardless of their size. However, organisations struggle to manage the sheer volume of vulnerabilities discovered on their networks. Moreover, vulnerability management tends to be more reactive in practice. Rigorous statistical models, simulating anticipated volume and dependence of vulnerability disclosures, will undoubtedly provide important insights to organisations and help them become more proactive in the management of cyber risks. By leveraging the rich yet complex historical vulnerability data, our proposed novel and rigorous framework has enabled this new capability. By utilising this sound framework, we initiated an important study on not only handling persistent volatilities in the data but also further unveiling multivariate dependence structure amongst different vulnerability risks. In sharp contrast to the existing studies on univariate time series, we consider the more general multivariate case striving to capture their intriguing relationships. Through our extensive empirical studies using the real world vulnerability data, we have shown that a composite model can effectively capture and preserve long-term dependency between different vulnerability and exploit disclosures. In addition, the paper paves the way for further study on the stochastic perspective of vulnerability proliferation towards building more accurate measures for better cyber risk management as a whole.",fullPaper,jv260
Computer Science,p1120,d3,0214a84a2a1e9ac5dc9600c7ee40a6ea48a5f95c,j69,IEEE Transactions on Services Computing,Cybersecurity in Big Data Era: From Securing Big Data to Data-Driven Security,"‘‘Knowledge is power” is an old adage that has been found to be true in today’s information age. Knowledge is derived from having access to information. The ability to gather information from large volumes of data has become an issue of relative importance. Big Data Analytics (BDA) is the term coined by researchers to describe the art of processing, storing and gathering large amounts of data for future examination. Data is being produced at an alarming rate. The rapid growth of the Internet, Internet of Things (IoT) and other technological advances are the main culprits behind this sustained growth. The data generated is a reflection of the environment it is produced out of, thus we can use the data we get out of systems to figure out the inner workings of that system. This has become an important feature in cybersecurity where the goal is to protect assets. Furthermore, the growing value of data has made big data a high value target. In this paper, we explore recent research works in cybersecurity in relation to big data. We highlight how big data is protected and how big data can also be used as a tool for cybersecurity. We summarize recent works in the form of tables and have presented trends, open research challenges and problems. With this paper, readers can have a more thorough understanding of cybersecurity in the big data era, as well as research trends and open challenges in this active research area.",fullPaper,jv69
Computer Science,p1121,d3,0ae18b28d8dc00cd4641488084ead5df2a449c89,j7,Journal of Big Data,A survey on data storage and placement methodologies for Cloud-Big Data ecosystem,Abstract,fullPaper,jv7
Computer Science,p1123,d3,a7f8b8e6124901c1e22e940092e87b5b93776ab3,j152,IEEE Access,Machine Learning With Big Data: Challenges and Approaches,"The Big Data revolution promises to transform how we live, work, and think by enabling process optimization, empowering insight discovery and improving decision making. The realization of this grand potential relies on the ability to extract value from such massive data through data analytics; machine learning is at its core because of its ability to learn from data and provide data driven insights, decisions, and predictions. However, traditional machine learning approaches were developed in a different era, and thus are based upon multiple assumptions, such as the data set fitting entirely into memory, what unfortunately no longer holds true in this new context. These broken assumptions, together with the Big Data characteristics, are creating obstacles for the traditional techniques. Consequently, this paper compiles, summarizes, and organizes machine learning challenges with Big Data. In contrast to other research that discusses challenges, this work highlights the cause–effect relationship by organizing challenges according to Big Data Vs or dimensions that instigated the issue: volume, velocity, variety, or veracity. Moreover, emerging machine learning approaches and techniques are discussed in terms of how they are capable of handling the various challenges with the ultimate objective of helping practitioners select appropriate solutions for their use cases. Finally, a matrix relating the challenges and approaches is presented. Through this process, this paper provides a perspective on the domain, identifies research gaps and opportunities, and provides a strong foundation and encouragement for further research in the field of machine learning with Big Data.",fullPaper,jv152
Computer Science,p1124,d3,fa863e822087b6101274cece1e2184fdf943a78c,j277,Tourism Management,Big data in tourism research: A literature review,Abstract,fullPaper,jv277
Computer Science,p1126,d3,3ca7f49ce3554ea9366744c54413d8f5e401f48d,j278,Information Systems and E-Business Management,Big data analytics capabilities: a systematic literature review and research agenda,Abstract,fullPaper,jv278
Computer Science,p1127,d3,7e6818390a82838ff3b3df1d7c680a14cd5b2e20,j152,IEEE Access,Disease Prediction by Machine Learning Over Big Data From Healthcare Communities,"With big data growth in biomedical and healthcare communities, accurate analysis of medical data benefits early disease detection, patient care, and community services. However, the analysis accuracy is reduced when the quality of medical data is incomplete. Moreover, different regions exhibit unique characteristics of certain regional diseases, which may weaken the prediction of disease outbreaks. In this paper, we streamline machine learning algorithms for effective prediction of chronic disease outbreak in disease-frequent communities. We experiment the modified prediction models over real-life hospital data collected from central China in 2013–2015. To overcome the difficulty of incomplete data, we use a latent factor model to reconstruct the missing data. We experiment on a regional chronic disease of cerebral infarction. We propose a new convolutional neural network (CNN)-based multimodal disease risk prediction algorithm using structured and unstructured data from hospital. To the best of our knowledge, none of the existing work focused on both data types in the area of medical big data analytics. Compared with several typical prediction algorithms, the prediction accuracy of our proposed algorithm reaches 94.8% with a convergence speed, which is faster than that of the CNN-based unimodal disease risk prediction algorithm.",fullPaper,jv152
Computer Science,p1128,d3,3a83d8595e6727269c876fcebd23ee9ddd524b76,j4,IEEE Transactions on Knowledge and Data Engineering,A Survey on Data Collection for Machine Learning: A Big Data - AI Integration Perspective,"Data collection is a major bottleneck in machine learning and an active research topic in multiple communities. There are largely two reasons data collection has recently become a critical issue. First, as machine learning is becoming more widely-used, we are seeing new applications that do not necessarily have enough labeled data. Second, unlike traditional machine learning, deep learning techniques automatically generate features, which saves feature engineering costs, but in return may require larger amounts of labeled data. Interestingly, recent research in data collection comes not only from the machine learning, natural language, and computer vision communities, but also from the data management community due to the importance of handling large amounts of data. In this survey, we perform a comprehensive study of data collection from a data management point of view. Data collection largely consists of data acquisition, data labeling, and improvement of existing data or models. We provide a research landscape of these operations, provide guidelines on which technique to use when, and identify interesting research challenges. The integration of machine learning and data management for data collection is part of a larger trend of Big data and Artificial Intelligence (AI) integration and opens many opportunities for new research.",fullPaper,jv4
Mathematics,p1128,d6,3a83d8595e6727269c876fcebd23ee9ddd524b76,j4,IEEE Transactions on Knowledge and Data Engineering,A Survey on Data Collection for Machine Learning: A Big Data - AI Integration Perspective,"Data collection is a major bottleneck in machine learning and an active research topic in multiple communities. There are largely two reasons data collection has recently become a critical issue. First, as machine learning is becoming more widely-used, we are seeing new applications that do not necessarily have enough labeled data. Second, unlike traditional machine learning, deep learning techniques automatically generate features, which saves feature engineering costs, but in return may require larger amounts of labeled data. Interestingly, recent research in data collection comes not only from the machine learning, natural language, and computer vision communities, but also from the data management community due to the importance of handling large amounts of data. In this survey, we perform a comprehensive study of data collection from a data management point of view. Data collection largely consists of data acquisition, data labeling, and improvement of existing data or models. We provide a research landscape of these operations, provide guidelines on which technique to use when, and identify interesting research challenges. The integration of machine learning and data management for data collection is part of a larger trend of Big data and Artificial Intelligence (AI) integration and opens many opportunities for new research.",fullPaper,jv4
Computer Science,p1132,d3,116927fbe4c9732fd1e392035a100c33b14e9d59,j16,International Journal of Digital Earth,Big Data and cloud computing: innovation opportunities and challenges,"ABSTRACT Big Data has emerged in the past few years as a new paradigm providing abundant data and opportunities to improve and/or enable research and decision-support applications with unprecedented value for digital earth applications including business, sciences and engineering. At the same time, Big Data presents challenges for digital earth to store, transport, process, mine and serve the data. Cloud computing provides fundamental support to address the challenges with shared computing resources including computing, storage, networking and analytical software; the application of these resources has fostered impressive Big Data advancements. This paper surveys the two frontiers – Big Data and cloud computing – and reviews the advantages and consequences of utilizing cloud computing to tackling Big Data in the digital earth and relevant science domains. From the aspects of a general introduction, sources, challenges, technology status and research opportunities, the following observations are offered: (i) cloud computing and Big Data enable science discoveries and application developments; (ii) cloud computing provides major solutions for Big Data; (iii) Big Data, spatiotemporal thinking and various application domains drive the advancement of cloud computing and relevant technologies with new requirements; (iv) intrinsic spatiotemporal principles of Big Data and geospatial sciences provide the source for finding technical and theoretical solutions to optimize cloud computing and processing Big Data; (v) open availability of Big Data and processing capability pose social challenges of geospatial significance and (vi) a weave of innovations is transforming Big Data into geospatial research, engineering and business values. This review introduces future innovations and a research agenda for cloud computing supporting the transformation of the volume, velocity, variety and veracity into values of Big Data for local to global digital earth science and applications.",fullPaper,jv16
Computer Science,p1136,d3,208261e342a57fdf9818f474bb01271b706f9c2f,c27,International Conference Geographic Information Science,Big Data Analysis and Mining,"Big data analysis and mining aims to discover implicit, previously unknown, and potentially useful information and knowledge from big databases that contain high volumes of valuable veracious data collected or generated at a high velocity from a wide variety of data sources. Among different big data mining tasks, this chapter focuses on big data analysis and mining for frequent patterns. By relying on the MapReduce programming model, researchers only need to specify the “map” and “reduce” functions to discover frequent patterns from (1) big databases of precise data in a breadth-first manner or in a depth-first manner and/or from (2) big databases of uncertain data. Such a big data analysis and mining process can be sped up. The resulting (constrained or unconstrained) frequent patterns mined from big databases provide users with new insights and a sound understanding of users' patterns. Such knowledge is useful is many real-life information science and technology applications.",poster,cp27
Computer Science,p1137,d3,afe3e7d85e3eb46fe23f64518bb5f7006d5b1b84,j246,IEEE Internet of Things Journal,IoT-Based Big Data Storage Systems in Cloud Computing: Perspectives and Challenges,"Internet of Things (IoT) related applications have emerged as an important field for both engineers and researchers, reflecting the magnitude and impact of data-related problems to be solved in contemporary business organizations especially in cloud computing. This paper first provides a functional framework that identifies the acquisition, management, processing and mining areas of IoT big data, and several associated technical modules are defined and described in terms of their key characteristics and capabilities. Then current research in IoT application is analyzed, moreover, the challenges and opportunities associated with IoT big data research are identified. We also report a study of critical IoT application publications and research topics based on related academic and industry publications. Finally, some open issues and some typical examples are given under the proposed IoT-related research framework.",fullPaper,jv246
Computer Science,p1140,d3,a607e0c6a0d5d1321eca383939dbcb2b638613a2,j284,IEEE transactions on consumer electronics,A smart home energy management system using IoT and big data analytics approach,"Increasing cost and demand of energy has led many organizations to find smart ways for monitoring, controlling and saving energy. A smart Energy Management System (EMS) can contribute towards cutting the costs while still meeting energy demand. The emerging technologies of Internet of Things (IoT) and Big Data can be utilized to better manage energy consumption in residential, commercial, and industrial sectors. This paper presents an Energy Management System (EMS) for smart homes. In this system, each home device is interfaced with a data acquisition module that is an IoT object with a unique IP address resulting in a large mesh wireless network of devices. The data acquisition System on Chip (SoC) module collects energy consumption data from each device of each smart home and transmits the data to a centralized server for further processing and analysis. This information from all residential areas accumulates in the utility’s server as Big Data. The proposed EMS utilizes off-the-shelf Business Intelligence (BI) and Big Data analytics software packages to better manage energy consumption and to meet consumer demand. Since air conditioning contributes to 60% of electricity consumption in Arab Gulf countries, HVAC (Heating, Ventilation and Air Conditioning) Units have been taken as a case study to validate the proposed system. A prototype was built and tested in the lab to mimic small residential area HVAC systems1.",fullPaper,jv284
Computer Science,p1141,d3,6b9f26445f5620746689b91ab451f997b391724c,c39,Online World Conference on Soft Computing in Industrial Applications,Big Data and Climate Change,"Climate science as a data-intensive subject has overwhelmingly affected by the era of big data and relevant technological revolutions. The big successes of big data analytics in diverse areas over the past decade have also prompted the expectation of big data and its efficacy on the big problem—climate change. As an emerging topic, climate change has been at the forefront of the big climate data analytics implementations and exhaustive research have been carried out covering a variety of topics. This paper aims to present an outlook of big data in climate change studies over the recent years by investigating and summarising the current status of big data applications in climate change related studies. It is also expected to serve as a one-stop reference directory for researchers and stakeholders with an overview of this trending subject at a glance, which can be useful in guiding future research and improvements in the exploitation of big climate data.",poster,cp39
Computer Science,p1142,d3,b7919fadb4c1bf959b1e410463594afacfda7dc6,j285,Information Fusion,A survey on deep learning for big data,Abstract,fullPaper,jv285
Computer Science,p1143,d3,09e72c1ddaa2f5b26f6c3a04583005bddaa030c7,c110,Biometrics and Identity Management,How ‘Big Data’ Can Make Big Impact: Findings from a Systematic Review and a Longitudinal Case Study,Abstract,poster,cp110
Computer Science,p1144,d3,b2239452680e97c503a90f62ccdc8137a893b1e9,j17,Big Data & Society,Big Data and quality data for fake news and misinformation detection,"Fake news has become an important topic of research in a variety of disciplines including linguistics and computer science. In this paper, we explain how the problem is approached from the perspective of natural language processing, with the goal of building a system to automatically detect misinformation in news. The main challenge in this line of research is collecting quality data, i.e., instances of fake and real news articles on a balanced distribution of topics. We review available datasets and introduce the MisInfoText repository as a contribution of our lab to the community. We make available the full text of the news articles, together with veracity labels previously assigned based on manual assessment of the articles’ truth content. We also perform a topic modelling experiment to elaborate on the gaps and sources of imbalance in currently available datasets to guide future efforts. We appeal to the community to collect more data and to make it available for research purposes.",fullPaper,jv17
Computer Science,p1145,d3,82bbf69ede840a8604d153ff23dcd95b8e5ff317,c88,International Conference on Big Data Computing and Communications,A Survey on Deep Learning in Big Data,"Big Data means extremely huge large data sets that can be analyzed to find patterns, trends. One technique that can be used for data analysis so that able to help us find abstract patterns in Big Data is Deep Learning. If we apply Deep Learning to Big Data, we can find unknown and useful patterns that were impossible so far. With the help of Deep Learning, AI is getting smart. There is a hypothesis in this regard, the more data, the more abstract knowledge. So a handy survey of Big Data, Deep Learning and its application in Big Data is necessary. In this paper, we provide a comprehensive survey on what is Big Data, comparing methods, its research problems, and trends. Then a survey of Deep Learning, its methods, comparison of frameworks, and algorithms is presented. And at last, application of Deep Learning in Big Data, its challenges, open research problems and future trends are presented.",poster,cp88
Computer Science,p1146,d3,a4b2fcd91638be1f98b0b49036ddf4b9d4aa196c,c107,Annual Haifa Experimental Systems Conference,Big Data: Principles and best practices of scalable realtime data systems,"Services like social networks, web analytics, and intelligent e-commerce often need to manage data at a scale too big for a traditional database. As scale and demand increase, so does Complexity. Fortunately, scalability and simplicity are not mutually exclusiverather than using some trendy technology, a different approach is needed. Big data systems use many machines working in parallel to store and process data, which introduces fundamental challenges unfamiliar to most developers. Big Data shows how to build these systems using an architecture that takes advantage of clustered hardware along with new tools designed specifically to capture and analyze web-scale data. It describes a scalable, easy to understand approach to big data systems that can be built and run by a small team. Following a realistic example, this book guides readers through the theory of big data systems, how to use them in practice, and how to deploy and operate them once they're built. Purchase of the print book comes with an offer of a free PDF, ePub, and Kindle eBook from Manning. Also available is all code from the book.",poster,cp107
Computer Science,p1150,d3,b12a264640c86c4b0fb1abb5ea24ad605e84e705,c109,Computer Vision and Pattern Recognition,Big Data: A Survey,Abstract,poster,cp109
Computer Science,p1154,d3,ae27c78d63e56c2b7610bd94fd704fa1b5dcdd29,j288,Journal of King Saud University: Computer and Information Sciences,Big Data technologies: A survey,Abstract,fullPaper,jv288
Computer Science,p1155,d3,00d536b61baecedb647ddd10b91cc9eeddd11fa4,j7,Journal of Big Data,Big Data: Deep Learning for financial sentiment analysis,Abstract,fullPaper,jv7
Computer Science,p1156,d3,08f767340bedd7d63523dab0816b0fcc612d1ee2,j278,Information Systems and E-Business Management,Big data and business analytics ecosystems: paving the way towards digital transformation and sustainable societies,Abstract,fullPaper,jv278
Business,p1156,d9,08f767340bedd7d63523dab0816b0fcc612d1ee2,j278,Information Systems and E-Business Management,Big data and business analytics ecosystems: paving the way towards digital transformation and sustainable societies,Abstract,fullPaper,jv278
Computer Science,p1157,d3,2c7fc4bb092929408a6f6954c984b1b1d5dd75d6,j289,Journal of Computational Information Systems,Understanding the Factors Affecting the Organizational Adoption of Big Data,"ABSTRACT Big data is rapidly becoming a major driver for firms seeking to gain a competitive advantage. Grounded in the Diffusion of Innovation theory (DOI), the institutional theory, and the Tech-nology–Organization–Environment (TOE) framework, this study applies the results of a content analysis to develop a framework to identify the main factors affecting the organizational adoption of big data. The content analysis is based on the retrieval and review of relevant papers in the business intelligence & analytics (BI&A) literature published during the period 2009–2015. The 26 factors identified by this review are then integrated into a TOE framework. The findings of this research enrich the current big data literature and enhance practitioners’ understanding of the decision-making processes involved in a firm’s adoption of big data.",fullPaper,jv289
Business,p1157,d9,2c7fc4bb092929408a6f6954c984b1b1d5dd75d6,j289,Journal of Computational Information Systems,Understanding the Factors Affecting the Organizational Adoption of Big Data,"ABSTRACT Big data is rapidly becoming a major driver for firms seeking to gain a competitive advantage. Grounded in the Diffusion of Innovation theory (DOI), the institutional theory, and the Tech-nology–Organization–Environment (TOE) framework, this study applies the results of a content analysis to develop a framework to identify the main factors affecting the organizational adoption of big data. The content analysis is based on the retrieval and review of relevant papers in the business intelligence & analytics (BI&A) literature published during the period 2009–2015. The 26 factors identified by this review are then integrated into a TOE framework. The findings of this research enrich the current big data literature and enhance practitioners’ understanding of the decision-making processes involved in a firm’s adoption of big data.",fullPaper,jv289
Computer Science,p1162,d3,e13af509fabacf0e3d27202c9ef6995ef71d0129,c66,International Conference on Web and Social Media,Database Resources of the BIG Data Center in 2019,Abstract,poster,cp66
Computer Science,p1163,d3,44454bc0c23fb3d515540eb64c8636de727abe85,j246,IEEE Internet of Things Journal,"Context-Aware Computing, Learning, and Big Data in Internet of Things: A Survey","Internet of Things (IoT) has been growing rapidly due to recent advancements in communications and sensor technologies. Meanwhile, with this revolutionary transformation, researchers, implementers, deployers, and users are faced with many challenges. IoT is a complicated, crowded, and complex field; there are various types of devices, protocols, communication channels, architectures, middleware, and more. Standardization efforts are plenty, and this chaos will continue for quite some time. What is clear, on the other hand, is that IoT deployments are increasing with accelerating speed, and this trend will not stop in the near future. As the field grows in numbers and heterogeneity, “intelligence” becomes a focal point in IoT. Since data now becomes “big data,” understanding, learning, and reasoning with big data is paramount for the future success of IoT. One of the major problems in the path to intelligent IoT is understanding “context,” or making sense of the environment, situation, or status using data from sensors, and then acting accordingly in autonomous ways. This is called “context-aware computing,” and it now requires both sensing and, increasingly, learning, as IoT systems get more data and better learning from this big data. In this survey, we review the field, first, from a historical perspective, covering ubiquitous and pervasive computing, ambient intelligence, and wireless sensor networks, and then, move to context-aware computing studies. Finally, we review learning and big data studies related to IoT. We also identify the open issues and provide an insight for future study areas for IoT researchers.",fullPaper,jv246
Computer Science,p1165,d3,6f0af225c3aa849c2083d7257b4ff9ed4b4eb042,c85,IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering,Big Data consumer analytics and the transformation of marketing,Abstract,poster,cp85
Computer Science,p1166,d3,079b2cfe950a96d5a43a3febc983d151fb533b53,j294,Neurocomputing,Machine learning on big data: Opportunities and challenges,Abstract,fullPaper,jv294
Computer Science,p1167,d3,ac469d36498a54ecaa129c6ef4dc09745bfa3b03,c90,International Conference on Collaboration Technologies and Systems,The role of big data analytics in Internet of Things,Abstract,poster,cp90
Computer Science,p1172,d3,f673051978825fc796b2b8812808ca6cd8de83f2,j152,IEEE Access,"A Survey on Big Data Market: Pricing, Trading and Protection","Big data is considered to be the key to unlocking the next great waves of growth in productivity. The amount of collected data in our world has been exploding due to a number of new applications and technologies that permeate our daily lives, including mobile and social networking applications, and Internet of Thing-based smart-world systems (smart grid, smart transportation, smart cities, and so on). With the exponential growth of data, how to efficiently utilize the data becomes a critical issue. This calls for the development of a big data market that enables efficient data trading. Via pushing data as a kind of commodity into a digital market, the data owners and consumers are able to connect with each other, sharing and further increasing the utility of data. Nonetheless, to enable such an effective market for data trading, several challenges need to be addressed, such as determining proper pricing for the data to be sold or purchased, designing a trading platform and schemes to enable the maximization of social welfare of trading participants with efficiency and privacy preservation, and protecting the traded data from being resold to maintain the value of the data. In this paper, we conduct a comprehensive survey on the lifecycle of data and data trading. To be specific, we first study a variety of data pricing models, categorize them into different groups, and conduct a comprehensive comparison of the pros and cons of these models. Then, we focus on the design of data trading platforms and schemes, supporting efficient, secure, and privacy-preserving data trading. Finally, we review digital copyright protection mechanisms, including digital copyright identifier, digital rights management, digital encryption, watermarking, and others, and outline challenges in data protection in the data trading lifecycle.",fullPaper,jv152
Computer Science,p1174,d3,2e8e061b3548dc181d783bacacbb6b0bd54851a5,j296,Energy Informatics,Big data analytics in smart grids: a review,Abstract,fullPaper,jv296
Computer Science,p1181,d3,a7797aed6d23d7b599e71ad129211c2834925c0d,c108,IEEE International Conference on Multimedia and Expo,EMG Pattern Recognition in the Era of Big Data and Deep Learning,"The increasing amount of data in electromyographic (EMG) signal research has greatly increased the importance of developing advanced data analysis and machine learning techniques which are better able to handle “big data”. Consequently, more advanced applications of EMG pattern recognition have been developed. This paper begins with a brief introduction to the main factors that expand EMG data resources into the era of big data, followed by the recent progress of existing shared EMG data sets. Next, we provide a review of recent research and development in EMG pattern recognition methods that can be applied to big data analytics. These modern EMG signal analysis methods can be divided into two main categories: (1) methods based on feature engineering involving a promising big data exploration tool called topological data analysis; and (2) methods based on feature learning with a special emphasis on “deep learning”. Finally, directions for future research in EMG pattern recognition are outlined and discussed.",poster,cp108
Computer Science,p1182,d3,fb5d6279f1967fc73e83b7466679b2fbb9ec71c1,j152,IEEE Access,"Fog Based Intelligent Transportation Big Data Analytics in The Internet of Vehicles Environment: Motivations, Architecture, Challenges, and Critical Issues","The intelligent transportation system (ITS) concept was introduced to increase road safety, manage traffic efficiently, and preserve our green environment. Nowadays, ITS applications are becoming more data-intensive and their data are described using the “5Vs of Big Data”. Thus, to fully utilize such data, big data analytics need to be applied. The Internet of vehicles (IoV) connects the ITS devices to cloud computing centres, where data processing is performed. However, transferring huge amount of data from geographically distributed devices creates network overhead and bottlenecks, and it consumes the network resources. In addition, following the centralized approach to process the ITS big data results in high latency which cannot be tolerated by the delay-sensitive ITS applications. Fog computing is considered a promising technology for real-time big data analytics. Basically, the fog technology complements the role of cloud computing and distributes the data processing at the edge of the network, which provides faster responses to ITS application queries and saves the network resources. However, implementing fog computing and the lambda architecture for real-time big data processing is challenging in the IoV dynamic environment. In this regard, a novel architecture for real-time ITS big data analytics in the IoV environment is proposed in this paper. The proposed architecture merges three dimensions including intelligent computing (i.e. cloud and fog computing) dimension, real-time big data analytics dimension, and IoV dimension. Moreover, this paper gives a comprehensive description of the IoV environment, the ITS big data characteristics, the lambda architecture for real-time big data analytics, several intelligent computing technologies. More importantly, this paper discusses the opportunities and challenges that face the implementation of fog computing and real-time big data analytics in the IoV environment. Finally, the critical issues and future research directions section discusses some issues that should be considered in order to efficiently implement the proposed architecture.",fullPaper,jv152
Computer Science,p1183,d3,b016670e4e0986a85cc70a625e8627a80217a0a7,c59,Australian Software Engineering Conference,DATA MINING FOR BIG DATA,"Big data"" is pervasive, and yet still the notion engenders confusion. Big data has been used to convey all sorts of concepts, including: huge quantities of data, social media analytics, next generation data management capabilities, real-time data, and much more. Whatever the label, organizations are starting to understand and explore how to process and analyze a vast array of information in new ways. Big data is the term for a collection of data sets which are large and complex, it contain structured and unstructured both type of data. Data comes from everywhere, sensors used to gather climate information, posts to social media sites, digital pictures and videos etc This data is known as big data. Useful data can be extracted from this big data with the help of data mining. Data mining is a technique for discovering interesting patterns as well as descriptive, understandable models from large scale data. In this paper we overviewed types of big data and challenges in big data for future.",poster,cp59
Computer Science,p1185,d3,5b2812a083ae67de9da889b32ddd6aa464df845c,j300,International Journal of Emerging Technologies in Learning (iJET),Big Data Emerging Technology: Insights into Innovative Environment for Online Learning Resources,"Digital devices like tablets, smart phones, and laptop have become increasingly raised and utilised in higher education. As a result, current trends on ICT (information and communication technology) used in education begin widely with focusing on teaching and learning. The new concept of big data in recent ICT domain extends the promising research direction on online learning and big data integration through promising content that can be tailored for each student based on the context and Internet behaviour of users in online learning. This paper aims to explore innovative design for innovative online learning in Higher education using Big Data approach. Critical review from referred journals and books was conducted using thematic analysis. This paper proposes model reference which can be implemented with the technology in teaching and learning to improve student learning environment and outcomes and to enhance students’ development, performance and achievement in learning process in higher education.",fullPaper,jv300
Computer Science,p1188,d3,92b8a8124872dfa576fbe9ea44d2a8ab723fe477,c80,International Symposium on Software Engineering for Adaptive and Self-Managing Systems,"The Data Revolution: Big Data, Open Data, Data Infrastructures and Their Consequences","Despite their importance, little conceptual attention has traditionally been paid to data, at least in comparison with the sophistication and depth of the debate on the nature of knowledge and of information. In contrast, efforts have largely been directed towards the development of sound methods for sampling and analysing in order to ensure the validity of the conclusions made on the back of data. However, these are not—despite their name—a neutral given, but a wholly social construct; therefore, their unpacking is necessary in order to understand how data come into existence, how they do work in the world, and with what consequences. The book does this by introducing the notion of the data assemblage, and using it to examine three key components of the data revolution: data infrastructures, open data and Big Data.",poster,cp80
Computer Science,p1189,d3,9b0dd87208a03e78105491e3727213b9b8ac0419,c18,International Conference on Exploring Services Science,Big Data: New Tricks for Econometrics,"Computers are now involved in many economic transactions and can capture data associated with these transactions, which can then be manipulated and analyzed. Conventional statistical and econometric techniques such as regression often work well, but there are issues unique to big datasets that may require different tools. First, the sheer size of the data involved may require more powerful data manipulation tools. Second, we may have more potential predictors than appropriate for estimation, so we need to do some kind of variable selection. Third, large datasets may allow for more flexible relationships than simple linear models. Machine learning techniques such as decision trees, support vector machines, neural nets, deep learning, and so on may allow for more effective ways to model complex relationships. In this essay, I will describe a few of these tools for manipulating and analyzing big data. I believe that these methods have a lot to offer and should be more widely known and used by economists.",poster,cp18
Computer Science,p1192,d3,8a7ccb4ffb953fafd44b01018eb53771d93d8c70,j258,IEEE Transactions on Industrial Informatics,Deep Convolutional Computation Model for Feature Learning on Big Data in Internet of Things,"Currently, a large number of industrial data, usually referred to big data, are collected from Internet of Things (IoT). Big data are typically heterogeneous, i.e., each object in big datasets is multimodal, posing a challenging issue on the convolutional neural network (CNN) that is one of the most representative deep learning models. In this paper, a deep convolutional computation model (DCCM) is proposed to learn hierarchical features of big data by using the tensor representation model to extend the CNN from the vector space to the tensor space. To make full use of the local features and topologies contained in the big data, a tensor convolution operation is defined to prevent overfitting and improve the training efficiency. Furthermore, a high-order backpropagation algorithm is proposed to train the parameters of the deep convolutional computational model in the high-order space. Finally, experiments on three datasets, i.e., CUAVE, SNAE2, and STL-10 are carried out to verify the performance of the DCCM. Experimental results show that the deep convolutional computation model can give higher classification accuracy than the deep computation model or the multimodal model for big data in IoT.",fullPaper,jv258
Computer Science,p1193,d3,6bf9d589f80823735084956f056728ae1a7bcfa8,j48,BioScience,"Situating Ecology as a Big-Data Science: Current Advances, Challenges, and Solutions","Ecology has joined a world of big data. Two complementary frameworks define big data: data that exceed the analytical capacities of individuals or disciplines or the “Four Vs” axes of volume, variety, veracity, and velocity. Variety predominates in ecoinformatics and limits the scalability of ecological science. Volume varies widely. Ecological velocity is low but growing as data throughput and societal needs increase. Ecological big-data systems include in situ and remote sensors, community data resources, biodiversity databases, citizen science, and permanent stations. Technological solutions include the development of open code- and data-sharing platforms, flexible statistical models that can handle heterogeneous data and sources of uncertainty, and cloud-computing delivery of high-velocity computing to large-volume analytics. Cultural solutions include training targeted to early and current scientific workforce and strengthening collaborations among ecologists and data scientists. The broader goal is to maximize the power, scalability, and timeliness of ecological insights and forecasting.",fullPaper,jv48
Computer Science,p1195,d3,c113b3f17d8f08bdaaa18ae88c24afd4d41cb543,j303,Journal of the AIS,Big Data Research in Information Systems: Toward an Inclusive Research Agenda,": Big data has received considerable attention from the information systems (IS) discipline over the past few years, with several recent commentaries, editorials, and special issue introductions on the topic appearing in leading IS outlets. These papers present varying perspectives on promising big data research topics and highlight some of the challenges that big data poses. In this editorial, we synthesize and contribute further to this discourse. We offer a first step toward an inclusive big data research agenda for IS by focusing on the interplay between big data’s characteristics, the information value chain encompassing people-process-technology, and the three dominant IS research traditions (behavioral, design, and economics of IS). We view big data as a disruption to the value chain that has widespread impacts, which include but are not limited to changing the way academics conduct scholarly work. Importantly, we critically discuss the opportunities and challenges for behavioral, design science, and economics of IS research and the emerging implications for theory and methodology arising due to big data’s disruptive effects.",fullPaper,jv303
Computer Science,p1196,d3,812ac6e60df272dab8ff7e835c8813cc0f46697c,c3,Knowledge Discovery and Data Mining,Detecting Anomaly in Big Data System Logs Using Convolutional Neural Network,"Nowadays, big data systems are being widely adopted by many domains for offering effective data solutions, such as manufacturing, healthcare, education, and media. Big data systems produce tons of unstructured logs that contain buried valuable information. However, it is a daunting task to manually unearth the information and detect system anomalies. A few automatic methods have been developed, where the cutting-edge machine learning technique is one of the most promising ways. In this paper, we propose a novel approach for anomaly detection from big data system logs by leveraging Convolutional Neural Networks (CNN). Different from other existing statistical methods or traditional rule-based machine learning approaches, our CNN-based model can automatically learn event relationships in system logs and detect anomaly with high accuracy. Our deep neural network consists of logkey2vec embeddings, three 1D convolutional layers, dropout layer, and max-pooling. According to our experiment, our CNN-based approach has better accuracy(reaches to 99%) compared to other approaches using Long Short term memory (LSTM) and Multilayer Perceptron (MLP) on detecting anomaly in Hadoop Distributed File System (HDFS) logs.",poster,cp3
Computer Science,p1198,d3,021332da6de9f907f77a39a0a6aefb110140d2cd,j304,IEEE wireless communications,"Synergy of Big Data and 5G Wireless Networks: Opportunities, Approaches, and Challenges","This article presents the synergistic and complementary features of big data and 5G wireless networks. An overview of their interplay is provided first, including big-data-driven networking and big data assisted networking. The former exploits heterogeneous resources such as communication, caching, and computing in 5G wireless networks to support big data applications and services, by catering for big data's features such as volume, velocity, and variety. The latter leverages big data techniques to collect wireless big data and extract in-depth knowledge regarding the networks and users to improve network planning and operation. To further illustrate the mutual benefits, two case studies on network aided data acquisition and big data assisted edge content caching are provided. Finally, some interesting open research issues are discussed.",fullPaper,jv304
Computer Science,p1200,d3,12c068d270f26135d60f23bd3dfc6aced55a08a7,c19,International Conference on Conceptual Structures,"Big Data Analytics: Applications, Prospects and Challenges",Abstract,poster,cp19
Business,p1200,d9,12c068d270f26135d60f23bd3dfc6aced55a08a7,c19,International Conference on Conceptual Structures,"Big Data Analytics: Applications, Prospects and Challenges",Abstract,poster,cp19
Computer Science,p1202,d3,753dec0e435c76a3f60e5a3db50e78670e63d33e,j222,IEEE Transactions on Parallel and Distributed Systems,A Parallel Random Forest Algorithm for Big Data in a Spark Cloud Computing Environment,"With the emergence of the big data age, the issue of how to obtain valuable knowledge from a dataset efficiently and accurately has attracted increasingly attention from both academia and industry. This paper presents a Parallel Random Forest (PRF) algorithm for big data on the Apache Spark platform. The PRF algorithm is optimized based on a hybrid approach combining data-parallel and task-parallel optimization. From the perspective of data-parallel optimization, a vertical data-partitioning method is performed to reduce the data communication cost effectively, and a data-multiplexing method is performed is performed to allow the training dataset to be reused and diminish the volume of data. From the perspective of task-parallel optimization, a dual parallel approach is carried out in the training process of RF, and a task Directed Acyclic Graph (DAG) is created according to the parallel training process of PRF and the dependence of the Resilient Distributed Datasets (RDD) objects. Then, different task schedulers are invoked for the tasks in the DAG. Moreover, to improve the algorithm's accuracy for large, high-dimensional, and noisy data, we perform a dimension-reduction approach in the training process and a weighted voting approach in the prediction process prior to parallelization. Extensive experimental results indicate the superiority and notable advantages of the PRF algorithm over the relevant algorithms implemented by Spark MLlib and other studies in terms of the classification accuracy, performance, and scalability. With the expansion of the scale of the random forest model and the Spark cluster, the advantage of the PRF algorithm is more obvious.",fullPaper,jv222
Computer Science,p1203,d3,fdd1ccdea15d3000bcf66a45d64d7a243006b021,c57,IEEE International Geoscience and Remote Sensing Symposium,A formal definition of Big Data based on its essential features,"Purpose – The purpose of this paper is to identify and describe the most prominent research areas connected with “Big Data” and propose a thorough definition of the term. Design/methodology/approach – The authors have analysed a conspicuous corpus of industry and academia articles linked with Big Data to find commonalities among the topics they treated. The authors have also compiled a survey of existing definitions with a view of generating a more solid one that encompasses most of the work happening in the field. Findings – The main themes of Big Data are: information, technology, methods and impact. The authors propose a new definition for the term that reads as follows: “Big Data is the Information asset characterized by such a High Volume, Velocity and Variety to require specific Technology and Analytical Methods for its transformation into Value.” Practical implications – The formal definition that is proposed can enable a more coherent development of the concept of Big Data, as it solely relies on ...",poster,cp57
Computer Science,p1205,d3,dcb401a9ff0ff14300e181c6fcd35479b719725d,j152,IEEE Access,Internet of Things and Big Data Analytics for Smart and Connected Communities,"This paper promotes the concept of smart and connected communities SCC, which is evolving from the concept of smart cities. SCC are envisioned to address synergistically the needs of remembering the past (preservation and revitalization), the needs of living in the present (livability), and the needs of planning for the future (attainability). Therefore, the vision of SCC is to improve livability, preservation, revitalization, and attainability of a community. The goal of building SCC for a community is to live in the present, plan for the future, and remember the past. We argue that Internet of Things (IoT) has the potential to provide a ubiquitous network of connected devices and smart sensors for SCC, and big data analytics has the potential to enable the move from IoT to real-time control desired for SCC. We highlight mobile crowdsensing and cyber-physical cloud computing as two most important IoT technologies in promoting SCC. As a case study, we present TreSight, which integrates IoT and big data analytics for smart tourism and sustainable cultural heritage in the city of Trento, Italy.",fullPaper,jv152
Computer Science,p1209,d3,27245e65a27bde90b5b0bb25d157bb75a0ad8b5a,j136,EURASIP Journal on Advances in Signal Processing,A survey of machine learning for big data processing,Abstract,fullPaper,jv136
Computer Science,p1211,d3,b997492c79f7980999b0f734cca76ef2761b4274,j219,International Journal of Production Research,"Modelling quality dynamics, business value and firm performance in a big data analytics environment","Big data analytics have become an increasingly important component for firms across advanced economies. This paper examines the quality dynamics in big data environment that are linked with enhancing business value and firm performance (FPER). The study identifies that system quality (i.e. system reliability, accessibility, adaptability, integration, response time and privacy) and information quality (i.e. completeness, accuracy, format and currency) are key to enhance business value and FPER in a big data environment. The study also proposes that the relationship between quality and FPER is mediated by business value of big data. Drawing on the resource-based theory and the information systems success literature, this study extends knowledge in this domain by linking system quality, information quality, business value and FPER.",fullPaper,jv219
Computer Science,p1214,d3,fdf5033947321dd1a89975eb67ba1ed6ed0eefc8,j308,Information Processing & Management,A survey towards an integration of big data analytics to big insights for value-creation,Abstract,fullPaper,jv308
Computer Science,p1218,d3,f1559567fbcc1dad4d99574eff3d6dcc216c51f6,j258,IEEE Transactions on Industrial Informatics,"Next-Generation Big Data Analytics: State of the Art, Challenges, and Future Research Topics","The term big data occurs more frequently now than ever before. A large number of fields and subjects, ranging from everyday life to traditional research fields (i.e., geography and transportation, biology and chemistry, medicine and rehabilitation), involve big data problems. The popularizing of various types of network has diversified types, issues, and solutions for big data more than ever before. In this paper, we review recent research in data types, storage models, privacy, data security, analysis methods, and applications related to network big data. Finally, we summarize the challenges and development of big data to predict current and future trends.",fullPaper,jv258
Computer Science,p1219,d3,fc5747192aa13a30ba70cdba54fd10b65d1abe1b,c105,International Conference on Automatic Face and Gesture Recognition,Big Data in Healthcare Management: A Review of Literature,"A systematic literature review of papers on big data in healthcare published between 2010 and 2015 was conducted. This paper reviews the definition, process, and use of big data in healthcare management. Unstructured data are growing very faster than semi-structured and structured data. 90 percentages of the big data are in a form of unstructured data, major steps of big data management in healthcare industry are data acquisition, storage of data, managing the data, analysis on data and data visualization. Recent researches targets on big data visualization tools. In this paper the authors analysed the effective tools used for visualization of big data and suggesting new visualization tools to manage the big data in healthcare industry. This article will be helpful to understand the processes and use of big data in healthcare management.",poster,cp105
Computer Science,p1220,d3,90a9f18396ffcccf295eb849a866b8934b9b21c5,c85,IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering,The big data analysis,"This article tells about the Big Data files, their features. The readers will know about methods and programms, helping to deal with Big Data. Perspectives of development and possible problems.",fullPaper,cp85
Computer Science,p1221,d3,0ea7937f7e87eb8967e724d9261ee925ed254a27,j7,Journal of Big Data,Intrusion detection model using machine learning algorithm on Big Data environment,Abstract,fullPaper,jv7
Computer Science,p1222,d3,bd1b111a2f840e88cf6e646cdfab82fe3910f896,j260,IEEE Transactions on Big Data,A Big Data-as-a-Service Framework: State-of-the-Art and Perspectives,"Due to the rapid advances of information technologies, Big Data, recognized with 4Vs characteristics (volume, variety, veracity, and velocity), bring significant benefits as well as many challenges. A major benefit of Big Data is to provide timely information and proactive services for humans. The primary purpose of this paper is to review the current state-of-the-art of Big Data from the aspects of organization and representation, cleaning and reduction, integration and processing, security and privacy, analytics and applications, then present a novel framework to provide high-quality so called Big Data-as-a-Service. The framework consists of three planes, namely sensing plane, cloud plane and application plane, to systemically address all challenges of the above aspects. Also, to clearly demonstrate the working process of the proposed framework, a tensor-based multiple clustering on bicycle renting and returning data is illustrated, which can provide several suggestions for rebalancing of the bicycle-sharing system. Finally, some challenges about the proposed framework are discussed.",fullPaper,jv260
Computer Science,p1225,d3,579c2aad3834e525a90740913fb58a8c8e9ef218,c104,North American Chapter of the Association for Computational Linguistics,Big Data Methods,"Advances in data science, such as data mining, data visualization, and machine learning, are extremely well-suited to address numerous questions in the organizational sciences given the explosion of available data. Despite these opportunities, few scholars in our field have discussed the specific ways in which the lens of our science should be brought to bear on the topic of big data and big data's reciprocal impact on our science. The purpose of this paper is to provide an overview of the big data phenomenon and its potential for impacting organizational science in both positive and negative ways. We identifying the biggest opportunities afforded by big data along with the biggest obstacles, and we discuss specifically how we think our methods will be most impacted by the data analytics movement. We also provide a list of resources to help interested readers incorporate big data methods into their existing research. Our hope is that we stimulate interest in big data, motivate future research using big data sources, and encourage the application of associated data science techniques more broadly in the organizational sciences.",poster,cp104
Computer Science,p1226,d3,46da3f3d592ea58211ce6bf8bec05ef0b5e792c2,c98,Vision,Transforming big data into smart data: An insight on the use of the k‐nearest neighbors algorithm to obtain quality data,"The k‐nearest neighbors algorithm is characterized as a simple yet effective data mining technique. The main drawback of this technique appears when massive amounts of data—likely to contain noise and imperfections—are involved, turning this algorithm into an imprecise and especially inefficient technique. These disadvantages have been subject of research for many years, and among others approaches, data preprocessing techniques such as instance reduction or missing values imputation have targeted these weaknesses. As a result, these issues have turned out as strengths and the k‐nearest neighbors rule has become a core algorithm to identify and correct imperfect data, removing noisy and redundant samples, or imputing missing values, transforming Big Data into Smart Data—which is data of sufficient quality to expect a good outcome from any data mining algorithm. The role of this smart data gleaning algorithm in a supervised learning context are investigated. This includes a brief overview of Smart Data, current and future trends for the k‐nearest neighbor algorithm in the Big Data context, and the existing data preprocessing techniques based on this algorithm. We present the emerging big data‐ready versions of these algorithms and develop some new methods to cope with Big Data. We carry out a thorough experimental analysis in a series of big datasets that provide guidelines as to how to use the k‐nearest neighbor algorithm to obtain Smart/Quality Data for a high‐quality data mining process. Moreover, multiple Spark Packages have been developed including all the Smart Data algorithms analyzed.",poster,cp98
Computer Science,p1227,d3,f257e3ac714cd8fcd3b22d7d27ac6fab2db34097,j50,ACM Computing Surveys,Multimedia Big Data Analytics,"With the proliferation of online services and mobile technologies, the world has stepped into a multimedia big data era. A vast amount of research work has been done in the multimedia area, targeting different aspects of big data analytics, such as the capture, storage, indexing, mining, and retrieval of multimedia big data. However, very few research work provides a complete survey of the whole pine-line of the multimedia big data analytics, including the management and analysis of the large amount of data, the challenges and opportunities, and the promising research directions. To serve this purpose, we present this survey, which conducts a comprehensive overview of the state-of-the-art research work on multimedia big data analytics. It also aims to bridge the gap between multimedia challenges and big data solutions by providing the current big data frameworks, their applications in multimedia analyses, the strengths and limitations of the existing methods, and the potential future directions in multimedia big data analytics. To the best of our knowledge, this is the first survey that targets the most recent multimedia management techniques for very large-scale data and also provides the research studies and technologies advancing the multimedia analyses in this big data era.",fullPaper,jv50
Computer Science,p1228,d3,83aa94353bb6b870e9f57a2567358b29fcb83507,j289,Journal of Computational Information Systems,Big Data Analytics Services for Enhancing Business Intelligence,"ABSTRACT This article examines how to use big data analytics services to enhance business intelligence (BI). More specifically, this article proposes an ontology of big data analytics and presents a big data analytics service-oriented architecture (BASOA), and then applies BASOA to BI, where our surveyed data analysis shows that the proposed BASOA is viable for enhancing BI and enterprise information systems. This article also explores temporality, expectability, and relativity as the characteristics of intelligence in BI. These characteristics are what customers and decision makers expect from BI in terms of systems, products, and services of organizations. The proposed approach in this article might facilitate the research and development of business analytics, big data analytics, and BI as well as big data science and big data computing.",fullPaper,jv289
Computer Science,p1230,d3,0d5047af65ddbce7fa21726cc5512844e32cbc0a,j7,Journal of Big Data,Big Data fraud detection using multiple medicare data sources,Abstract,fullPaper,jv7
Computer Science,p1232,d3,6a229b2c1fb7a117d8fa24598f66c51955c6d6ca,j152,IEEE Access,Big Data Challenges and Data Aggregation Strategies in Wireless Sensor Networks,"The emergence of new data handling technologies and analytics enabled the organization of big data in processes as an innovative aspect in wireless sensor networks (WSNs). Big data paradigm, combined with WSN technology, involves new challenges that are necessary to resolve in parallel. Data aggregation is a rapidly emerging research area. It represents one of the processing challenges of big sensor networks. This paper introduces the big data paradigm, its main dimensions that represent one of the most challenging concepts, and its principle analytic tools which are more and more introduced in the WSNs technology. The paper also presents the big data challenges that must be overcome to efficiently manipulate the voluminous data, and proposes a new classification of these challenges based on the necessities and the challenges of WSNs. As the big data aggregation challenge represents the center of our interest, this paper surveys its proposed strategies in WSNs.",fullPaper,jv152
Computer Science,p1233,d3,ae3c5a309638b59f0a86f36a33d360581d65c4d1,j312,Annals of Operations Research,Big data and disaster management: a systematic review and agenda for future research,Abstract,fullPaper,jv312
Political Science,p1233,d15,ae3c5a309638b59f0a86f36a33d360581d65c4d1,j312,Annals of Operations Research,Big data and disaster management: a systematic review and agenda for future research,Abstract,fullPaper,jv312
Computer Science,p1235,d3,474de4adfce86e14a59d02ccce59cf158fdc4c36,j78,Future generations computer systems,Big Data for Internet of Things: A Survey,Abstract,fullPaper,jv78
Computer Science,p1237,d3,cbe6b1066a9a9489b9b52d7f59af97141828fe58,j66,Data Science Journal,The Challenges of Data Quality and Data Quality Assessment in the Big Data Era,"High-quality data are the precondition for analyzing and using big data and for guaranteeing the value of the data. Currently, comprehensive analysis and research of quality standards and quality assessment methods for big data are lacking. First, this paper summarizes reviews of data quality research. Second, this paper analyzes the data characteristics of the big data environment, presents quality challenges faced by big data, and formulates a hierarchical data quality framework from the perspective of data users. This framework consists of big data quality dimensions, quality characteristics, and quality indexes. Finally, on the basis of this framework, this paper constructs a dynamic assessment process for data quality. This process has good expansibility and adaptability and can meet the needs of big data quality assessment. The research results enrich the theoretical scope of big data and lay a solid foundation for the future by establishing an assessment model and studying evaluation algorithms.",fullPaper,jv66
Computer Science,p1239,d3,60119658af638693f6de23d8466968e60c428ac7,j313,Journal of Integrative Agriculture,Agricultural remote sensing big data: Management and applications,Abstract,fullPaper,jv313
Computer Science,p1240,d3,db1daa777f253a5949883c3a60364a23d6a65726,c85,IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering,Privacy Issues and Data Protection in Big Data: A Case Study Analysis under GDPR,"Big data has become a great asset for many organizations, promising improved operations and new business opportunities. However, big data has increased access to sensitive information that when processed can directly jeopardize the privacy of individuals and violate data protection laws. As a consequence, data controllers and data processors may be imposed tough penalties for non-compliance that can result even to bankruptcy. In this paper, we discuss the current state of the legal regulations and analyse different data protection and privacy-preserving techniques in the context of big data analysis. In addition, we present and analyse two real-life research projects as case studies dealing with sensitive data and actions for complying with the data regulation laws. We show which types of information might become a privacy risk, the employed privacy-preserving techniques in accordance with the legal requirements, and the influence of these techniques on the data processing phase and the research results.",poster,cp85
Computer Science,p1242,d3,d5b65fac5a6f511c0261a14c93d702b6697ef98e,c19,International Conference on Conceptual Structures,Digitalisation and Big Data Mining in Banking,"Banking as a data intensive subject has been progressing continuously under the promoting influences of the era of big data. Exploring the advanced big data analytic tools like Data Mining (DM) techniques is key for the banking sector, which aims to reveal valuable information from the overwhelming volume of data and achieve better strategic management and customer satisfaction. In order to provide sound direction for the future research and development, a comprehensive and most up to date review of the current research status of DM in banking will be extremely beneficial. Since existing reviews only cover the applications until 2013, this paper aims to fill this research gap and presents the significant progressions and most recent DM implementations in banking post 2013. By collecting and analyzing the trends of research focus, data resources, technological aids, and data analytical tools, this paper contributes to bringing valuable insights with regard to the future developments of both DM and the banking sector along with a comprehensive one stop reference table. Moreover, we identify the key obstacles and present a summary for all interested parties that are facing the challenges of big data.",poster,cp19
Business,p1242,d9,d5b65fac5a6f511c0261a14c93d702b6697ef98e,c19,International Conference on Conceptual Structures,Digitalisation and Big Data Mining in Banking,"Banking as a data intensive subject has been progressing continuously under the promoting influences of the era of big data. Exploring the advanced big data analytic tools like Data Mining (DM) techniques is key for the banking sector, which aims to reveal valuable information from the overwhelming volume of data and achieve better strategic management and customer satisfaction. In order to provide sound direction for the future research and development, a comprehensive and most up to date review of the current research status of DM in banking will be extremely beneficial. Since existing reviews only cover the applications until 2013, this paper aims to fill this research gap and presents the significant progressions and most recent DM implementations in banking post 2013. By collecting and analyzing the trends of research focus, data resources, technological aids, and data analytical tools, this paper contributes to bringing valuable insights with regard to the future developments of both DM and the banking sector along with a comprehensive one stop reference table. Moreover, we identify the key obstacles and present a summary for all interested parties that are facing the challenges of big data.",poster,cp19
Computer Science,p1243,d3,8b924846eef928a9f30c88cdd9733662668335c0,j65,International Journal of Information Management,Big data: From beginning to future,Abstract,fullPaper,jv65
Computer Science,p1245,d3,ca72f601cc7c3f40f2c6300e9b115610dabf4f5b,j314,ACM Transactions on Knowledge Discovery from Data,Tensor Completion Algorithms in Big Data Analytics,"Tensor completion is a problem of filling the missing or unobserved entries of partially observed tensors. Due to the multidimensional character of tensors in describing complex datasets, tensor completion algorithms and their applications have received wide attention and achievement in areas like data mining, computer vision, signal processing, and neuroscience. In this survey, we provide a modern overview of recent advances in tensor completion algorithms from the perspective of big data analytics characterized by diverse variety, large volume, and high velocity. We characterize these advances from the following four perspectives: general tensor completion algorithms, tensor completion with auxiliary information (variety), scalable tensor completion algorithms (volume), and dynamic tensor completion algorithms (velocity). Further, we identify several tensor completion applications on real-world data-driven problems and present some common experimental frameworks popularized in the literature along with several available software repositories. Our goal is to summarize these popular methods and introduce them to researchers and practitioners for promoting future research and applications. We conclude with a discussion of key challenges and promising research directions in this community for future exploration.",fullPaper,jv314
Mathematics,p1245,d6,ca72f601cc7c3f40f2c6300e9b115610dabf4f5b,j314,ACM Transactions on Knowledge Discovery from Data,Tensor Completion Algorithms in Big Data Analytics,"Tensor completion is a problem of filling the missing or unobserved entries of partially observed tensors. Due to the multidimensional character of tensors in describing complex datasets, tensor completion algorithms and their applications have received wide attention and achievement in areas like data mining, computer vision, signal processing, and neuroscience. In this survey, we provide a modern overview of recent advances in tensor completion algorithms from the perspective of big data analytics characterized by diverse variety, large volume, and high velocity. We characterize these advances from the following four perspectives: general tensor completion algorithms, tensor completion with auxiliary information (variety), scalable tensor completion algorithms (volume), and dynamic tensor completion algorithms (velocity). Further, we identify several tensor completion applications on real-world data-driven problems and present some common experimental frameworks popularized in the literature along with several available software repositories. Our goal is to summarize these popular methods and introduce them to researchers and practitioners for promoting future research and applications. We conclude with a discussion of key challenges and promising research directions in this community for future exploration.",fullPaper,jv314
Computer Science,p1246,d3,f957ec0ae0a4bc19a3958a2bcff9223f97f58567,c62,International Conference on Advanced Data and Information Engineering,"Big data: Dimensions, evolution, impacts, and challenges",Abstract,poster,cp62
Computer Science,p1247,d3,7d969b2a0a5ec818006f1fd7afb9e898eb913357,j260,IEEE Transactions on Big Data,Cloud Infrastructure Resource Allocation for Big Data Applications,"Increasing popular big data applications bring about invaluable information, but along with challenges to industrial community and academia. Cloud computing with unlimited resources seems to be the way out. However, this panacea cannot play its role if we do not arrange fine allocation for cloud infrastructure resources. In this paper, we present a multi-objective optimization algorithm to trade off the performance, availability, and cost of Big Data application running on Cloud. After analyzing and modeling the interlaced relations among these objectives, we design and implement our approach on experimental environment. Finally, three sets of experiments show that our approach can run about 20 percent faster than traditional optimization approaches, and can achieve about 15 percent higher performance than other heuristic algorithms, while saving 4 to 20 percent cost.",fullPaper,jv260
Computer Science,p1252,d3,352cf2e54105798d228cbd0962880bc19059a1cd,j317,Journal of Internet Services and Applications,Applications of big data to smart cities,Abstract,fullPaper,jv317
Computer Science,p1253,d3,363cf3b41ea1f98de4935994e45a166695f41f34,c86,International Conference on Big Data and Education,"The 10 Vs, Issues and Challenges of Big Data","In this emerging computing and digital globe, information and Knowledge are created and then collected with a rapid approach by wide range of applications through scientific computing and commercial workloads. Over 3.8 billion people out of 7.6 billion population of the world are connected to the internet. Out of 13.4 billion devices, 8.06 billion devices have a mobile connection. In 2020, 38.5 billion devices will be connected and globally internet traffic will be 92 times greater than it was in 2005. The use of such devices and internet not only increase the data volume but the velocity of market brings in fast-track and accelerates as information is transferred and shared with light speed on optic fiber and wireless networks. This fast generation of huge data creates numerous challenges. The existing approaches addressing issues such as, Volume, Variety, Velocity and Value in big data research perspective. The objectives of the paper are to investigate and analyze the current status of Big Data and furthermore a comprehensive overview of various aspects has discussed, and additionally has been described all 10 Vs' (Issues) of Big Data.",fullPaper,cp86
Computer Science,p1254,d3,08a02c91597115e4dad97950adf01804862ec049,c49,ACM/SIGCOMM Internet Measurement Conference,"Big-Crypto: Big Data, Blockchain and Cryptocurrency","Cryptocurrency has been a trending topic over the past decade, pooling tremendous technological power and attracting investments valued over trillions of dollars on a global scale. The cryptocurrency technology and its network have been endowed with many superior features due to its unique architecture, which also determined its worldwide efficiency, applicability and data intensive characteristics. This paper introduces and summarises the interactions between two significant concepts in the digitalized world, i.e., cryptocurrency and Big Data. Both subjects are at the forefront of technological research, and this paper focuses on their convergence and comprehensively reviews the very recent applications and developments after 2016. Accordingly, we aim to present a systematic review of the interactions between Big Data and cryptocurrency and serve as the one stop reference directory for researchers with regard to identifying research gaps and directing future explorations.",poster,cp49
Computer Science,p1257,d3,dc967b697caec331ec7f0e95d23b1e98873b4c1e,c89,Conference on Uncertainty in Artificial Intelligence,Big Data in Health Care: Applications and Challenges,"Abstract The concept of Big Data is popular in a variety of domains. The purpose of this review was to summarize the features, applications, analysis approaches, and challenges of Big Data in health care. Big Data in health care has its own features, such as heterogeneity, incompleteness, timeliness and longevity, privacy, and ownership. These features bring a series of challenges for data storage, mining, and sharing to promote health-related research. To deal with these challenges, analysis approaches focusing on Big Data in health care need to be developed and laws and regulations for making use of Big Data in health care need to be enacted. From a patient perspective, application of Big Data analysis could bring about improved treatment and lower costs. In addition to patients, government, hospitals, and research institutions could also benefit from the Big Data in health care.",poster,cp89
Computer Science,p1258,d3,e282659f372b67219567938a770648903713cb0a,c35,"International Conference on Internet of Things, Big Data and Security",An insight into imbalanced Big Data classification: outcomes and challenges,Abstract,poster,cp35
Computer Science,p1261,d3,d92897c169ebb44e6d690e1dd69266894ad5c8b8,c63,International Conference on Evaluation & Assessment in Software Engineering,Mystiko—Blockchain Meets Big Data,"Blockchain is a peer-to-peer distributed storage that stores chronological series of transactions in a tamper-resistant manner. Blockchain became popular in various industries due to its decentralized trust ecosystem. When integrating blockchain with big data, one encounters many challenges. Current public blockchain does not support high transaction throughput; it does not scale in terms of big data storage and management; it does not provide keyword-based search and retrieval; and so on. As a result, it is hard to incorporate existing blockchain systems for big data applications. In this research, we propose a new blockchain storage ""Mystiko"" that is built over the Apache Cassandra distributed database to incorporate big data. Mystiko supports high transaction throughput, high scalability, high availability and full text search features. With Mystiko, we make big data more secure, structured and meaningful, and allows further data analytics on big data to be more easily performed.",poster,cp63
Computer Science,p1262,d3,f158cad7a51f46be0d8831a00a2869ee97524048,j229,Journal on Data Semantics,Big Data Semantics,Abstract,fullPaper,jv229
Computer Science,p1265,d3,b9cc31a2c718b54d5798b8dab3902acc33284d93,c45,IEEE Symposium on Security and Privacy,Big data challenges and opportunities in the hype of Industry 4.0,"The world of industrial automation technology is at the outset of a new era of innovation with the hype of Industry 4.0. Global modern industrial system converges the power of machines, computing, analytics, connectivity, cyber-physical systems, Internet of things, automation, cloud system and data exchange. Industry 4.0 is a revolution towards the digital world of digital factories and smart products. Big data is an integration of multi-disciplinary technologies and facilitates customer by bringing incredible services to a click. Internet of things connected the world of machines by adding communication capability in every device to connect to other devices or access the Internet. Big Data inflict a new horizon of opportunities in these systems. In this paper, challenges and opportunities of industrial big data are revealed in the context of Industry 4.0 with a different perspective. The current study helps the researchers to threshold these modern systems of Industry 4.0 in designing big data algorithms and techniques.",poster,cp45
Computer Science,p1266,d3,93ac3365c6c878c5daaf32eb2c8db2a7089a4c73,j65,International Journal of Information Management,Big data with cognitive computing: A review for the future,Abstract,fullPaper,jv65
Computer Science,p1273,d3,0d2a99f498221d284adb9782332e8950192c3b84,j7,Journal of Big Data,Analysis of agriculture data using data mining techniques: application of big data,Abstract,fullPaper,jv7
Computer Science,p1275,d3,e87f503e838ce7c2ae8a90bfa3cda7bbac8c919a,c71,International Joint Conference on Artificial Intelligence,Privacy-Preserving Record Linkage for Big Data: Current Approaches and Research Challenges,Abstract,poster,cp71
Computer Science,p1276,d3,dcbd4a20c1b636376b0818b89943e9087f2914f3,c48,"Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing",SECURITY ISSUES ASSOCIATED WITH BIG DATA IN CLOUD COMPUTING,"In this paper, we discuss security issues for cloud computing, Big data, Map Reduce and Hadoop environment. The main focus is on security issues in cloud computing that are associated with big data. Big data applications are a great benefit to organizations, business, companies and many large scale and small scale industries. We also discuss various possible solutions for the issues in cloud computing security and Hadoop. Cloud computing security is developing at a rapid pace which includes computer security, network security, information security, and data privacy. Cloud computing plays a very vital role in protecting data, applications and the related infrastructure with the help of policies, technologies, controls, and big data tools. Moreover, cloud computing, big data and its applications, advantages are likely to represent the most promising new frontiers in science.",poster,cp48
Computer Science,p1281,d3,1d9e78fa5bd16b0a87375712f9150c074c9a651d,c121,International Conference on Interaction Sciences,Forecasting Destination Weekly Hotel Occupancy with Big Data,"Hospitality constituencies need accurate forecasting of future performance of hotels in specific destinations to benchmark their properties and better optimize operations. As competition increases, hotel managers have urgent need for accurate short-term forecasts. In this study, time-series models incorporating several tourism big data sources, including search engine queries, website traffic, and weekly weather information, are tested in order to construct an accurate forecasting model of weekly hotel occupancy for a destination. The results show the superiority of ARMAX models with both search engine queries and website traffic data in accurate forecasting. Also, the results suggest that weekly dummies are superior to Fourier terms in capturing the hotel seasonality. The limitations of the inclusion of multiple big data sources are noted since the reduction in forecasting error is minimal.",poster,cp121
Computer Science,p1282,d3,fc51107f341fff07a17139ad31ee16587ad1cd45,c40,European Conference on Computer Vision,Exploring the path to big data analytics success in healthcare,Abstract,poster,cp40
Computer Science,p1283,d3,f08ad50f72abbfdaaf3da16d1f36f98caa499c1d,j265,IEEE Communications Surveys and Tutorials,Networking for Big Data: A Survey,"Complementary to the fancy big data applications, networking for big data is an indispensable supporting platform for these applications in practice. This emerging research branch has gained extensive attention from both academia and industry in recent years. In this new territory, researchers are facing many unprecedented theoretical and practical challenges. We are therefore motivated to solicit the latest works in this area, aiming to pave a comprehensive and solid starting ground for interested readers. We first clarify the definition of networking for big data based on the cross disciplinary nature and integrated needs of the domain. Second, we present the current understanding of big data from different levels, including its formation, networking features, mathematical representations, and the networking technologies. Third, we discuss the challenges and opportunities from various perspectives in this hopeful field. We further summarize the lessons we learned based on the survey. We humbly hope this paper will shed light for forthcoming researchers to further explore the uncharted part of this promising land.",fullPaper,jv265
Computer Science,p1285,d3,18682ba1feb27958f96bea076bdd7decca7c4d96,j7,Journal of Big Data,"Big Data management in smart grid: concepts, requirements and implementation",Abstract,fullPaper,jv7
Computer Science,p1287,d3,b184eb1a4aaa47993c7d7fb7fb994f6abd2b274a,c40,European Conference on Computer Vision,The Role of Big Data and Predictive Analytics in Retailing,Abstract,poster,cp40
Computer Science,p1288,d3,b14c291d99f32580fe987c20e52a248c62838e68,c16,International Conference on Data Science and Advanced Analytics,"The data revolution : big data, open data, data infrastructures & their consequences","Chapter 1: Conceptualising Data What are data? Kinds of data Data, information, knowledge, wisdom Framing data Thinking critically about databases and data infrastructures Data assemblages and the data revolution Chapter 2: Small Data, Data Infrastructures and Data Brokers Data holdings, data archives and data infrastructures Rationale for research data infrastructures The challenges of building data infrastructures The challenges of building data infrastructuresData brokers and markets Chapter 3: Open and Linked Data Open data Linked data The case for open data The economics of open data Concerns with respect to opening data Chapter 4: Big Data Volume Exhaustive Resolution and indexicality Relationality Velocity Variety Flexibility Chapter 5: Enablers and Sources of Big Data The enablers of big data Sources of big data Directed Data Automated data Volunteered data Chapter 6: Data Analytics Pre-analytics Machine learning Data mining and pattern recognition Data visualisation and visual analytics Statistical analysis Prediction, simulation and optimization Chapter 7: The Governmental and Business Rationale for Big Data Governing people Managing organisations Leveraging value and producing capital Creating better places Chapter 8: The Reframing of Science, Social Science and Humanities Research The fourth paradigm in science? The re-emergence of empiricism The fallacies of empiricism Data-driven science Computational social sciences and digital humanities Chapter 9: Technical and Organisational Issues Deserts and deluges Access Data quality, veracity and lineage Data integration and interoperability Poor analysis and ecological fallacies Skills and human resourcing Chapter 10: Ethical, Political, Social and Legal Concerns Data shadows and dataveillance Privacy Data security Profiling, social sorting and redlining Secondary uses, control creep and anticipatory governance Modes of governance and technological lock-ins Chapter 11: Making Sense of the Data Revolution Understanding data and the data revolution Researching data assemblages Final thoughts",poster,cp16
Sociology,p1288,d4,b14c291d99f32580fe987c20e52a248c62838e68,c16,International Conference on Data Science and Advanced Analytics,"The data revolution : big data, open data, data infrastructures & their consequences","Chapter 1: Conceptualising Data What are data? Kinds of data Data, information, knowledge, wisdom Framing data Thinking critically about databases and data infrastructures Data assemblages and the data revolution Chapter 2: Small Data, Data Infrastructures and Data Brokers Data holdings, data archives and data infrastructures Rationale for research data infrastructures The challenges of building data infrastructures The challenges of building data infrastructuresData brokers and markets Chapter 3: Open and Linked Data Open data Linked data The case for open data The economics of open data Concerns with respect to opening data Chapter 4: Big Data Volume Exhaustive Resolution and indexicality Relationality Velocity Variety Flexibility Chapter 5: Enablers and Sources of Big Data The enablers of big data Sources of big data Directed Data Automated data Volunteered data Chapter 6: Data Analytics Pre-analytics Machine learning Data mining and pattern recognition Data visualisation and visual analytics Statistical analysis Prediction, simulation and optimization Chapter 7: The Governmental and Business Rationale for Big Data Governing people Managing organisations Leveraging value and producing capital Creating better places Chapter 8: The Reframing of Science, Social Science and Humanities Research The fourth paradigm in science? The re-emergence of empiricism The fallacies of empiricism Data-driven science Computational social sciences and digital humanities Chapter 9: Technical and Organisational Issues Deserts and deluges Access Data quality, veracity and lineage Data integration and interoperability Poor analysis and ecological fallacies Skills and human resourcing Chapter 10: Ethical, Political, Social and Legal Concerns Data shadows and dataveillance Privacy Data security Profiling, social sorting and redlining Secondary uses, control creep and anticipatory governance Modes of governance and technological lock-ins Chapter 11: Making Sense of the Data Revolution Understanding data and the data revolution Researching data assemblages Final thoughts",poster,cp16
Business,p1288,d9,b14c291d99f32580fe987c20e52a248c62838e68,c16,International Conference on Data Science and Advanced Analytics,"The data revolution : big data, open data, data infrastructures & their consequences","Chapter 1: Conceptualising Data What are data? Kinds of data Data, information, knowledge, wisdom Framing data Thinking critically about databases and data infrastructures Data assemblages and the data revolution Chapter 2: Small Data, Data Infrastructures and Data Brokers Data holdings, data archives and data infrastructures Rationale for research data infrastructures The challenges of building data infrastructures The challenges of building data infrastructuresData brokers and markets Chapter 3: Open and Linked Data Open data Linked data The case for open data The economics of open data Concerns with respect to opening data Chapter 4: Big Data Volume Exhaustive Resolution and indexicality Relationality Velocity Variety Flexibility Chapter 5: Enablers and Sources of Big Data The enablers of big data Sources of big data Directed Data Automated data Volunteered data Chapter 6: Data Analytics Pre-analytics Machine learning Data mining and pattern recognition Data visualisation and visual analytics Statistical analysis Prediction, simulation and optimization Chapter 7: The Governmental and Business Rationale for Big Data Governing people Managing organisations Leveraging value and producing capital Creating better places Chapter 8: The Reframing of Science, Social Science and Humanities Research The fourth paradigm in science? The re-emergence of empiricism The fallacies of empiricism Data-driven science Computational social sciences and digital humanities Chapter 9: Technical and Organisational Issues Deserts and deluges Access Data quality, veracity and lineage Data integration and interoperability Poor analysis and ecological fallacies Skills and human resourcing Chapter 10: Ethical, Political, Social and Legal Concerns Data shadows and dataveillance Privacy Data security Profiling, social sorting and redlining Secondary uses, control creep and anticipatory governance Modes of governance and technological lock-ins Chapter 11: Making Sense of the Data Revolution Understanding data and the data revolution Researching data assemblages Final thoughts",poster,cp16
History,p1288,d12,b14c291d99f32580fe987c20e52a248c62838e68,c16,International Conference on Data Science and Advanced Analytics,"The data revolution : big data, open data, data infrastructures & their consequences","Chapter 1: Conceptualising Data What are data? Kinds of data Data, information, knowledge, wisdom Framing data Thinking critically about databases and data infrastructures Data assemblages and the data revolution Chapter 2: Small Data, Data Infrastructures and Data Brokers Data holdings, data archives and data infrastructures Rationale for research data infrastructures The challenges of building data infrastructures The challenges of building data infrastructuresData brokers and markets Chapter 3: Open and Linked Data Open data Linked data The case for open data The economics of open data Concerns with respect to opening data Chapter 4: Big Data Volume Exhaustive Resolution and indexicality Relationality Velocity Variety Flexibility Chapter 5: Enablers and Sources of Big Data The enablers of big data Sources of big data Directed Data Automated data Volunteered data Chapter 6: Data Analytics Pre-analytics Machine learning Data mining and pattern recognition Data visualisation and visual analytics Statistical analysis Prediction, simulation and optimization Chapter 7: The Governmental and Business Rationale for Big Data Governing people Managing organisations Leveraging value and producing capital Creating better places Chapter 8: The Reframing of Science, Social Science and Humanities Research The fourth paradigm in science? The re-emergence of empiricism The fallacies of empiricism Data-driven science Computational social sciences and digital humanities Chapter 9: Technical and Organisational Issues Deserts and deluges Access Data quality, veracity and lineage Data integration and interoperability Poor analysis and ecological fallacies Skills and human resourcing Chapter 10: Ethical, Political, Social and Legal Concerns Data shadows and dataveillance Privacy Data security Profiling, social sorting and redlining Secondary uses, control creep and anticipatory governance Modes of governance and technological lock-ins Chapter 11: Making Sense of the Data Revolution Understanding data and the data revolution Researching data assemblages Final thoughts",poster,cp16
Political Science,p1288,d15,b14c291d99f32580fe987c20e52a248c62838e68,c16,International Conference on Data Science and Advanced Analytics,"The data revolution : big data, open data, data infrastructures & their consequences","Chapter 1: Conceptualising Data What are data? Kinds of data Data, information, knowledge, wisdom Framing data Thinking critically about databases and data infrastructures Data assemblages and the data revolution Chapter 2: Small Data, Data Infrastructures and Data Brokers Data holdings, data archives and data infrastructures Rationale for research data infrastructures The challenges of building data infrastructures The challenges of building data infrastructuresData brokers and markets Chapter 3: Open and Linked Data Open data Linked data The case for open data The economics of open data Concerns with respect to opening data Chapter 4: Big Data Volume Exhaustive Resolution and indexicality Relationality Velocity Variety Flexibility Chapter 5: Enablers and Sources of Big Data The enablers of big data Sources of big data Directed Data Automated data Volunteered data Chapter 6: Data Analytics Pre-analytics Machine learning Data mining and pattern recognition Data visualisation and visual analytics Statistical analysis Prediction, simulation and optimization Chapter 7: The Governmental and Business Rationale for Big Data Governing people Managing organisations Leveraging value and producing capital Creating better places Chapter 8: The Reframing of Science, Social Science and Humanities Research The fourth paradigm in science? The re-emergence of empiricism The fallacies of empiricism Data-driven science Computational social sciences and digital humanities Chapter 9: Technical and Organisational Issues Deserts and deluges Access Data quality, veracity and lineage Data integration and interoperability Poor analysis and ecological fallacies Skills and human resourcing Chapter 10: Ethical, Political, Social and Legal Concerns Data shadows and dataveillance Privacy Data security Profiling, social sorting and redlining Secondary uses, control creep and anticipatory governance Modes of governance and technological lock-ins Chapter 11: Making Sense of the Data Revolution Understanding data and the data revolution Researching data assemblages Final thoughts",poster,cp16
Computer Science,p1289,d3,ec4bf3c2a63d2f212045d6774ff2d35a221837c1,j246,IEEE Internet of Things Journal,A Secure Mechanism for Big Data Collection in Large Scale Internet of Vehicle,"As an extension for Internet of Things (IoT), Internet of Vehicles (IoV) achieves unified management in smart transportation area. With the development of IoV, an increasing number of vehicles are connected to the network. Large scale IoV collects data from different places and various attributes, which conform with heterogeneous nature of big data in size, volume, and dimensionality. Big data collection between vehicle and application platform becomes more and more frequent through various communication technologies, which causes evolving security attack. However, the existing protocols in IoT cannot be directly applied in big data collection in large scale IoV. The dynamic network structure and growing amount of vehicle nodes increases the complexity and necessary of the secure mechanism. In this paper, a secure mechanism for big data collection in large scale IoV is proposed for improved security performance and efficiency. To begin with, vehicles need to register in the big data center to connect into the network. Afterward, vehicles associate with big data center via mutual authentication and single sign-on algorithm. Two different secure protocols are proposed for business data and confidential data collection. The collected big data is stored securely using distributed storage. The discussion and performance evaluation result shows the security and efficiency of the proposed secure mechanism.",fullPaper,jv246
Computer Science,p1291,d3,d6119ea48453b12b4928f3ab36a4b04f31aac929,c87,International Conference on Big Data Research,Big Data with Ten Big Characteristics,"This paper reveals ten big characteristics (10 Bigs) of big data and explores their non-linear interrelationships through presenting a unified framework of big data. The framework has three levels: fundamental level, technological level, and socio-economic level. The fundamental level has four big fundamental characteristics of big data. The technological level consists of three big technological characteristics of big data. The socioeconomic level has three big socioeconomic characteristics of big data. The paper looks at each level of the proposed framework from a service-oriented perspective. The proposed approach in this paper might facilitate the research and development of big data, big data analytics, business intelligence, and business analytics.",fullPaper,cp87
Computer Science,p1293,d3,becb893edf5629482f8d8acc566a12054477aa3b,j322,Global Journal of Flexible Systems Management,Big Data Analytics: A Review on Theoretical Contributions and Tools Used in Literature,Abstract,fullPaper,jv322
Computer Science,p1294,d3,259dbbd8fe040863d6f7cf7bbdc03cc1c3f193be,j292,Information Manager (The),A Big Data Analytics Method for Tourist Behaviour Analysis,Abstract,fullPaper,jv292
Computer Science,p1298,d3,859edc821f821b74fc9c818e45bcecb850603d07,j152,IEEE Access,Toward Scalable Systems for Big Data Analytics: A Technology Tutorial,"Recent technological advancements have led to a deluge of data from distinctive domains (e.g., health care and scientific sensors, user-generated data, Internet and financial companies, and supply chain systems) over the past two decades. The term big data was coined to capture the meaning of this emerging trend. In addition to its sheer volume, big data also exhibits other unique characteristics as compared with traditional data. For instance, big data is commonly unstructured and require more real-time analysis. This development calls for new system architectures for data acquisition, transmission, storage, and large-scale data processing mechanisms. In this paper, we present a literature survey and system tutorial for big data analytics platforms, aiming to provide an overall picture for nonexpert readers and instill a do-it-yourself spirit for advanced audiences to customize their own big-data solutions. First, we present the definition of big data and discuss big data challenges. Next, we present a systematic framework to decompose big data systems into four sequential modules, namely data generation, data acquisition, data storage, and data analytics. These four modules form a big data value chain. Following that, we present a detailed survey of numerous approaches and mechanisms from research and industry communities. In addition, we present the prevalent Hadoop framework for addressing big data challenges. Finally, we outline several evaluation benchmarks and potential research directions for big data systems.",fullPaper,jv152
Computer Science,p1299,d3,e8048433239796b1eb6d65c5121512b0f811185e,j208,Multimedia tools and applications,A resource-efficient encryption algorithm for multimedia big data,Abstract,fullPaper,jv208
Computer Science,p1300,d3,be07d32a3eb02807f910291cc865a83a9afdc708,j13,International Journal of Data Science and Analysis,Sports analytics and the big-data era,Abstract,fullPaper,jv13
Psychology,p1300,d10,be07d32a3eb02807f910291cc865a83a9afdc708,j13,International Journal of Data Science and Analysis,Sports analytics and the big-data era,Abstract,fullPaper,jv13
Computer Science,p1301,d3,594180789a8d77b9122ed010cd43744828f2561e,j323,Journal of Knowledge Management,Big data text analytics: an enabler of knowledge management,"Purpose 
 
 
 
 
The purpose of this paper is to examine the role of big data text analytics as an enabler of knowledge management (KM). The paper argues that big data text analytics represents an important means to visualise and analyse data, especially unstructured data, which have the potential to improve KM within organisations. 
 
 
 
 
Design/methodology/approach 
 
 
 
 
The study uses text analytics to review 196 articles published in two of the leading KM journals – Journal of Knowledge Management and Journal of Knowledge Management Research & Practice – in 2013 and 2014. The text analytics approach is used to process, extract and analyse the 196 papers to identify trends in terms of keywords, topics and keyword/topic clusters to show the utility of big data text analytics. 
 
 
 
 
Findings 
 
 
 
 
The findings show how big data text analytics can have a key enabler role in KM. Drawing on the 196 articles analysed, the paper shows the power of big data-oriented text analytics tools in supporting KM through the visualisation of data. In this way, the authors highlight the nature and quality of the knowledge generated through this method for efficient KM in developing a competitive advantage. 
 
 
 
 
Research limitations/implications 
 
 
 
 
The research has important implications concerning the role of big data text analytics in KM, and specifically the nature and quality of knowledge produced using text analytics. The authors use text analytics to exemplify the value of big data in the context of KM and highlight how future studies could develop and extend these findings in different contexts. 
 
 
 
 
Practical implications 
 
 
 
 
Results contribute to understanding the role of big data text analytics as a means to enhance the effectiveness of KM. The paper provides important insights that can be applied to different business functions, from supply chain management to marketing management to support KM, through the use of big data text analytics. 
 
 
 
 
Originality/value 
 
 
 
 
The study demonstrates the practical application of the big data tools for data visualisation, and, with it, improving KM.",fullPaper,jv323
Computer Science,p1302,d3,76925b455073c8021c05157d64332a616f0c8aab,j304,IEEE wireless communications,Wireless Big Data Computing in Smart Grid,"The development of smart grid brings great improvement in the efficiency, reliability, and economics to power grid. However, at the same time, the volume and complexity of data in the grid explode. To address this challenge, big data technology is a strong candidate for the analysis and processing of smart grid data. In this article, we propose a big data computing architecture for smart grid analytics, which involves data resources, transmission, storage, and analysis. In order to enable big data computing in smart grid, a communication architecture is then described consisting of four main domains. Key technologies to enable big-data-aware wireless communication for smart grid are investigated. As a case study of the proposed architecture, we introduce a big-data- enabled storage planning scheme based on wireless big data computing. A hybrid approach is adopted for the optimization including GA for storage planning and a game theoretic inner optimization for daily energy scheduling. Simulation results indicate that the proposed storage planning scheme greatly reduce",fullPaper,jv304
Computer Science,p1303,d3,569fa8f129e33c09529bc28fa8d3fbf2862a7257,c6,Annual Conference on Genetic and Evolutionary Computation,"Big Data, Big Insights? Advancing Service Innovation and Design With Machine Learning","Service innovation is intertwined with service design, and knowledge from both fields should be integrated to advance theoretical and normative insights. However, studies bridging service innovation and service design are in their infancy. This is because the body of service innovation and service design research is large and heterogeneous, which makes it difficult, if not impossible, for any human to read and understand its entire content and to delineate appropriate guidelines on how to broaden the scope of either field. Our work addresses this challenge by presenting the first application of topic modeling, a type of machine learning, to review and analyze currently available service innovation and service design research (n = 641 articles with 10,543 pages of written text or 4,119,747 words). We provide an empirical contribution to service research by identifying and analyzing 69 distinct research topics in the published text corpus, a theoretical contribution by delineating an extensive research agenda consisting of four research directions and 12 operationalizable guidelines to facilitate cross-fertilization between the two fields, and a methodological contribution by introducing and demonstrating the applicability of topic modeling and machine learning as a novel type of big data analytics to our discipline.",poster,cp6
Computer Science,p1306,d3,26a0611b59bc73cf29a99fdd5d130d5b5e5658a9,c42,IEEE Working Conference on Mining Software Repositories,A Survey on Emerging Computing Paradigms for Big Data,"The explosive growth of data volume and the ever-increasing demands of data value extraction have driven us into the era of big data. The “5V” (Variety, Velocity, Volume, Value, and Veracity) characteristics of big data pose great challenges to traditional computing paradigms and motivate the emergence of new solutions. Cloud computing is one of the representative technologies that can perform massive-scale and complex data computing by taking advantages of virtualized resources, parallel processing and data service integration with scalable data storage. However, as we are also experiencing the revolution of Internet-of-things (IoT), the limitations of cloud computing on supporting lightweight end devices significantly impede the flourish of cloud computing at the intersection of big data and IoT era. It also promotes the urgency of proposing new computing paradigms. We provide an overview on the topic of big data, and a comprehensive survey on how cloud computing as well as its related technologies can address the challenges arisen by big data. Then, we analyze the disadvantages of cloud computing when big data encounters IoT, and introduce two promising computing paradigms, including fog computing and transparent computing, to support the big data services of IoT. Finally, some open challenges and future directions are summarized to foster continued research efforts into this evolving field of study.",poster,cp42
Computer Science,p1309,d3,f4e66bd035e195f539f1b65a5aaec0e873cdee29,j53,Computer Applications in Engineering Education,Data science in education: Big data and learning analytics,This paper considers the data science and the summaries significance of Big Data and Learning Analytics in education. The widespread platform of making high‐quality benefits that could be achieved by exhausting big data techniques in the field of education is considered. One principal architecture framework to support education research is proposed.,fullPaper,jv53
Computer Science,p1311,d3,a4584baccf772622e5dfd57db4f19290ac5f0d73,c48,"Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing",A survey of big data architectures and machine learning algorithms in healthcare,"Big Data has gained much attention from researchers in healthcare, bioinformatics, and information sciences. As a result, data production at this stage will be 44 times greater than that in 2009. Hence, the volume, velocity, and variety of data rapidly increase. Hence, it is difficult to store, process and visualise this huge data using traditional technologies. Many organisations such as Twitter, LinkedIn, and Facebook are used big data for different use cases in the social networking domain. Also, implementations of such architectures of the use cases have been published worldwide. However, a conceptual architecture for specific big data application has been limited. The intention of this paper is application-oriented architecture for big data systems, which is based on a study of published big data architectures for specific use cases. This paper also provides an overview of the state-of-the-art machine learning algorithms for processing big data in healthcare and other applications.",poster,cp48
Computer Science,p1312,d3,1ff13ea4f9b9c9096062d120c5eb7a26e660ad96,j152,IEEE Access,Big Health Application System based on Health Internet of Things and Big Data,"The world is facing problems, such as uneven distribution of medical resources, the growing chronic diseases, and the increasing medical expenses. Blending the latest information technology into the healthcare system will greatly mitigate the problems. This paper presents the big health application system based on the health Internet of Things and big data. The system architecture, key technologies, and typical applications of big health system are introduced in detail.",fullPaper,jv152
Computer Science,p1313,d3,1d984051486471b0ec819113e26fc4440ebff21b,j7,Journal of Big Data,The core enabling technologies of big data analytics and context-aware computing for smart sustainable cities: a review and synthesis,Abstract,fullPaper,jv7
Computer Science,p1314,d3,8ee4eda834e95124aca1e5ff05a1b8ce7d1487ec,j150,SIAM Journal on Mathematics of Data Science,Why Are Big Data Matrices Approximately Low Rank?,"Matrices of (approximate) low rank are pervasive in data science, appearing in movie preferences, text documents, survey data, medical records, and genomics. While there is a vast literature on how...",fullPaper,jv150
Computer Science,p1315,d3,fd323c20df356d0b7f082806e22ca92eaa0bcf03,j134,Big Data Research,Significance and Challenges of Big Data Research,Abstract,fullPaper,jv134
Computer Science,p1316,d3,1cbe7243f3e91f95069020bdaf5fa753fb663439,c78,ACM/IEEE International Conference on Model Driven Engineering Languages and Systems,‘Hypernudge’: Big Data as a mode of regulation by design,"ABSTRACT This paper draws on regulatory governance scholarship to argue that the analytic phenomenon currently known as ‘Big Data’ can be understood as a mode of ‘design-based’ regulation. Although Big Data decision-making technologies can take the form of automated decision-making systems, this paper focuses on algorithmic decision-guidance techniques. By highlighting correlations between data items that would not otherwise be observable, these techniques are being used to shape the informational choice context in which individual decision-making occurs, with the aim of channelling attention and decision-making in directions preferred by the ‘choice architect’. By relying upon the use of ‘nudge’ – a particular form of choice architecture that alters people’s behaviour in a predictable way without forbidding any options or significantly changing their economic incentives, these techniques constitute a ‘soft’ form of design-based control. But, unlike the static Nudges popularised by Thaler and Sunstein [(2008). Nudge. London: Penguin Books] such as placing the salad in front of the lasagne to encourage healthy eating, Big Data analytic nudges are extremely powerful and potent due to their networked, continuously updated, dynamic and pervasive nature (hence ‘hypernudge’). I adopt a liberal, rights-based critique of these techniques, contrasting liberal theoretical accounts with selective insights from science and technology studies (STS) and surveillance studies on the other. I argue that concerns about the legitimacy of these techniques are not satisfactorily resolved through reliance on individual notice and consent, touching upon the troubling implications for democracy and human flourishing if Big Data analytic techniques driven by commercial self-interest continue their onward march unchecked by effective and legitimate constraints.",poster,cp78
Computer Science,p1317,d3,fe944ce6918c27e20e8c410d05950e84bd89987f,c99,Symposium on the Theory of Computing,Heterogeneous Data and Big Data Analytics,"Heterogeneity is one of major features of big data and heterogeneous data result in problems in data integration and Big Data analytics. This paper introduces data processing methods for heterogeneous data and Big Data analytics, Big Data tools, some traditional data mining (DM) and machine learning (ML) methods. Deep learning and its potential in Big Data analytics are analysed. The benefits of the confluences among Big Data analytics, deep learning, high performance computing (HPC), and heterogeneous computing are presented. Challenges of dealing with heterogeneous data and Big Data analytics are also discussed.",poster,cp99
Computer Science,p1318,d3,8af6996d540a3bd1a9ceb0fc24fc50d7a5caa0c7,j323,Journal of Knowledge Management,Does big data mean big knowledge? KM perspectives on big data and analytics,"Purpose 
 
 
 
 
This viewpoint study aims to make the case that the field of knowledge management (KM) must respond to the significant changes that big data/analytics is bringing to operationalizing the production of organizational data and information. 
 
 
 
 
Design/methodology/approach 
 
 
 
 
This study expresses the opinions of the guest editors of “Does Big Data Mean Big Knowledge? Knowledge Management Perspectives on Big Data and Analytics”. 
 
 
 
 
Findings 
 
 
 
 
A Big Data/Analytics-Knowledge Management (BDA-KM) model is proposed that illustrates the centrality of knowledge as the guiding principle in the use of big data/analytics in organizations. 
 
 
 
 
Research limitations/implications 
 
 
 
 
This is an opinion piece, and the proposed model still needs to be empirically verified. 
 
 
 
 
Practical implications 
 
 
 
 
It is suggested that academics and practitioners in KM must be capable of controlling the application of big data/analytics, and calls for further research investigating how KM can conceptually and operationally use and integrate big data/analytics to foster organizational knowledge for better decision-making and organizational value creation. 
 
 
 
 
Originality/value 
 
 
 
 
The BDA-KM model is one of the early models placing knowledge as the primary consideration in the successful organizational use of big data/analytics.",fullPaper,jv323
Computer Science,p1319,d3,0a4569cfcb193a548de7445106135445bae5950f,c117,Very Large Data Bases Conference,"Big Data: Challenges, Opportunities and Realities","With the advent of Internet of Things (IoT) and Web 2.0 technologies, there has been a tremendous growth in the amount of data generated. This chapter emphasizes on the need for big data, technological advancements, tools and techniques being used to process big data are discussed. Technological improvements and limitations of existing storage techniques are also presented. Since, the traditional technologies like Relational Database Management System (RDBMS) have their own limitations to handle big data, new technologies have been developed to handle them and to derive useful insights. This chapter presents an overview of big data analytics, its application, advantages, and limitations. Few research issues and future directions are presented in this chapter.",poster,cp117
Computer Science,p1320,d3,a197b5e02d4effb1df5a249d41b975a8aa70d9b5,j7,Journal of Big Data,A bibliometric approach to tracking big data research trends,Abstract,fullPaper,jv7
Computer Science,p1322,d3,8a076dc0034c9c92e444c60dbd3f5f31ac28ab8b,j7,Journal of Big Data,Big data analytics: does organizational factor matters impact technology acceptance?,Abstract,fullPaper,jv7
Computer Science,p1323,d3,236c874bad9845d2750e9579d0c23a97c71f4cfb,c10,Americas Conference on Information Systems,Big Data Visualization Tools,Abstract,poster,cp10
Computer Science,p1326,d3,f34769765f75f94dc81b21faa2e50594ef1fcbfb,j324,IEEE Transactions on Emerging Topics in Computing,A Survey of Clustering Algorithms for Big Data: Taxonomy and Empirical Analysis,"Clustering algorithms have emerged as an alternative powerful meta-learning tool to accurately analyze the massive volume of data generated by modern applications. In particular, their main goal is to categorize data into clusters such that objects are grouped in the same cluster when they are similar according to specific metrics. There is a vast body of knowledge in the area of clustering and there has been attempts to analyze and categorize them for a larger number of applications. However, one of the major issues in using clustering algorithms for big data that causes confusion amongst practitioners is the lack of consensus in the definition of their properties as well as a lack of formal categorization. With the intention of alleviating these problems, this paper introduces concepts and algorithms related to clustering, a concise survey of existing (clustering) algorithms as well as providing a comparison, both from a theoretical and an empirical perspective. From a theoretical perspective, we developed a categorizing framework based on the main properties pointed out in previous studies. Empirically, we conducted extensive experiments where we compared the most representative algorithm from each of the categories using a large number of real (big) data sets. The effectiveness of the candidate clustering algorithms is measured through a number of internal and external validity metrics, stability, runtime, and scalability tests. In addition, we highlighted the set of clustering algorithms that are the best performing for big data.",fullPaper,jv324
Computer Science,p1327,d3,213976a51735aaabc96eb53444b02f2c7ca04f4f,c89,Conference on Uncertainty in Artificial Intelligence,Big Data and Management,The authors reflect on management of big data by organizations. They comment on service level agreements (SLA) which define the nature and quality of information technology services and mention big data-sharing agreements tend to be poorly structured and informal. They reflect on the methodologies of analyzing big data and state it is easy to get false correlations when using typical statistical tools in analyzing big data. They talk about the use of big data in management and behavior research.,poster,cp89
Computer Science,p1331,d3,d5bf31824ec3560262525868fcd89211b2f50475,c88,International Conference on Big Data Computing and Communications,Big Data Model of Security Sharing Based on Blockchain,"The rise of big data age in the Internet has led to the explosive growth of data size. However, trust issue has become the biggest problem of big data, leading to the difficulty in data safe circulation and industry development. The blockchain technology provides a new solution to this problem by combining non-tampering, traceable features with smart contracts that automatically execute default instructions. In this paper, we present a credible big data sharing model based on blockchain technology and smart contract to ensure the safe circulation of data resources.",fullPaper,cp88
Computer Science,p1332,d3,8e641ea510c06485f33a61ea65555cf791efad2d,c16,International Conference on Data Science and Advanced Analytics,Big data and its technical challenges,Exploring the inherent technical challenges in realizing the potential of Big Data.,poster,cp16
Computer Science,p1334,d3,025917fd73695c87b2b35d8059b2961f433ae048,c1,International Conference on Human Factors in Computing Systems,Big data machine learning using apache spark MLlib,"Artificial intelligence, and particularly machine learning, has been used in many ways by the research community to turn a variety of diverse and even heterogeneous data sources into high quality facts and knowledge, providing premier capabilities to accurate pattern discovery. However, applying machine learning strategies on big and complex datasets is computationally expensive, and it consumes a very large amount of logical and physical resources, such as data file space, CPU, and memory. A sophisticated platform for efficient big data analytics is becoming more important these days as the data amount generated in a daily basis exceeds over quintillion bytes. Apache Spark MLlib is one of the most prominent platforms for big data analysis which offers a set of excellent functionalities for different machine learning tasks ranging from regression, classification, and dimension reduction to clustering and rule extraction. In this contribution, we explore, from the computational perspective, the expanding body of the Apache Spark MLlib 2.0 as an open-source, distributed, scalable, and platform independent machine learning library. Specifically, we perform several real world machine learning experiments to examine the qualitative and quantitative attributes of the platform. Furthermore, we highlight current trends in big data machine learning research and provide insights for future work.",poster,cp1
Computer Science,p1337,d3,644ba94b6fe1d19708feee478cdf31bd457ff6ba,j17,Big Data & Society,"What makes Big Data, Big Data? Exploring the ontological characteristics of 26 datasets","Big Data has been variously defined in the literature. In the main, definitions suggest that Big Data possess a suite of key traits: volume, velocity and variety (the 3Vs), but also exhaustivity, resolution, indexicality, relationality, extensionality and scalability. However, these definitions lack ontological clarity, with the term acting as an amorphous, catch-all label for a wide selection of data. In this paper, we consider the question ‘what makes Big Data, Big Data?’, applying Kitchin’s taxonomy of seven Big Data traits to 26 datasets drawn from seven domains, each of which is considered in the literature to constitute Big Data. The results demonstrate that only a handful of datasets possess all seven traits, and some do not possess either volume and/or variety. Instead, there are multiple forms of Big Data. Our analysis reveals that the key definitional boundary markers are the traits of velocity and exhaustivity. We contend that Big Data as an analytical category needs to be unpacked, with the genus of Big Data further delineated and its various species identified. It is only through such ontological work that we will gain conceptual clarity about what constitutes Big Data, formulate how best to make sense of it, and identify how it might be best used to make sense of the world.",fullPaper,jv17
Computer Science,p1338,d3,312c3d413fabb11478b8374bf307c24a90be213f,c55,Design Automation Conference,Handbook of Big Data Technologies,Abstract,poster,cp55
Computer Science,p1339,d3,d517b13f2b152c913b81ce534a149493517dbdad,j152,IEEE Access,Big Data Deep Learning: Challenges and Perspectives,"Deep learning is currently an extremely active research area in machine learning and pattern recognition society. It has gained huge successes in a broad area of applications such as speech recognition, computer vision, and natural language processing. With the sheer size of data available today, big data brings big opportunities and transformative potential for various sectors; on the other hand, it also presents unprecedented challenges to harnessing data and information. As the data keeps getting bigger, deep learning is coming to play a key role in providing big data predictive analytics solutions. In this paper, we provide a brief overview of deep learning, and highlight current research efforts and the challenges to big data, as well as the future trends.",fullPaper,jv152
Computer Science,p1341,d3,d4dafb2f797630d1ff45d7833e385a848380e449,j178,Data & Knowledge Engineering,Big data technologies and Management: What conceptual modeling can do,Abstract,fullPaper,jv178
Computer Science,p1342,d3,99b7c4f33ae58fcbc118571e6635f80d025576eb,j17,Big Data & Society,Conceptualising the right to data protection in an era of Big Data,"In 2009, with the enactment of the Lisbon Treaty, the Charter of Fundamental Rights of the European Union entered into force. Under Article 8 of the Charter, for the first time, a stand-alone fundamental right to data protection was declared. The creation of this right, standing as a distinct right to the right to privacy, is undoubtedly significant, and it is unique to the European legal order, being absent from other international human rights instruments. This commentary examines the parameters of this new right to data protection, asking what are the principles underpinning the right. It argues that the right reflects some key values inherent in the European legal order, namely: privacy, transparency, autonomy and nondiscrimination. It also analyses some of the challenges in implementing this right in an era of ubiquitous veillance practices and Big Data.",fullPaper,jv17
Political Science,p1342,d15,99b7c4f33ae58fcbc118571e6635f80d025576eb,j17,Big Data & Society,Conceptualising the right to data protection in an era of Big Data,"In 2009, with the enactment of the Lisbon Treaty, the Charter of Fundamental Rights of the European Union entered into force. Under Article 8 of the Charter, for the first time, a stand-alone fundamental right to data protection was declared. The creation of this right, standing as a distinct right to the right to privacy, is undoubtedly significant, and it is unique to the European legal order, being absent from other international human rights instruments. This commentary examines the parameters of this new right to data protection, asking what are the principles underpinning the right. It argues that the right reflects some key values inherent in the European legal order, namely: privacy, transparency, autonomy and nondiscrimination. It also analyses some of the challenges in implementing this right in an era of ubiquitous veillance practices and Big Data.",fullPaper,jv17
Computer Science,p1343,d3,fa4fa3418559e97418af35c8361628a7f81b7932,c28,International Conference on Contemporary Computing,Big data preprocessing: methods and prospects,Abstract,poster,cp28
Computer Science,p1347,d3,8f2a211fe0386b539ab6de383516fe17a0c47345,j15,Journal on spesial topics in mobile networks and applications,Smart Clothing: Connecting Human with Clouds and Big Data for Sustainable Health Monitoring,Abstract,fullPaper,jv15
Computer Science,p1348,d3,71d51302877734e2146f5c5772eb6df3e238a6d9,j260,IEEE Transactions on Big Data,Privacy-Preserving Data Encryption Strategy for Big Data in Mobile Cloud Computing,"Privacy has become a considerable issue when the applications of big data are dramatically growing in cloud computing. The benefits of the implementation for these emerging technologies have improved or changed service models and improve application performances in various perspectives. However, the remarkably growing volume of data sizes has also resulted in many challenges in practice. The execution time of the data encryption is one of the serious issues during the data processing and transmissions. Many current applications abandon data encryptions in order to reach an adoptive performance level companioning with privacy concerns. In this paper, we concentrate on privacy and propose a novel data encryption approach, which is called Dynamic Data Encryption Strategy (D2ES). Our proposed approach aims to selectively encrypt data and use privacy classification methods under timing constraints. This approach is designed to maximize the privacy protection scope by using a selective encryption strategy within the required execution time requirements. The performance of D2ES has been evaluated in our experiments, which provides the proof of the privacy enhancement.",fullPaper,jv260
Computer Science,p1349,d3,11d6ab00e757b75497414e589d7c03951c433217,c61,"International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems",Big data analytics for security and criminal investigations,"Applications of various data analytics technologies to security and criminal investigation during the past three decades have demonstrated the inception, growth, and maturation of criminal analytics. We first identify five cutting‐edge data mining technologies such as link analysis, intelligent agents, text mining, neural networks, and machine learning. Then, we explore their recent applications to the criminal analytics domain, and discuss the challenges arising from these innovative applications. We also extend our study to big data analytics which provides some state‐of‐the‐art technologies to reshape criminal investigations. In this paper, we review the recent literature, and examine the potentials of big data analytics for security intelligence under a criminal analytics framework. We examine some common data sources, analytics methods, and applications related to two important aspects of social network analysis namely, structural analysis and positional analysis that lay the foundation of criminal analytics. Another contribution of this paper is that we also advocate a novel criminal analytics methodology that is underpinned by big data analytics. We discuss the merits and challenges of applying big data analytics to the criminal analytics domain. Finally, we highlight the future research directions of big data analytics enhanced criminal investigations. WIREs Data Mining Knowl Discov 2017, 7:e1208. doi: 10.1002/widm.1208",poster,cp61
Computer Science,p1351,d3,d6d793cf8cfbee473125c322b046e6df26361efd,j210,IEEE Communications Magazine,Big data caching for networking: moving from cloud to edge,"In order to cope with the relentless data tsunami in 5G wireless networks, current approaches such as acquiring new spectrum, deploying more BSs, and increasing nodes in mobile packet core networks are becoming ineffective in terms of scalability, cost and flexibility. In this regard, context- aware 5G networks with edge/cloud computing and exploitation of big data analytics can yield significant gains for mobile operators. In this article, proactive content caching in 5G wireless networks is investigated in which a big-data-enabled architecture is proposed. In this practical architecture, a vast amount of data is harnessed for content popularity estimation, and strategic contents are cached at BSs to achieve higher user satisfaction and backhaul offloading. To validate the proposed solution, we consider a real-world case study where several hours worth of mobile data traffic is collected from a major telecom operator in Turkey, and big-data-enabled analysis is carried out, leveraging tools from machine learning. Based on the available information and storage capacity, numerical studies show that several gains are achieved in terms of both user satisfaction and backhaul offloading. For example, in the case of 16 BSs with 30 percent of content ratings and 13 GB storage size (78 percent of total library size), proactive caching yields 100 percent user satisfaction and offloads 98 percent of the backhaul.",fullPaper,jv210
Computer Science,p1352,d3,5ea3ff5390ec5c36db5ecc8d7a04fa2d88559005,j226,Information Systems,Persisting big-data: The NoSQL landscape,Abstract,fullPaper,jv226
Computer Science,p1353,d3,da63f30bd5b3a1b16c261f75ca1b1daddfc5b44d,c106,International Conference on Biometrics,Big Data and Data Science Methods for Management Research,"The recent advent of remote sensing, mobile technologies, novel transaction systems, and highperformance computing offers opportunities to understand trends, behaviors, and actions in a manner that has not been previously possible. Researchers can thus leverage “big data” that are generated from a plurality of sources including mobile transactions, wearable technologies, social media, ambient networks, andbusiness transactions.An earlierAcademy of Management Journal (AMJ) editorial explored the potential implications for data science inmanagement research and highlighted questions for management scholarship as well as the attendant challenges of data sharing and privacy (George, Haas, & Pentland, 2014). This nascent field is evolving rapidly and at a speed that leaves scholars and practitioners alike attempting to make sense of the emergent opportunities that big datahold.With thepromiseof bigdata comequestions about the analytical value and thus relevance of these data for theory development—including concerns over the context-specific relevance, its reliability and its validity. To address this challenge, data science is emerging as an interdisciplinary field that combines statistics, data mining, machine learning, and analytics to understand and explainhowwecan generate analytical insights and prediction models from structured and unstructured big data. Data science emphasizes the systematic study of the organization, properties, and analysis of data and their role in inference, including our confidence in the inference (Dhar, 2013).Whereas both big data and data science terms are often used interchangeably, “big data” refer to large and varied data that can be collected and managed, whereas “data science” develops models that capture, visualize, andanalyze theunderlyingpatterns in thedata. In this editorial, we address both the collection and handling of big data and the analytical tools provided by data science for management scholars. At the current time, practitioners suggest that data science applications tackle the three core elements of big data: volume, velocity, and variety (McAfee & Brynjolfsson, 2012; Zikopoulos & Eaton, 2011). “Volume” represents the sheer size of the dataset due to the aggregation of a large number of variables and an even larger set of observations for each variable. “Velocity” reflects the speed atwhich these data are collected and analyzed, whether in real time or near real time from sensors, sales transactions, social media posts, and sentiment data for breaking news and social trends. “Variety” in big data comes from the plurality of structured and unstructured data sources such as text, videos, networks, and graphics among others. The combinations of volume, velocity, and variety reveal the complex task of generating knowledge from big data, which often runs into millions of observations, and deriving theoretical contributions from such data. In this editorial, we provide a primer or a “starter kit” for potential data science applications inmanagement research. We do so with a caveat that emerging fields outdate and improve uponmethodologies while often supplanting them with new applications. Nevertheless, this primer can guide management scholars who wish to use data science techniques to reach better answers to existing questions or explore completely new research questions.",poster,cp106
Computer Science,p1355,d3,f289a7872a4b19cf3210da9b0a96e0d2fea4ea95,j36,British Journal of Educational Technology,Big Data and analytics in higher education: Opportunities and challenges,Institutions of higher education are operating in an increasingly complex and competitive environment. This paper identifies contemporary challenges facing institutions of higher education worldwide and explores the potential of Big Data in addressing these challenges. The paper then outlines a number of opportunities and challenges associated with the implementation of Big Data in the context of higher education.The paper concludes by outlining future directions relating to the development and implementation of an institutional project on Big Data.,fullPaper,jv36
Computer Science,p1357,d3,e301beb0e17805dbabf5add06d99c53e8703ea34,c89,Conference on Uncertainty in Artificial Intelligence,Gaussian Processes for Big Data,We introduce stochastic variational inference for Gaussian process models. This enables the application of Gaussian process (GP) models to data sets containing millions of data points. We show how GPs can be variationally decomposed to depend on a set of globally relevant inducing variables which factorize the model in the necessary manner to perform variational inference. Our approach is readily extended to models with non-Gaussian likelihoods and latent variable models based around Gaussian processes. We demonstrate the approach on a simple toy problem and two real world data sets.,fullPaper,cp89
Mathematics,p1357,d6,e301beb0e17805dbabf5add06d99c53e8703ea34,c89,Conference on Uncertainty in Artificial Intelligence,Gaussian Processes for Big Data,We introduce stochastic variational inference for Gaussian process models. This enables the application of Gaussian process (GP) models to data sets containing millions of data points. We show how GPs can be variationally decomposed to depend on a set of globally relevant inducing variables which factorize the model in the necessary manner to perform variational inference. Our approach is readily extended to models with non-Gaussian likelihoods and latent variable models based around Gaussian processes. We demonstrate the approach on a simple toy problem and two real world data sets.,fullPaper,cp89
Computer Science,p1359,d3,81b7e5635241f0fae8eac9f703f604a1a0073038,j185,Proceedings of the IEEE,Learning to Hash for Indexing Big Data—A Survey,"The explosive growth in Big Data has attracted much attention in designing efficient indexing and search methods recently. In many critical applications such as large-scale search and pattern matching, finding the nearest neighbors to a query is a fundamental research problem. However, the straightforward solution using exhaustive comparison is infeasible due to the prohibitive computational complexity and memory requirement. In response, approximate nearest neighbor (ANN) search based on hashing techniques has become popular due to its promising performance in both efficiency and accuracy. Prior randomized hashing methods, e.g., locality-sensitive hashing (LSH), explore data-independent hash functions with random projections or permutations. Although having elegant theoretic guarantees on the search quality in certain metric spaces, performance of randomized hashing has been shown insufficient in many real-world applications. As a remedy, new approaches incorporating data-driven learning methods in development of advanced hash functions have emerged. Such learning-to-hash methods exploit information such as data distributions or class labels when optimizing the hash codes or functions. Importantly, the learned hash codes are able to preserve the proximity of neighboring data in the original feature spaces in the hash code spaces. The goal of this paper is to provide readers with systematic understanding of insights, pros, and cons of the emerging techniques. We provide a comprehensive survey of the learning-to-hash framework and representative techniques of various types, including unsupervised, semisupervised, and supervised. In addition, we also summarize recent hashing approaches utilizing the deep learning models. Finally, we discuss the future direction and trends of research in this area.",fullPaper,jv185
Mathematics,p1359,d6,81b7e5635241f0fae8eac9f703f604a1a0073038,j185,Proceedings of the IEEE,Learning to Hash for Indexing Big Data—A Survey,"The explosive growth in Big Data has attracted much attention in designing efficient indexing and search methods recently. In many critical applications such as large-scale search and pattern matching, finding the nearest neighbors to a query is a fundamental research problem. However, the straightforward solution using exhaustive comparison is infeasible due to the prohibitive computational complexity and memory requirement. In response, approximate nearest neighbor (ANN) search based on hashing techniques has become popular due to its promising performance in both efficiency and accuracy. Prior randomized hashing methods, e.g., locality-sensitive hashing (LSH), explore data-independent hash functions with random projections or permutations. Although having elegant theoretic guarantees on the search quality in certain metric spaces, performance of randomized hashing has been shown insufficient in many real-world applications. As a remedy, new approaches incorporating data-driven learning methods in development of advanced hash functions have emerged. Such learning-to-hash methods exploit information such as data distributions or class labels when optimizing the hash codes or functions. Importantly, the learned hash codes are able to preserve the proximity of neighboring data in the original feature spaces in the hash code spaces. The goal of this paper is to provide readers with systematic understanding of insights, pros, and cons of the emerging techniques. We provide a comprehensive survey of the learning-to-hash framework and representative techniques of various types, including unsupervised, semisupervised, and supervised. In addition, we also summarize recent hashing approaches utilizing the deep learning models. Finally, we discuss the future direction and trends of research in this area.",fullPaper,jv185
Computer Science,p1361,d3,5dfbb89afb2d77c3afcb6a2cc2d24e537963a55b,j152,IEEE Access,Big Privacy: Challenges and Opportunities of Privacy Study in the Age of Big Data,"One of the biggest concerns of big data is privacy. However, the study on big data privacy is still at a very early stage. We believe the forthcoming solutions and theories of big data privacy root from the in place research output of the privacy discipline. Motivated by these factors, we extensively survey the existing research outputs and achievements of the privacy field in both application and theoretical angles, aiming to pave a solid starting ground for interested readers to address the challenges in the big data case. We first present an overview of the battle ground by defining the roles and operations of privacy systems. Second, we review the milestones of the current two major research categories of privacy: data clustering and privacy frameworks. Third, we discuss the effort of privacy study from the perspectives of different disciplines, respectively. Fourth, the mathematical description, measurement, and modeling on privacy are presented. We summarize the challenges and opportunities of this promising topic at the end of this paper, hoping to shed light on the exciting and almost uncharted land.",fullPaper,jv152
Computer Science,p1362,d3,f77331fc287e4bc76c4b3c464121ec6453fd448b,j190,IEEE Network,When big data meets software-defined networking: SDN for big data and big data for SDN,"Both big data and software-defined networking (SDN) have attracted great interests from both academia and industry. These two important areas have traditionally been addressed separately in the most of previous works. However, on the one hand, the good features of SDN can greatly facilitate big data acquisition, transmission, storage, and processing. On the other hand, big data will have profound impacts on the design and operation of SDN. In this paper, we present the good features of SDN in solving several issues prevailing with big data applications, including big data processing in cloud data centers, data delivery, joint optimization, scientific big data architectures and scheduling issues. We show that SDN can manage the network efficiently for improving the performance of big data applications. In addition, we show that big data can benefit SDN as well, including traffic engineering, cross-layer design, defeating security attacks, and SDN-based intra and inter data center networks. Moreover, we discuss a number of open issues that need to be addressed to jointly consider big data and SDN in future research.",fullPaper,jv190
Computer Science,p1363,d3,0751d9c1738d4fb71d4624c05913b2dfa162b279,j7,Journal of Big Data,Big data privacy: a technological perspective and review,Abstract,fullPaper,jv7
Computer Science,p1365,d3,ac11fc3acdf5233bda411dee6d72a424e0b33c4e,j13,International Journal of Data Science and Analysis,Big data analytics on Apache Spark,Abstract,fullPaper,jv13
Computer Science,p1366,d3,31931aa8d814d45f9cfcd1765aff07e443cca99d,c117,Very Large Data Bases Conference,Understanding big data,"Give us 5 minutes and we will show you the best book to read today. This is it, the understanding big data that will be your best choice for better reading book. Your five times will not spend wasted by reading this website. You can take the book as a source to make better concept. Referring the books that can be situated with your needs is sometime difficult. But here, this is so easy. You can find the best thing of book that you can read.",poster,cp117
Computer Science,p1367,d3,094b19739855efbdacb17a83508c42582391c263,j329,Journal of Information and Technology,"New games, new rules: big data and the changing context of strategy",Abstract,fullPaper,jv329
Computer Science,p1368,d3,49a46baf194ac92e01f54b2eecc767514854edcb,j190,IEEE Network,Big data-driven optimization for mobile networks toward 5G,"Big data offers a plethora of opportunities to mobile network operators for improving quality of service. This article explores various means of integrating big data analytics with network optimization toward the objective of improving the user quality of experience. We first propose a framework of Big Data-Driven (BDD) mobile network optimization. We then present the characteristics of big data that are collected not only from user equipment but also from mobile networks. Moreover, several techniques in data collection and analytics are discussed from the viewpoint of network optimization. Certain user cases on the application of the proposed framework for improving network performance are also given in order to demonstrate the feasibility of the framework. With the integration of the emerging fifth generation (5G) mobile networks with big data analytics, the quality of our daily mobile life is expected to be tremendously enhanced.",fullPaper,jv190
Computer Science,p1369,d3,006c846c72e77cb913be4b2c76664967e9e01ee0,c84,EUROCON Conference,AI^2: Training a Big Data Machine to Defend,"We present AI2, an analyst-in-the-loop security system where Analyst Intuition (AI) is put together with state-of-the-art machine learning to build a complete end-to-end Artificially Intelligent solution (AI). The system presents four key features: a big data behavioral analytics platform, an outlier detection system, a mechanism to obtain feedback from security analysts, and a supervised learning module. We validate our system with a real-world data set consisting of 3.6 billion log lines and 70.2 million entities. The results show that the system is capable of learning to defend against unseen attacks. With respect to unsupervised outlier analysis, our system improves the detection rate in 2.92× and reduces false positives by more than 5×.",poster,cp84
Computer Science,p1370,d3,131cef9d6caf65a8cf7ad6b771c531c409e8930d,j312,Annals of Operations Research,Big Data and supply chain management: a review and bibliometric analysis,Abstract,fullPaper,jv312
Computer Science,p1371,d3,50f15da80fbc5098db7e6d7aa815e493ea580c54,c90,International Conference on Collaboration Technologies and Systems,Big data: A review,"Big data is a term for massive data sets having large, more varied and complex structure with the difficulties of storing, analyzing and visualizing for further processes or results. The process of research into massive amounts of data to reveal hidden patterns and secret correlations named as big data analytics. These useful informations for companies or organizations with the help of gaining richer and deeper insights and getting an advantage over the competition. For this reason, big data implementations need to be analyzed and executed as accurately as possible. This paper presents an overview of big data's content, scope, samples, methods, advantages and challenges and discusses privacy concern on it.",fullPaper,cp90
Computer Science,p1372,d3,75b868e844e58db707fb5dbf7acbe2e26ba8c122,j330,Knowledge and Information Systems,A survey on indexing techniques for big data: taxonomy and performance evaluation,Abstract,fullPaper,jv330
Computer Science,p1374,d3,6237ed8129f7bdb2ec52482725d5418ad8d61584,j152,IEEE Access,Protection of Big Data Privacy,"In recent years, big data have become a hot research topic. The increasing amount of big data also increases the chance of breaching the privacy of individuals. Since big data require high computational power and large storage, distributed systems are used. As multiple parties are involved in these systems, the risk of privacy violation is increased. There have been a number of privacy-preserving mechanisms developed for privacy protection at different stages (e.g., data generation, data storage, and data processing) of a big data life cycle. The goal of this paper is to provide a comprehensive overview of the privacy preservation mechanisms in big data and present the challenges for existing mechanisms. In particular, in this paper, we illustrate the infrastructure of big data and the state-of-the-art privacy-preserving mechanisms in each stage of the big data life cycle. Furthermore, we discuss the challenges and future research directions related to privacy preservation in big data.",fullPaper,jv152
Computer Science,p1375,d3,e5d9809beb3ba4887585fccc6283521f43fb21d6,j4,IEEE Transactions on Knowledge and Data Engineering,In-Memory Big Data Management and Processing: A Survey,"Growing main memory capacity has fueled the development of in-memory big data management and processing. By eliminating disk I/O bottleneck, it is now possible to support interactive data analytics. However, in-memory systems are much more sensitive to other sources of overhead that do not matter in traditional I/O-bounded disk-based systems. Some issues such as fault-tolerance and consistency are also more challenging to handle in in-memory environment. We are witnessing a revolution in the design of database systems that exploits main memory as its data storage layer. Many of these researches have focused along several dimensions: modern CPU and memory hierarchy utilization, time/space efficiency, parallelism, and concurrency control. In this survey, we aim to provide a thorough review of a wide range of in-memory data management and processing proposals and systems, including both data storage systems and data processing frameworks. We also give a comprehensive presentation of important technology in memory management, and some key factors that need to be considered in order to achieve efficient in-memory data management and processing.",fullPaper,jv4
Computer Science,p1376,d3,1c35bab85900cd6c09eaddc1b1a9541bbc0bbcc3,j7,Journal of Big Data,A survey of open source tools for machine learning with big data in the Hadoop ecosystem,Abstract,fullPaper,jv7
Computer Science,p1377,d3,be2745d3a246ce9ae19972bc033aeb184fe0974a,j17,Big Data & Society,Big Data in food and agriculture,"Farming is undergoing a digital revolution. Our existing review of current Big Data applications in the agri-food sector has revealed several collection and analytics tools that may have implications for relationships of power between players in the food system (e.g. between farmers and large corporations). For example, Who retains ownership of the data generated by applications like Monsanto Corproation's Weed I.D. “app”? Are there privacy implications with the data gathered by John Deere's precision agricultural equipment? Systematically tracing the digital revolution in agriculture, and charting the affordances as well as the limitations of Big Data applied to food and agriculture, should be a broad research goal for Big Data scholarship. Such a goal brings data scholarship into conversation with food studies and it allows for a focus on the material consequences of big data in society.",fullPaper,jv17
Computer Science,p1382,d3,d094f0faff376af9a0ee79a742b350531b3c89bf,j12,Communications of the ACM,Exascale computing and big data,Scientific discovery and engineering innovation requires unifying traditionally separated high-performance computing and big data analytics.,fullPaper,jv12
Computer Science,p1383,d3,766c86e927fab2e44926e7ec0044f4cba1bdb905,c13,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,Bayes and big data: the consensus Monte Carlo algorithm,"A useful definition of ‘big data’ is data that is too big to process comfortably on a single machine, either because of processor, memory, or disk bottlenecks. Graphics processing units can alleviate the processor bottleneck, but memory or disk bottlenecks can only be eliminated by splitting data across multiple machines. Communication between large numbers of machines is expensive (regardless of the amount of data being communicated), so there is a need for algorithms that perform distributed approximate Bayesian analyses with minimal communication. Consensus Monte Carlo operates by running a separate Monte Carlo algorithm on each machine, and then averaging individual Monte Carlo draws across machines. Depending on the model, the resulting draws can be nearly indistinguishable from the draws that would have been obtained by running a single-machine algorithm for a very long time. Examples of consensus Monte Carlo are shown for simple models where single-machine solutions are available, for large single-layer hierarchical models, and for Bayesian additive regression trees (BART).",poster,cp13
Computer Science,p1385,d3,2614a63a84e83cc8a7f906c08f219d52b158f92f,c46,Ideal,Big Data in Healthcare,Abstract,poster,cp46
Business,p1385,d9,2614a63a84e83cc8a7f906c08f219d52b158f92f,c46,Ideal,Big Data in Healthcare,Abstract,poster,cp46
Computer Science,p1386,d3,b169080ee7ada80cd80bc79974f45b26ef9ed1a4,j69,IEEE Transactions on Services Computing,Accelerated PSO Swarm Search Feature Selection for Data Stream Mining Big Data,"Big Data though it is a hype up-springing many technical challenges that confront both academic research communities and commercial IT deployment, the root sources of Big Data are founded on data streams and the curse of dimensionality. It is generally known that data which are sourced from data streams accumulate continuously making traditional batch-based model induction algorithms infeasible for real-time data mining. Feature selection has been popularly used to lighten the processing load in inducing a data mining model. However, when it comes to mining over high dimensional data the search space from which an optimal feature subset is derived grows exponentially in size, leading to an intractable demand in computation. In order to tackle this problem which is mainly based on the high-dimensionality and streaming format of data feeds in Big Data, a novel lightweight feature selection is proposed. The feature selection is designed particularly for mining streaming data on the fly, by using accelerated particle swarm optimization (APSO) type of swarm search that achieves enhanced analytical accuracy within reasonable processing time. In this paper, a collection of Big Data with exceptionally large degree of dimensionality are put under test of our new feature selection algorithm for performance evaluation.",fullPaper,jv69
Computer Science,p1389,d3,299bab6b327e34c3e4f97cc8d0f9c64c9741fa99,j17,Big Data & Society,Where are human subjects in Big Data research? The emerging ethics divide,"There are growing discontinuities between the research practices of data science and established tools of research ethics regulation. Some of the core commitments of existing research ethics regulations, such as the distinction between research and practice, cannot be cleanly exported from biomedical research to data science research. Such discontinuities have led some data science practitioners and researchers to move toward rejecting ethics regulations outright. These shifts occur at the same time as a proposal for major revisions to the Common Rule—the primary regulation governing human-subjects research in the USA—is under consideration for the first time in decades. We contextualize these revisions in long-running complaints about regulation of social science research and argue data science should be understood as continuous with social sciences in this regard. The proposed regulations are more flexible and scalable to the methods of non-biomedical research, yet problematically largely exclude data science methods from human-subjects regulation, particularly uses of public datasets. The ethical frameworks for Big Data research are highly contested and in flux, and the potential harms of data science research are unpredictable. We examine several contentious cases of research harms in data science, including the 2014 Facebook emotional contagion study and the 2016 use of geographical data techniques to identify the pseudonymous artist Banksy. To address disputes about application of human-subjects research ethics in data science, critical data studies should offer a historically nuanced theory of “data subjectivity” responsive to the epistemic methods, harms and benefits of data science and commerce.",fullPaper,jv17
Sociology,p1389,d4,299bab6b327e34c3e4f97cc8d0f9c64c9741fa99,j17,Big Data & Society,Where are human subjects in Big Data research? The emerging ethics divide,"There are growing discontinuities between the research practices of data science and established tools of research ethics regulation. Some of the core commitments of existing research ethics regulations, such as the distinction between research and practice, cannot be cleanly exported from biomedical research to data science research. Such discontinuities have led some data science practitioners and researchers to move toward rejecting ethics regulations outright. These shifts occur at the same time as a proposal for major revisions to the Common Rule—the primary regulation governing human-subjects research in the USA—is under consideration for the first time in decades. We contextualize these revisions in long-running complaints about regulation of social science research and argue data science should be understood as continuous with social sciences in this regard. The proposed regulations are more flexible and scalable to the methods of non-biomedical research, yet problematically largely exclude data science methods from human-subjects regulation, particularly uses of public datasets. The ethical frameworks for Big Data research are highly contested and in flux, and the potential harms of data science research are unpredictable. We examine several contentious cases of research harms in data science, including the 2014 Facebook emotional contagion study and the 2016 use of geographical data techniques to identify the pseudonymous artist Banksy. To address disputes about application of human-subjects research ethics in data science, critical data studies should offer a historically nuanced theory of “data subjectivity” responsive to the epistemic methods, harms and benefits of data science and commerce.",fullPaper,jv17
Computer Science,p1390,d3,c5b66bca85209e83f7f5de333938874a3dd999f1,j152,IEEE Access,Mobile Cloud Computing Model and Big Data Analysis for Healthcare Applications,"Mobile devices are increasingly becoming an indispensable part of people's daily life, facilitating to perform a variety of useful tasks. Mobile cloud computing integrates mobile and cloud computing to expand their capabilities and benefits and overcomes their limitations, such as limited memory, CPU power, and battery life. Big data analytics technologies enable extracting value from data having four Vs: volume, variety, velocity, and veracity. This paper discusses networked healthcare and the role of mobile cloud computing and big data analytics in its enablement. The motivation and development of networked healthcare applications and systems is presented along with the adoption of cloud computing in healthcare. A cloudlet-based mobile cloud-computing infrastructure to be used for healthcare big data applications is described. The techniques, tools, and applications of big data analytics are reviewed. Conclusions are drawn concerning the design of networked healthcare systems using big data and mobile cloud-computing technologies. An outlook on networked healthcare is given.",fullPaper,jv152
Computer Science,p1391,d3,4b2c57cf9516a1d9eb98877627697c288227d9fb,j332,Quality and Reliability Engineering International,How Can SMEs Benefit from Big Data? Challenges and a Path Forward,"Big data is big news, and large companies in all sectors are making significant advances in their customer relations, product selection and development and consequent profitability through using this valuable commodity. Small and medium enterprises (SMEs) have proved themselves to be slow adopters of the new technology of big data analytics and are in danger of being left behind. In Europe, SMEs are a vital part of the economy, and the challenges they encounter need to be addressed as a matter of urgency. This paper identifies barriers to SME uptake of big data analytics and recognises their complex challenge to all stakeholders, including national and international policy makers, IT, business management and data science communities.",fullPaper,jv332
Computer Science,p1395,d3,1b919ece23b4d72468bf3ea483ea436097502974,j334,The Journal of Information Systems,Big Data Analytics: Opportunity or Threat for the Accounting Profession?,"ABSTRACT Contrary to Frey and Osborne's (2013) prediction that the accounting profession faces extinction, we argue that accountants can still create value in a world of Big Data analytics. To advance this position, we provide a conceptual framework based on structured/unstructured data and problem-driven/exploratory analysis. We argue that accountants already excel at problem-driven analysis of structured data, are well positioned to play a leading role in the problem-driven analysis of unstructured data, and can support data scientists performing exploratory analysis on Big Data. Our argument rests on two pillars: accountants are familiar with structured datasets, easing the transition to working with unstructured data, and possess knowledge of business fundamentals. Thus, rather than replacing accountants, we argue that Big Data analytics complements accountants' skills and knowledge. However, educators, standard setters, and professional bodies must adjust their curricula, standards, and frameworks to...",fullPaper,jv334
Sociology,p1395,d4,1b919ece23b4d72468bf3ea483ea436097502974,j334,The Journal of Information Systems,Big Data Analytics: Opportunity or Threat for the Accounting Profession?,"ABSTRACT Contrary to Frey and Osborne's (2013) prediction that the accounting profession faces extinction, we argue that accountants can still create value in a world of Big Data analytics. To advance this position, we provide a conceptual framework based on structured/unstructured data and problem-driven/exploratory analysis. We argue that accountants already excel at problem-driven analysis of structured data, are well positioned to play a leading role in the problem-driven analysis of unstructured data, and can support data scientists performing exploratory analysis on Big Data. Our argument rests on two pillars: accountants are familiar with structured datasets, easing the transition to working with unstructured data, and possess knowledge of business fundamentals. Thus, rather than replacing accountants, we argue that Big Data analytics complements accountants' skills and knowledge. However, educators, standard setters, and professional bodies must adjust their curricula, standards, and frameworks to...",fullPaper,jv334
Computer Science,p1396,d3,fa87557e2d9a9695a99ec481673fb360fad2a897,c58,Extreme Science and Engineering Discovery Environment,"Big data: a revolution that will transform how we live, work, and think","Howard, P. (2006). New media and the managed citizen. New York: Cambridge University Press. Lathrop, D., & Ruma, L. (Eds.). (2010). Open government: Collaboration, transparency, and participation in practice. Sebastopol, CA: O’Reilly. Noveck, B. S. (2008). Wiki-Government: How open-source technology can make government decisionmaking more expert and more democratic. Washington, DC: Brookings Institution Press.",poster,cp58
Computer Science,p1397,d3,adf0daa3c4f5e21de8e98ce86dbc6b3d7d299a2d,c54,ACM SIGCOMM Symposium on Software Defined Networking Research,Towards cloud based big data analytics for smart future cities,Abstract,poster,cp54
Computer Science,p1399,d3,a5ba556a23c57d1e8b57a5ee0e3e01736e06e646,c3,Knowledge Discovery and Data Mining,Crime Rate Inference with Big Data,"Crime is one of the most important social problems in the country, affecting public safety, children development, and adult socioeconomic status. Understanding what factors cause higher crime is critical for policy makers in their efforts to reduce crime and increase citizens' life quality. We tackle a fundamental problem in our paper: crime rate inference at the neighborhood level. Traditional approaches have used demographics and geographical influences to estimate crime rates in a region. With the fast development of positioning technology and prevalence of mobile devices, a large amount of modern urban data have been collected and such big data can provide new perspectives for understanding crime. In this paper, we used large-scale Point-Of-Interest data and taxi flow data in the city of Chicago, IL in the USA. We observed significantly improved performance in crime rate inference compared to using traditional features. Such an improvement is consistent over multiple years. We also show that these new features are significant in the feature importance analysis.",fullPaper,cp3
Computer Science,p1401,d3,654723da3e334731c6d31bdbc1135616eb192cc5,j335,Information Systems Frontiers,Big data in the public sector: Uncertainties and readiness,Abstract,fullPaper,jv335
Business,p1401,d9,654723da3e334731c6d31bdbc1135616eb192cc5,j335,Information Systems Frontiers,Big data in the public sector: Uncertainties and readiness,Abstract,fullPaper,jv335
Computer Science,p1402,d3,a5ba556a23c57d1e8b57a5ee0e3e01736e06e646,c3,Knowledge Discovery and Data Mining,Crime Rate Inference with Big Data,"Crime is one of the most important social problems in the country, affecting public safety, children development, and adult socioeconomic status. Understanding what factors cause higher crime is critical for policy makers in their efforts to reduce crime and increase citizens' life quality. We tackle a fundamental problem in our paper: crime rate inference at the neighborhood level. Traditional approaches have used demographics and geographical influences to estimate crime rates in a region. With the fast development of positioning technology and prevalence of mobile devices, a large amount of modern urban data have been collected and such big data can provide new perspectives for understanding crime. In this paper, we used large-scale Point-Of-Interest data and taxi flow data in the city of Chicago, IL in the USA. We observed significantly improved performance in crime rate inference compared to using traditional features. Such an improvement is consistent over multiple years. We also show that these new features are significant in the feature importance analysis.",fullPaper,cp3
Computer Science,p1404,d3,b91334db900b1c066ed5c811c9e5eea57a7e7b08,j294,Neurocomputing,Efficient kNN classification algorithm for big data,Abstract,fullPaper,jv294
Computer Science,p1405,d3,65d61afd9c35b0a75d9de77c2a4a2428af0f7f7b,j181,IEEE Signal Processing Magazine,Big Data Analysis with Signal Processing on Graphs: Representation and processing of massive data sets with irregular structure,"Analysis and processing of very large data sets, or big data, poses a significant challenge. Massive data sets are collected and studied in numerous domains, from engineering sciences to social networks, biomolecular research, commerce, and security. Extracting valuable information from big data requires innovative approaches that efficiently process large amounts of data as well as handle and, moreover, utilize their structure. This article discusses a paradigm for large-scale data analysis based on the discrete signal processing (DSP) on graphs (DSPG). DSPG extends signal processing concepts and methodologies from the classical signal processing theory to data indexed by general graphs. Big data analysis presents several challenges to DSPG, in particular, in filtering and frequency analysis of very large data sets. We review fundamental concepts of DSPG, including graph signals and graph filters, graph Fourier transform, graph frequency, and spectrum ordering, and compare them with their counterparts from the classical signal processing theory. We then consider product graphs as a graph model that helps extend the application of DSPG methods to large data sets through efficient implementation based on parallelization and vectorization. We relate the presented framework to existing methods for large-scale data processing and illustrate it with an application to data compression.",fullPaper,jv181
Computer Science,p1406,d3,910d0314facfc9d5874a12957d6c3cea97dc159f,j134,Big Data Research,Big Data Systems Meet Machine Learning Challenges: Towards Big Data Science as a Service,Abstract,fullPaper,jv134
Computer Science,p1407,d3,1c2122e6e140301f5d9e56f8bae476105bc01fcb,j190,IEEE Network,Highly efficient data migration and backup for big data applications in elastic optical inter-data-center networks,"This article discusses the technologies for realizing highly efficient data migration and backup for big data applications in elastic optical inter-data-center (inter-DC) networks. We first describe the impacts of big data applications on underlying network infrastructure and introduce the concept of flexible-grid elastic optical inter-DC networks. Then we model the data migration in such networks as dynamic anycast and propose several efficient algorithms. Joint resource defragmentation is also discussed to further improve network performance. For efficient data backup, we leverage a mutual backup model and investigate how to avoid the prolonged negative impacts on DCs' normal operation by minimizing the DC backup window.",fullPaper,jv190
Computer Science,p1411,d3,12f26e0c38cdf9b8df735a6359384529e533bddc,c97,International Conference on Computational Logic,Creating Value with Big Data Analytics: Making Smarter Marketing Decisions,"Our newly digital world is generating an almost unimaginable amount of data about all of us. Such a vast amount of data is useless without plans and strategies that are designed to cope with its size and complexity, and which enable organisations to leverage the information to create value. This book is a refreshingly practical, yet theoretically sound roadmap to leveraging big data and analytics. Creating Value with Big Data Analytics provides a nuanced view of big data development, arguing that big data in itself is not a revolution but an evolution of the increasing availability of data that has been observed in recent times. Building on the authors extensive academic and practical knowledge, this book aims to provide managers and analysts withstrategic directions and practical analytical solutions on how to create value from existing and new big data. By tying data and analytics to specific goals and processes for implementation, this is a much-needed book that will be essential reading for students and specialists of data analytics, marketing research, and customer relationship management.",poster,cp97
Computer Science,p1414,d3,1b5cc68e0498629bf3f57ec07abb3da69a4ce3a1,j312,Annals of Operations Research,Environmental performance evaluation with big data: theories and methods,Abstract,fullPaper,jv312
Computer Science,p1415,d3,e3642ab645a3fdcce97f854503ba67f4b503b9ea,c91,International Symposium on High-Performance Computer Architecture,BigDataBench: A big data benchmark suite from internet services,"As architecture, systems, and data management communities pay greater attention to innovative big data systems and architecture, the pressure of benchmarking and evaluating these systems rises. However, the complexity, diversity, frequently changed workloads, and rapid evolution of big data systems raise great challenges in big data benchmarking. Considering the broad use of big data systems, for the sake of fairness, big data benchmarks must include diversity of data and workloads, which is the prerequisite for evaluating big data systems and architecture. Most of the state-of-the-art big data benchmarking efforts target evaluating specific types of applications or system software stacks, and hence they are not qualified for serving the purposes mentioned above. This paper presents our joint research efforts on this issue with several industrial partners. Our big data benchmark suite-BigDataBench not only covers broad application scenarios, but also includes diverse and representative data sets. Currently, we choose 19 big data benchmarks from dimensions of application scenarios, operations/ algorithms, data types, data sources, software stacks, and application types, and they are comprehensive for fairly measuring and evaluating big data systems and architecture. BigDataBench is publicly available from the project home page http://prof.ict.ac.cn/BigDataBench. Also, we comprehensively characterize 19 big data workloads included in BigDataBench with varying data inputs. On a typical state-of-practice processor, Intel Xeon E5645, we have the following observations: First, in comparison with the traditional benchmarks: including PARSEC, HPCC, and SPECCPU, big data applications have very low operation intensity, which measures the ratio of the total number of instructions divided by the total byte number of memory accesses; Second, the volume of data input has non-negligible impact on micro-architecture characteristics, which may impose challenges for simulation-based big data architecture research; Last but not least, corroborating the observations in CloudSuite and DCBench (which use smaller data inputs), we find that the numbers of L1 instruction cache (L1I) misses per 1000 instructions (in short, MPKI) of the big data applications are higher than in the traditional benchmarks; also, we find that L3 caches are effective for the big data applications, corroborating the observation in DCBench.",fullPaper,cp91
Computer Science,p1416,d3,1a1f3d8045c1efbaefecc30c89035705ec10ba73,c51,International Conference on Engineering Education,Big data for development: applications and techniques,Abstract,poster,cp51
Computer Science,p1417,d3,ff42d3ea6285282149b8aaa86665b0d70840112b,c69,Neural Information Processing Systems,"The Big Data Value Chain: Definitions, Concepts, and Theoretical Approaches",Abstract,poster,cp69
Computer Science,p1420,d3,9e7be12082f58cbf7ebdb84a8cbdc897a4e41683,j339,Foundations of Science,The Deluge of Spurious Correlations in Big Data,Abstract,fullPaper,jv339
Computer Science,p1421,d3,41a44da00b2e3de7a501ebe0782514b6b4afe5f9,j260,IEEE Transactions on Big Data,Deduplication on Encrypted Big Data in Cloud,"Cloud computing offers a new way of service provision by re-arranging various resources over the Internet. The most important and popular cloud service is data storage. In order to preserve the privacy of data holders, data are often stored in cloud in an encrypted form. However, encrypted data introduce new challenges for cloud data deduplication, which becomes crucial for big data storage and processing in cloud. Traditional deduplication schemes cannot work on encrypted data. Existing solutions of encrypted data deduplication suffer from security weakness. They cannot flexibly support data access control and revocation. Therefore, few of them can be readily deployed in practice. In this paper, we propose a scheme to deduplicate encrypted data stored in cloud based on ownership challenge and proxy re-encryption. It integrates cloud data deduplication with access control. We evaluate its performance based on extensive analysis and computer simulations. The results show the superior efficiency and effectiveness of the scheme for potential practical deployment, especially for big data deduplication in cloud storage.",fullPaper,jv260
Computer Science,p1422,d3,7ee8e3a67cf299276611cea6d8aa3e657177bd19,c69,Neural Information Processing Systems,Big Data and consumer behavior: imminent opportunities,"Purpose – The purpose of this paper is to assess how the study of consumer behavior can benefit from the presence of Big Data. Design/methodology/approach – This paper offers a conceptual overview of potential opportunities and changes to the study of consumer behavior that Big Data will likely bring. Findings – Big Data have the potential to further our understanding of each stage in the consumer decision-making process. While the field has traditionally moved forward using a priori theory followed by experimentation, it now seems that the nature of the feedback loop between theory and results may shift under the weight of Big Data. Research limitations/implications – A new data culture is now represented in marketing practice. The new group advocates inductive data mining and A/B testing rather than human intuition harnessed for deduction. The group brings with it interest in numerous secondary data sources. However, Big Data may be limited by poor quality, unrepresentativeness and volatility, among oth...",poster,cp69
Computer Science,p1423,d3,c5dae4440044b015fd4ae8fd59aba43d7515c889,j16,International Journal of Digital Earth,Big Data Analytics for Earth Sciences: the EarthServer approach,"Big Data Analytics is an emerging field since massive storage and computing capabilities have been made available by advanced e-infrastructures. Earth and Environmental sciences are likely to benefit from Big Data Analytics techniques supporting the processing of the large number of Earth Observation datasets currently acquired and generated through observations and simulations. However, Earth Science data and applications present specificities in terms of relevance of the geospatial information, wide heterogeneity of data models and formats, and complexity of processing. Therefore, Big Earth Data Analytics requires specifically tailored techniques and tools. The EarthServer Big Earth Data Analytics engine offers a solution for coverage-type datasets, built around a high performance array database technology, and the adoption and enhancement of standards for service interaction (OGC WCS and WCPS). The EarthServer solution, led by the collection of requirements from scientific communities and international initiatives, provides a holistic approach that ranges from query languages and scalability up to mobile access and visualization. The result is demonstrated and validated through the development of lighthouse applications in the Marine, Geology, Atmospheric, Planetary and Cryospheric science domains.",fullPaper,jv16
Computer Science,p1424,d3,4dfddbf5b6b0815414313039d29f41900f47d003,c24,International Conference on Data Technologies and Applications,"The Data Revolution. Big Data, Open Data, Data Infrastructures and Their Consequences","The last few years have witnessed an increasing production of data that have become open, accessible and available at low cost. Although many disciplines are already using ‘big data’ as instrument of analysis, social sciences have apparently missed the opportunity to exploit their potentialities fully. The purpose of this excellent book is to prove how these data do not exist independently from the ideas, techniques, technologies, people and context that produce, process, manage, analyze and store them. Moreover, the author explores the definition, characteristics and the technique to manage big data, but he also focuses his attention on the challenges of this way of thinking and on how big data are changing existing epistemology and science. Before the big data revolution, the scientific approach was based on computational science based on the simulation of complex phenomena. In the age of big data, however, an exploratory approach based on data-intensive, statistical exploration and data mining was used. In the age of big data, however, an exploratory approach based on data-intensive, statistical exploration and data mining is used. The author focuses his attention on how big data are changing the approaches and methodologies in four different fields, that is, governing people, managing organizations, leveraging value and producing capitals, creating better places in which to live. The aim of the book is threefold: to provide a detailed reflection on the nature of the data and their wider assemblages; to chart how these assemblages are shifting and mutating all along the development of new data infrastructures; and to reflect on the consequences that these new ways to assemble data may entail the making of sense and on the effects they produce in the world. The 11 chapters ideally can be divided into two main sections. The first section (chapters 1–6) deals with the big data characteristics and the techniques to manage them. The last section (chapters 7–11) consider how big data are changing the epistemology of science across all domains (arts and humanities, social and life sciences, engineering). The interest of these last chapters lies in the core idea of data being not self-meaningful, as their meaningfulness is proportionate to the information they can provide. This is particularly interesting as it fosters dense insights and ideas on further development in research. The book starts with the definition of big data and enhances the concept by which data do not exist independently from the ideas, instruments, practices, context and knowledge used to generate, process, analyze and draw conclusions from them. The book continues with an analysis of the data characteristics. Data vary by forms (qualitative and quantitative), structure (structured, semi-structured and unstructured), source (captured, derived, exhaust, transient), producer (primary, secondary, tertiary), and type (indexical, attribute, metadata). However, these different types of data share the same characteristic as they all form the basis of the knowledge pyramid where data precede information which, in turn, precedes knowledge. The latter precedes understanding and wisdom. In order to make sense of data, they are usually pooled into datasets and databases designed and organized to enable specific analysis. How they are structured has consequences on the queries and obtainable results. The author underlines the importance of the data assembly process as an issue that needs further attention and research. Chapter 4 explores big data characteristics, that is, volume, velocity, variety, exhaustivity, resolution/indexicality, relationality and flexibility/scalability. The author then examines the interest that the access to large data with those specific characteristics may have for society, governments and business organizations. Chapter 5 concerns the sources of big data. The starting point is that the production of big data has been facilitated by the confluence of five technological innovations: growing computational power, internet, pervasive and ubiquitous computing, indexical and machine readable identification, and massive distributed storage. The data production can be divided into three categories, that is, directed data (generated by traditional forms of surveillance), automated data (generated by automatic function of the device or systems) and volunteered data (traded or gifted by people to a system). Once again, with his critical approach, the author underlines the importance of developing empirical studies to examine in depth the various ways in which big data are being generated, Regional Studies, 2016",poster,cp24
Computer Science,p1425,d3,fa445325cf2edd5a03f45d15d67d24e4af5d0ab8,j72,IEEE Intelligent Systems,Challenges of Feature Selection for Big Data Analytics,"We're surrounded by huge amounts of large-scale high-dimensional data, but learning tasks require reduced data dimensionality. Feature selection has shown its effectiveness in many applications by building simpler and more comprehensive models, improving learning performance, and preparing clean, understandable data. Some unique characteristics of big data such as data velocity and data variety have presented challenges to the feature selection problem. In this article, the authors envision these challenges for big data analytics. To facilitate and promote feature selection research, they present an open source feature selection repository (scikit-feature) of popular algorithms.",fullPaper,jv72
Computer Science,p1427,d3,ce2bbc757a4009d83d4a1719925fc64caf890e23,c121,International Conference on Interaction Sciences,Learning Spark: Lightning-Fast Big Data Analytics,"The Web is getting faster, and the data it delivers is getting bigger. How can you handle everything efficiently? This book introduces Spark, an open source cluster computing system that makes data analytics fast to run and fast to write. Youll learn how to run programs faster, using primitives for in-memory cluster computing. With Spark, your job can load data into memory and query it repeatedly much quicker than with disk-based systems like Hadoop MapReduce. Written by the developers of Spark, this book will have you up and running in no time. Youll learn how to express MapReduce jobs with just a few simple lines of Spark code, instead of spending extra time and effort working with Hadoops raw Java API. Quickly dive into Spark capabilities such as collect, count, reduce, and save Use one programming paradigm instead of mixing and matching tools such as Hive, Hadoop, Mahout, and S4/Storm Learn how to run interactive, iterative, and incremental analyses Integrate with Scala to manipulate distributed datasets like local collections Tackle partitioning issues, data locality, default hash partitioning, user-defined partitioners, and custom serialization Use other languages by means of pipe() to achieve the equivalent of Hadoop streaming",poster,cp121
Computer Science,p1428,d3,0d8426ba72cb872062b1a87140f8cb2c2324ba65,j152,IEEE Access,Big Data Analytics in Mobile Cellular Networks,"Mobile cellular networks have become both the generators and carriers of massive data. Big data analytics can improve the performance of mobile cellular networks and maximize the revenue of operators. In this paper, we introduce a unified data model based on the random matrix theory and machine learning. Then, we present an architectural framework for applying the big data analytics in the mobile cellular networks. Moreover, we describe several illustrative examples, including big signaling data, big traffic data, big location data, big radio waveforms data, and big heterogeneous data, in mobile cellular networks. Finally, we discuss a number of open research challenges of the big data analytics in the mobile cellular networks.",fullPaper,jv152
Computer Science,p1429,d3,4271319294949d4ee6c88b653429af9ab7f58a9b,j244,The International Journal of Advanced Manufacturing Technology,The impact of big data on world-class sustainable manufacturing,Abstract,fullPaper,jv244
Computer Science,p1430,d3,92066c26f5c618b58a854b7bd3185b3addc00021,j50,ACM Computing Surveys,Computational Health Informatics in the Big Data Age,"The explosive growth and widespread accessibility of digital health data have led to a surge of research activity in the healthcare and data sciences fields. The conventional approaches for health data management have achieved limited success as they are incapable of handling the huge amount of complex data with high volume, high velocity, and high variety. This article presents a comprehensive overview of the existing challenges, techniques, and future directions for computational health informatics in the big data age, with a structured analysis of the historical and state-of-the-art methods. We have summarized the challenges into four Vs (i.e., volume, velocity, variety, and veracity) and proposed a systematic data-processing pipeline for generic big data in health informatics, covering data capturing, storing, sharing, analyzing, searching, and decision support. Specifically, numerous techniques and algorithms in machine learning are categorized and compared. On the basis of this material, we identify and discuss the essential prospects lying ahead for computational health informatics in this big data age.",fullPaper,jv50
Computer Science,p1432,d3,31485e1213dd886fa2b668eefcd9b13533d8a9fe,c30,PS,Big data and data science: what should we teach?,"The era of big data has arrived. Big data bring us the data‐driven paradigm and enlighten us to challenge new classes of problems we were not able to solve in the past. We are beginning to see the impacts of big data in every aspect of our lives and society. We need a science that can address these big data problems. Data science is a new emerging discipline that was termed to address challenges that we are facing and going to face in the big data era. Thus, education in data science is the key to success, and we need concrete strategies and approaches to better educate future data scientists. In this paper, we discuss general concepts on big data, data science, and data scientists and show the results of an extensive survey on current data science education in United States. Finally, we propose various approaches that data science education should aim to accomplish.",poster,cp30
Computer Science,p1433,d3,6d379fe4781c77b949fe02c8de78386e93b5e3de,j17,Big Data & Society,Questioning Big Data: Crowdsourcing crisis data towards an inclusive humanitarian response,"The aim of this paper is to critically explore whether crowdsourced Big Data enables an inclusive humanitarian response at times of crisis. We argue that all data, including Big Data, are socially constructed artefacts that reflect the contexts and processes of their creation. To support our argument, we qualitatively analysed the process of ‘Big Data making’ that occurred by way of crowdsourcing through open data platforms, in the context of two specific humanitarian crises, namely the 2010 earthquake in Haiti and the 2015 earthquake in Nepal. We show that the process of creating Big Data from local and global sources of knowledge entails the transformation of information as it moves from one distinct group of contributors to the next. The implication of this transformation is that locally based, affected people and often the original ‘crowd’ are excluded from the information flow, and from the interpretation process of crowdsourced crisis knowledge, as used by formal responding organizations, and are marginalized in their ability to benefit from Big Data in support of their own means. Our paper contributes a critical perspective to the debate on participatory Big Data, by explaining the process of in and exclusion during data making, towards more responsive humanitarian relief.",fullPaper,jv17
Sociology,p1433,d4,6d379fe4781c77b949fe02c8de78386e93b5e3de,j17,Big Data & Society,Questioning Big Data: Crowdsourcing crisis data towards an inclusive humanitarian response,"The aim of this paper is to critically explore whether crowdsourced Big Data enables an inclusive humanitarian response at times of crisis. We argue that all data, including Big Data, are socially constructed artefacts that reflect the contexts and processes of their creation. To support our argument, we qualitatively analysed the process of ‘Big Data making’ that occurred by way of crowdsourcing through open data platforms, in the context of two specific humanitarian crises, namely the 2010 earthquake in Haiti and the 2015 earthquake in Nepal. We show that the process of creating Big Data from local and global sources of knowledge entails the transformation of information as it moves from one distinct group of contributors to the next. The implication of this transformation is that locally based, affected people and often the original ‘crowd’ are excluded from the information flow, and from the interpretation process of crowdsourced crisis knowledge, as used by formal responding organizations, and are marginalized in their ability to benefit from Big Data in support of their own means. Our paper contributes a critical perspective to the debate on participatory Big Data, by explaining the process of in and exclusion during data making, towards more responsive humanitarian relief.",fullPaper,jv17
Computer Science,p1434,d3,c10b6ba92e63d9056a81c7bff5f499536e9ea73f,c92,International Symposium on Computer Architecture,Machine Learning Models and Algorithms for Big Data Classification,Abstract,poster,cp92
Computer Science,p1435,d3,f96efbad191c388706ce370dd9be5d2c25a7b3dc,c92,International Symposium on Computer Architecture,Biscuit: A Framework for Near-Data Processing of Big Data Workloads,"Data-intensive queries are common in business intelligence, data warehousing and analytics applications. Typically, processing a query involves full inspection of large in-storage data sets by CPUs. An intuitive way to speed up such queries is to reduce the volume of data transferred over the storage network to a host system. This can be achieved by filtering out extraneous data within the storage, motivating a form of near-data processing. This work presents Biscuit, a novel near-data processing framework designed for modern solid-state drives. It allows programmers to write a data-intensive application to run on the host system and the storage system in a distributed, yet seamless manner. In order to offer a high-level programming model, Biscuit builds on the concept of data flow. Data processing tasks communicate through typed and data-ordered ports. Biscuit does not distinguish tasks that run on the host system and the storage system. As the result, Biscuit has desirable traits like generality and expressiveness, while promoting code reuse and naturally exposing concurrency. We implement Biscuit on a host system that runs the Linux OS and a high-performance solid-state drive. We demonstrate the effectiveness of our approach and implementation with experimental results. When data filtering is done by hardware in the solid-state drive, the average speed-up obtained for the top five queries of TPC-H is over 15x.",fullPaper,cp92
Computer Science,p1437,d3,d07e640a48da232cd3a49c039273f7e91a0a9b20,c107,Annual Haifa Experimental Systems Conference,"Big data, bigger dilemmas: A critical review","The recent interest in Big Data has generated a broad range of new academic, corporate, and policy practices along with an evolving debate among its proponents, detractors, and skeptics. While the practices draw on a common set of tools, techniques, and technologies, most contributions to the debate come either from a particular disciplinary perspective or with a focus on a domain‐specific issue. A close examination of these contributions reveals a set of common problematics that arise in various guises and in different places. It also demonstrates the need for a critical synthesis of the conceptual and practical dilemmas surrounding Big Data. The purpose of this article is to provide such a synthesis by drawing on relevant writings in the sciences, humanities, policy, and trade literature. In bringing these diverse literatures together, we aim to shed light on the common underlying issues that concern and affect all of these areas. By contextualizing the phenomenon of Big Data within larger socioeconomic developments, we also seek to provide a broader understanding of its drivers, barriers, and challenges. This approach allows us to identify attributes of Big Data that require more attention—autonomy, opacity, generativity, disparity, and futurity—leading to questions and ideas for moving beyond dilemmas.",poster,cp107
Sociology,p1437,d4,d07e640a48da232cd3a49c039273f7e91a0a9b20,c107,Annual Haifa Experimental Systems Conference,"Big data, bigger dilemmas: A critical review","The recent interest in Big Data has generated a broad range of new academic, corporate, and policy practices along with an evolving debate among its proponents, detractors, and skeptics. While the practices draw on a common set of tools, techniques, and technologies, most contributions to the debate come either from a particular disciplinary perspective or with a focus on a domain‐specific issue. A close examination of these contributions reveals a set of common problematics that arise in various guises and in different places. It also demonstrates the need for a critical synthesis of the conceptual and practical dilemmas surrounding Big Data. The purpose of this article is to provide such a synthesis by drawing on relevant writings in the sciences, humanities, policy, and trade literature. In bringing these diverse literatures together, we aim to shed light on the common underlying issues that concern and affect all of these areas. By contextualizing the phenomenon of Big Data within larger socioeconomic developments, we also seek to provide a broader understanding of its drivers, barriers, and challenges. This approach allows us to identify attributes of Big Data that require more attention—autonomy, opacity, generativity, disparity, and futurity—leading to questions and ideas for moving beyond dilemmas.",poster,cp107
Computer Science,p1440,d3,774170fb9eef527c2f32c865fe0516d5c000c440,j152,IEEE Access,Energy big data: A survey,"As a significant application of energy, smart grid is a complicated interconnected power grid that involves sensors, deployment strategies, smart meters, and real-time data processing. It continuously generates data with large volume, high velocity, and diverse variety. In this paper, we first give a brief introduction on big data, smart grid, and big data application in the smart grid scenario. Then, recent studies and developments are summarized in the context of integrated architecture and key enabling technologies. Meanwhile, security issues are specifically addressed. Finally, we introduce several typical big data applications and point out future challenges in the energy domain.",fullPaper,jv152
Computer Science,p1441,d3,1992422fbb960bd3f1adcc0f4df77d78df5984b0,j341,Data Science and Engineering,Medical Big Data: Neurological Diseases Diagnosis Through Medical Data Analysis,Abstract,fullPaper,jv341
Computer Science,p1442,d3,aeede2d75d7cb3e10bc3b732a897ca1a7bfc12c5,c48,"Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing","Data Science and Big Data Analytics: Discovering, Analyzing, Visualizing and Presenting Data","Data Science and Big Data Analytics: Discovering, Analyzing, Visualizing and Presenting Data By EMC Education Services Data Science and Big Data Analytics is about harnessing the power of data for new insights. The book covers the breadth of activities and methods and tools that Data Scientists use. The content focuses on concepts, principles and practical applications that are applicable to any industry and technology environment, and the learning is supported and explained with examples that you can replicate using open-source software. This book will help you: Become a contributor on a data science team ●",poster,cp48
Computer Science,p1443,d3,6e42797e4ab69d56a0d6071cad7f18dd686431f4,j341,Data Science and Engineering,Big Data Reduction Methods: A Survey,Abstract,fullPaper,jv341
Computer Science,p1444,d3,aec372fd2af3de28e896165e04ec86a26d0ae61d,j342,IEEE Transactions on Smart Grid,A Big Data Architecture Design for Smart Grids Based on Random Matrix Theory,"Model-based analysis tools, built on assumptions and simplifications, are difficult to handle smart grids with data characterized by volume, velocity, variety, and veracity (i.e., 4Vs data). This paper, using random matrix theory (RMT), motivates data-driven tools to perceive the complex grids in high-dimension; meanwhile, an architecture with detailed procedures is proposed. In algorithm perspective, the architecture performs a high-dimensional analysis and compares the findings with RMT predictions to conduct anomaly detections. Mean spectral radius (MSR), as a statistical indicator, is defined to reflect the correlations of system data in different dimensions. In management mode perspective, a group-work mode is discussed for smart grids operation. This mode breaks through regional limitations for energy flows and data flows, and makes advanced big data analyses possible. For a specific large-scale zone-dividing system with multiple connected utilities, each site, operating under the group-work mode, is able to work out the regional MSR only with its own measured/simulated data. The large-scale interconnected system, in this way, is naturally decoupled from statistical parameters perspective, rather than from engineering models perspective. Furthermore, a comparative analysis of these distributed MSRs, even with imperceptible different raw data, will produce a contour line to detect the event and locate the source. It demonstrates that the architecture is compatible with the block calculation only using the regional small database; beyond that, this architecture, as a data-driven solution, is sensitive to system situation awareness, and practical for real large-scale interconnected systems. Five case studies and their visualizations validate the designed architecture in various fields of power systems. To our best knowledge, this paper is the first attempt to apply big data technology into smart grids.",fullPaper,jv342
Mathematics,p1444,d6,aec372fd2af3de28e896165e04ec86a26d0ae61d,j342,IEEE Transactions on Smart Grid,A Big Data Architecture Design for Smart Grids Based on Random Matrix Theory,"Model-based analysis tools, built on assumptions and simplifications, are difficult to handle smart grids with data characterized by volume, velocity, variety, and veracity (i.e., 4Vs data). This paper, using random matrix theory (RMT), motivates data-driven tools to perceive the complex grids in high-dimension; meanwhile, an architecture with detailed procedures is proposed. In algorithm perspective, the architecture performs a high-dimensional analysis and compares the findings with RMT predictions to conduct anomaly detections. Mean spectral radius (MSR), as a statistical indicator, is defined to reflect the correlations of system data in different dimensions. In management mode perspective, a group-work mode is discussed for smart grids operation. This mode breaks through regional limitations for energy flows and data flows, and makes advanced big data analyses possible. For a specific large-scale zone-dividing system with multiple connected utilities, each site, operating under the group-work mode, is able to work out the regional MSR only with its own measured/simulated data. The large-scale interconnected system, in this way, is naturally decoupled from statistical parameters perspective, rather than from engineering models perspective. Furthermore, a comparative analysis of these distributed MSRs, even with imperceptible different raw data, will produce a contour line to detect the event and locate the source. It demonstrates that the architecture is compatible with the block calculation only using the regional small database; beyond that, this architecture, as a data-driven solution, is sensitive to system situation awareness, and practical for real large-scale interconnected systems. Five case studies and their visualizations validate the designed architecture in various fields of power systems. To our best knowledge, this paper is the first attempt to apply big data technology into smart grids.",fullPaper,jv342
Computer Science,p1445,d3,c3aa26e6b39625ccc95b67df7cbe94cf32a0d1de,j210,IEEE Communications Magazine,Wireless communications in the era of big data,"The rapidly growing wave of wireless data service is pushing against the boundary of our communication network's processing power. The pervasive and exponentially increasing data traffic present imminent challenges to all aspects of wireless system design, such as spectrum efficiency, computing capabilities, and fronthaul/backhaul link capacity. In this article, we discuss the challenges and opportunities in the design of scalable wireless systems to embrace the big data era. On one hand, we review the state-of-the-art networking architectures and signal processing techniques adaptable for managing big data traffic in wireless networks. On the other hand, instead of viewing mobile big data as an unwanted burden, we introduce methods to capitalize on the vast data traffic, for building a big-data-aware wireless network with better wireless service quality and new mobile applications. We highlight several promising future research directions for wireless communications in the mobile big data era.",fullPaper,jv210
Computer Science,p1447,d3,837daca8463dc161bb23da089a0dbb0093dc5565,c27,International Conference Geographic Information Science,Big Data Analytics in Financial Statement Audits,"SYNOPSIS: Big Data analytics is the process of inspecting, cleaning, transforming, and modeling Big Data to discover and communicate useful information and patterns, suggest conclusions, and support decision making. Big Data has been used for advanced analytics in many domains but hardly, if at all, by auditors. This article hypothesizes that Big Data analytics can improve the efficiency and effectiveness of financial statement audits. We explain how Big Data analytics applied in other domains might be applied in auditing. We also discuss the characteristics of Big Data analytics, which set it apart from traditional auditing, and its implications for practical implementation.",poster,cp27
Computer Science,p1449,d3,dbfea34a4d2239f7595841f1162e0f37ed8bac97,j7,Journal of Big Data,Visualizing Big Data with augmented and virtual reality: challenges and research agenda,Abstract,fullPaper,jv7
Computer Science,p1450,d3,aa740792d3d1afaa8acbdb2700582c2226986cd2,c65,International Symposium on Empirical Software Engineering and Measurement,Data discretization: taxonomy and big data challenge,"Discretization of numerical data is one of the most influential data preprocessing tasks in knowledge discovery and data mining. The purpose of attribute discretization is to find concise data representations as categories which are adequate for the learning task retaining as much information in the original continuous attribute as possible. In this article, we present an updated overview of discretization techniques in conjunction with a complete taxonomy of the leading discretizers. Despite the great impact of discretization as data preprocessing technique, few elementary approaches have been developed in the literature for Big Data. The purpose of this article is twofold: a comprehensive taxonomy of discretization techniques to help the practitioners in the use of the algorithms is presented; the article aims is to demonstrate that standard discretization methods can be parallelized in Big Data platforms such as Apache Spark, boosting both performance and accuracy. We thus propose a distributed implementation of one of the most well‐known discretizers based on Information Theory, obtaining better results than the one produced by: the entropy minimization discretizer proposed by Fayyad and Irani. Our scheme goes beyond a simple parallelization and it is intended to be the first to face the Big Data challenge. WIREs Data Mining Knowl Discov 2016, 6:5–21. doi: 10.1002/widm.1173",poster,cp65
Computer Science,p1451,d3,ae5d6372cc21a2938b7602fb0f903eedd6d27eb4,c76,Group,Big Data and Cycling,"Abstract Big Data has begun to create significant impacts in urban and transport planning. This paper covers the explosion in data-driven research on cycling, most of which has occurred in the last ten years. We review the techniques, objectives and findings of a growing number of studies we have classified into three groups according to the nature of the data they are based on: GPS data (spatio-temporal data collected using the global positioning system (GPS)), live point data and journey data. We discuss the movement from small-scale GPS studies to the ‘Big GPS’ data sets held by fitness and leisure apps or specific cycling initiatives, the impact of Bike Share Programmes (BSP) on the availability of timely point data and the potential of historical journey data for trend analysis and pattern recognition. We conclude by pointing towards the possible new insights through combining these data sets with each other – and with more conventional health, socio-demographic or transport data.",poster,cp76
Computer Science,p1452,d3,8cb6c2711afd3e504400ee12d3b582cc06348b08,c62,International Conference on Advanced Data and Information Engineering,Digital Data Streams: Creating Value from the Real-Time Flow of Big Data,"There is no escaping the Big Data hype. Vendors are peddling Big Data solutions; consulting firms employ Big Data specialists; Big Data conferences are aplenty. There is a rush to extract golden nuggets (of insight) from mountains (of data). By focusing merely on the mountain (of Big Data), these adventurers are overlooking the source of the revolution—namely, the many digital data streams (DDSs) that create Big Data—and the opportunity to improve real-time decision making. This article discusses the characteristics of DDSs, describes their common structure, and offers guidelines to enable firms to profit from their untapped potential.",poster,cp62
Computer Science,p1456,d3,f64c7a9a3be492f749dfe7ac3c2c8111ed5d0139,j190,IEEE Network,Big data in mobile social networks: a QoE-oriented framework,"Due to the rapid development of mobile social networks, mobile big data play an important role in providing mobile social users with various mobile services. However, as mobile big data have inherent properties, current MSNs face a challenge to provide mobile social user with a satisfactory quality of experience. Therefore, in this article, we propose a novel framework to deliver mobile big data over content- centric mobile social networks. At first, the characteristics and challenges of mobile big data are studied. Then the content-centric network architecture to deliver mobile big data in MSNs is presented, where each datum consists of interest packets and data packets, respectively. Next, how to select the agent node to forward interest packets and the relay node to transmit data packets are given by defining priorities of interest packets and data packets. Finally, simulation results show the performance of our framework with varied parameters.",fullPaper,jv190
Computer Science,p1457,d3,fb40b7d4a9b7c1040173c0420b69ccbb7bcdbe5c,c3,Knowledge Discovery and Data Mining,Forecasting Fine-Grained Air Quality Based on Big Data,"In this paper, we forecast the reading of an air quality monitoring station over the next 48 hours, using a data-driven method that considers current meteorological data, weather forecasts, and air quality data of the station and that of other stations within a few hundred kilometers. Our predictive model is comprised of four major components: 1) a linear regression-based temporal predictor to model the local factors of air quality, 2) a neural network-based spatial predictor to model global factors, 3) a dynamic aggregator combining the predictions of the spatial and temporal predictors according to meteorological data, and 4) an inflection predictor to capture sudden changes in air quality. We evaluate our model with data from 43 cities in China, surpassing the results of multiple baseline methods. We have deployed a system with the Chinese Ministry of Environmental Protection, providing 48-hour fine-grained air quality forecasts for four major Chinese cities every hour. The forecast function is also enabled on Microsoft Bing Map and MS cloud platform Azure. Our technology is general and can be applied globally for other cities.",fullPaper,cp3
Environmental Science,p1457,d14,fb40b7d4a9b7c1040173c0420b69ccbb7bcdbe5c,c3,Knowledge Discovery and Data Mining,Forecasting Fine-Grained Air Quality Based on Big Data,"In this paper, we forecast the reading of an air quality monitoring station over the next 48 hours, using a data-driven method that considers current meteorological data, weather forecasts, and air quality data of the station and that of other stations within a few hundred kilometers. Our predictive model is comprised of four major components: 1) a linear regression-based temporal predictor to model the local factors of air quality, 2) a neural network-based spatial predictor to model global factors, 3) a dynamic aggregator combining the predictions of the spatial and temporal predictors according to meteorological data, and 4) an inflection predictor to capture sudden changes in air quality. We evaluate our model with data from 43 cities in China, surpassing the results of multiple baseline methods. We have deployed a system with the Chinese Ministry of Environmental Protection, providing 48-hour fine-grained air quality forecasts for four major Chinese cities every hour. The forecast function is also enabled on Microsoft Bing Map and MS cloud platform Azure. Our technology is general and can be applied globally for other cities.",fullPaper,cp3
Computer Science,p1458,d3,9dee8f8c93e14647124aaf6298aec6eb8f52b921,j78,Future generations computer systems,Remote sensing big data computing: Challenges and opportunities,Abstract,fullPaper,jv78
Computer Science,p1460,d3,494f901ab46bd67de8ce46c7325822d43dffd8ce,j341,Data Science and Engineering,On the Meaningfulness of “Big Data Quality” (Invited Paper),Abstract,fullPaper,jv341
Computer Science,p1461,d3,87a7e55b4c3116751edb4b0f74e0484eaf7a853d,j67,Information systems research,"Editorial - Big Data, Data Science, and Analytics: The Opportunity and Challenge for IS Research","We address key questions related to the explosion of interest in the emerging fields of big data, analytics, and data science. We discuss the novelty of the fields and whether the underlying questions are fundamentally different, the strengths that the information systems IS community brings to this discourse, interesting research questions for IS scholars, the role of predictive and explanatory modeling, and how research in this emerging area should be evaluated for contribution and significance.",fullPaper,jv67
Economics,p1461,d11,87a7e55b4c3116751edb4b0f74e0484eaf7a853d,j67,Information systems research,"Editorial - Big Data, Data Science, and Analytics: The Opportunity and Challenge for IS Research","We address key questions related to the explosion of interest in the emerging fields of big data, analytics, and data science. We discuss the novelty of the fields and whether the underlying questions are fundamentally different, the strengths that the information systems IS community brings to this discourse, interesting research questions for IS scholars, the role of predictive and explanatory modeling, and how research in this emerging area should be evaluated for contribution and significance.",fullPaper,jv67
Computer Science,p1464,d3,503dc3243d206ac86ac13bdc2bf38d1b3272b962,j152,IEEE Access,High-Performance Extreme Learning Machines: A Complete Toolbox for Big Data Applications,This paper presents a complete approach to a successful utilization of a high-performance extreme learning machines (ELMs) Toolbox for Big Data. It summarizes recent advantages in algorithmic performance; gives a fresh view on the ELM solution in relation to the traditional linear algebraic performance; and reaps the latest software and hardware performance achievements. The results are applicable to a wide range of machine learning problems and thus provide a solid ground for tackling numerous Big Data challenges. The included toolbox is targeted at enabling the full potential of ELMs to the widest range of users.,fullPaper,jv152
Computer Science,p1466,d3,8bcc25651b62532f639fe713da9d7b67c742d562,j134,Big Data Research,Efficient Machine Learning for Big Data: A Review,Abstract,fullPaper,jv134
Mathematics,p1466,d6,8bcc25651b62532f639fe713da9d7b67c742d562,j134,Big Data Research,Efficient Machine Learning for Big Data: A Review,Abstract,fullPaper,jv134
Computer Science,p1467,d3,0882e7abd738267e4f4f38ca37557b916b130f7e,j7,Journal of Big Data,"Big data, Big bang?",Abstract,fullPaper,jv7
Computer Science,p1468,d3,a8a40298b05466adf011b6f7f842427e6a1e5d02,c104,North American Chapter of the Association for Computational Linguistics,Big Data: The V's of the Game Changer Paradigm,"The Big Data is the most prominent paradigm now-a-days. The Big Data starts rule slowly from 2003, and expected to rule and dominate the IT industries at least up to 2030. Furthermore, the Big Data conquer the technological war and easily capture the entire market since 2009. The Big Data is blasting everywhere around the World in every domain. The Big Data, a massive amount of data, able to generate billions of revenue. The secret behind of these billions of revenue is ever growing volume. This paper presents the redefinition of volume of Big Data. The volume is redefined by engaging three other V's, namely, voluminosity, vacuum, and vitality. Furthermore, this paper augments two new V's to the Big Data paradigm, namely, vendee and vase. This paper explores all V's of Big Data. There are lots of controversy and confusion regarding V's of Big Data. This paper uncovers the confusions of the V family of the Big Data.",poster,cp104
Computer Science,p1471,d3,b95ca56172201797aa5e51200fbf07309613c315,c49,ACM/SIGCOMM Internet Measurement Conference,Towards an IoT Big Data Analytics Framework: Smart Buildings Systems,"There is a growing interest in IoT-enabled smart buildings. However, the storage and analysis of large amount of high-speed real-time smart building data is a challenging task. There are a number of contemporary Big Data management technologies and advanced analytics techniques that can be used to deal with this challenge. There is a need for an integrated IoT Big Data Analytics (IBDA) framework to fill the research gap in the Big Data Analytics domain. This paper presents one such IBDA framework for the storage and analysis of real time data generated from IoT sensors deployed inside the smart building. The initial version of the IBDA framework has been developed by using Python and the Big Data Cloudera platform. The applicability of the framework is demonstrated with the help of a scenario involving the analysis of real-time smart building data for automatically managing the oxygen level, luminosity and smoke/hazardous gases in different parts of the smart building. The initial results indicate that the proposed framework is fit for the purpose and seems useful for IoT-enabled Big Data Analytics for smart buildings. The key contribution of this paper is the complex integration of Big Data Analytics and IoT for addressing the large volume and velocity challenge of real-time data in the smart building domain. This framework will be further evaluated and extended through its implementation in other domains.",poster,cp49
Computer Science,p1473,d3,ffa05258ba9617925c697c99d0dfb5e3ba100844,c94,International Conferences on Contemporary Computing and Informatics,Big data visualization: Tools and challenges,"In today's world where everything is recorded digitally, right from our web surfing patterns to our medical records, we are generating and processing petabytes of data every day. Big data will be transformative in every sphere of life. But just to process and analyze those data is not enough, human brain tends to find pattern more efficiently when data is represented visually. Data Visualization and Analytics plays important role in decision making in various sectors. It also leads to new opportunities in the visualization domain representing the innovative ideation for solving the big-data problem via visual means. It is quite a challenge to visualize such a mammoth amount of data in real time or in static form. In this paper, we discuss why big data visualization is of utmost importance, what are the challenges related to it and review some big data visualization tools.",fullPaper,cp94
Computer Science,p1474,d3,b0150dd118ebedbc3ece68726e065f9afaaf3b18,c24,International Conference on Data Technologies and Applications,Big data analytics and big data science: a survey,"Big data has attracted much attention from academia and industry. But the discussion of big data is disparate, fragmented and distributed among different outlets. This paper conducts a systematic and extensive review on 186 journal publications about big data from 2011 to 2015 in the Science Citation Index (SCI) and the Social Science Citation Index (SSCI) database aiming to provide scholars and practitioners with a comprehensive overview and big picture about research on big data. The selected papers are grouped into 20 research categories. The contents of the paper(s) in each research category are summarized. Research directions for each category are outlined as well. The results in this study indicate that the selected papers were mainly published between 2013 and 2015 and focus on technological issues regarding big data. Diverse new approaches, methods, frameworks and systems are proposed for data collection, storage, transport, processing and analysis in the selected papers. Possible directions for f...",poster,cp24
Computer Science,p1476,d3,4c7fa768a4c67a5332c0c58ed79043eda3a775d7,c68,Symposium on Advances in Databases and Information Systems,Building a Big Data Platform for Smart Cities: Experience and Lessons from Santander,"The Internet of Things (IoT) is now shaping our cities to make them more connected, convenient, and intelligent. However, this change will highly rely on extracted values and insights from the big data generated by our cities via sensors, devices, and human activities. Many existing studies and projects have been done to make our cities smart, focusing more on how to deploy various sensors and devices and then collect data from them. However, this is just the first step towards smart cities and next step will be to make good use of the collected data and enable context-awareness and intelligence into all kinds of applications and services via a flexible big data platform. In this paper, we introduce the system architecture and the major design issues of a live City Data and Analytics Platform, namely CiDAP. More importantly, we share our experience and lessons learned from building this practical system for a large scale running smart city test bed, SmartSantander. Our work provides a valuable example to future Smart City platform designers so that they can foresee some practice issues and refer to our solution when building their own smart city data platforms.",poster,cp68
Computer Science,p1477,d3,1d74b9a3cc22b479e1dab4762b3c16da8a2bda8a,j7,Journal of Big Data,An industrial big data pipeline for data-driven analytics maintenance applications in large-scale smart manufacturing facilities,Abstract,fullPaper,jv7
Computer Science,p1478,d3,c8ce1460f584a6fe1197d03f1e71d6cd9d9147a8,j345,Information Technology & Tourism,"Big data related technologies, challenges and future prospects",Abstract,fullPaper,jv345
Computer Science,p1479,d3,3e7f5f4382ac6f9c4fef6197dd21abf74456acd1,c69,Neural Information Processing Systems,Big Self-Supervised Models are Strong Semi-Supervised Learners,"One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to most previous approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of a big (deep and wide) network during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2 (a modification of SimCLR), supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9\% ImageNet top-1 accuracy with just 1\% of the labels ($\le$13 labeled images per class) using ResNet-50, a $10\times$ improvement in label efficiency over the previous state-of-the-art. With 10\% of labels, ResNet-50 trained with our method achieves 77.5\% top-1 accuracy, outperforming standard supervised training with all of the labels.",fullPaper,cp69
Mathematics,p1479,d6,3e7f5f4382ac6f9c4fef6197dd21abf74456acd1,c69,Neural Information Processing Systems,Big Self-Supervised Models are Strong Semi-Supervised Learners,"One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to most previous approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of a big (deep and wide) network during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2 (a modification of SimCLR), supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9\% ImageNet top-1 accuracy with just 1\% of the labels ($\le$13 labeled images per class) using ResNet-50, a $10\times$ improvement in label efficiency over the previous state-of-the-art. With 10\% of labels, ResNet-50 trained with our method achieves 77.5\% top-1 accuracy, outperforming standard supervised training with all of the labels.",fullPaper,cp69
Computer Science,p1480,d3,62ed4d3b414792a5d5cbddad37869281de9908be,j345,Information Technology & Tourism,"Jose Maria Cavanillas, Edward Curry, and Wolfgang Wahlster (editors): new horizons for a data-driven economy: a roadmap for usage and exploitation of big data in Europe",Abstract,fullPaper,jv345
Political Science,p1480,d15,62ed4d3b414792a5d5cbddad37869281de9908be,j345,Information Technology & Tourism,"Jose Maria Cavanillas, Edward Curry, and Wolfgang Wahlster (editors): new horizons for a data-driven economy: a roadmap for usage and exploitation of big data in Europe",Abstract,fullPaper,jv345
Computer Science,p1481,d3,95d559c91d01e28450653847a1d682aacd324ca0,c93,ASE BigData & SocialInformatics,Ethical Issues in the Big Data Industry,"Big Data combines information from diverse sources to create knowledge, make better predictions and tailor services. This article analyzes Big Data as an industry, not a technology, and identifies the ethical issues it faces. These issues arise from reselling consumers’ data to the secondary market for Big Data. Remedies for the issues are proposed, with the goal of fostering a sustainable Big Data Industry.",poster,cp93
Business,p1481,d9,95d559c91d01e28450653847a1d682aacd324ca0,c93,ASE BigData & SocialInformatics,Ethical Issues in the Big Data Industry,"Big Data combines information from diverse sources to create knowledge, make better predictions and tailor services. This article analyzes Big Data as an industry, not a technology, and identifies the ethical issues it faces. These issues arise from reselling consumers’ data to the secondary market for Big Data. Remedies for the issues are proposed, with the goal of fostering a sustainable Big Data Industry.",poster,cp93
Computer Science,p1482,d3,adc180e1fe404b650fca3bb7970e43bdce34a611,c95,Cyber ..,A Big Data Modeling Methodology for Apache Cassandra,"Apache Cassandra is a leading distributed database of choice when it comes to big data management with zero downtime, linear scalability, and seamless multiple data center deployment. With increasingly wider adoption of Cassandra for online transaction processing by hundreds of Web-scale companies, there is a growing need for a rigorous and practical data modeling approach that ensures sound and efficient schema design. This work i) proposes the first query-driven big data modeling methodology for Apache Cassandra, ii) defines important data modeling principles, mapping rules, and mapping patterns to guide logical data modeling, iii) presents visual diagrams for Cassandra logical and physical data models, and iv) demonstrates a data modeling tool that automates the entire data modeling process.",poster,cp95
Computer Science,p1483,d3,a0ff514a8a64ba5a7cd7430ca04245fd037d040c,c19,International Conference on Conceptual Structures,Business Analytics in the Context of Big Data: A Roadmap for Research,"This paper builds on academic and industry discussions from the 2012 and 2013 pre-ICIS events: BI Congress III and the Special Interest Group on Decision Support Systems (SIGDSS) workshop, respectively. Recognizing the potential of “big data” to offer new insights for decision making and innovation, panelists at the two events discussed how organizations can use and manage big data for competitive advantage. In addition, expert panelists helped to identify research gaps. While emerging research in the academic community identifies some of the issues in acquiring, analyzing, and using big data, many of the new developments are occurring in the practitioner community. We bridge the gap between academic and practitioner research by presenting a big data analytics framework that depicts a process view of the components needed for big data analytics in organizations. Using practitioner interviews and literature from both academia and practice, we identify the current state of big data research guided by the framework and propose potential areas for future research to increase the relevance of academic research to practice.",poster,cp19
Computer Science,p1485,d3,5d2f4d6516686fb337605057452058d56444c53d,c3,Knowledge Discovery and Data Mining,Efficient Online Evaluation of Big Data Stream Classifiers,"The evaluation of classifiers in data streams is fundamental so that poorly-performing models can be identified, and either improved or replaced by better-performing models. This is an increasingly relevant and important task as stream data is generated from more sources, in real-time, in large quantities, and is now considered the largest source of big data. Both researchers and practitioners need to be able to effectively evaluate the performance of the methods they employ. However, there are major challenges for evaluation in a stream. Instances arriving in a data stream are usually time-dependent, and the underlying concept that they represent may evolve over time. Furthermore, the massive quantity of data also tends to exacerbate issues such as class imbalance. Current frameworks for evaluating streaming and online algorithms are able to give predictions in real-time, but as they use a prequential setting, they build only one model, and are thus not able to compute the statistical significance of results in real-time. In this paper we propose a new evaluation methodology for big data streams. This methodology addresses unbalanced data streams, data where change occurs on different time scales, and the question of how to split the data between training and testing, over multiple models.",fullPaper,cp3
Computer Science,p1486,d3,1a69ec1864d490e121e6574b2bfe7817f475b7b9,c109,Computer Vision and Pattern Recognition,Big data stream computing in healthcare real-time analytics,"The healthcare industry is changing at a dramatic rate. There are multiple processes going on within the health sector. These processes not only impact the care of individuals but also help medical practitioners and the delivery of care and services. The industry can take advantage of big data analytics to ensure that all the multiple processes within the industry are running smoothly. Big data analytics is not just an opportunity but a necessity. Recently, big data stream computing has been studied in order to improve the quality of healthcare services and reduce costs by capability support prediction, thus making decisions in real-time. This paper proposes a generic architecture for big data healthcare analytic by using open sources, including Hadoop, Apache Storm, Kafka and NoSQL Cassandra. The combination of high throughput publish-subscribe messaging for streams, distributed real-time computing, and distributed storage system can effectively analyze a huge amount of healthcare data coming with a rapid rate.",poster,cp109
Computer Science,p1488,d3,405d646cab73cfcbf441d239f9942bf1529e27ce,j7,Journal of Big Data,Big data in manufacturing: a systematic mapping study,Abstract,fullPaper,jv7
Computer Science,p1490,d3,0e834f8594abda09b005aaf241a1e6b25abe7eb0,c75,International Conference on Predictive Models in Software Engineering,"Big Data Fundamentals: Concepts, Drivers & Techniques","This text should be required reading for everyone in contemporary business. --Peter Woodhull, CEO, Modus21 The one book that clearly describes and links Big Data concepts to business utility. --Dr. Christopher Starr, PhD Simply, this is the best Big Data book on the market! --Sam Rostam, Cascadian IT Group ...one of the most contemporary approaches Ive seen to Big Data fundamentals... --Joshua M. Davis, PhD The Definitive Plain-English Guide to Big Data for Business and Technology Professionals Big Data Fundamentals provides a pragmatic, no-nonsense introduction to Big Data. Best-selling IT author Thomas Erl and his team clearly explain key Big Data concepts, theory and terminology, as well as fundamental technologies and techniques. All coverage is supported with case study examples and numerous simple diagrams. The authors begin by explaining how Big Data can propel an organization forward by solving a spectrum of previously intractable business problems. Next, they demystify key analysis techniques and technologies and show how a Big Data solution environment can be built and integrated to offer competitive advantages. Discovering Big Datas fundamental concepts and what makes it different from previous forms of data analysis and data science Understanding the business motivations and drivers behind Big Data adoption, from operational improvements through innovation Planning strategic, business-driven Big Data initiatives Addressing considerations such as data management, governance, and security Recognizing the 5 V characteristics of datasets in Big Data environments: volume, velocity, variety, veracity, and value Clarifying Big Datas relationships with OLTP, OLAP, ETL, data warehouses, and data marts Working with Big Data in structured, unstructured, semi-structured, and metadata formats Increasing value by integrating Big Data resources with corporate performance monitoring Understanding how Big Data leverages distributed and parallel processing Using NoSQL and other technologies to meet Big Datas distinct data processing requirements Leveraging statistical approaches of quantitative and qualitative analysis Applying computational analysis methods, including machine learning",poster,cp75
Computer Science,p1492,d3,6729aeba2d6d72bc75476b4ecd1f5556dce40342,c68,Symposium on Advances in Databases and Information Systems,Big Data Analysis: Apache Storm Perspective,"— the boom in the technology has resulted in emergence of new concepts and challenges. Big data is one of those spoke about terms today. Big data is becoming a synonym for competitive advantages in business rivalries. Despite enormous benefits, big data accompanies some serious challenges and when it comes to analyzing of big data, it requires some serious thought. This study explores Big Data terminology and its analysis concepts using sample from Twitter data with the help of one of the most industry trusted real time processing and fault tolerant tool called Apache Storm.",poster,cp68
Computer Science,p1493,d3,654e10be1113c6d73c9b65163b6f71d414406b6e,c16,International Conference on Data Science and Advanced Analytics,Big Data Analysis: Apache Spark Perspective,"the boom in the technology has resulted in emergence of new concepts and challenges. Big data is one of those spoke about terms today. Big data is becoming a synonym for competitive advantages in business rivalries. Despite enormous benefits, big data accompanies some serious challenges and when it comes to analyzing of big data, it requires some serious thought. This study explores Big Data terminology and its analysis concepts using sample from Twitter data with the help of one of the most industry trusted real time processing and fault tolerant tool called Apache Storm.",poster,cp16
Computer Science,p1494,d3,4b2b061a7e004216e61dadbf076c80a8301c4361,c9,Big Data,BigDansing: A System for Big Data Cleansing,"Data cleansing approaches have usually focused on detecting and fixing errors with little attention to scaling to big datasets. This presents a serious impediment since data cleansing often involves costly computations such as enumerating pairs of tuples, handling inequality joins, and dealing with user-defined functions. In this paper, we present BigDansing, a Big Data Cleansing system to tackle efficiency, scalability, and ease-of-use issues in data cleansing. The system can run on top of most common general purpose data processing platforms, ranging from DBMSs to MapReduce-like frameworks. A user-friendly programming interface allows users to express data quality rules both declaratively and procedurally, with no requirement of being aware of the underlying distributed platform. BigDansing takes these rules into a series of transformations that enable distributed computations and several optimizations, such as shared scans and specialized joins operators. Experimental results on both synthetic and real datasets show that BigDansing outperforms existing baseline systems up to more than two orders of magnitude without sacrificing the quality provided by the repair algorithms.",poster,cp9
Computer Science,p1497,d3,6985316725136e7b66ba6b886fce92194387841f,j260,IEEE Transactions on Big Data,Architecting Time-Critical Big-Data Systems,"Current infrastructures for developing big-data applications are able to process –via big-data analytics- huge amounts of data, using clusters of machines that collaborate to perform parallel computations. However, current infrastructures were not designed to work with the requirements of time-critical applications; they are more focused on general-purpose applications rather than time-critical ones. Addressing this issue from the perspective of the real-time systems community, this paper considers time-critical big-data. It deals with the definition of a time-critical big-data system from the point of view of requirements, analyzing the specific characteristics of some popular big-data applications. This analysis is complemented by the challenges stemmed from the infrastructures that support the applications, proposing an architecture and offering initial performance patterns that connect application costs with infrastructure performance.",fullPaper,jv260
Computer Science,p1498,d3,4df00051e84846933299cd823a40c15e47adb5c4,c58,Extreme Science and Engineering Discovery Environment,"Big data for education data mining, data analytics and web dashboards","In this era of big data, school and universities are gathering tons of information. But much of that data is stored in ways that make it difficult for teachers and managers to access it. Usually written reports tell only one story or display just one piece of information. Many educational institutions use Moodle as educational environment in the process of learning and when it comes to a bigger number of users and course participants it becomes hard to follow their activity in the courses. To make studying more effective, it is important to supply personalization of the participants, based on their activity, an opportunity to analyze their activities in different courses, predict the results of the participants and get better survey of the activities of the students. The goal of this work is, by the use of data mining techniques to describe the process of selection and acquiring data from the Moodle database, and to create dashboard - web based application, that would communicate with Moodle and supply multilevel approach, and practically improve the approach to evaluation of larger groups of participants in the learning process and will help teachers to learn more about how students learn.",poster,cp58
Computer Science,p1511,d3,d2c733e34d48784a37d717fe43d9e93277a8c53e,c21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,ImageNet: A large-scale hierarchical image database,"The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.",poster,cp21
Computer Science,p1520,d3,68c03788224000794d5491ab459be0b2a2c38677,c96,Human Language Technology - The Baltic Perspectiv,WordNet: A Lexical Database for English,"Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].",fullPaper,cp96
Computer Science,p1524,d3,6a074a3fa856e86b2e6bc60e83d66cc488090ae9,j349,The International Journal of Life Cycle Assessment,The ecoinvent database version 3 (part I): overview and methodology,Abstract,fullPaper,jv349
Computer Science,p1529,d3,c6b3ca4f939e36a9679a70e14ce8b1bbbc5618f3,c33,Workshop on Python for High-Performance and Scientific Computing,Labeled Faces in the Wild: A Database forStudying Face Recognition in Unconstrained Environments,"Most face databases have been created under controlled conditions to facilitate the study of specific parameters on the face recognition problem. These parameters include such variables as position, pose, lighting, background, camera quality, and gender. While there are many applications for face recognition technology in which one can control the parameters of image acquisition, there are also many applications in which the practitioner has little or no control over such parameters. This database, Labeled Faces in the Wild, is provided as an aid in studying the latter, unconstrained, recognition problem. The database contains labeled face photographs spanning the range of conditions typically encountered in everyday life. The database exhibits “natural” variability in factors such as pose, lighting, race, accessories, occlusions, and background. In addition to describing the details of the database, we provide specific experimental paradigms for which the database is suitable. This is done in an effort to make research performed with the database as consistent and comparable as possible. We provide baseline results, including results of a state of the art face recognition system combined with a face alignment system. To facilitate experimentation on the database, we provide several parallel databases, including an aligned version.",poster,cp33
Computer Science,p1532,d3,8b3b8848a311c501e704c45c6d50430ab7068956,c98,Vision,HMDB: A large video database for human motion recognition,"With nearly one billion online videos viewed everyday, an emerging new frontier in computer vision research is recognition and search in video. While much effort has been devoted to the collection and annotation of large scalable static image datasets containing thousands of image categories, human action datasets lag far behind. Current action recognition databases contain on the order of ten different action categories collected under fairly controlled conditions. State-of-the-art performance on these datasets is now near ceiling and thus there is a need for the design and creation of new benchmarks. To address this issue we collected the largest action video database to-date with 51 action categories, which in total contain around 7,000 manually annotated clips extracted from a variety of sources ranging from digitized movies to YouTube. We use this database to evaluate the performance of two representative computer vision systems for action recognition and explore the robustness of these methods under various conditions such as camera motion, viewpoint, video quality and occlusion.",fullPaper,cp98
Computer Science,p1537,d3,1976c9eeccc7115d18a04f1e7fb5145db6b96002,c43,European Conference on Machine Learning,Freebase: a collaboratively created graph database for structuring human knowledge,"Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Freebase currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.",poster,cp43
Computer Science,p1546,d3,46f74231b9afeb0c290d6d550043c55045284e5f,j181,IEEE Signal Processing Magazine,The MNIST Database of Handwritten Digit Images for Machine Learning Research [Best of the Web],"In this issue, “Best of the Web” presents the modified National Institute of Standards and Technology (MNIST) resources, consisting of a collection of handwritten digit images used extensively in optical character recognition and machine learning research.",fullPaper,jv181
Computer Science,p1552,d3,82524ddee00fa0895dfca43995a7ec8bdb16f0d5,c6,Annual Conference on Genetic and Evolutionary Computation,Fundamentals of Database Systems,"From the Publisher: 
Fundamentals of Database Systems combines clear explanations of theory and design, broad coverage of models and real systems, and excellent examples with up-to-date introductions to modern database technologies. This edition is completely revised and updated, and reflects the latest trends in technological and application development. Professors Elmasri and Navathe focus on the relational model and include coverage of recent object-oriented developments. They also address advanced modeling and system enhancements in the areas of active databases, temporal and spatial databases, and multimedia information systems. This edition also surveys the latest application areas of data warehousing, data mining, web databases, digital libraries, GIS, and genome databases. New to the Third Edition 
Reorganized material on data modeling to clearly separate entity relationship modeling, extended entity relationship modeling, and object-oriented modeling Expanded coverage of the object-oriented and object/relational approach to data management, including ODMG and SQL3 Uses examples from real database systems including OracleTM and Microsoft AccessAE Includes discussion of decision support applications of data warehousing and data mining, as well as emerging technologies of web databases, multimedia, and mobile databases Covers advanced modeling in the areas of active, temporal, and spatial databases Provides coverage of issues of physical database tuning Discusses current database application areas of GIS, genome, and digital libraries",poster,cp6
Computer Science,p1557,d3,bb967168ead7a14adcb0121dcf24a930d1a383b3,c58,Extreme Science and Engineering Discovery Environment,The HITRAN 2008 molecular spectroscopic database,Abstract,poster,cp58
Computer Science,p1558,d3,4bd970a37c59c97804ff93cbb2c108e081de3a37,c21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,Introduction to WordNet: An On-line Lexical Database,"Standard alphabetical procedures for organizing lexical information put together words that are spelled alike and scatter words with similar or related meanings haphazardly through the list. Unfortunately, there is no obvious alternative, no other simple way for lexicographers to keep track of what has been done or for readers to find the word they are looking for. But a frequent objection to this solution is that finding things on an alphabetical list can be tedious and time-consuming. Many people who would like to refer to a dictionary decide not to bother with it because finding the information would interrupt their work and break their train of thought.",poster,cp21
Computer Science,p1564,d3,9667f8264745b626c6173b1310e2ff0298b09cfc,c69,Neural Information Processing Systems,Learning Deep Features for Scene Recognition using Places Database,"Scene recognition is one of the hallmark tasks of computer vision, allowing definition of a context for object recognition. Whereas the tremendous recent progress in object recognition tasks is due to the availability of large datasets like ImageNet and the rise of Convolutional Neural Networks (CNNs) for learning high-level features, performance at scene recognition has not attained the same level of success. This may be because current deep features trained from ImageNet are not competitive enough for such tasks. Here, we introduce a new scene-centric database called Places with over 7 million labeled pictures of scenes. We propose new methods to compare the density and diversity of image datasets and show that Places is as dense as other scene datasets and has more diversity. Using CNN, we learn deep features for scene recognition tasks, and establish new state-of-the-art results on several scene-centric datasets. A visualization of the CNN layers' responses allows us to show differences in the internal representations of object-centric and scene-centric networks.",fullPaper,cp69
Computer Science,p1565,d3,908091b4a8757c3b2f7d9cfa2c4f616ee12c5157,c14,Hawaii International Conference on System Sciences,SUN database: Large-scale scene recognition from abbey to zoo,"Scene categorization is a fundamental problem in computer vision. However, scene understanding research has been constrained by the limited scope of currently-used databases which do not capture the full variety of scene categories. Whereas standard databases for object categorization contain hundreds of different classes of objects, the largest available dataset of scene categories contains only 15 classes. In this paper we propose the extensive Scene UNderstanding (SUN) database that contains 899 categories and 130,519 images. We use 397 well-sampled categories to evaluate numerous state-of-the-art algorithms for scene recognition and establish new bounds of performance. We measure human scene classification performance on the SUN database and compare this with computational methods. Additionally, we study a finer-grained scene representation to detect scenes embedded inside of larger scenes.",poster,cp14
Computer Science,p1566,d3,c369c9a40a36b013be3ef9c19a068f2d6578e4c3,c114,Chinese Conference on Biometric Recognition,Greengenes: Chimera-checked 16S rRNA gene database and workbench compatible in ARB,"Title: Greengenes: Chimera-checked 16S rRNA gene database and workbench compatible with ARB Authors: DeSantis, T.Z. 1 Hugenholtz, P. 2 Larsen, N. 3 Rojas, M. 4 Brodie, E.L. 1 Keller, K. 5 Huber, T. 6 Dalevi, D. 7 Hu, P. 1 Andersen, G.L. 1 Center for Environmental Biotechnology Lawrence Berkeley National Laboratory 1 Cyclotron Road, Mail Stop 70A-3317 Berkeley, CA 94720 USA Microbial Ecology Program DOE Joint Genome Institute 2800 Mitchell Drive Bldg 400-404 Walnut Creek, CA 94598 USA Danish Genome Institute Gustav Wieds vej 10 C DK-8000 Aarhus C Denmark Department of Bioinformatics Baylor University P.O. Box 97356, 1311 S. 5th St. Waco, TX 76798-7356 USA Department of Bioengineering University of California Berkeley, CA 94720 USA Departments of Biochemistry and Mathematics The University of Queensland Brisbane Qld 4072 Australia Department of Computer Science Chalmers University of Technology",poster,cp114
Biology,p1566,d5,c369c9a40a36b013be3ef9c19a068f2d6578e4c3,c114,Chinese Conference on Biometric Recognition,Greengenes: Chimera-checked 16S rRNA gene database and workbench compatible in ARB,"Title: Greengenes: Chimera-checked 16S rRNA gene database and workbench compatible with ARB Authors: DeSantis, T.Z. 1 Hugenholtz, P. 2 Larsen, N. 3 Rojas, M. 4 Brodie, E.L. 1 Keller, K. 5 Huber, T. 6 Dalevi, D. 7 Hu, P. 1 Andersen, G.L. 1 Center for Environmental Biotechnology Lawrence Berkeley National Laboratory 1 Cyclotron Road, Mail Stop 70A-3317 Berkeley, CA 94720 USA Microbial Ecology Program DOE Joint Genome Institute 2800 Mitchell Drive Bldg 400-404 Walnut Creek, CA 94598 USA Danish Genome Institute Gustav Wieds vej 10 C DK-8000 Aarhus C Denmark Department of Bioinformatics Baylor University P.O. Box 97356, 1311 S. 5th St. Waco, TX 76798-7356 USA Department of Bioengineering University of California Berkeley, CA 94720 USA Departments of Biochemistry and Mathematics The University of Queensland Brisbane Qld 4072 Australia Department of Computer Science Chalmers University of Technology",poster,cp114
Computer Science,p1568,d3,5a2892f91addeea2f4600d28b23e684be32f5b2c,j357,IEEE Transactions on Affective Computing,DEAP: A Database for Emotion Analysis ;Using Physiological Signals,"We present a multimodal data set for the analysis of human affective states. The electroencephalogram (EEG) and peripheral physiological signals of 32 participants were recorded as each watched 40 one-minute long excerpts of music videos. Participants rated each video in terms of the levels of arousal, valence, like/dislike, dominance, and familiarity. For 22 of the 32 participants, frontal face video was also recorded. A novel method for stimuli selection is proposed using retrieval by affective tags from the last.fm website, video highlight detection, and an online assessment tool. An extensive analysis of the participants' ratings during the experiment is presented. Correlates between the EEG signal frequencies and the participants' ratings are investigated. Methods and results are presented for single-trial classification of arousal, valence, and like/dislike ratings using the modalities of EEG, peripheral physiological signals, and multimedia content analysis. Finally, decision fusion of the classification results from different modalities is performed. The data set is made publicly available and we encourage other researchers to use it for testing their own affective state estimation methods.",fullPaper,jv357
Psychology,p1568,d10,5a2892f91addeea2f4600d28b23e684be32f5b2c,j357,IEEE Transactions on Affective Computing,DEAP: A Database for Emotion Analysis ;Using Physiological Signals,"We present a multimodal data set for the analysis of human affective states. The electroencephalogram (EEG) and peripheral physiological signals of 32 participants were recorded as each watched 40 one-minute long excerpts of music videos. Participants rated each video in terms of the levels of arousal, valence, like/dislike, dominance, and familiarity. For 22 of the 32 participants, frontal face video was also recorded. A novel method for stimuli selection is proposed using retrieval by affective tags from the last.fm website, video highlight detection, and an online assessment tool. An extensive analysis of the participants' ratings during the experiment is presented. Correlates between the EEG signal frequencies and the participants' ratings are investigated. Methods and results are presented for single-trial classification of arousal, valence, and like/dislike ratings using the modalities of EEG, peripheral physiological signals, and multimedia content analysis. Finally, decision fusion of the classification results from different modalities is performed. The data set is made publicly available and we encourage other researchers to use it for testing their own affective state estimation methods.",fullPaper,jv357
Computer Science,p1570,d3,092c275005ae49dc1303214f6d02d134457c7053,j358,International Journal of Computer Vision,LabelMe: A Database and Web-Based Tool for Image Annotation,Abstract,fullPaper,jv358
Computer Science,p1576,d3,6d96f946aaabc734af7fe3fc4454cf8547fcd5ed,c20,ACM Conference on Economics and Computation,The AR face database,Abstract,poster,cp20
Computer Science,p1584,d3,5cf0d213f3253cd46673d955209f8463db73cc51,j361,Language Resources and Evaluation,IEMOCAP: interactive emotional dyadic motion capture database,Abstract,fullPaper,jv361
Computer Science,p1586,d3,804836b8ad86ef8042e3dcbd45442a52f031ee03,c100,IEEE International Conference on Computer Vision,A Database and Evaluation Methodology for Optical Flow,Abstract,fullPaper,cp100
Computer Science,p1590,d3,e3f2391513693647e0ea87bfa86cd89e468f51d0,c66,International Conference on Web and Social Media,Comprehensive database for facial expression analysis,"Within the past decade, significant effort has occurred in developing methods of facial expression analysis. Because most investigators have used relatively limited data sets, the generalizability of these various methods remains unknown. We describe the problem space for facial expression analysis, which includes level of description, transitions among expressions, eliciting conditions, reliability and validity of training and test data, individual differences in subjects, head orientation and scene complexity image characteristics, and relation to non-verbal behavior. We then present the CMU-Pittsburgh AU-Coded Face Expression Image Database, which currently includes 2105 digitized image sequences from 182 adult subjects of varying ethnicity, performing multiple tokens of most primary FACS action units. This database is the most comprehensive testbed to date for comparative studies of facial expression analysis.",poster,cp66
Computer Science,p1598,d3,dc8b25e35a3acb812beb499844734081722319b4,j363,Image and Vision Computing,The FERET database and evaluation procedure for face-recognition algorithms,Abstract,fullPaper,jv363
Computer Science,p1608,d3,026668472fd8f0fa2ca710ce276be35d362637c2,c96,Human Language Technology - The Baltic Perspectiv,"Federated database systems for managing distributed, heterogeneous, and autonomous databases","A federated database system (FDBS) is a collection of cooperating database systems that are autonomous and possibly heterogeneous. In this paper, we define a reference architecture for distributed database management systems from system and schema viewpoints and show how various FDBS architectures can be developed. We then define a methodology for developing one of the popular architectures of an FDBS. Finally, we discuss critical issues related to developing and operating an FDBS.",poster,cp96
Computer Science,p1609,d3,60258897d250a41f11cfee27de828a0130110b5e,c52,Workshop on Applied Computational Geometry,The CELEX Lexical Database (CD-ROM),Abstract,poster,cp52
Computer Science,p1617,d3,2157f202c8c89d924dd4da4d1bcf92d16fcd8893,j366,Signal processing. Image communication,"Image database TID2013: Peculiarities, results and perspectives",Abstract,fullPaper,jv366
Computer Science,p1621,d3,e4beeff8cf47dcc0faf6efc8f4c1b3fefc052afb,c101,Interspeech,A database of German emotional speech,"The article describes a database of emotional speech. Ten actors (5 female and 5 male) simulated the emotions, producing 10 German utterances (5 short and 5 longer sentences) which could be used in everyday communication and are interpretable in all applied emotions. The recordings were taken in an anechoic chamber with high-quality recording equipment. In addition to the sound electro-glottograms were recorded. The speech material comprises about 800 sentences (seven emotions * ten actors * ten sentences + some second versions). The complete database was evaluated in a perception test regarding the recognisability of emotions and their naturalness. Utterances recognised better than 80% and judged as natural by more than 60% of the listeners were phonetically labelled in a narrow transcription with special markers for voice-quality, phonatory and articulatory settings and articulatory features. The database can be accessed by the public via the internet (http://www.expressive-speech.net/emodb/).",fullPaper,cp101
Computer Science,p1628,d3,07b58c8bb6b9084fba84d464f5324b7933720665,c53,International Conference on Learning Representations,Principles of Distributed Database Systems,Abstract,poster,cp53
Computer Science,p1638,d3,62a134740314b4469c83c8921ae2e1beea22b8f5,j173,IEEE Transactions on Pattern Analysis and Machine Intelligence,A Database for Handwritten Text Recognition Research,"An image database for handwritten text recognition research is described. Digital images of approximately 5000 city names, 5000 state names, 10000 ZIP Codes, and 50000 alphanumeric characters are included. Each image was scanned from mail in a working post office at 300 pixels/in in 8-bit gray scale on a high-quality flat bed digitizer. The data were unconstrained for the writer, style, and method of preparation. These characteristics help overcome the limitations of earlier databases that contained only isolated characters or were prepared in a laboratory setting under prescribed circumstances. Also, the database is divided into explicit training and testing sets to facilitate the sharing of results among researchers as well as performance comparisons. >",fullPaper,jv173
Computer Science,p1642,d3,2de0a40e9a5d4f1feb07d61af5a5d87a069653f0,j4,IEEE Transactions on Knowledge and Data Engineering,Data Mining: An Overview from a Database Perspective,"Mining information and knowledge from large databases has been recognized by many researchers as a key research topic in database systems and machine learning, and by many industrial companies as an important area with an opportunity of major revenues. Researchers in many different fields have shown great interest in data mining. Several emerging applications in information-providing services, such as data warehousing and online services over the Internet, also call for various data mining techniques to better understand user behavior, to improve the service provided and to increase business opportunities. In response to such a demand, this article provides a survey, from a database researcher's point of view, on the data mining techniques developed recently. A classification of the available data mining techniques is provided and a comparative study of such techniques is presented.",fullPaper,jv4
Computer Science,p1644,d3,7ffa7a36e5414a0f2b16b1d8f93442ab15e2235d,j173,IEEE Transactions on Pattern Analysis and Machine Intelligence,"The CMU Pose, Illumination, and Expression Database","In the Fall of 2000, we collected a database of more than 40,000 facial images of 68 people. Using the Carnegie Mellon University 3D Room, we imaged each person across 13 different poses, under 43 different illumination conditions, and with four different expressions. We call this the CMU pose, illumination, and expression (PIE) database. We describe the imaging hardware, the collection procedure, the organization of the images, several possible uses, and how to obtain the database.",fullPaper,jv173
Computer Science,p1645,d3,448752b56fe4b2fc8fb15f22d9430c17aa306392,c60,Network and Distributed System Security Symposium,FEEDBACK ON A PUBLICLY DISTRIBUTED IMAGE DATABASE: THE MESSIDOR DATABASE,"The Messidor database, which contains hundreds of eye fundus images, has been publicly distributed since 2008. It was created by the Messidor project in order to evaluate automatic lesion segmentation and diabetic retinopathy grading methods. Designing, producing and maintaining such a database entails significant costs. By publicly sharing it, one hopes to bring a valuable resource to the public research community. However, the real interest and benefit of the research community is not easy to quantify. We analyse here the feedback on the Messidor database, after more than 6 years of diffusion. This analysis should apply to other similar research databases.",poster,cp60
Computer Science,p1667,d3,b89f0e4f43570688dd983813c9a3efa2fa7e7ebc,c104,North American Chapter of the Association for Computational Linguistics,"The CMU Pose, Illumination, and Expression (PIE) database","Between October 2000 and December 2000, we collected a database of over 40,000 facial images of 68 people. Using the CMU (Carnegie Mellon University) 3D Room, we imaged each person across 13 different poses, under 43 different illumination conditions, and with four different expressions. We call this database the CMU Pose, Illumination and Expression (PIE) database. In this paper, we describe the imaging hardware, the collection procedure, the organization of the database, several potential uses of the database, and how to obtain the database.",poster,cp104
Computer Science,p1670,d3,3a60678ad2b862fa7c27b11f04c93c010cc6c430,j357,IEEE Transactions on Affective Computing,A Multimodal Database for Affect Recognition and Implicit Tagging,"MAHNOB-HCI is a multimodal database recorded in response to affective stimuli with the goal of emotion recognition and implicit tagging research. A multimodal setup was arranged for synchronized recording of face videos, audio signals, eye gaze data, and peripheral/central nervous system physiological signals. Twenty-seven participants from both genders and different cultural backgrounds participated in two experiments. In the first experiment, they watched 20 emotional videos and self-reported their felt emotions using arousal, valence, dominance, and predictability as well as emotional keywords. In the second experiment, short videos and images were shown once without any tag and then with correct or incorrect tags. Agreement or disagreement with the displayed tags was assessed by the participants. The recorded videos and bodily responses were segmented and stored in a database. The database is made available to the academic community via a web-based system. The collected data were analyzed and single modality and modality fusion results for both emotion recognition and implicit tagging experiments are reported. These results show the potential uses of the recorded modalities and the significance of the emotion elicitation protocol.",fullPaper,jv357
Psychology,p1670,d10,3a60678ad2b862fa7c27b11f04c93c010cc6c430,j357,IEEE Transactions on Affective Computing,A Multimodal Database for Affect Recognition and Implicit Tagging,"MAHNOB-HCI is a multimodal database recorded in response to affective stimuli with the goal of emotion recognition and implicit tagging research. A multimodal setup was arranged for synchronized recording of face videos, audio signals, eye gaze data, and peripheral/central nervous system physiological signals. Twenty-seven participants from both genders and different cultural backgrounds participated in two experiments. In the first experiment, they watched 20 emotional videos and self-reported their felt emotions using arousal, valence, dominance, and predictability as well as emotional keywords. In the second experiment, short videos and images were shown once without any tag and then with correct or incorrect tags. Agreement or disagreement with the displayed tags was assessed by the participants. The recorded videos and bodily responses were segmented and stored in a database. The database is made available to the academic community via a web-based system. The collected data were analyzed and single modality and modality fusion results for both emotion recognition and implicit tagging experiments are reported. These results show the potential uses of the recorded modalities and the significance of the emotion elicitation protocol.",fullPaper,jv357
Computer Science,p1689,d3,7def002796277facffe02aa09e3a1bb101ec0785,c102,ACM SIGMOD Conference,Access path selection in a relational database management system,"In a high level query and data manipulation language such as SQL, requests are stated non-procedurally, without reference to access paths. This paper describes how System R chooses access paths for both simple (single relation) and complex queries (such as joins), given a user specification of desired data as a boolean expression of predicates. System R is an experimental database management system developed to carry out research on the relational model of data. System R was designed and built by members of the IBM San Jose Research Laboratory.",fullPaper,cp102
Computer Science,p1696,d3,2e1b489e9f5964eee1f09c08cf36f5377c2688c6,c21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,Introduction to Database Systems,Abstract,poster,cp21
Computer Science,p1697,d3,49ed15db181c74c7067ec01800fb5392411c868c,c53,International Conference on Learning Representations,Epidemic algorithms for replicated database maintenance,"When a database is replicated at many sites, maintaining mutual consistency among the sites in the face of updates is a significant problem. This paper describes several randomized algorithms for distributing updates and driving the replicas toward consistency. The algorithms are very simple and require few guarantees from the underlying communication system, yet they ensure that the effect of every update is eventually reflected in all replicas. The cost and performance of the algorithms are tuned by choosing appropriate distributions in the randomization step. The algorithms are closely analogous to epidemics, and the epidemiology literature aids in understanding their behavior. One of the algorithms has been implemented in the Clearinghouse servers of the Xerox Corporate Internet. solving long-standing problems of high traffic and database inconsistency.",poster,cp53
Computer Science,p1699,d3,6dc7b1da99e30592b54f2148c85e8725563011f7,c102,ACM SIGMOD Conference,Storing and querying ordered XML using a relational database system,"XML is quickly becoming the de facto standard for data exchange over the Internet. This is creating a new set of data management requirements involving XML, such as the need to store and query XML documents. Researchers have proposed using relational database systems to satisfy these requirements by devising ways to ""shred"" XML documents into relations, and translate XML queries into SQL queries over these relations. However, a key issue with such an approach, which has largely been ignored in the research literature, is how (and whether) the ordered XML data model can be efficiently supported by the unordered relational data model. This paper shows that XML's ordered data model can indeed be efficiently supported by a relational database system. This is accomplished by encoding order as a data value. We propose three order encoding methods that can be used to represent XML order in the relational data model, and also propose algorithms for translating ordered XPath expressions into SQL using these encoding methods. Finally, we report the results of an experimental study that investigates the performance of the proposed order encoding methods on a workload of ordered XML queries and updates.",fullPaper,cp102
Computer Science,p1706,d3,b62628ac06bbac998a3ab825324a41a11bc3a988,c21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,XM2VTSDB: The Extended M2VTS Database,"Keywords: vision Reference EPFL-CONF-82502 URL: ftp://ftp.idiap.ch/pub/papers/vision/avbpa99.pdf Record created on 2006-03-10, modified on 2017-05-10",poster,cp21
Computer Science,p1713,d3,d57ca29d73272e139c04f118d5c3107dfb964596,c4,Conference on Innovative Data Systems Research,Survey of graph database models,"Graph database models can be defined as those in which data structures for the schema and instances are modeled as graphs or generalizations of them, and data manipulation is expressed by graph-oriented operations and type constructors. These models took off in the eighties and early nineties alongside object-oriented models. Their influence gradually died out with the emergence of other database models, in particular geographical, spatial, semistructured, and XML. Recently, the need to manage information with graph-like nature has reestablished the relevance of this area. The main objective of this survey is to present the work that has been conducted in the area of graph database modeling, concentrating on data structures, query languages, and integrity constraints.",poster,cp4
Computer Science,p1714,d3,e0867d523f610b32267fa7ec35a510936b8b595f,j357,IEEE Transactions on Affective Computing,DISFA: A Spontaneous Facial Action Intensity Database,"Access to well-labeled recordings of facial expression is critical to progress in automated facial expression recognition. With few exceptions, publicly available databases are limited to posed facial behavior that can differ markedly in conformation, intensity, and timing from what occurs spontaneously. To meet the need for publicly available corpora of well-labeled video, we collected, ground-truthed, and prepared for distribution the Denver intensity of spontaneous facial action database. Twenty-seven young adults were video recorded by a stereo camera while they viewed video clips intended to elicit spontaneous emotion expression. Each video frame was manually coded for presence, absence, and intensity of facial action units according to the facial action unit coding system. Action units are the smallest visibly discriminable changes in facial action; they may occur individually and in combinations to comprise more molar facial expressions. To provide a baseline for use in future research, protocols and benchmarks for automated action unit intensity measurement are reported. Details are given for accessing the database for research in computer vision, machine learning, and affective and behavioral science.",fullPaper,jv357
Psychology,p1714,d10,e0867d523f610b32267fa7ec35a510936b8b595f,j357,IEEE Transactions on Affective Computing,DISFA: A Spontaneous Facial Action Intensity Database,"Access to well-labeled recordings of facial expression is critical to progress in automated facial expression recognition. With few exceptions, publicly available databases are limited to posed facial behavior that can differ markedly in conformation, intensity, and timing from what occurs spontaneously. To meet the need for publicly available corpora of well-labeled video, we collected, ground-truthed, and prepared for distribution the Denver intensity of spontaneous facial action database. Twenty-seven young adults were video recorded by a stereo camera while they viewed video clips intended to elicit spontaneous emotion expression. Each video frame was manually coded for presence, absence, and intensity of facial action units according to the facial action unit coding system. Action units are the smallest visibly discriminable changes in facial action; they may occur individually and in combinations to comprise more molar facial expressions. To provide a baseline for use in future research, protocols and benchmarks for automated action unit intensity measurement are reported. Details are given for accessing the database for research in computer vision, machine learning, and affective and behavioral science.",fullPaper,jv357
Computer Science,p1723,d3,ef12383f516840ec1ec998cd5921dfc6e197c9b2,c104,North American Chapter of the Association for Computational Linguistics,PPDB: The Paraphrase Database,"We present the 1.0 release of our paraphrase database, PPDB. Its English portion, PPDB:Eng, contains over 220 million paraphrase pairs, consisting of 73 million phrasal and 8 million lexical paraphrases, as well as 140 million paraphrase patterns, which capture many meaning-preserving syntactic transformations. The paraphrases are extracted from bilingual parallel corpora totaling over 100 million sentence pairs and over 2 billion English words. We also release PPDB:Spa, a collection of 196 million Spanish paraphrases. Each paraphrase pair in PPDB contains a set of associated scores, including paraphrase probabilities derived from the bitext data and a variety of monolingual distributional similarity scores computed from the Google n-grams and the Annotated Gigaword corpus. Our release includes pruning tools that allow users to determine their own precision/recall tradeoff.",fullPaper,cp104
Computer Science,p1724,d3,cc589c499dcf323fe4a143bbef0074c3e31f9b60,c105,International Conference on Automatic Face and Gesture Recognition,A 3D facial expression database for facial behavior research,"Traditionally, human facial expressions have been studied using either 2D static images or 2D video sequences. The 2D-based analysis is incapable of handing large pose variations. Although 3D modeling techniques have been extensively used for 3D face recognition and 3D face animation, barely any research on 3D facial expression recognition using 3D range data has been reported. A primary factor for preventing such research is the lack of a publicly available 3D facial expression database. In this paper, we present a newly developed 3D facial expression database, which includes both prototypical 3D facial expression shapes and 2D facial textures of 2,500 models from 100 subjects. This is the first attempt at making a 3D facial expression database available for the research community, with the ultimate goal of fostering the research on affective computing and increasing the general understanding of facial behavior and the fine 3D structure inherent in human facial expressions. The new database can be a valuable resource for algorithm assessment, comparison and evaluation",fullPaper,cp105
Computer Science,p1728,d3,e3bc4caca9a5115c61281acb99ab9b978edd6387,j378,ACM Transactions on Graphics,The sketchy database,"We present the Sketchy database, the first large-scale collection of sketch-photo pairs. We ask crowd workers to sketch particular photographic objects sampled from 125 categories and acquire 75,471 sketches of 12,500 objects. The Sketchy database gives us fine-grained associations between particular photos and sketches, and we use this to train cross-domain convolutional networks which embed sketches and photographs in a common feature space. We use our database as a benchmark for fine-grained retrieval and show that our learned representation significantly outperforms both hand-crafted features as well as deep features trained for sketch or photo classification. Beyond image retrieval, we believe the Sketchy database opens up new opportunities for sketch and image understanding and synthesis.",fullPaper,jv378
Computer Science,p1732,d3,c2b381b24aabf237394059fed7920cd6fd0e67b8,j4,IEEE Transactions on Knowledge and Data Engineering,Database Mining: A Performance Perspective,"The authors' perspective of database mining as the confluence of machine learning techniques and the performance emphasis of database technology is presented. Three classes of database mining problems involving classification, associations, and sequences are described. It is argued that these problems can be uniformly viewed as requiring discovery of rules embedded in massive amounts of data. A model and some basic operations for the process of rule discovery are described. It is shown how the database mining problems considered map to this model, and how they can be solved by using the basic operations proposed. An example is given of an algorithm for classification obtained by combining the basic rule discovery operations. This algorithm is efficient in discovering classification rules and has accuracy comparable to ID3, one of the best current classifiers. >",fullPaper,jv4
Computer Science,p1736,d3,b02d2cf90e5b06e3278e23e7984c29b0307c5ef3,c2,International Conference on Software Engineering,Principles of database and knowledge- base systems,Abstract,poster,cp2
Computer Science,p1739,d3,d53bcbac7ea19173e95d3bd855b998fab765737d,c5,Technical Symposium on Computer Science Education,WordNet: An Electronic Lexical Database,Abstract,poster,cp5
Computer Science,p1741,d3,7bbe0235f583b27f96c0f288043876f86795d6c2,c18,International Conference on Exploring Services Science,AVA: A large-scale database for aesthetic visual analysis,"With the ever-expanding volume of visual content available, the ability to organize and navigate such content by aesthetic preference is becoming increasingly important. While still in its nascent stage, research into computational models of aesthetic preference already shows great potential. However, to advance research, realistic, diverse and challenging databases are needed. To this end, we introduce a new large-scale database for conducting Aesthetic Visual Analysis: AVA. It contains over 250,000 images along with a rich variety of meta-data including a large number of aesthetic scores for each image, semantic labels for over 60 categories as well as labels related to photographic style. We show the advantages of AVA with respect to existing databases in terms of scale, diversity, and heterogeneity of annotations. We then describe several key insights into aesthetic preference afforded by AVA. Finally, we demonstrate, through three applications, how the large scale of AVA can be leveraged to improve performance on existing preference tasks.",poster,cp18
Computer Science,p1745,d3,1e9e0538ae54be22a515430199dab0befb9e29cb,c85,IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering,Database: The Journal of Biological Databases and Curation,"Evolution provides the unifying framework with which to understand biology. The coherent investigation of genic and genomic data often requires comparative genomics analyses based on whole-genome alignments, sets of homologous genes and other relevant datasets in order to evaluate and answer evolutionary-related questions. However, the complexity and computational requirements of producing such data are substantial: this has led to only a small number of reference resources that are used for most comparative analyses. The Ensembl comparative genomics resources are one such reference set that facilitates comprehensive and reproducible analysis of chordate genome data. Ensembl computes pairwise and multiple whole-genome alignments from which large-scale synteny, per-base conservation scores and constrained elements are obtained. Gene alignments are used to define Ensembl Protein Families, GeneTrees and homologies for both protein-coding and non-coding RNA genes. These resources are updated frequently and have a consistent informatics infrastructure and data presentation across all supported species. Specialized web-based visualizations are also available including synteny displays, collapsible gene tree plots, a gene family locator and different alignment views. The Ensembl comparative genomics infrastructure is extensively reused for the analysis of non-vertebrate species by other projects including Ensembl Genomes and Gramene and much of the information here is relevant to these projects. The consistency of the annotation across species and the focus on vertebrates makes Ensembl an ideal system to perform and support vertebrate comparative genomic analyses. We use robust software and pipelines to produce reference comparative data and make it freely available.Database URL: http://www.ensembl.org.",poster,cp85
Computer Science,p1748,d3,0b9182d502e62fb7e1ebd7e01de7523005d35677,c14,Hawaii International Conference on System Sciences,The notions of consistency and predicate locks in a database system,"In database systems, users access shared data under the assumption that the data satisfies certain consistency constraints. This paper defines the concepts of transaction, consistency and schedule and shows that consistency requires that a transaction cannot request new locks after releasing a lock. Then it is argued that a transaction needs to lock a logical rather than a physical subset of the database. These subsets may be specified by predicates. An implementation of predicate locks which satisfies the consistency condition is suggested.",poster,cp14
Computer Science,p1750,d3,0ec2a62edef364f4bef1e1b265d3d7869bb4444a,c91,International Symposium on High-Performance Computer Architecture,Database Systems: The Complete Book,"From the Publisher: 
This introduction to database systems offers a readable comprehensive approach with engaging, real-world examplesusers will learn how to successfully plan a database application before building it. The first half of the book provides in-depth coverage of databases from the point of view of the database designer, user, and application programmer, while the second half of the book provides in-depth coverage of databases from the point of view of the DBMS implementor. The first half of the book focuses on database design, database use, and implementation of database applications and database management systemsit covers the latest database standards SQL:1999, SQL/PSM, SQL/CLI, JDBC, ODL, and XML, with broader coverage of SQL than most other books. The second half of the book focuses on storage structures, query processing, and transaction managementit covers the main techniques in these areas with broader coverage of query optimization than most other books, along with advanced topics including multidimensional and bitmap indexes, distributed transactions, and information integration techniques. A professional reference for database designers, users, and application programmers.",poster,cp91
Computer Science,p1751,d3,ff2218b349f89026ffaaccdf807228fa497c04bd,c119,International Conference on Business Process Management,THE DIGITAL DATABASE FOR SCREENING MAMMOGRAPHY,Abstract,poster,cp119
Computer Science,p1757,d3,0d3e8b3a4bb5ffc8bc0527c443a99eb1438956a0,c106,International Conference on Biometrics,A face antispoofing database with diverse attacks,"Face antispoofing has now attracted intensive attention, aiming to assure the reliability of face biometrics. We notice that currently most of face antispoofing databases focus on data with little variations, which may limit the generalization performance of trained models since potential attacks in real world are probably more complex. In this paper we release a face antispoofing database which covers a diverse range of potential attack variations. Specifically, the database contains 50 genuine subjects, and fake faces are made from the high quality records of the genuine faces. Three imaging qualities are considered, namely the low quality, normal quality and high quality. Three fake face attacks are implemented, which include warped photo attack, cut photo attack and video attack. Therefore each subject contains 12 videos (3 genuine and 9 fake), and the final database contains 600 video clips. Test protocol is provided, which consists of 7 scenarios for a thorough evaluation from all possible aspects. A baseline algorithm is also given for comparison, which explores the high frequency information in the facial region to determine the liveness. We hope such a database can serve as an evaluation platform for future researches in the literature.",fullPaper,cp106
Computer Science,p1761,d3,04a10e1b25f35a9ac1a4d4344bfbdb34b253cb59,j382,International Journal on Document Analysis and Recognition,The IAM-database: an English sentence database for offline handwriting recognition,Abstract,fullPaper,jv382
Computer Science,p1766,d3,c80b987fe2d52214772f435417cb6666f60613d2,c89,Conference on Uncertainty in Artificial Intelligence,An Introduction to Database Systems,"From the Publisher: 
For over 25 years, C. J. Date's An Introduction to Database Systems has been the authoritative resource for readers interested in gaining insight into and understanding of the principles of database systems. This revision continues to provide a solid grounding in the foundations of database technology and to provide some ideas as to how the field is likely to develop in the future.. ""Readers of this book will gain a strong working knowledge of the overall structure, concepts, and objectives of database systems and will become familiar with the theoretical principles underlying the construction of such systems.",poster,cp89
Computer Science,p1767,d3,2bb7426e6ecdab0f120c89f6a324cf0c2a7266d4,c42,IEEE Working Conference on Mining Software Repositories,BrainWeb: Online Interface to a 3D MRI Simulated Brain Database,"Introduction: The increased importance of automated computer techniques for anatomical brain mapping from MR images and quantitative brain image analysis methods leads to an increased need for validation and evaluation of the effect of image acquisition parameters on performance of these procedures. Validation of analysis techniques of in-vivo acquired images is complicated due to the lack of reference data (“ground truth”). Also, optimal selection of the MR imaging parameters is difficult due to the large parameter space. BrainWeb makes available to the neuroimaging community, online on WWW, a set of realistic simulated brain MR image volumes (Simulated Brain Database, SBD) that allows the above issues to be examined in a controlled, systematic way.",poster,cp42
Computer Science,p1778,d3,a6e72ff479fb58f0b714f07b0292c612dfe4ff05,c7,International Symposium on Intelligent Data Analysis,Principles Of Database And Knowledge-Base Systems,"This book goes into the details of database conception and use, it tells you everything on relational databases. from theory to the actual used algorithms.",poster,cp7
Computer Science,p1781,d3,03cea6194b4d402f83f48382cbaf52b369d3d700,c61,"International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems",Physical Database Design for Relational Databases,Abstract,poster,cp61
Computer Science,p1787,d3,a5edce377759894482464a133cb9ec6791709eb2,c92,International Symposium on Computer Architecture,Database System Concepts,"From the Publisher: 
This acclaimed revision of a classic database systems text offers a complete background in the basics of database design, languages, and system implementation. It provides the latest information combined with real-world examples to help readers master concepts. All concepts are presented in a technically complete yet easy-to-understand style with notations kept to a minimum. A running example of a bank enterprise illustrates concepts at work. To further optimize comprehension, figures and examples, rather than proofs, portray concepts and anticipate results.",poster,cp92
Computer Science,p1788,d3,6dd37b57f3391b438fa588f98a1c2067365ae5ca,c55,Design Automation Conference,TID2008 – A database for evaluation of full-reference visual quality assessment metrics,"— In this paper, a new image database, TID2008, for evaluation of full-reference visual quality assessment metrics is described. It contains 1700 test images (25 reference images, 17 types of distortions for each reference image, 4 different levels of each type of distortion). Mean Opinion Scores (MOS) for this database have been obtained as a result of more than 800 experiments. During these tests, observers from three countries (Finland, Italy, and Ukraine) have carried out about 256000 individual human quality judgments. The obtained MOS can be used for effective testing of different visual quality metrics as well as for the design of new metrics. Using the designed image database, we have tested several known quality metrics. The designed test image database is freely available for downloading and utilization in scientific investigations.",poster,cp55
Computer Science,p1790,d3,1dd0140d51e870a713340ae30734c8438b03d1a3,c60,Network and Distributed System Security Symposium,Unit selection in a concatenative speech synthesis system using a large speech database,"One approach to the generation of natural-sounding synthesized speech waveforms is to select and concatenate units from a large speech database. Units (in the current work, phonemes) are selected to produce a natural realisation of a target phoneme sequence predicted from text which is annotated with prosodic and phonetic context information. We propose that the units in a synthesis database can be considered as a state transition network in which the state occupancy cost is the distance between a database unit and a target, and the transition cost is an estimate of the quality of concatenation of two consecutive units. This framework has many similarities to HMM-based speech recognition. A pruned Viterbi search is used to select the best units for synthesis from the database. This approach to waveform synthesis permits training from natural speech: two methods for training from speech are presented which provide weights which produce more natural speech than can be obtained by hand-tuning.",poster,cp60
Computer Science,p1792,d3,81e89f25baed869a690ffc6f93cd0306c58efe14,c8,Frontiers in Education Conference,The Mammographic Image Analysis Society digital mammogram database,"A clamp or grip for heavy duty work with twisted wire cables and the like, such as in marine and industrial uses and especially where reasonably easy application of the cable grip to the cable is important and undue bending moments on the heavy cables are to be avoided. The feature of a removable jaw is coupled with dual link bar structure for the jaws without sacrificing strength and with a considerable reduction in overall weight of the clamp as compared to presently available equipment, this being accomplished in part by elimination of a frame as such and providing the principal jaw with a slotted stabilizing arm having a sliding connection with a unique hanger bar, which latter is designed to be connected to the lift hook of a crane or the like.",poster,cp8
Computer Science,p1794,d3,5c814bc6b49f21a1b84cbc3d4dc662a24165baee,c107,Annual Haifa Experimental Systems Conference,Spanner: Google's globally-distributed database,"Spanner is Google's scalable, multi-version, globally-distributed, and synchronously-replicated database. It provides strong transactional semantics, consistent replication, and high performance reads and writes for a variety of Google's applications. I'll discuss the design and implementation of Spanner, as well as some of the lessons we have learned along the way. I'll also discuss some open challenges that we still see in building scalable distributed storage systems.",fullPaper,cp107
Computer Science,p1796,d3,2055c63fd081abf321ad0ff61987df112f8871c4,j389,ACM Transactions on Database Systems,Extending the database relational model to capture more meaning,"During the last three or four years several investigators have been exploring “semantic models” for formatted databases. The intent is to capture (in a more or less formal way) more of the meaning of the data so that database design can become more systematic and the database system itself can behave more intelligently. Two major thrusts are clear. (1) the search for meaningful units that are as small as possible—atomic semantics; (2) the search for meaningful units that are larger than the usual n-ary relation—molecular semantics. In this paper we propose extensions to the relational model to support certain atomic and molecular semantics. These extensions represent a synthesis of many ideas from the published work in semantic modeling plus the introduction of new rules for insertion, update, and deletion, as well as new algebraic operators.",fullPaper,jv389
Computer Science,p1800,d3,8162d4f3bfce2055c9a53c267af66103c3bfd167,c41,IEEE International Conference on Data Engineering,Survey on NoSQL database,"With the development of the Internet and cloud computing, there need databases to be able to store and process big data effectively, demand for high-performance when reading and writing, so the traditional relational database is facing many new challenges. Especially in large scale and high-concurrency applications, such as search engines and SNS, using the relational database to store and query dynamic user data has appeared to be inadequate. In this case, NoSQL database created. This paper describes the background, basic characteristics, data model of NoSQL. In addition, this paper classifies NoSQL databases according to the CAP theorem. Finally, the mainstream NoSQL databases are separately described in detail, and extract some properties to help enterprises to choose NoSQL.",poster,cp41
Computer Science,p1803,d3,2a75f34663a60ab1b04a0049ed1d14335129e908,c108,IEEE International Conference on Multimedia and Expo,Web-based database for facial expression analysis,"In the last decade, the research topic of automatic analysis of facial expressions has become a central topic in machine vision research. Nonetheless, there is a glaring lack of a comprehensive, readily accessible reference set of face images that could be used as a basis for benchmarks for efforts in the field. This lack of easily accessible, suitable, common testing resource forms the major impediment to comparing and extending the issues concerned with automatic facial expression analysis. In this paper, we discuss a number of issues that make the problem of creating a benchmark facial expression database difficult. We then present the MMI facial expression database, which includes more than 1500 samples of both static images and image sequences of faces in frontal and in profile view displaying various expressions of emotion, single and multiple facial muscle activation. It has been built as a Web-based direct-manipulation application, allowing easy access and easy search of the available images. This database represents the most comprehensive reference set of images for studies on facial expression analysis to date.",fullPaper,cp108
Computer Science,p1806,d3,8445ff6a398279d3179bbedae01bab7c034c31ff,c74,International Conference on Computational Linguistics,The Diverse Environments Multi-channel Acoustic Noise Database (DEMAND): A database of multichannel environmental noise recordings,"Multi-microphone arrays allow for the use of spatial filtering techniques that can greatly improve noise reduction and source separation. However, for speech and audio data, work on noise reduction or separation has focused primarily on one- or two-channel systems. Because of this, databases of multichannel environmental noise are not widely available. DEMAND (Diverse Environments Multi-channel Acoustic Noise Database) addresses this problem by providing a set of 16-channel noise files recorded in a variety of indoor and outdoor settings. The data was recorded using a planar microphone array consisting of four staggered rows, with the smallest distance between microphones being 5 cm and the largest being 21.8 cm. DEMAND is freely available under a Creative Commons license to encourage research into algorithms beyond the stereo setup.",poster,cp74
Computer Science,p1812,d3,71e9a23138a5d7c35b28bd98fd616c81719b1b7a,c97,International Conference on Computational Logic,"NoSQL Database: New Era of Databases for Big data Analytics - Classification, Characteristics and Comparison","Digital world is growing very fast and become more complex in the volume (terabyte to petabyte), variety (structured and un-structured and hybrid), velocity (high speed in growth) in nature. This refers to as ‘Big Data’ that is a global phenomenon. This is typically considered to be a data collection that has grown so large it can’t be effectively managed or exploited using conventional data management tools: e.g., classic relational database management systems (RDBMS) or conventional search engines. To handle this problem, traditional RDBMS are complemented by specifically designed a rich set of alternative DBMS; such as - NoSQL, NewSQL and Search-based systems. This paper motivation is to provide - classification, characteristics and evaluation of NoSQL databases in Big Data Analytics. This report is intended to help users, especially to the organizations to obtain an independent understanding of the strengths and weaknesses of various NoSQL database approaches to supporting applications that process huge volumes of data.",poster,cp97
Computer Science,p1814,d3,b0c5efdf2f90322784283290a052797eb073b554,j66,Data Science Journal,The World Ocean Database,"The World Ocean Database (WOD) is the most comprehensive global ocean profile-plankton database available internationally without restriction. All data are in one well-documented format and are available both on DVDs for a minimal charge and on-line without charge. The latest DVD version of the WOD is the World Ocean Database 2009 (WOD09). All data in the WOD are associated with as much metadata as possible, and every ocean data value has a quality control flag associated with it. The WOD is a product of the U.S. National Oceanographic Data Center and its co-located World Data Center for Oceanography. However, the WOD exists because of the international oceanographic data exchange that has occurred under the auspices of the Intergovernmental Oceanographic Commission (IOC) and the International Council of Science (ICSU) World Data Center (WDC) system. World Data Centers are part of the ICSU World Data System.",fullPaper,jv66
Computer Science,p1816,d3,d6cff906315e29b61afcf14bf7e6fe40f4d74ea5,c109,Computer Vision and Pattern Recognition,A large-scale hierarchical image database,Abstract,fullPaper,cp109
Computer Science,p1817,d3,5ab169aed6e76c20621a23c411f651aac423efe3,c21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,"Principles of Database and Knowledge-Base Systems, Volume II",Abstract,poster,cp21
Computer Science,p1818,d3,f322e882eec09709f7f7c2d7824722509b79f5e9,c20,ACM Conference on Economics and Computation,Principles of Database Systems,"A large part is a description of relations, their algebra and calculus, and the query languages that have been designed using these concepts. There are explanations of how the theory can be used to design good systems. A description of the optimization of queries in relation-based query languages is provided, and a chapter is devoted to the recently developed protocols for guaranteeing consistency in databases that are operated on by many processes concurrently",poster,cp20
Computer Science,p1822,d3,2485c98aa44131d1a2f7d1355b1e372f2bb148ad,c46,Ideal,The CAS-PEAL Large-Scale Chinese Face Database and Baseline Evaluations,"In this paper, we describe the acquisition and contents of a large-scale Chinese face database: the CAS-PEAL face database. The goals of creating the CAS-PEAL face database include the following: 1) providing the worldwide researchers of face recognition with different sources of variations, particularly pose, expression, accessories, and lighting (PEAL), and exhaustive ground-truth information in one uniform database; 2) advancing the state-of-the-art face recognition technologies aiming at practical applications by using off-the-shelf imaging equipment and by designing normal face variations in the database; and 3) providing a large-scale face database of Mongolian. Currently, the CAS-PEAL face database contains 99 594 images of 1040 individuals (595 males and 445 females). A total of nine cameras are mounted horizontally on an arc arm to simultaneously capture images across different poses. Each subject is asked to look straight ahead, up, and down to obtain 27 images in three shots. Five facial expressions, six accessories, and 15 lighting changes are also included in the database. A selected subset of the database (CAS-PEAL-R1, containing 30 863 images of the 1040 subjects) is available to other researchers now. We discuss the evaluation protocol based on the CAS-PEAL-R1 database and present the performance of four algorithms as a baseline to do the following: 1) elementarily assess the difficulty of the database for face recognition algorithms; 2) preference evaluation results for researchers using the database; and 3) identify the strengths and weaknesses of the commonly used algorithms.",poster,cp46
Computer Science,p1826,d3,bef5bdc4d49c9da09d125f4d86b15509ebff52cd,c31,Information Security Solutions Europe,Parallel database systems: the future of high performance database systems,Abstract,poster,cp31
Computer Science,p1831,d3,e72c49c059ac9926cc7e25d18971300f7ec2feef,c115,International Conference on Information Integration and Web-based Applications & Services,Principles of transaction-oriented database recovery,"In this paper, a terminological framework is provided for describing different transactionoriented recovery schemes for database systems in a conceptual rather than an implementation-dependent way. By introducing the terms materialized database, propagation strategy, and checkpoint, we obtain a means for classifying arbitrary implementations from a unified viewpoint. This is complemented by a classification scheme for logging techniques, which are precisely defined by using the other terms. It is shown that these criteria are related to all relevant questions such as speed and scope of recovery and amount of redundant information required. The primary purpose of this paper, however, is to establish an adequate and precise terminology for a topic in which the confusion of concepts and implementational aspects still imposes a lot of problems.",poster,cp115
Business,p1831,d9,e72c49c059ac9926cc7e25d18971300f7ec2feef,c115,International Conference on Information Integration and Web-based Applications & Services,Principles of transaction-oriented database recovery,"In this paper, a terminological framework is provided for describing different transactionoriented recovery schemes for database systems in a conceptual rather than an implementation-dependent way. By introducing the terms materialized database, propagation strategy, and checkpoint, we obtain a means for classifying arbitrary implementations from a unified viewpoint. This is complemented by a classification scheme for logging techniques, which are precisely defined by using the other terms. It is shown that these criteria are related to all relevant questions such as speed and scope of recovery and amount of redundant information required. The primary purpose of this paper, however, is to establish an adequate and precise terminology for a topic in which the confusion of concepts and implementational aspects still imposes a lot of problems.",poster,cp115
Computer Science,p1838,d3,eda424538bab229d38f03a97d3ed1731e2a2c871,c112,British Machine Vision Conference,"Semantic database modeling: survey, applications, and research issues","Most common database management systems represent information in a simple record-based format. Semantic modeling provides richer data structuring capabilities for database applications. In particular, research in this area has articulated a number of constructs that provide mechanisms for representing structurally complex interrelations among data typically arising in commercial applications. In general terms, semantic modeling complements work on knowledge representation (in artificial intelligence) and on the new generation of database models based on the object-oriented paradigm of programming languages.
This paper presents an in-depth discussion of semantic data modeling. It reviews the philosophical motivations of semantic models, including the need for high-level modeling abstractions and the reduction of semantic overloading of data type constructors. It then provides a tutorial introduction to the primary components of semantic models, which are the explicit representation of objects, attributes of and relationships among objects, type constructors for building complex types, ISA relationships, and derived schema components. Next, a survey of the prominent semantic models in the literature is presented. Further, since a broad area of research has developed around semantic modeling, a number of related topics based on these models are discussed, including data languages, graphical interfaces, theoretical investigations, and physical implementation strategies.",poster,cp112
Computer Science,p1843,d3,f82d09363e9a657749d14e97e9d6d1410331b436,c117,Very Large Data Bases Conference,Database Management Systems,"Database Management Systems Introduction Data Models Relational Model Entity-Relationship (E/R) Model Key-Value Model Entity-Relationship (E/R) Model Relationships Subclasses Constraints Converting ER to Relational Model Functional Dependencies (FD) FD Properties Closure Algorithm Superkeys & Minimal Basis Schema Normalization Decomposition Lossless-Join & Chase Algorithm Dependency Preserving Boyce-Codd Normal Form (BCNF) Third Normal Form (3NF) Relational Algebra Basic Operators Derived Operators Extended RA Syntax Data Storage Buffer Manager File Organization Page Organization Record Format Index Structures Indexing Basics Hash Tables B+ Trees Bitmaps & Bitslices Supporting SQL Operators External Sorting Selection Operator Projection Operator Join Operator Aggregation Operation Query Optimizer Annotated RA Trees Optimization Plans Cost Estimation Transaction Management Definition of Transactions The ""ACID"" Principle Write-Ahead Logging (WAL) Transaction Concurrency",poster,cp117
Computer Science,p1844,d3,c9fc7bf985fde7246f6139809cc7b019fd1ae007,c32,International Conference on Smart Data Services,The Forest Inventory and Analysis Database: Database Description and Users Manual Version 4.0 for Phase 2,"This document is based on previous documentation of the nationally standardized Forest Inventory and Analysis database (Hansen and others 1992; Woudenberg and Farrenkopf 1995; Miles and others 2001). Documentation of the structure of the Forest Inventory and Analysis database (FIADB) for Phase 2 data, as well as codes and definitions, is provided. Examples for producing population level estimates are also presented. This database provides a consistent framework for storing forest inventory data across all ownerships for the entire United States. These data are available to the public.",poster,cp32
Computer Science,p1847,d3,28e702e1a352854cf0748b9a6a9ad6679b1d4e83,c10,Americas Conference on Information Systems,Progressive skyline computation in database systems,"The skyline of a d-dimensional dataset contains the points that are not dominated by any other point on all dimensions. Skyline computation has recently received considerable attention in the database community, especially for progressive methods that can quickly return the initial results without reading the entire database. All the existing algorithms, however, have some serious shortcomings which limit their applicability in practice. In this article we develop branch-and-bound skyline (BBS), an algorithm based on nearest-neighbor search, which is I/O optimal, that is, it performs a single access only to those nodes that may contain skyline points. BBS is simple to implement and supports all types of progressive processing (e.g., user preferences, arbitrary dimensionality, etc). Furthermore, we propose several interesting variations of skyline computation, and show how BBS can be applied for their efficient processing.",poster,cp10
Computer Science,p1848,d3,7dabd56ccd524f78f0eda5073dc358f28893a45d,c99,Symposium on the Theory of Computing,The CIPIC HRTF database,"This paper describes a public-domain database of high-spatial-resolution head-related transfer functions measured at the UC Davis CIPIC Interface Laboratory and the methods used to collect the data.. Release 1.0 (see http://interface.cipic.ucdavis.edu) includes head-related impulse responses for 45 subjects at 25 different azimuths and 50 different elevations (1250 directions) at approximately 5/spl deg/ angular increments. In addition, the database contains anthropometric measurements for each subject. Statistics of anthropometric parameters and correlations between anthropometry and some temporal and spectral features of the HRTFs are reported.",poster,cp99
Computer Science,p1849,d3,b791d488eef45ef79da812f7569fc2cc83196aa5,j393,Springer Netherlands,EuroWordNet: A multilingual database with lexical semantic networks,Abstract,fullPaper,jv393
Computer Science,p1851,d3,c2eb8cbfa71ead3e30d08fa5f2712a51950c6a40,c116,International Society for Music Information Retrieval Conference,"Principles of database and knowledge-base systems, Vol. I",Abstract,poster,cp116
Computer Science,p1853,d3,695557ab15e44bee66d532d52b81a37decd87d70,c78,ACM/IEEE International Conference on Model Driven Engineering Languages and Systems,The Object-Oriented Database System Manifesto,Abstract,poster,cp78
Computer Science,p1854,d3,c3b16176728c7f785802f84df5aacffbc82ad431,c89,Conference on Uncertainty in Artificial Intelligence,Database abstractions: aggregation and generalization,"Two kinds of abstraction that are fundamentally important in database design and usage are defined. Aggregation is an abstraction which turns a relationship between objects into an aggregate object. Generalization is an abstraction which turns a class of objects into a generic object. It is suggested that all objects (individual, aggregate, generic) should be given uniform treatment in models of the real world. A new data type, called generic, is developed as a primitive for defining such models. Models defined with this primitive are structured as a set of aggregation hierarchies intersecting with a set of generalization hierarchies. Abstract objects occur at the points of intersection. This high level structure provides a discipline for the organization of relational databases. In particular this discipline allows: (i) an important class of views to be integrated and maintained; (ii) stability of data and programs under certain evolutionary changes; (iii) easier understanding of complex models and more natural query formulation; (iv) a more systematic approach to database design; (v) more optimization to be performed at lower implementation levels. The generic type is formalized by a set of invariant properties. These properties should be satisfied by all relations in a database if abstractions are to be preserved. A triggering mechanism for automatically maintaining these invariants during update operations is proposed. A simple mapping of aggregation/generalization hierarchies onto owner-coupled set structures is given.",poster,cp89
Mathematics,p1854,d6,c3b16176728c7f785802f84df5aacffbc82ad431,c89,Conference on Uncertainty in Artificial Intelligence,Database abstractions: aggregation and generalization,"Two kinds of abstraction that are fundamentally important in database design and usage are defined. Aggregation is an abstraction which turns a relationship between objects into an aggregate object. Generalization is an abstraction which turns a class of objects into a generic object. It is suggested that all objects (individual, aggregate, generic) should be given uniform treatment in models of the real world. A new data type, called generic, is developed as a primitive for defining such models. Models defined with this primitive are structured as a set of aggregation hierarchies intersecting with a set of generalization hierarchies. Abstract objects occur at the points of intersection. This high level structure provides a discipline for the organization of relational databases. In particular this discipline allows: (i) an important class of views to be integrated and maintained; (ii) stability of data and programs under certain evolutionary changes; (iii) easier understanding of complex models and more natural query formulation; (iv) a more systematic approach to database design; (v) more optimization to be performed at lower implementation levels. The generic type is formalized by a set of invariant properties. These properties should be satisfied by all relations in a database if abstractions are to be preserved. A triggering mechanism for automatically maintaining these invariants during update operations is proposed. A simple mapping of aggregation/generalization hierarchies onto owner-coupled set structures is given.",poster,cp89
Computer Science,p1855,d3,2acf7e58f0a526b957be2099c10aab693f795973,c110,Biometrics and Identity Management,Bosphorus Database for 3D Face Analysis,Abstract,fullPaper,cp110
Psychology,p1855,d10,2acf7e58f0a526b957be2099c10aab693f795973,c110,Biometrics and Identity Management,Bosphorus Database for 3D Face Analysis,Abstract,fullPaper,cp110
Computer Science,p1860,d3,aa7b1246e367b5a1154bdc877558a8a4a8474f96,c22,Grid Computing Environments,The MRC Psycholinguistic Database,"This paper describes a computerised database of psycholinguistic information. Semantic, syntactic, phonological and orthographic information about some or all of the 98,538 words in the database is accessible, by using a specially-written and very simple programming language. Word-association data are also included in the database. Some examples are given of the use of the database for selection of stimuli to be used in psycholinguistic experimentation or linguistic research.",poster,cp22
Computer Science,p1865,d3,5f47123f5d86019c79c89f75ef6b44a60039f347,j208,Multimedia tools and applications,SCface – surveillance cameras face database,Abstract,fullPaper,jv208
Computer Science,p1869,d3,d70c182a71aea05a145391b24d6bc3cdeede32a5,c111,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,Database-friendly random projections,"A classic result of Johnson and Lindenstrauss asserts that any set of n points in d-dimensional Euclidean space can be embedded into k-dimensional Euclidean space where k is logarithmic in n and independent of d so that all pairwise distances are maintained within an arbitrarily small factor. All known constructions of such embeddings involve projecting the n points onto a random k-dimensional hyperplane. We give a novel construction of the embedding, suitable for database applications, which amounts to computing a simple aggregate over k random attribute partitions.",fullPaper,cp111
Mathematics,p1869,d6,d70c182a71aea05a145391b24d6bc3cdeede32a5,c111,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,Database-friendly random projections,"A classic result of Johnson and Lindenstrauss asserts that any set of n points in d-dimensional Euclidean space can be embedded into k-dimensional Euclidean space where k is logarithmic in n and independent of d so that all pairwise distances are maintained within an arbitrarily small factor. All known constructions of such embeddings involve projecting the n points onto a random k-dimensional hyperplane. We give a novel construction of the embedding, suitable for database applications, which amounts to computing a simple aggregate over k random attribute partitions.",fullPaper,cp111
Computer Science,p1872,d3,7a350beede1b8eda39ce22bca62732bcd6677ebd,c24,International Conference on Data Technologies and Applications,Concurrency Control in Distributed Database Systems,"In this paper we survey, consolidate, and present the state of the art in distributed database concurrency control. The heart of our analysts is a decomposition of the concurrency control problem into two major subproblems: read-write and write-write synchronization. We describe a series of synchromzation techniques for solving each subproblem and show how to combine these techniques into algorithms for solving the entire concurrency control problem. Such algorithms are called ""concurrency control methods."" We describe 48 principal methods, including all practical algorithms that have appeared m the literature plus several new ones. We concentrate on the structure and correctness of concurrency control algorithms. Issues of performance are given only secondary treatment.",poster,cp24
Computer Science,p1884,d3,a6859f695e6b2bd967df7cdb8becf8c9465b472a,c81,ACM Symposium on Applied Computing,The 'Dresden Image Database' for benchmarking digital image forensics,"This paper introduces and documents a novel image database specifically built for the purpose of development and bench-marking of camera-based digital forensic techniques. More than 14,000 images of various indoor and outdoor scenes have been acquired under controlled and thus widely comparable conditions from altogether 73 digital cameras. The cameras were drawn from only 25 different models to ensure that device-specific and model-specific characteristics can be disentangled and studied separately, as validated with results in this paper. In addition, auxiliary images for the estimation of device-specific sensor noise pattern were collected for each camera. Another subset of images to study model-specific JPEG compression algorithms has been compiled for each model. The 'Dresden Image Database' will be made freely available for scientific purposes when this accompanying paper is presented. The database is intended to become a useful resource for researchers and forensic investigators. Using a standard database as a benchmark not only makes results more comparable and reproducible, but it is also more economical and avoids potential copyright and privacy issues that go along with self-sampled benchmark sets from public photo communities on the Internet.",fullPaper,cp81
Computer Science,p1892,d3,387e290dfd914cc2bfa4a8d76386cad4c0b882d0,c102,ACM SIGMOD Conference,On supporting containment queries in relational database management systems,"Virtually all proposals for querying XML include a class of query we term “containment queries”. It is also clear that in the foreseeable future, a substantial amount of XML data will be stored in relational database systems. This raises the question of how to support these containment queries. The inverted list technology that underlies much of Information Retrieval is well-suited to these queries, but should we implement this technology (a) in a separate loosely-coupled IR engine, or (b) using the native tables and query execution machinery of the RDBMS? With option (b), more than twenty years of work on RDBMS query optimization, query execution, scalability, and concurrency control and recovery immediately extend to the queries and structures that implement these new operations. But all this will be irrelevant if the performance of option (b) lags that of (a) by too much. In this paper, we explore some performance implications of both options using native implementations in two commercial relational database systems and in a special purpose inverted list engine. Our performance study shows that while RDBMSs are generally poorly suited for such queries, under conditions they can outperform an inverted list engine. Our analysis further identifies two significant causes that differentiate the performance of the IR and RDBMS implementations: the join algorithms employed and the hardware cache utilization. Our results suggest that contrary to most expectations, with some modifications, a native implementations in an RDBMS can support this class of query much more efficiently.",fullPaper,cp102
Computer Science,p1905,d3,0d74d07ee64ae04e01a1893d798a93680a9211c8,c93,ASE BigData & SocialInformatics,Conceptual Database Design: An Entity-Relationship Approach,I. CONCEPTUAL DATABASE DESIGN. 1. An Introduction to Database Design. 2. Data Modeling Concepts. 3. Methodologies for Conceptual Design. 4. View Design. 5. View Integration. 6. Improving the Quality of a Database Schema. 7. Schema Documentation and Maintenance. II. FUNCTIONAL ANALYSIS FOR DATABASE DESIGN. 1. Functional Analysis Using the Dataflow Model. 2. Joint Data and Functional Analysis. 3. Case Study. III. LOGICAL DESIGN AND DESIGN TOOLS. 1. High-Level Logical Design Using the Entity-Relationship Model. 2. Logical Design for the Relational Model. 3. Logical Design for the Network Model. 4. Logical Design for the Hierarchical Model. 5. Database Design Tools. Index. 0805302441T04062001,poster,cp93
Computer Science,p1906,d3,83a500fcc7cd98db063b73461277ac885c8fe7c3,c113,International Conference on Mobile Data Management,Towards Sensor Database Systems,Abstract,fullPaper,cp113
Computer Science,p1912,d3,06d16ecff3eeaecd86380de53d26f520068cdb9b,c24,International Conference on Data Technologies and Applications,"Database Systems: A Practical Approach to Design, Implementation and Management","This best-selling text introduces the theory behind databases in a concise yet comprehensive manner, providing database design methodology that can be used by both technical and non-technical readers. The methodology for relational Database Management Systems is presented in simple, step-by-step instructions in conjunction with a realistic worked example using three explicit phasesconceptual, logical, and physical database design. Background: Introduction to Databases; Database Environment; Database Architectures and the Web. The Relational Model and Languages: The Relational model; Relational Algebra and Relational Calculus; SQL: Data Manipulation; SQL: Data Definition; Query-By-Example (QBE). Database Analysis and Design: Database System Lifecycle; Database Analysis and the DreamHome Case Study; EntityRelationship Modeling; Enhanced EntityRelationship Modeling; Normalization; Advanced Normalization. Methodology: MethodologyConceptual Database Design; MethodologyLogical Database Design for Relational Model; MethodologyPhysical Database Design for Relational Databases; MethodologyMonitoring and Tuning the Operational System. Selected Database Issues: Security and Administration; Professional, Legal, and Ethical Issues; Transaction Management; Query Processing. Distributed DBMSs and Replication: Distributed DBMSsConcepts and Design; Distributed DBMSsAdvanced Concepts; Replication and Mobile Databases. Object DBMSs: Object-Oriented DBMSsConcepts and Design; Object-Oriented DBMSsStandards and Languages; Object-Relational DBMSs. Web and DBMSs: Web Technology and DBMSs; Semistructured Data and XML. Business Intelligence Technologies: Data Warehousing Concepts; Data Warehousing Design; OLAP; Data Mining. Appendices: Users' Requirements Specification for DreamHome Case Study; Other Case Studies; Alternative Data Modeling Notations; Summary of the Database Design Methodology for Relational Databases; Introduction to PyrrhoA Liteweight RDBMS. Web Appendices: File Organization and Storage Structures; When Is a DBMS Relational?; Commercial DBMSs: Access and Oracle; Programmatic SQL; Estimating Disk Space Requirements; Introduction to Object-Orientation; Example Web Scripts. This book is ideal for readers interested in database management or database design.",poster,cp24
Computer Science,p1915,d3,fb2896bb515ad483260f2b937202d0e7289ddd16,c114,Chinese Conference on Biometric Recognition,SDUMLA-HMT: A Multimodal Biometric Database,Abstract,fullPaper,cp114
Computer Science,p1920,d3,5bf9cebe3658cfbf7f67c0a2680c8233509aa5e4,c0,International Conference on Machine Learning,UCI Repository of Machine Learning Database,Abstract,poster,cp0
Computer Science,p1922,d3,5524cacaae93810945f1b21e77f565f6c8bdcdef,c4,Conference on Innovative Data Systems Research,Relational Cloud: a Database Service for the cloud,"This paper introduces a new transactional “database-as-a-service” (DBaaS) called Relational Cloud. A DBaaS promises to move much of the operational burden of provisioning, configuration, scaling, performance tuning, backup, privacy, and access control from the database users to the service operator, offering lower overall costs to users. Early DBaaS efforts include Amazon RDS and Microsoft SQL Azure, which are promising in terms of establishing the market need for such a service, but which do not address three important challenges: efficient multi-tenancy, elastic scalability, and database privacy. We argue that these three challenges must be overcome before outsourcing database software and management becomes attractive to many users, and cost-effective for service providers. The key technical features of Relational Cloud include: (1) a workload-aware approach to multi-tenancy that identifies the workloads that can be co-located on a database server, achieving higher consolidation and better performance than existing approaches; (2) the use of a graph-based data partitioning algorithm to achieve near-linear elastic scale-out even for complex transactional workloads; and (3) an adjustable security scheme that enables SQL queries to run over encrypted data, including ordering operations, aggregates, and joins. An underlying theme in the design of the components of Relational Cloud is the notion of workload awareness: by monitoring query patterns and data accesses, the system obtains information useful for various optimization and security functions, reducing the configuration effort for users and operators.",fullPaper,cp4
Computer Science,p1925,d3,e4b9a5ed3f838da72a8f3169a01be3268c4d3c2c,c64,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",NGA Project Strong-Motion Database,"A key component of the NGA research project was the development of a strong-motion database with improved quality and content that could be used for ground-motion research as well as for engineering practice. Development of the NGA database was executed through the Lifelines program of the PEER Center with contributions from several research organizations and many individuals in the engineering and seismological communities. Currently, the data set consists of 3551 publicly available multi-component records from 173 shallow crustal earthquakes, ranging in magnitude from 4.2 to 7.9. Each acceleration time series has been corrected and filtered, and pseudo absolute spectral acceleration at multiple damping levels has been computed for each of the 3 components of the acceleration time series. The lowest limit of usable spectral frequency was determined based on the type of filter and the filter corner frequency. For NGA model development, the two horizontal acceleration components were further rotated to form the orientation-independent measure of horizontal ground motion (GMRotI50). In addition to the ground-motion parameters, a large and comprehensive list of metadata characterizing the recording conditions of each record was also developed. NGA data have been systematically checked and reviewed by experts and NGA developers.",poster,cp64
Computer Science,p1927,d3,9e9801ef47bff55074165440a17d5122520837a2,c29,ACM-SIAM Symposium on Discrete Algorithms,Concurrency Control and Recovery in Database Systems,"This book is an introduction to the design and implementation of concurrency control and recovery mechanisms for transaction management in centralized and distributed database systems. Concurrency control and recovery have become increasingly important as businesses rely more and more heavily on their on-line data processing activities. For high performance, the system must maximize concurrency by multiprogramming transactions. But this can lead to interference between queries and updates, which concurrency control mechanisms must avoid. In addition, a satisfactory recovery system is necessary to ensure that inevitable transaction and database system failures do not corrupt the database.",poster,cp29
Computer Science,p1935,d3,523a87607f06f7ed56a0506bdb4671f76244264a,c111,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,Introducing the Global Terrorism Database,"Compared to most types of criminal violence, terrorism poses special data collection challenges. In response, there has been growing interest in open source terrorist event data bases. One of the major problems with these data bases in the past is that they have been limited to international events—those involving a national or group of nationals from one country attacking targets physically located in another country. Past research shows that domestic incidents greatly outnumber international incidents. In this paper we describe a previously unavailable open source data base that includes some 70,000 domestic and international incidents since 1970. We began the Global Terrorism Database (GTD) by computerizing data originally collected by the Pinkerton Global Intelligence Service (PGIS). Following computerization, our research team has been working for the past two years to validate and extend the data to real time. In this paper, we describe our data collection efforts, the strengths and weaknesses of open source data in general and the GTD in particular, and provide descriptive statistics on the contents of this new resource.",poster,cp111
Computer Science,p1937,d3,a5881da1c592ea11d24f90992f5b210beaa3ea73,c102,ACM SIGMOD Conference,Gigascope: a stream database for network applications,"We have developed Gigascope, a stream database for network applications including traffic analysis, intrusion detection, router configuration analysis, network research, network monitoring, and performance monitoring and debugging. Gigascope is undergoing installation at many sites within the AT&T network, including at OC48 routers, for detailed monitoring. In this paper we describe our motivation for and constraints in developing Gigascope, the Gigascope architecture and query language, and performance issues. We conclude with a discussion of stream database research problems we have found in our application.",fullPaper,cp102
Computer Science,p1947,d3,714e6dc60dba65431ecae63310da52462defd9c9,c118,International Conference on Image Analysis and Processing,The Object Database Standard: ODMG-93,Abstract,poster,cp118
Computer Science,p1948,d3,e2a5593f587970018b9e826e5d876125b374b801,c80,International Symposium on Software Engineering for Adaptive and Self-Managing Systems,Integrating compression and execution in column-oriented database systems,"Column-oriented database system architectures invite a re-evaluation of how and when data in databases is compressed. Storing data in a column-oriented fashion greatly increases the similarity of adjacent records on disk and thus opportunities for compression. The ability to compress many adjacent tuples at once lowers the per-tuple cost of compression, both in terms of CPU and space overheads.In this paper, we discuss how we extended C-Store (a column-oriented DBMS) with a compression sub-system. We show how compression schemes not traditionally used in row-oriented DBMSs can be applied to column-oriented systems. We then evaluate a set of compression schemes and show that the best scheme depends not only on the properties of the data but also on the nature of the query workload.",poster,cp80
Computer Science,p1949,d3,58efda5a28e5791adfde9ef6e330caf7b89349c6,c22,Grid Computing Environments,Providing database as a service,"We explore a novel paradigm for data management in which a third party service provider hosts ""database as a service"", providing its customers with seamless mechanisms to create, store, and access their databases at the host site. Such a model alleviates the need for organizations to purchase expensive hardware and software, deal with software upgrades, and hire professionals for administrative and maintenance tasks which are taken over by the service provider. We have developed and deployed a database service on the Internet, called NetDB2, which is in constant use. In a sense, a data management model supported by NetDB2 provides an effective mechanism for organizations to purchase data management as a service, thereby freeing them to concentrate on their core businesses. Among the primary challenges introduced by ""database as a service"" are the additional overhead of remote access to data, an infrastructure to guarantee data privacy, and user interface design for such a service. These issues are investigated. We identify data privacy as a particularly vital problem and propose alternative solutions based on data encryption. The paper is meant as a challenge for the database community to explore a rich set of research issues that arise in developing such a service.",poster,cp22
Computer Science,p1953,d3,f472655c370e4b3209f35a2834e01fb4e77ade9b,c115,International Conference on Information Integration and Web-based Applications & Services,NoSQL databases: a step to database scalability in web environment,"The paper is focused on so called NoSQL databases. In context of cloud computing, architectures and basic features of these databases are studied, particularly their horizontal scalability and concurrency model, that is mostly weaker than ACID transactions in relational SQL-like database systems. Some characteristics like a data model and querying capabilities are discussed in more detail. The paper also contains an overview of some representatives of NoSQL databases.",fullPaper,cp115
Computer Science,p1955,d3,4a30343f3230dddd96fd6f79547fef9407262dbf,c48,"Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing",A comparison of a graph database and a relational database: a data provenance perspective,"Relational databases have been around for many decades and are the database technology of choice for most traditional data-intensive storage and retrieval applications. Retrievals are usually accomplished using SQL, a declarative query language. Relational database systems are generally efficient unless the data contains many relationships requiring joins of large tables. Recently there has been much interest in data stores that do not use SQL exclusively, the so-called NoSQL movement. Examples are Google's BigTable and Facebook's Cassandra. This paper reports on a comparison of one such NoSQL graph database called Neo4j with a common relational database system, MySQL, for use as the underlying technology in the development of a software system to record and query data provenance information.",poster,cp48
Computer Science,p1956,d3,0b115b72214b501923852e6278b20401fee27f85,c51,International Conference on Engineering Education,Database Repairing and Consistent Query Answering,"Integrity constraints are semantic conditions that a database should satisfy in order to be an appropriate model of external reality. In practice, and for many reasons, a database may not satisfy those integrity constraints, and for that reason it is said to be inconsistent. However, and most likely, a large portion of the database is still semantically correct, in a sense that has to be made precise. After having provided a formal characterization of consistent data in an inconsistent database, the natural problem emerges of extracting that semantically correct data, as query answers. The consistent data in an inconsistent database is usually characterized as the data that persists across all the database instances that are consistent and minimally differ from the inconsistent instance. Those are the so-called repairs of the database. In particular, the consistent answers to a query posed to the inconsistent database are those answers that can be simultaneously obtained from all the database repairs. As expected, the notion of repair requires an adequate notion of distance that allows for the comparison of databases with respect to how much they differ from the inconsistent instance. On this basis, the minimality condition on repairs can be properly formulated. In this monograph we present and discuss these fundamental concepts, different repair semantics, algorithms for computing consistent answers to queries, and also complexity-theoretic results related to the computation of repairs and doing consistent query answering. Table of Contents: Introduction / The Notions of Repair and Consistent Answer / Tractable CQA and Query Rewriting / Logically Specifying Repairs / Decision Problems in CQA: Complexity and Algorithms / Repairs and Data Cleaning",poster,cp51
Computer Science,p1957,d3,e7ab23d011e5183db78cfea48e303210f6e57e2e,c59,Australian Software Engineering Conference,The serializability of concurrent database updates,"A sequence of interleaved user transactions in a database system may not be ser:ahzable, t e, equivalent to some sequential execution of the individual transactions Using a simple transaction model, it ~s shown that recognizing the transaction histories that are serlahzable is an NP-complete problem. Several efficiently recognizable subclasses of the class of senahzable histories are therefore introduced; most of these subclasses correspond to senahzabdity principles existing in the hterature and used in practice Two new principles that subsume all previously known ones are also proposed Necessary and sufficient conditions are given for a class of histories to be the output of an efficient history scheduler, these conditions imply that there can be no efficient scheduler that outputs all of senahzable histories, and also that all subclasses of senalizable histories studied above have an efficient scheduler Finally, it is shown how these results can be extended to far more general transaction models, to transactions with partly interpreted functions, and to distributed database systems",poster,cp59
Computer Science,p1965,d3,f86cefe05621180a48856c9f23a55bf587d7476b,c111,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,Principles of Database Systems,Abstract,fullPaper,cp111
Computer Science,p1967,d3,78b20577738126cb80ed80ad9d1ef96ed813a14a,c11,European Conference on Modelling and Simulation,System R: relational approach to database management,"System R is a database management system which provides a high level relational data interface. The systems provides a high level of data independence by isolating the end user as much as possible from underlying storage structures. The system permits definition of a variety of relational views on common underlying data. Data control features are provided, including authorization, integrity assertions, triggered transactions, a logging and recovery subsystem, and facilities for maintaining data consistency in a shared-update environment.
This paper contains a description of the overall architecture and design of the system. At the present time the system is being implemented and the design evaluated. We emphasize that System R is a vehicle for research in database architecture, and is not planned as a product.",poster,cp11
Computer Science,p1973,d3,f1af714b92372c8e606485a3982eab2f16772ad8,c92,International Symposium on Computer Architecture,The MUG facial expression database,This paper presents a new extended collection of posed and induced facial expression image sequences. All sequences were captured in a controlled laboratory environment with high resolution and no occlusions. The collection consists of two parts: The first part depicts eighty six subjects performing the six basic expressions according to the “emotion prototypes” as defined in the Investigator's Guide in the FACS manual. The second part contains the same subjects recorded while they were watching an emotion inducing video. Most of the database recordings are available to the scientific community. Beyond the emotion related annotation the database contains also manual and automatic annotation of 80 facial landmark points for a significant number of frames. The database contains sufficient material for the development and the statistical evaluation of facial expression recognition systems using posed and induced expressions.,poster,cp92
Computer Science,p1975,d3,ef47742e72bd64fb1ae5359cd6d5dd6dfad34dc8,c102,ACM SIGMOD Conference,Implementation techniques for main memory database systems,"With the availability of very large, relatively inexpensive main memories, it is becoming possible keep large databases resident in main memory In this paper we consider the changes necessary to permit a relational database system to take advantage of large amounts of main memory We evaluate AVL vs B+-tree access methods for main memory databases, hash-based query processing strategies vs sort-merge, and study recovery issues when most or all of the database fits in main memory As expected, B+-trees are the preferred storage mechanism unless more than 80--90% of the database fits in main memory A somewhat surprising result is that hash based query processing strategies are advantageous for large memory situations",fullPaper,cp102
Computer Science,p1979,d3,de31aa8e914c60189a425e174af223967e2722cc,c116,International Society for Music Information Retrieval Conference,"RWC Music Database: Popular, Classical and Jazz Music Databases","paper describes the design policy and specifications of the RWC Music Database , a music database (DB) that is available to researchers for common use and research purposes. Various com- monly available DBs have been built in other research fields and have made a significant contribution to the research in those fields. The field of musical information processing, however, has lacked a commonly available music DB. We therefore built the RWC Mu- sic Database which contains four original DBs: the Popular Music Database (100 pieces), Royalty-Free Music Database(15 pieces), Classical Music Database(50 pieces), and Jazz Music Database (50 pieces). Each consists of originally-recorded music compact discs, standard MIDI files, and text files of lyrics. These DBs are now available in Japan at a cost equal to only duplication, shipping, and handling charges (virtually for free), and we plan to make them available outside Japan. We hope that our DB will encourage further advances in musical information processing research.",fullPaper,cp116
Computer Science,p1980,d3,b069e125442995787119db3bfa71dff5d965f3aa,c34,International Conference on Data Warehousing and Knowledge Discovery,MCYT baseline corpus: a bimodal biometric database,"The current need for large multimodal databases to evaluate automatic biometric recognition systems has motivated the development of the MCYT bimodal database. The main purpose has been to consider a large scale population, with statistical significance, in a real multimodal procedure, and including several sources of variability that can be found in real environments. The acquisition process, contents and availability of the single-session baseline corpus are fully described. Some experiments showing consistency of data through the different acquisition sites and assessing data quality are also presented.",poster,cp34
Computer Science,p1984,d3,5160fb34a6719bbf8d60743d0d27db0ed5df3d2a,c117,Very Large Data Bases Conference,Foundations of Preferences in Database Systems,Abstract,fullPaper,cp117
Computer Science,p1985,d3,88b98d7b20ede342fe471b2889ace70d082e4db7,c92,International Symposium on Computer Architecture,Query by humming: musical information retrieval in an audio database,"The emergence of audio and video data types in databases will require new information retrieval methods adapted to the specific characteristics and needs of these data types. An effective and natural way of querying a musical audio database is by humming the tune of a song. In this paper, a system for querying an audio database by humming is described along with a scheme for representing the melodic information in a song as relative pitch changes. Relevant difficulties involved with tracking pitch are enumerated, along with the approach we followed, and the performance results of system indicating its effectiveness are presented.",poster,cp92
Computer Science,p1987,d3,a6089c7eca1d77dc199e462763ab13f99f85c663,c89,Conference on Uncertainty in Artificial Intelligence,Global wood density database,Abstract,poster,cp89
Computer Science,p1989,d3,be9124db35e5f451f508e095fdc25727c067a9ef,c118,International Conference on Image Analysis and Processing,UBIRIS: A Noisy Iris Image Database,Abstract,fullPaper,cp118
Computer Science,p1990,d3,c17ee327e563536f8adaf214eb6d3bde33b73dd6,j206,Computer,Chabot: Retrieval from a Relational Database of Images,"Selecting from a large, expanding collection of images requires carefully chosen search criteria. We present an approach that integrates a relational database retrieval system with a color analysis technique. The Chabot project was initiated at our university to study storage and retrieval of a vast collection of digitized images. These images are from the State of California Department of Water Resources. The goal was to integrate a relational database retrieval system with content analysis techniques that would give our querying system a better method for handling images. Our simple color analysis method, if used in conjunction with other search criteria, improves our ability to retrieve images efficiently. The best result is obtained when text-based search criteria are combined with content-based criteria and when a coarse granularity is used for content analysis. >",fullPaper,jv206
Computer Science,p1992,d3,d4a8e93f004c86267eead89edecbd332518dbf21,c36,International Conference on Information Technology Based Higher Education and Training,Database description with SDM: a semantic database model,"SDM is a high-level semantics-based database description and structuring formalism (database model) for databases. This database model is designed to capture more of the meaning of an application environment than is possible with contemporary database models. An SDM specification describes a database in terms of the kinds of entities that exist in the application environment, the classifications and groupings of those entities, and the structural interconnections among them. SDM provides a collection of high-level modeling primitives to capture the semantics of an application environment. By accommodating derived information in a database structural specification, SDM allows the same information to be viewed in several ways; this makes it possible to directly accommodate the variety of needs and processing requirements typically present in database applications. The design of the present SDM is based on our experience in using a preliminary version of it.
SDM is designed to enhance the effectiveness and usability of database systems. An SDM database description can serve as a formal specification and documentation tool for a database; it can provide a basis for supporting a variety of powerful user interface facilities, it can serve as a conceptual database model in the database design process; and, it can be used as the database model for a new kind of database management system.",poster,cp36
Computer Science,p1993,d3,666dfa8258914bf17970b20d2f7247c7c1468307,c68,Symposium on Advances in Databases and Information Systems,On the Desirability of Acyclic Database Schemes,"A class of database schemes, called acychc, was recently introduced. It is shown that this class has a number of desirable properties. In particular, several desirable properties that have been studied by other researchers m very different terms are all shown to be eqmvalent to acydicity. In addition, several equivalent charactenzauons of the class m terms of graphs and hypergraphs are given, and a smaple algorithm for determining acychclty is presented. Also given are several eqmvalent characterizations of those sets M of multivalued dependencies such that M is the set of muRlvalued dependencies that are the consequences of a given join dependency. Several characterizations for a conflict-free (in the sense of Lien) set of muluvalued dependencies are provided.",poster,cp68
Mathematics,p1993,d6,666dfa8258914bf17970b20d2f7247c7c1468307,c68,Symposium on Advances in Databases and Information Systems,On the Desirability of Acyclic Database Schemes,"A class of database schemes, called acychc, was recently introduced. It is shown that this class has a number of desirable properties. In particular, several desirable properties that have been studied by other researchers m very different terms are all shown to be eqmvalent to acydicity. In addition, several equivalent charactenzauons of the class m terms of graphs and hypergraphs are given, and a smaple algorithm for determining acychclty is presented. Also given are several eqmvalent characterizations of those sets M of multivalued dependencies such that M is the set of muRlvalued dependencies that are the consequences of a given join dependency. Several characterizations for a conflict-free (in the sense of Lien) set of muluvalued dependencies are provided.",poster,cp68
Computer Science,p1994,d3,599cc88971d2f5b375ff23b6342f17855e01791c,c47,EUROMICRO Conference on Software Engineering and Advanced Applications,The CMU Motion of Body (MoBo) Database,"In March 2001 we started to collect the CMU Motion of Body (MoBo) database. To date the database contains 25 individuals walking on a treadmill in the CMU 3D room. The subjects perform four different walk patterns: slow walk, fast walk, incline walk and walking with a ball. All subjects are captured using six high resolution color cameras distributed evenly around the treadmill. In this technical report we describe the capture setup, the collection procedure and the organization of the database.",poster,cp47
Computer Science,p1996,d3,74e10ff37b568e76c5166ce8b0eddf2abfdcbac9,c104,North American Chapter of the Association for Computational Linguistics,A common database approach for OLTP and OLAP using an in-memory column database,"When SQL and the relational data model were introduced 25 years ago as a general data management concept, enterprise software migrated quickly to this new technology. It is fair to say that SQL and the various implementations of RDBMSs became the backbone of enterprise systems. In those days. we believed that business planning, transaction processing and analytics should reside in one single system. Despite the incredible improvements in computer hardware, high-speed networks, display devices and the associated software, speed and flexibility remained an issue. The nature of RDBMSs, being organized along rows, prohibited us from providing instant analytical insight and finally led to the introduction of so-called data warehouses. This paper will question some of the fundamentals of the OLAP and OLTP separation. Based on the analysis of real customer environments and experience in some prototype implementations, a new proposal for an enterprise data management concept will be presented. In our proposal, the participants in enterprise applications, customers, orders, accounting documents, products, employees etc. will be modeled as objects and also stored and maintained as such. Despite that, the vast majority of business functions will operate on an in memory representation of their objects. Using the relational algebra and a column-based organization of data storage will allow us to revolutionize transactional applications while providing an optimal platform for analytical data processing. The unification of OLTP and OLAP workloads on a shared architecture and the reintegration of planning activities promise significant gains in application development while simplifying enterprise systems drastically. The latest trends in computer technology -- e.g. blade architecture, multiple CPUs per blade with multiple cores per CPU allow for a significant parallelization of application processes. The organization of data in columns supports the parallel use of cores for filtering and aggregation. Elements of application logic can be implemented as highly efficient stored procedures operating on columns. The vast increase in main memory combined with improvements in L1--, L2--, L3--caching, together with the high data compression rate column storage will allow us to support substantial data volumes on one single blade. Distributing data across multiple blades using a shared nothing approach provides further scalability.",poster,cp104
Computer Science,p1997,d3,5737a1f6fd8d928b88726ada916d7874afdfe0d7,c117,Very Large Data Bases Conference,Approximate String Joins in a Database (Almost) for Free,"String data is ubiquitous, and its management has taken on particular importance in the past few years. Approximate queries are very important on string data especially for more complex queries involving joins. This is due, for example, to the prevalence of typographical errors in data, and multiple conventions for recording attributes such as name and address. Commercial databases do not support approximate string joins directly, and it is a challenge to implement this functionality efficiently with user-defined functions (UDFs). In this paper, we develop a technique for building approximate string join capabilities on top of commercial databases by exploiting facilities already available in them. At the core, our technique relies on matching short substrings of length , called -grams, and taking into account both positions of individual matches and the total number of such matches. Our approach applies to both approximate full string matching and approximate substring matching, with a variety of possible edit distance functions. The approximate string match predicate, with a suitable edit distance threshold, can be mapped into a vanilla relational expression and optimized by conventional relational optimizers. We demonstrate experimentally the benefits of our technique over the direct use of UDFs, using commercial database systems and real data. To study the I/O and CPU behavior of approximate string join algorithms with variations in edit distance and -gram length, we also describe detailed experiments based on a prototype implementation.",fullPaper,cp117
Computer Science,p1998,d3,a0d251f893e043175d0a6d0e12e6e166ff523255,c17,International Conference on Statistical and Scientific Database Management,Documentation Mocap Database HDM05,"Preface In the past two decades, motion capture (mocap) systems have been developed that allow to track and record human motions at high spatial and temporal resolutions. The resulting motion capture data is used to analyze human motions in fields such as sports sciences and biometrics (person identification), and to synthesize realistic motion sequences in data-driven computer animation. Such applications require efficient methods and tools for the automatic analysis, synthesis and classification of motion capture data, which constitutes an active research area with many yet unsolved problems. Even though there is a rapidly growing corpus of motion capture data, the academic research community still lacks publicly available motion data, as supplied by [4], that can be freely used for systematic research on motion analysis, synthesis, and classification. Furthermore, a common dataset of annotated and well-documented motion capture data would be extremely valuable to the research community in view of an objective comparison and evaluation of the achieved research results. It is the objective of our motion capture database HDM05 1 to supply free motion capture data for research purposes. HDM05 contains more than tree hours of systematically recorded and well-documented motion capture data in the C3D as well as in the ASF/AMC data format. Furthermore, HDM05 contains for each of roughly 70 motion classes 10 to 50 realizations executed by various actors amounting to roughly 1, 500 motion clips. In this documentation, we give a detailed description of our mocap database HDM05. In Sect. 1, we provide some general information on motion capture data including references to various application fields. A detailed description of the database structure of HDM05 as well as of the content of each mocap file can be found in Sect. 2. We also provide several MATLAB tools comprising a parser for ASF/AMC and C3D as well as visualization, renaming and cutting tools, which are described in Sect. 3. Finally, Sect. 4 summarizes some facts on the mocap file formats ASF/AMC and C3D as used in our database. We appreciate any comments and suggestions for improvement. 1 The motion capture data has been recorded at the Hochschule der Medien (HDM) in the year 2005 under the supervision of Bernhard Eberhardt.",poster,cp17
Computer Science,p2001,d3,bef028d7d7fcb4705c24451304b089f4912920d4,c44,Italian National Conference on Sensors,Research Methods for Business Students,"How to use this book Guided tour Preface Contributors 1 The nature of business and management research and structure of this book 2 Formulating and clarifying the research topic 3 Critically reviewing the literature 4 Understanding research philosophies and approaches 5 Formulating the research design 6 Negotiating access and research ethics 7 Selecting samples 8 Using secondary data 9 Collecting primary data through observation 10 Collecting primary data using semi-structured, in-depth and group interviews 11 Collecting primary data using questionnaires 12 Analysing quantitative data 13 Analysing qualitative data 14 Writing and presenting your project report Appendices Glossary Index",poster,cp44
Computer Science,p2015,d3,a5b65b05c765025b30503a3705a13a5c490c2fde,c96,Human Language Technology - The Baltic Perspectiv,Research Methods for Business : A Skill Building Approach (5th Edition),Abstract,poster,cp96
Computer Science,p2021,d3,450d54303f7086059d0082fa3db98e29bca4c2a8,c51,International Conference on Engineering Education,Research Methods for Business Students (5th edn),""" "" I'm confused by all these different philosophies"" "" I've got my data; what do I write first?"" ΠK then, open this book to read: * Regular checklists and ...",poster,cp51
Sociology,p2021,d4,450d54303f7086059d0082fa3db98e29bca4c2a8,c51,International Conference on Engineering Education,Research Methods for Business Students (5th edn),""" "" I'm confused by all these different philosophies"" "" I've got my data; what do I write first?"" ΠK then, open this book to read: * Regular checklists and ...",poster,cp51
Computer Science,p2022,d3,672dd6422b29fa7cbc31187e91009f25c4a7cf63,j403,Journal of the Operational Research Society,Business Dynamics—Systems Thinking and Modeling for a Complex World,Abstract,fullPaper,jv403
Business,p2022,d9,672dd6422b29fa7cbc31187e91009f25c4a7cf63,j403,Journal of the Operational Research Society,Business Dynamics—Systems Thinking and Modeling for a Complex World,Abstract,fullPaper,jv403
Computer Science,p2025,d3,7a3b1e5a1e4e077a6b2c7187c761c2f8649a72a5,c79,IEEE Symposium on Computational Intelligence for Engineering Solutions,Business Research Methods,"Designed to lead readers through the entire research process from start to finish, the text is divided into three parts. Essentials of Research: this section introduces the whys of research, outlines the process and the proposal stage, and finally explores the ethics involved with all research undertakings. Research Set-Up: now readers are familiar with the research process, this section introduces the different types of research. Beginning with a chapter on qualitative research, this part also features chapters on sampling strategies, interviews, surveys, observational research, ethnography and ends with a chapter on experiments. Conducting the Research: this third section illustrates how best to conduct research, exploring the theory and practice of secondary data, measurements, field work and questionnaires. The text ends with a chapter that explains how best to write and present your research once it is complete, an essential skill for any good researcher!A fourth part of the book (which covers the Statistical Background of research) is made available to readers on a CD-ROM that comes free with every copy of ""Business Research Methods"". Designed to be used easily in conjunction with the text by those that need a more statistical perspective on research, the CD includes chapters on exploring, displaying and examining data, testing hypothesis, measuring association and an overview on multivariate analysis. Providing integrated coverage of advanced statistical methods, techniques and analysis, these chapters provide the perfect add-on for those that need it. Indexed and page numbered, it can be used easily in conjunction with the rest of the text.",poster,cp79
Computer Science,p2030,d3,fa02855ba4688a077b22607f420f5be7fd4f9139,c52,Workshop on Applied Computational Geometry,Review: Information Technology and Organizational Performance: An Integrative Model of IT Business Value,"Despite the importance to researchers, managers, and policy makers of how information technology (IT) contributes to organizational performance, there is uncertainty and debate about what we know and don't know. A review of the literature reveals that studies examining the association between information technology and organizational performance are divergent in how they conceptualize key constructs and their interrelationships. We develop a model of IT business value based on the resource-based view of the firm that integrates the various strands of research into a single framework. We apply the integrative model to synthesize what is known about IT business value and guide future research by developing propositions and suggesting a research agenda. A principal finding is that IT is valuable, but the extent and dimensions are dependent upon internal and external factors, including complementary organizational resources of the firm and its trading partners, as well as the competitive and macro environment. Our analysis provides a blueprint to guide future research and facilitate knowledge accumulation and creation concerning the organizational performance impacts of information technology.",poster,cp52
Economics,p2030,d11,fa02855ba4688a077b22607f420f5be7fd4f9139,c52,Workshop on Applied Computational Geometry,Review: Information Technology and Organizational Performance: An Integrative Model of IT Business Value,"Despite the importance to researchers, managers, and policy makers of how information technology (IT) contributes to organizational performance, there is uncertainty and debate about what we know and don't know. A review of the literature reveals that studies examining the association between information technology and organizational performance are divergent in how they conceptualize key constructs and their interrelationships. We develop a model of IT business value based on the resource-based view of the firm that integrates the various strands of research into a single framework. We apply the integrative model to synthesize what is known about IT business value and guide future research by developing propositions and suggesting a research agenda. A principal finding is that IT is valuable, but the extent and dimensions are dependent upon internal and external factors, including complementary organizational resources of the firm and its trading partners, as well as the competitive and macro environment. Our analysis provides a blueprint to guide future research and facilitate knowledge accumulation and creation concerning the organizational performance impacts of information technology.",poster,cp52
Computer Science,p2039,d3,c9253ce46a869d921a304c30ca604820089c8d9d,c83,"International Convention on Information and Communication Technology, Electronics and Microelectronics",Business process execution language for web services,"processes are rarely used. The most common scenario is to use them as a template to define executable processes. Abstract processes can be used to replace sets of rules usually expressed in natural language, which is often ambiguous. In this book, we will first focus on executable processes and come back to abstract processes in Chapter 4. 21 This material is copyright and is licensed for the sole use by Encarnacion Bellido on 20th February 2006 Via Alemania, 10, bajos, , Palma de Mallorca, Baleares, 07006",poster,cp83
Computer Science,p2042,d3,44bf936716687ce4d8d59726c43be85bdb6ef83d,j118,Review of Economics and Statistics,Measuring Business Cycles: Approximate Band-Pass Filters for Economic Time Series,Band-pass filters are useful in a wide range of economic contexts. This paper develops a set of approximate band-pass filters and illustrates their application to measuring the business-cycle component of macroeconomic activity. Detailed comparisons are made with several alternative filters commonly used for extracting business-cycle components.,fullPaper,jv118
Materials Science,p2042,d7,44bf936716687ce4d8d59726c43be85bdb6ef83d,j118,Review of Economics and Statistics,Measuring Business Cycles: Approximate Band-Pass Filters for Economic Time Series,Band-pass filters are useful in a wide range of economic contexts. This paper develops a set of approximate band-pass filters and illustrates their application to measuring the business-cycle component of macroeconomic activity. Detailed comparisons are made with several alternative filters commonly used for extracting business-cycle components.,fullPaper,jv118
Economics,p2042,d11,44bf936716687ce4d8d59726c43be85bdb6ef83d,j118,Review of Economics and Statistics,Measuring Business Cycles: Approximate Band-Pass Filters for Economic Time Series,Band-pass filters are useful in a wide range of economic contexts. This paper develops a set of approximate band-pass filters and illustrates their application to measuring the business-cycle component of macroeconomic activity. Detailed comparisons are made with several alternative filters commonly used for extracting business-cycle components.,fullPaper,jv118
Computer Science,p2043,d3,55b9b07f404e7e165a9840134f5080417b4a8f7f,c98,Vision,Business models and dynamic capabilities,Abstract,poster,cp98
Computer Science,p2048,d3,0ac62b0cd70d057c10d980d185c31944b7de6561,c110,Biometrics and Identity Management,Reengineering the Corporation: A Manifesto for Business Revolution,Abstract,poster,cp110
Computer Science,p2049,d3,1bfb01fc10cf40d307f5d9b99e9b94de3fb85685,c113,International Conference on Mobile Data Management,Web Services Business Process Execution Language Version 2.0,"Processes serve a descriptive role, with more than one use case. One such use case might be to describe the observable behavior of some or all of the services offered by an Executable Process. Another use case would be to define a process template that embodies domain-specific best practices. Such a process template would capture essential process logic in a manner compatible with a design-time representation, while excluding execution details to be completed when mapping to an Executable Process. Regardless of the specific use case and purpose, all Abstract Processes share a common syntactic base. They have different requirements for the level of opacity and restrictions on which parts of a process definition may be omitted or hidden. Tailored uses of Abstract Processes have different effects on the consistency constraints and on the semantics of that process. Some of these required constraints are not enforceable by the XML Schema. A common base specifies the features that define the syntactic universe of Abstract Processes. Given this common base, a usage profile provides the necessary specializations and semantics based on Executable WS-BPEL for a particular use of an Abstract Process. As mentioned above it is possible to use WS-BPEL to define an Executable Business Process. While a WS-BPEL Abstract Process definition is not required to be fully specified, the language effectively defines a portable execution format for business processes that rely exclusively on Web Service resources and XML data. Moreover, such processes execute and interact with their partners in a consistent way regardless of the supporting platform or programming model used by the implementation of the hosting environment. The continuity of the basic conceptual model between Abstract and Executable Processes in WSBPEL makes it possible to export and import the public aspects embodied in Abstract Processes as process or role templates while maintaining the intent and structure of the observable behavior. This applies even where private implementation aspects use platform dependent functionality.",poster,cp113
Computer Science,p2056,d3,7f17906f57732ca49396da5a0ac968f0ba6b35d4,c114,Chinese Conference on Biometric Recognition,Digital Business Strategy: Toward a Next Generation of Insights,"Over the last three decades, the prevailing view of information technology strategy has been that it is a functional-level strategy that must be aligned with the firm's chosen business strategy. Even within this so-called alignment view, business strategy directed IT strategy. During the last decade, the business infrastructure has become digital with increased interconnections among products, processes, and services. Across many firms spanning different industries and sectors, digital technologies (viewed as combinations of information, computing, communication, and connectivity technologies) are fundamentally transforming business strategies, business processes, firm capabilities, products and services, and key interfirm relationships in extended business networks. Accordingly, we argue that the time is right to rethink the role of IT strategy, from that of a functional-level strategy--aligned but essentially always subordinate to business strategy--to one that reflects a fusion between IT strategy and business strategy. This fusion is herein termed digital business strategy. 
 
We identify four key themes to guide our thinking on digital business strategy and help provide a framework to define the next generation of insights. The four themes are (1) the scope of digital business strategy, (2) the scale of digital business strategy, (3) the speed of digital business strategy, and (4) the sources of business value creation and capture in digital business strategy. After elaborating on each of these four themes, we discuss the success metrics and potential performance implications from pursuing a digital business strategy. We also show how the papers in the special issue shed light on digital strategies and offer directions to advance insights and shape future research.",poster,cp114
Business,p2056,d9,7f17906f57732ca49396da5a0ac968f0ba6b35d4,c114,Chinese Conference on Biometric Recognition,Digital Business Strategy: Toward a Next Generation of Insights,"Over the last three decades, the prevailing view of information technology strategy has been that it is a functional-level strategy that must be aligned with the firm's chosen business strategy. Even within this so-called alignment view, business strategy directed IT strategy. During the last decade, the business infrastructure has become digital with increased interconnections among products, processes, and services. Across many firms spanning different industries and sectors, digital technologies (viewed as combinations of information, computing, communication, and connectivity technologies) are fundamentally transforming business strategies, business processes, firm capabilities, products and services, and key interfirm relationships in extended business networks. Accordingly, we argue that the time is right to rethink the role of IT strategy, from that of a functional-level strategy--aligned but essentially always subordinate to business strategy--to one that reflects a fusion between IT strategy and business strategy. This fusion is herein termed digital business strategy. 
 
We identify four key themes to guide our thinking on digital business strategy and help provide a framework to define the next generation of insights. The four themes are (1) the scope of digital business strategy, (2) the scale of digital business strategy, (3) the speed of digital business strategy, and (4) the sources of business value creation and capture in digital business strategy. After elaborating on each of these four themes, we discuss the success metrics and potential performance implications from pursuing a digital business strategy. We also show how the papers in the special issue shed light on digital strategies and offer directions to advance insights and shape future research.",poster,cp114
Computer Science,p2058,d3,5b3d051dfc7b68161c0308eebee720686a4203a0,c84,EUROCON Conference,Business Model Design: An Activity System Perspective,Abstract,poster,cp84
Computer Science,p2060,d3,b6b9e1943c9e2e7ca09c28cdc50c25b3918f3735,c14,Hawaii International Conference on System Sciences,Cloud Computing - The Business Perspective,"If cloud computing (CC) is to achieve its potential, there needs to be a clear understanding of the various issues involved, both from the perspectives of the providers and the consumers of the technology. There is an equally urgent need for understanding the business-related issues surrounding CC. We interviewed several industry executives who are either involved as developers or are evaluating CC as an enterprise user. We identify the strengths, weaknesses, opportunities and threats for the industry. We also identify the various issues that will affect the different stakeholders of CC. We issue a set of recommendations for the practitioners who will provide and manage this technology. For IS researchers, we outline the different areas of research that need attention so that we are in a position to advise the industry in the years to come. Finally, we outline some of the key issues facing governmental agencies who will be involved in the regulation of cloud computing.",fullPaper,cp14
Business,p2060,d9,b6b9e1943c9e2e7ca09c28cdc50c25b3918f3735,c14,Hawaii International Conference on System Sciences,Cloud Computing - The Business Perspective,"If cloud computing (CC) is to achieve its potential, there needs to be a clear understanding of the various issues involved, both from the perspectives of the providers and the consumers of the technology. There is an equally urgent need for understanding the business-related issues surrounding CC. We interviewed several industry executives who are either involved as developers or are evaluating CC as an enterprise user. We identify the strengths, weaknesses, opportunities and threats for the industry. We also identify the various issues that will affect the different stakeholders of CC. We issue a set of recommendations for the practitioners who will provide and manage this technology. For IS researchers, we outline the different areas of research that need attention so that we are in a position to advise the industry in the years to come. Finally, we outline some of the key issues facing governmental agencies who will be involved in the regulation of cloud computing.",fullPaper,cp14
Computer Science,p2063,d3,87bbedf0efbf010515ed54086bdf31c7cb33e4a3,c113,International Conference on Mobile Data Management,The business model ontology a proposition in a design science approach,"The goal of this dissertation is to find and provide the basis for a managerial tool that allows a firm to easily express its business logic. The methodological basis for this work is design science, where the researcher builds an artifact to solve a specific problem. In this case the aim is to provide an ontology that makes it possible to explicit a firm's business model. In other words, the proposed artifact helps a firm to formally describe its value proposition, its customers, the relationship with them, the necessary intra- and inter-firm infrastructure and its profit model. Such an ontology is relevant because until now there is no model that expresses a company's global business logic from a pure business point of view. Previous models essentially take an organizational or process perspective or cover only parts of a firm's business logic. The four main pillars of the ontology, which are inspired by management science and enterprise- and processmodeling, are product, customer interface, infrastructure and finance. The ontology is validated by case studies, a panel of experts and managers. The dissertation also provides a software prototype to capture a company's business model in an information system. The last part of the thesis consists of a demonstration of the value of the ontology in business strategy and Information Systems (IS) alignment.
Structure of this thesis:
The dissertation is structured in nine parts:
Chapter 1 presents the motivations of this research, the research methodology with which the goals shall be achieved and why this dissertation present a contribution to research.
Chapter 2 investigates the origins, the term and the concept of business models. It defines what is meant by business models in this dissertation and how they are situated in the context of the firm. In addition this chapter outlines the possible uses of the business model concept.
Chapter 3 gives an overview of the research done in the field of business models and enterprise ontologies.
Chapter 4 introduces the major contribution of this dissertation: the business model ontology. In this part of the thesis the elements, attributes and relationships of the ontology are explained and described in detail.
Chapter 5 presents a case study of the Montreux Jazz Festival which's business model was captured by applying the structure and concepts of the ontology. In fact, it gives an impression of how a business model description based on the ontology looks like.
Chapter 6 shows an instantiation of the ontology into a prototype tool: the Business Model Modelling Language BM2L. This is an XML-based description language that allows to capture and describe the business model of a firm and has a large potential for further applications.
Chapter 7 is about the evaluation of the business model ontology. The evaluation builds on literature review, a set of interviews with practitioners and case studies.
Chapter 8 gives an outlook on possible future research and applications of the business model ontology. The main areas of interest are alignment of business and information technology IT/information systems IS and business model comparison. Finally, chapter 9 presents some conclusions.",poster,cp113
Computer Science,p2086,d3,46514325524c3841be258f71893cc69f4d917596,c33,Workshop on Python for High-Performance and Scientific Computing,From Strategy to Business Models and onto Tactics,Abstract,poster,cp33
Computer Science,p2087,d3,415fa442467423b083999cf76bfbb5d5cd8088c5,j62,Business & Information Systems Engineering,Digitalization: Opportunity and Challenge for the Business and Information Systems Engineering Community,Abstract,fullPaper,jv62
Computer Science,p2088,d3,88526607f05ffd9ccbf060bd6801db9d2cd6c1ee,c90,International Conference on Collaboration Technologies and Systems,"Strategic Orientation of Business Enterprises: The Construct, Dimensionality and Measurement","This paper reports the results of a research study aimed at conceptualizing and developing valid measurements of key dimensions of a strategy construct-termed Strategic Orientation of Business Enterprises. This construct is first defined by addressing four theoretical questions of scope; hierarchical level; domain; and intentions versus realizations, and then conceptualized in terms of six dimensions. Subsequently, operational indicators are developed for the six dimensions in terms of managerial perceptions across 200 business units in a field study. An evaluation of the measurement properties within an analysis of covariance structures framework indicated that the operational measures developed here largely satisfy the criteria for unidimensionality, convergent, discriminant, and predictive validity. Implications and lines of extensions are outlined.",poster,cp90
Computer Science,p2090,d3,a67f9ccf159b32317595bda9c2111859902b17a4,c112,British Machine Vision Conference,Business Process Model and Notation - BPMN,Abstract,poster,cp112
Computer Science,p2107,d3,8cabea77a299f8a070ac26e41ba3b0868f10fbf4,c119,International Conference on Business Process Management,Untrusted Business Process Monitoring and Execution Using Blockchain,Abstract,fullPaper,cp119
Computer Science,p2108,d3,19c042e7bde02e999a359564e267390652120c7c,c40,European Conference on Computer Vision,Business Process Management,Abstract,poster,cp40
Computer Science,p2111,d3,95e4caa83879fddb8bec8c6fa0e0c236261deeed,c54,ACM SIGCOMM Symposium on Software Defined Networking Research,Research Methods for Business Students (2nd edition),Abstract,poster,cp54
Computer Science,p2114,d3,34279a915f10330d5d2264f0da3e9793e4977902,c35,"International Conference on Internet of Things, Big Data and Security","Measuring Business Model Innovation: Conceptualization, Scale Development, and Proof of Performance","Business model innovation is a topic that has received much attention from academia as well as from business practice. After extensive research on the definition and conceptualization of the concept and publication of many case-based results, recently scholars have been calling for more generalizable results, large-scale investigations and greater empirical sophistication. Despite the great importance of measuring business model innovation for various purposes, a validated measurement scale is still not available. I fill this gap by systematically developing a new scale for business model innovation. I follow a rigorous scale development approach to ensure validity and reliability. Specifically, I collected two large-scale samples of 126 and 232 firms to specify and assess the scale. As a result, I provide a hierarchical three-level scale for measuring business model innovation. At the first level, 41 reflective items are provided to measure ten subconstructs of business model innovation. These can be used as formative measures of three dimensions of business model innovation at the second level, namely value creation innovation, value proposition innovation and value capture innovation. At the third level, these three dimensions form the metaconstruct of business model innovation.",poster,cp35
Computer Science,p2116,d3,36bc4fbe76e9c31785c5ee71ab046288446253e6,c19,International Conference on Conceptual Structures,Applied Business research: Qualitative and Quantitative Methods,"Chapter 1 Introduction to business research Chapter 2 Business research values and methodology Chapter 3 Problem definition and preliminary data gathering Chapter 4 Theorising and the research question Chapter 5 Research planning and management Chapter 6 Qualitative data gathering Chapter 7 Qualitative data analysis Chapter 9 Measurement of variables Chapter 10 Measurement: Scaling, reliability, validity Chapter 11 Questionnaire design Chapter 12 Sampling Chapter 13 Other research methods Chapter 14 Quantitative data analysis Chapter 15 The research report Appendix 1 A refresher on some statistical terms Glossary of terms References Statistical tables",poster,cp19
Computer Science,p2125,d3,2ac5d0dbcc10238315fde351759d1ae7386d4c9c,c97,International Conference on Computational Logic,Business Model Generation,"“Design Tomorrow’s Enterprises” Upstart technologies and lightning-fast implementation drive various new business models that are radically altering industries and commerce. Just look at how Apple’s iPod has revolutionized the music industry, how Skype has turned the telecommunications field upside down and how Grameen Bank has “popularized microlending” as a new form of finance. A business model – defined as “the rationale of how an organization creates, delivers and captures value” – can launch an entrepreneur’s new idea or readjust the thrust of an existing company. The Swatch Group is a case in point: Previously specializing in high-end watches, the Swiss company diversified into the production of the low-priced Swatch line of watches in response to Asian rivals. The Swatch Group has thrived in both the upper and lower segments of its market largely because it chose to serve these segments by using different business models. Swatch is an example of a company that manages not just one but a “portfolio” of business models. Constructing a Business Model Strategizing an innovative business model, a process which sometimes can resemble structured chaos, demands a fresh approach to creativity. The “Business Model Canvas” delivers a framework for focused brainstorming and staff inspiration. Major companies such as Ericsson, IBM and Deloitte have adopted it for their “business model generation.” The Business Model Canvas provides a flexible template for capturing the nine essential parts of a business model. The “canvas” is usually a large piece of paper with sections “In today’s climate, it’s best to assume that most business models, even successful ones, will have a short lifespan.” “A business model can best be described through nine basic building blocks that show the logic of how a company intends to make money.”",poster,cp97
Computer Science,p2130,d3,aa14067957eabe58c12cccb9c2c735bc00fde134,j67,Information systems research,Post-Adoption Variations in Usage and Value of E-Business by Organizations: Cross-Country Evidence from the Retail Industry,"Grounded in the innovation diffusion literature and the resource-based theory, this paper develops an integrative research model for assessing the diffusion and consequence of e-business at the firm level. Unlike the typical focus on adoption as found in the literature, we focus on postadoption stages, that is, actual usage and value creation. The model thus moves beyond dichotomous ""adoption versus nonadoption"" and accounts for the ""missing link""--actual usage--as a critical stage of value creation. The model links technological, organizational, and environmental factors to e-business use and value, based on which a series of hypotheses are developed. The theoretical model is tested by using structural equation modeling on a dataset of 624 firms across 10 countries in the retail industry. To probe deeper into whether e-business use and value are influenced by economic environments, two subsamples from developed and developing countries are compared. The study finds that technology competence, firm size, financial commitment, competitive pressure, and regulatory support are important antecedents of e-business use. In addition, the study finds that, while both front-end and back-end capabilities contribute to e-business value, back-end integration has a much stronger impact. While front-end functionalities are becoming commodities, e-businesses are more differentiated by back-end integration. This is consistent with the resource-based theory because back-end integration possesses the value-creating characteristics of resources (e.g., firm specific, difficult to imitate), which are strengthened by the Internet-enabled connectivity. Our study also adds an international dimension to the innovation diffusion literature, showing that careful attention must be paid to the economic and regulatory factors that may affect technology diffusion across different countries.",fullPaper,jv67
Economics,p2130,d11,aa14067957eabe58c12cccb9c2c735bc00fde134,j67,Information systems research,Post-Adoption Variations in Usage and Value of E-Business by Organizations: Cross-Country Evidence from the Retail Industry,"Grounded in the innovation diffusion literature and the resource-based theory, this paper develops an integrative research model for assessing the diffusion and consequence of e-business at the firm level. Unlike the typical focus on adoption as found in the literature, we focus on postadoption stages, that is, actual usage and value creation. The model thus moves beyond dichotomous ""adoption versus nonadoption"" and accounts for the ""missing link""--actual usage--as a critical stage of value creation. The model links technological, organizational, and environmental factors to e-business use and value, based on which a series of hypotheses are developed. The theoretical model is tested by using structural equation modeling on a dataset of 624 firms across 10 countries in the retail industry. To probe deeper into whether e-business use and value are influenced by economic environments, two subsamples from developed and developing countries are compared. The study finds that technology competence, firm size, financial commitment, competitive pressure, and regulatory support are important antecedents of e-business use. In addition, the study finds that, while both front-end and back-end capabilities contribute to e-business value, back-end integration has a much stronger impact. While front-end functionalities are becoming commodities, e-businesses are more differentiated by back-end integration. This is consistent with the resource-based theory because back-end integration possesses the value-creating characteristics of resources (e.g., firm specific, difficult to imitate), which are strengthened by the Internet-enabled connectivity. Our study also adds an international dimension to the innovation diffusion literature, showing that careful attention must be paid to the economic and regulatory factors that may affect technology diffusion across different countries.",fullPaper,jv67
Computer Science,p2134,d3,5ad634039bbc16dd58283595df8ca44a708ad02c,c55,Design Automation Conference,Essentials of Business Research Methods,"Managers increasingly must make decisions based on almost unlimited information. How can they navigate and organize this vast amount of data? Essentials of Business Research Methods provides research techniques for people who aren't data analysts. The authors offer a straightforward, hands-on approach to the vital managerial process of gathering and using data to make clear business decisions. They include such critical topics as the increasing role of online research, ethical issues, data mining, customer relationship management, and how to conduct information-gathering activities more effectively in a rapidly changing business environment. This is the only such book that includes a chapter on qualitative data analysis, and the coverage of quantitative data analysis is more extensive and much easier to understand than in other works. The book features a realistic continuing case throughout the text that enables students to see how business research information is used in the real world. It includes applied research examples in all chapters, as well as Ethical Dilemma mini - cases, and interactive Internet applications and exercises.",poster,cp55
Business,p2134,d9,5ad634039bbc16dd58283595df8ca44a708ad02c,c55,Design Automation Conference,Essentials of Business Research Methods,"Managers increasingly must make decisions based on almost unlimited information. How can they navigate and organize this vast amount of data? Essentials of Business Research Methods provides research techniques for people who aren't data analysts. The authors offer a straightforward, hands-on approach to the vital managerial process of gathering and using data to make clear business decisions. They include such critical topics as the increasing role of online research, ethical issues, data mining, customer relationship management, and how to conduct information-gathering activities more effectively in a rapidly changing business environment. This is the only such book that includes a chapter on qualitative data analysis, and the coverage of quantitative data analysis is more extensive and much easier to understand than in other works. The book features a realistic continuing case throughout the text that enables students to see how business research information is used in the real world. It includes applied research examples in all chapters, as well as Ethical Dilemma mini - cases, and interactive Internet applications and exercises.",poster,cp55
Computer Science,p2135,d3,f572bcaa97e36d79e0cd01fb18dadb2f58eebebd,j414,Peer-to-Peer Networking and Applications,The IoT electric business model: Using blockchain technology for the internet of things,Abstract,fullPaper,jv414
Computer Science,p2142,d3,8558d7c06f7fde090d2bf3e860f0fd1ef41f09fd,c24,International Conference on Data Technologies and Applications,Business Model Evolution: In Search of Dynamic Consistency,Abstract,poster,cp24
Computer Science,p2150,d3,092b82b043985b053abc3d385794bab77ccf2af4,j62,Business & Information Systems Engineering,Blockchain Technology in Business and Information Systems Research,Abstract,fullPaper,jv62
Computer Science,p2157,d3,ee6871ee034d0b56bc098e64393bfbdbf5581b99,c21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,Factors That Influence the Social Dimension of Alignment Between Business and Information Technology Objectives,"The establishment of strong alignment between information technology (IT) and organizational objectives has consistently been reported as one of the key concerns of information systems managers. This paper presents findings from a study which investigated the influence of several factors on the social dimension of alignment within 10 business units in the Canadian life insurance industry. The social dimension of alignment refers 'Lynda M. Applegate was the accepting senior editor for this paper. to the state in which business and IT executives understand and are committed to the business and IT mission, objectives, and plans. The research model included four factors that would potentially influence alignment: (1) shared domain knowledge between business and IT executives, (2) IT implementation success, (3) communication between business and IT executives, and (4) connections between business and IT planning processes. The outcome, alignment, was operationalized in two ways: the degree of mutual understanding of current objectives (shortterm alignment) and the congruence of IT vision (long-term alignment) between business and IT executives. A total of 57 semi-structured interviews were held with 45 informants. Written business and IT strategic plans, minutes from IT steering committee meetings, and other strategy documents were collected and analyzed from each of the 10 business units. All four factors in the model (shared domain knowledge, IT implementation success, communication between business and IT executives, and connections between business and IT planning) were found to influence short-term alignment. Only shared domain knowledge was found to influence long-term alignment A new factor, strategic business plans, was found to influence both short and long-term alignment. MIS Quarterly Vol. 24 No. 1, pp. 81-113/March 2000 81 I~~~~~fl This content downloaded from 207.46.13.115 on Sat, 08 Oct 2016 04:53:48 UTC All use subject to http://about.jstor.org/terms Reich & Benbasat/Alignment Between Business and IT Objectives The findings suggest that both practitioners and researchers should direct significant effort toward understanding shared domain knowledge, the factor which had the strongest influence on the alignment between IT and business executives. There is also a call for further research into the creation of an IT vision.",poster,cp21
Business,p2157,d9,ee6871ee034d0b56bc098e64393bfbdbf5581b99,c21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,Factors That Influence the Social Dimension of Alignment Between Business and Information Technology Objectives,"The establishment of strong alignment between information technology (IT) and organizational objectives has consistently been reported as one of the key concerns of information systems managers. This paper presents findings from a study which investigated the influence of several factors on the social dimension of alignment within 10 business units in the Canadian life insurance industry. The social dimension of alignment refers 'Lynda M. Applegate was the accepting senior editor for this paper. to the state in which business and IT executives understand and are committed to the business and IT mission, objectives, and plans. The research model included four factors that would potentially influence alignment: (1) shared domain knowledge between business and IT executives, (2) IT implementation success, (3) communication between business and IT executives, and (4) connections between business and IT planning processes. The outcome, alignment, was operationalized in two ways: the degree of mutual understanding of current objectives (shortterm alignment) and the congruence of IT vision (long-term alignment) between business and IT executives. A total of 57 semi-structured interviews were held with 45 informants. Written business and IT strategic plans, minutes from IT steering committee meetings, and other strategy documents were collected and analyzed from each of the 10 business units. All four factors in the model (shared domain knowledge, IT implementation success, communication between business and IT executives, and connections between business and IT planning) were found to influence short-term alignment. Only shared domain knowledge was found to influence long-term alignment A new factor, strategic business plans, was found to influence both short and long-term alignment. MIS Quarterly Vol. 24 No. 1, pp. 81-113/March 2000 81 I~~~~~fl This content downloaded from 207.46.13.115 on Sat, 08 Oct 2016 04:53:48 UTC All use subject to http://about.jstor.org/terms Reich & Benbasat/Alignment Between Business and IT Objectives The findings suggest that both practitioners and researchers should direct significant effort toward understanding shared domain knowledge, the factor which had the strongest influence on the alignment between IT and business executives. There is also a call for further research into the creation of an IT vision.",poster,cp21
Computer Science,p2169,d3,685fc606ed5cf144b2a1d5e63e2ba6a3a5d733ff,c37,International Workshop on the Semantic Web,Business Model Design and the Performance of Entrepreneurial Firms,"We focus on the design of an organization's set of boundary-spanning transactions---business model design---and ask how business model design affects the performance of entrepreneurial firms. By extending and integrating theoretical perspectives that inform the study of boundary-spanning organization design, we propose hypotheses about the impact of efficiency-centered and novelty-centered business model design on the performance of entrepreneurial firms. To test these hypotheses, we developed and analyzed a unique data set of 190 entrepreneurial firms that were publicly listed on U.S. and European stock exchanges. The empirical results show that novelty-centered business model design matters to the performance of entrepreneurial firms. Our analysis also shows that this positive relationship is remarkably stable across time, even under varying environmental regimes. Additionally, we find indications of potential diseconomies of scope in design; that is, entrepreneurs' attempts to incorporate both efficiency-and novelty-centered design elements into their business models may be counterproductive.",poster,cp37
Business,p2169,d9,685fc606ed5cf144b2a1d5e63e2ba6a3a5d733ff,c37,International Workshop on the Semantic Web,Business Model Design and the Performance of Entrepreneurial Firms,"We focus on the design of an organization's set of boundary-spanning transactions---business model design---and ask how business model design affects the performance of entrepreneurial firms. By extending and integrating theoretical perspectives that inform the study of boundary-spanning organization design, we propose hypotheses about the impact of efficiency-centered and novelty-centered business model design on the performance of entrepreneurial firms. To test these hypotheses, we developed and analyzed a unique data set of 190 entrepreneurial firms that were publicly listed on U.S. and European stock exchanges. The empirical results show that novelty-centered business model design matters to the performance of entrepreneurial firms. Our analysis also shows that this positive relationship is remarkably stable across time, even under varying environmental regimes. Additionally, we find indications of potential diseconomies of scope in design; that is, entrepreneurs' attempts to incorporate both efficiency-and novelty-centered design elements into their business models may be counterproductive.",poster,cp37
Computer Science,p2175,d3,7a001216124a7927c1bf1345bfbc85cfe92b4927,c46,Ideal,"Productivity, Business Profitability, and Consumer Surplus: Three Different Measures of Information Technology Value","The business value of information technology (IT) has been debated for a number of years. While some authors have attributed large productivity improvements and substantial consumer benefits to IT, others report that IT has not had any bottom line impact on business profitability. This paper focuses on the fact that while productivity, consumer value, and bush ness profitability are related, they are ultimate1 Allen Lee was the accepting senior editor for this paper. 2 An earlier version of this paper appears in the",poster,cp46
Economics,p2175,d11,7a001216124a7927c1bf1345bfbc85cfe92b4927,c46,Ideal,"Productivity, Business Profitability, and Consumer Surplus: Three Different Measures of Information Technology Value","The business value of information technology (IT) has been debated for a number of years. While some authors have attributed large productivity improvements and substantial consumer benefits to IT, others report that IT has not had any bottom line impact on business profitability. This paper focuses on the fact that while productivity, consumer value, and bush ness profitability are related, they are ultimate1 Allen Lee was the accepting senior editor for this paper. 2 An earlier version of this paper appears in the",poster,cp46
Computer Science,p2178,d3,998f11fc99c41d7ffa987af3dac8e5322253ae0d,c120,SIGSAND-Europe Symposium,Business Models of Internet of Things,Abstract,fullPaper,cp120
Business,p2178,d9,998f11fc99c41d7ffa987af3dac8e5322253ae0d,c120,SIGSAND-Europe Symposium,Business Models of Internet of Things,Abstract,fullPaper,cp120
Computer Science,p2181,d3,d093b4efc97458f171a476578ca0719ef0beb7c1,j67,Information systems research,Introduction to the Special Issue - Social Media and Business Transformation: A Framework for Research,"Social media are fundamentally changing the way we communicate, collaborate, consume, and create. They represent one of the most transformative impacts of information technology on business, both within and outside firm boundaries. This special issue was designed to stimulate innovative investigations of the relationship between social media and business transformation. In this paper we outline a broad research agenda for understanding the relationships among social media, business, and society. We place the papers comprising the special issue within this research framework and identify areas where further research is needed. We hope that the flexible framework we outline will help guide future research and develop a cumulative research tradition in this area.",fullPaper,jv67
Sociology,p2181,d4,d093b4efc97458f171a476578ca0719ef0beb7c1,j67,Information systems research,Introduction to the Special Issue - Social Media and Business Transformation: A Framework for Research,"Social media are fundamentally changing the way we communicate, collaborate, consume, and create. They represent one of the most transformative impacts of information technology on business, both within and outside firm boundaries. This special issue was designed to stimulate innovative investigations of the relationship between social media and business transformation. In this paper we outline a broad research agenda for understanding the relationships among social media, business, and society. We place the papers comprising the special issue within this research framework and identify areas where further research is needed. We hope that the flexible framework we outline will help guide future research and develop a cumulative research tradition in this area.",fullPaper,jv67
Computer Science,p2184,d3,66c9bb3981d82bcaa2108806d1eeb4fc750b1031,c83,"International Convention on Information and Communication Technology, Electronics and Microelectronics",Business Process Management: A Comprehensive Survey,"Business Process Management (BPM) research resulted in a plethora of methods, techniques, and tools to support the design, enactment, management, and analysis of operational business processes. This survey aims to structure these results and provide an overview of the state-of-the-art in BPM. In BPM the concept of a process model is fundamental. Process models may be used to configure information systems, but may also be used to analyze, understand, and improve the processes they describe. Hence, the introduction of BPM technology has both managerial and technical ramifications and may enable significant productivity improvements, cost savings, and flow-time reductions. The practical relevance of BPM and rapid developments over the last decade justify a comprehensive survey.",poster,cp83
Computer Science,p2191,d3,94c24e9bf176ca3598c29c70f35ecba844f344c3,c119,International Conference on Business Process Management,Business Process Management: A Survey,Abstract,fullPaper,cp119
Business,p2191,d9,94c24e9bf176ca3598c29c70f35ecba844f344c3,c119,International Conference on Business Process Management,Business Process Management: A Survey,Abstract,fullPaper,cp119
Computer Science,p2205,d3,441125787767c91af8c58f86e4629dc019f0c9cb,c119,International Conference on Business Process Management,Business Process Management Architectures,Abstract,fullPaper,cp119
Computer Science,p2223,d3,59730bdee2651cc22557925e885d5bcce727fc65,c76,Group,The Scope and Evolution of Business Process Management,Abstract,poster,cp76
Business,p2223,d9,59730bdee2651cc22557925e885d5bcce727fc65,c76,Group,The Scope and Evolution of Business Process Management,Abstract,poster,cp76
Computer Science,p2227,d3,e0d78af45e63d5b9cc032844d4a5aa24d7c3a7d8,c119,International Conference on Business Process Management,Optimized Execution of Business Processes on Blockchain,Abstract,fullPaper,cp119
Computer Science,p2228,d3,4192c1818ed948316db08c534195890733e6967d,j67,Information systems research,"Alignment Between Business and IS Strategies: A Study of Prospectors, Analyzers, and Defenders","Alignment between business strategy and IS strategy is widely believed to improve business performance. This paper examines the impact of alignment on perceived business performance using Miles and Snow's popular classification of Defender, Analyzer, and Prospector business strategies. A priori theoretical profiles for these business strategies are developed using Venkatraman's (1989a) measure of business strategy. Theoretical profiles for IS strategies are developed in terms of four types of systems--operational support systems, market information systems, strategic decision-support systems, and interorganizational systems. Empirical data from two multirespondent surveys of 164 and 62 companies, respectively, are analyzed. Results indicate that alignment affects perceived business performance but only insome organizations. Alignment seems to influence overall business success in Prospectors and Analyzers but not in Defenders. Implications for future research and practice are discussed.",fullPaper,jv67
Business,p2228,d9,4192c1818ed948316db08c534195890733e6967d,j67,Information systems research,"Alignment Between Business and IS Strategies: A Study of Prospectors, Analyzers, and Defenders","Alignment between business strategy and IS strategy is widely believed to improve business performance. This paper examines the impact of alignment on perceived business performance using Miles and Snow's popular classification of Defender, Analyzer, and Prospector business strategies. A priori theoretical profiles for these business strategies are developed using Venkatraman's (1989a) measure of business strategy. Theoretical profiles for IS strategies are developed in terms of four types of systems--operational support systems, market information systems, strategic decision-support systems, and interorganizational systems. Empirical data from two multirespondent surveys of 164 and 62 companies, respectively, are analyzed. Results indicate that alignment affects perceived business performance but only insome organizations. Alignment seems to influence overall business success in Prospectors and Analyzers but not in Defenders. Implications for future research and practice are discussed.",fullPaper,jv67
Computer Science,p2238,d3,8604dc534ca4e75dcdb25bbbf8dc6fe64ae1a523,c119,International Conference on Business Process Management,Research Methods in Business Studies,Provisional TOC PART ONE: CHALLENGES AND AMBIGUITIES OF BUSINESS RESEARCH 1. Introduction 2. Research in business PART TWO: THE RESEARCH PROCESS 3. The Process Perspective 4. Research Problems 5. Research Design 6. Measurements 7. Data sources 8. Data Collection 9. Sampling in empirical research 10. Preparation and analysis of data PART THREE: IMPLEMENTATION 11. Quantitative data analysis 12. Qualitative Data analysis 13. Writing the final report,poster,cp119
Computer Science,p2240,d3,b456dcabed7fb251bd34167f27b00ed35978d03e,j416,Journal of Small Business and Entrepreneurship,Cloud computing technology: improving small business performance using the Internet,"Cloud computing technology (CCT) is a revolutionary new way of leveraging the power of the Internet to provide software and infrastructure solutions to businesses around the world. 2017 is predicted to be a major breakout year for this technology, with many organizations small and large making a shift to this platform. Employing this technology empowers communication between companies and has the potential to generate significant financial and operational benefits for small businesses. The key objective of this article is to propose a conceptual model for successful implementation of CCT in small businesses. This article also examines some of the potential benefits of CCT as applied to small businesses and explores implementation challenges that can be expected. Furthermore, this study reviews key attributes of successful CCT and illustrates some of the routes that might be taken to implement this technology in small businesses. Finally, this article examines the specific application of CCT to small businesses, highlights developing technologies and trends, investigates the deployment of CCT in different types of small businesses, and provides a case example of success.",fullPaper,jv416
Computer Science,p2243,d3,08e42af44607ce507547f0f6adda06b3ac656fa0,c114,Chinese Conference on Biometric Recognition,Business Research Methods and Statistics Using SPSS,"PART ONE: GENERAL ORIENTATION TO RESEARCH IN BUSINESS AND MANAGEMENT Research, Statistics and Business Decisions Contrasting Philosophies and Approaches to Research Ethical Issues in Research Selecting the Topic and Conducting a Literature Review Theory, Problem Definition, Frameworks and Research Design PART TWO: ENTERING, DESCRIBING AND OBTAINING DATA How To Enter Data into SPSS and Undertake Initial Screening Describing and Presenting your Data Normal Distribution, Probability and Statistical Significance Sampling Issues Hypothesis Formation, Types of Error and Estimation Power and Effect Size PART THREE: STATISTICALLY ANALYSING DATA Hypothesis Testing for Differences between Means and Proportions Analysis of Variance Techniques (ANOVA) Chi Square Methods of Correlation Testing Hypotheses of Relationships Prediction and Regression Reliability and Validity Factor Analysis PART FOUR: SURVEY METHODS FOR RESEARCH IN BUSINESS AND MANAGEMENT Attitude Questionnaires and Measurement Structured Interview and Questionnaires Surveys PART FIVE: REPORTING AND PRESENTING RESEARCH Writing Up and Communicating Research",poster,cp114
Computer Science,p2247,d3,ab1c2525ff960654f7045c1f67e239fdaabe1d79,c60,Network and Distributed System Security Symposium,Modern methods for business research,"Contents: Preface. G.A. Marcoulides, Applied Generalizability Theory Models. K.M. Schmidt McCollam, Latent Trait and Latent Class Models. M.E. Lunz, J.M. Linacre, Measurement Designs Using Multifacet Rasch Modeling. Z. Drezner, T. Drezner, Applied Location Theory Models. A.C. Soteriou, S.A. Zenios, Data Envelopment Analysis: An Introduction and an Application to Bank Branch Performance Assessment. S. Salhi, Heuristic Search Methods. R.H. Heck, Factor Analysis: Exploratory and Confirmatory Approaches. S.L. Hershberger, Dynamic Factor Analysis. E.E. Ridgon, Structural Equation Modeling. W.W. Chin, The Partial Least Squares Approach for Structural Equation Modeling. D. Kaplan, Methods for Multilevel Data Analysis. J.J. McArdle, Modeling Longitudinal Data by Latent Growth Curve Methods. P.K. Wood, Structural and Configural Models for Longitudinal Categorical Data.",poster,cp60
Computer Science,p2248,d3,ae8e00f98427eead226b2c75a0ee562f2efd32e3,j62,Business & Information Systems Engineering,Self-Service Business Intelligence,Abstract,fullPaper,jv62
Political Science,p2248,d15,ae8e00f98427eead226b2c75a0ee562f2efd32e3,j62,Business & Information Systems Engineering,Self-Service Business Intelligence,Abstract,fullPaper,jv62
Computer Science,p2251,d3,e536ba7e37bedc07d5cf665738d53616d758c856,j12,Communications of the ACM,An overview of business intelligence technology,BI technologies are essential to running today's businesses and this technology is going through sea changes.,fullPaper,jv12
Business,p2251,d9,e536ba7e37bedc07d5cf665738d53616d758c856,j12,Communications of the ACM,An overview of business intelligence technology,BI technologies are essential to running today's businesses and this technology is going through sea changes.,fullPaper,jv12
Computer Science,p2252,d3,b9e1e264e1c650ffa24271f0afe18b445c9ed27a,j423,European Journal of Information Systems,Developing a unified framework of the business model concept,Abstract,fullPaper,jv423
Computer Science,p2260,d3,704401579cde72ebebba80c3c483746f1800f7c3,j303,Journal of the AIS,Business Value of IT: An Essay on Expanding Research Directions to Keep up with the Times,Rajiv Kohli Mason School of Business The College of William and Mary rajiv.kohli@mason.wm.edu,fullPaper,jv303
Economics,p2260,d11,704401579cde72ebebba80c3c483746f1800f7c3,j303,Journal of the AIS,Business Value of IT: An Essay on Expanding Research Directions to Keep up with the Times,Rajiv Kohli Mason School of Business The College of William and Mary rajiv.kohli@mason.wm.edu,fullPaper,jv303
Computer Science,p2262,d3,782a0d779fc9246b89b48d8ba4437eeb553d88da,j67,Information systems research,"Business Strategic Orientation, Information Systems Strategic Orientation, and Strategic Alignment","Information systems strategic alignment---the fit between business strategic orientation and information systems (IS) strategic orientation---is an important concept. This study measured business strategic orientation, IS strategic orientation, and IS strategic alignment, and investigated their implications for perceived IS effectiveness and business performance. Analyses of data gathered in a mail survey of North American financial services and manufacturing firms indicated that 1) business strategic orientation, IS strategic orientation, and IS strategic alignment are modeled best by utilizing holistic, ‘systems’ approaches instead of dimension-specific, ‘bivariate’ approaches, 2) three generic IS strategic orientations can be detected, 3) user information satisfaction does not capture important strategic aspects of IS effectiveness, 4) IS strategic alignment is a better predictor of IS effectiveness than is strategic orientation, and 5) business strategic orientation, IS strategic alignment, and IS effectiveness have positive impacts on business performance.",fullPaper,jv67
Business,p2262,d9,782a0d779fc9246b89b48d8ba4437eeb553d88da,j67,Information systems research,"Business Strategic Orientation, Information Systems Strategic Orientation, and Strategic Alignment","Information systems strategic alignment---the fit between business strategic orientation and information systems (IS) strategic orientation---is an important concept. This study measured business strategic orientation, IS strategic orientation, and IS strategic alignment, and investigated their implications for perceived IS effectiveness and business performance. Analyses of data gathered in a mail survey of North American financial services and manufacturing firms indicated that 1) business strategic orientation, IS strategic orientation, and IS strategic alignment are modeled best by utilizing holistic, ‘systems’ approaches instead of dimension-specific, ‘bivariate’ approaches, 2) three generic IS strategic orientations can be detected, 3) user information satisfaction does not capture important strategic aspects of IS effectiveness, 4) IS strategic alignment is a better predictor of IS effectiveness than is strategic orientation, and 5) business strategic orientation, IS strategic alignment, and IS effectiveness have positive impacts on business performance.",fullPaper,jv67
Computer Science,p2270,d3,acfcb78eb995e1755e08cfd39e863e8dd7f28587,j67,Information systems research,Information Technologies and Business Value: An Analytic and Empirical Investigation,"An important management question today is whether the anticipated economic benefits of Information Technology IT are being realized. In this paper, we consider this problem to be measurement related, and propose and test a new process-oriented methodology for ex post measurement to audit IT impacts on a strategic business unit SBU or profit center's performance. The IT impacts on a given SBU are measured relative to a group of SBUs in the industry. The methodology involves a two-stage analysis of intermediate and higher level output variables that also accounts for industry and economy wide exogenous variables for tracing and measuring IT contributions. The data for testing the proposed model were obtained from SBUs in the manufacturing sector. Our results show significant positive impacts of IT at the intermediate level. The theoretical contribution of the study is a methodology that attempts to circumvent some of the measurement problems in this domain. It also provides a practical management tool to address the question of why or why not certain IT impacts occur. Additionally, through its process orientation, the suggested approach highlights key variables that may require managerial attention and subsequent action.",fullPaper,jv67
Economics,p2270,d11,acfcb78eb995e1755e08cfd39e863e8dd7f28587,j67,Information systems research,Information Technologies and Business Value: An Analytic and Empirical Investigation,"An important management question today is whether the anticipated economic benefits of Information Technology IT are being realized. In this paper, we consider this problem to be measurement related, and propose and test a new process-oriented methodology for ex post measurement to audit IT impacts on a strategic business unit SBU or profit center's performance. The IT impacts on a given SBU are measured relative to a group of SBUs in the industry. The methodology involves a two-stage analysis of intermediate and higher level output variables that also accounts for industry and economy wide exogenous variables for tracing and measuring IT contributions. The data for testing the proposed model were obtained from SBUs in the manufacturing sector. Our results show significant positive impacts of IT at the intermediate level. The theoretical contribution of the study is a methodology that attempts to circumvent some of the measurement problems in this domain. It also provides a practical management tool to address the question of why or why not certain IT impacts occur. Additionally, through its process orientation, the suggested approach highlights key variables that may require managerial attention and subsequent action.",fullPaper,jv67
Computer Science,p2278,d3,ce04e0daac53c7f905b5a0a43d04ea2e43fd5822,c30,PS,Business models: A challenging agenda,"Most research on business models lies in the literature on strategy and competitive advantage and focuses on their role as descriptors of actual phenomenon, often by reference to taxonomic categories. In this article, we explore how business models can be seen as a set of cognitive configurations that can be manipulable in the minds of managers (and academics). By proposing a typology of business models that emphasizes the connecting of traditional value chain descriptors with how customers are identified and satisfied, and how the firm monetizes its value, we explore how business model configurations can extend current work on cognitive categorization and open up new possibilities for organization research.",poster,cp30
Sociology,p2278,d4,ce04e0daac53c7f905b5a0a43d04ea2e43fd5822,c30,PS,Business models: A challenging agenda,"Most research on business models lies in the literature on strategy and competitive advantage and focuses on their role as descriptors of actual phenomenon, often by reference to taxonomic categories. In this article, we explore how business models can be seen as a set of cognitive configurations that can be manipulable in the minds of managers (and academics). By proposing a typology of business models that emphasizes the connecting of traditional value chain descriptors with how customers are identified and satisfied, and how the firm monetizes its value, we explore how business model configurations can extend current work on cognitive categorization and open up new possibilities for organization research.",poster,cp30
Business,p2278,d9,ce04e0daac53c7f905b5a0a43d04ea2e43fd5822,c30,PS,Business models: A challenging agenda,"Most research on business models lies in the literature on strategy and competitive advantage and focuses on their role as descriptors of actual phenomenon, often by reference to taxonomic categories. In this article, we explore how business models can be seen as a set of cognitive configurations that can be manipulable in the minds of managers (and academics). By proposing a typology of business models that emphasizes the connecting of traditional value chain descriptors with how customers are identified and satisfied, and how the firm monetizes its value, we explore how business model configurations can extend current work on cognitive categorization and open up new possibilities for organization research.",poster,cp30
Computer Science,p2280,d3,1fd54646ad1749c7532c3ee6647c2412e46d111f,j427,Informatik-Spektrum,Business Intelligence,Abstract,fullPaper,jv427
Computer Science,p2283,d3,8c3297b84b09f8d256bf941a2ebe1c8696065023,c97,International Conference on Computational Logic,What is Business Process Management?,Abstract,poster,cp97
Computer Science,p2291,d3,8b8c3143eb43af01fadc79ede7e3f051a5195e1b,j423,European Journal of Information Systems,Electronic business adoption by European firms: a cross-country assessment of the facilitators and inhibitors,Abstract,fullPaper,jv423
Business,p2291,d9,8b8c3143eb43af01fadc79ede7e3f051a5195e1b,j423,European Journal of Information Systems,Electronic business adoption by European firms: a cross-country assessment of the facilitators and inhibitors,Abstract,fullPaper,jv423
Political Science,p2291,d15,8b8c3143eb43af01fadc79ede7e3f051a5195e1b,j423,European Journal of Information Systems,Electronic business adoption by European firms: a cross-country assessment of the facilitators and inhibitors,Abstract,fullPaper,jv423
Computer Science,p2301,d3,01325a01eb0728573724050aa687d1d787e4b5c5,j424,Business Process Management Journal,Maturity models in business process management,"Purpose – Maturity models are a prospering approach to improving a company's processes and business process management (BPM) capabilities. In fact, the number of corresponding maturity models is so high that practitioners and scholars run the risk of losing track. This paper therefore aims to provide a systematic in‐depth review of BPM maturity models.Design/methodology/approach – The paper follows the accepted research process for literature reviews. It analyzes a sample of ten BPM maturity models according to a framework of general design principles. The framework particularly focuses on the applicability and usefulness of maturity models.Findings – The analyzed maturity models sufficiently address basic design principles as well as principles for a descriptive purpose of use. The design principles for a prescriptive use, however, are hardly met. Thus, BPM maturity models provide limited guidance for identifying desirable maturity levels and for implementing improvement measures.Research limitations/imp...",fullPaper,jv424
Computer Science,p2305,d3,d28a63bfe54d607e419f389e6ad8c4000c6a803e,j268,Journal of Management Information Systems,Executives’ Perceptions of the Business Value of Information Technology: A Process-Oriented Approach,"Abstract: Despite significant progress in evaluating the productivity payoffs from information technology (IT), the inability of traditional firm-level economic analysis to account fully for the intangible impacts of IT has led to calls for a more inclusive and comprehensive approach to measuring IT business value. In response to this call, we develop a process-oriented model to assess the impacts of IT on critical business activities within the value chain. Our model incorporates corporate goals for IT and management practices as key determinants of realized IT payoffs. Using survey data from 304 business executives worldwide, we found that corporate goals for IT can be classified into one of four types: unfocused, operations focus, market focus, and dual focus. Our analysis confirms that these goals are useful indicators of payoffs from IT in that executives in firms with more focused goals for IT perceive greater payoffs from IT across the value chain. In addition, we found that management practices such as strategic alignment and IT investment evaluation contribute to higher perceived levels of IT business value.",fullPaper,jv268
Sociology,p2305,d4,d28a63bfe54d607e419f389e6ad8c4000c6a803e,j268,Journal of Management Information Systems,Executives’ Perceptions of the Business Value of Information Technology: A Process-Oriented Approach,"Abstract: Despite significant progress in evaluating the productivity payoffs from information technology (IT), the inability of traditional firm-level economic analysis to account fully for the intangible impacts of IT has led to calls for a more inclusive and comprehensive approach to measuring IT business value. In response to this call, we develop a process-oriented model to assess the impacts of IT on critical business activities within the value chain. Our model incorporates corporate goals for IT and management practices as key determinants of realized IT payoffs. Using survey data from 304 business executives worldwide, we found that corporate goals for IT can be classified into one of four types: unfocused, operations focus, market focus, and dual focus. Our analysis confirms that these goals are useful indicators of payoffs from IT in that executives in firms with more focused goals for IT perceive greater payoffs from IT across the value chain. In addition, we found that management practices such as strategic alignment and IT investment evaluation contribute to higher perceived levels of IT business value.",fullPaper,jv268
Business,p2305,d9,d28a63bfe54d607e419f389e6ad8c4000c6a803e,j268,Journal of Management Information Systems,Executives’ Perceptions of the Business Value of Information Technology: A Process-Oriented Approach,"Abstract: Despite significant progress in evaluating the productivity payoffs from information technology (IT), the inability of traditional firm-level economic analysis to account fully for the intangible impacts of IT has led to calls for a more inclusive and comprehensive approach to measuring IT business value. In response to this call, we develop a process-oriented model to assess the impacts of IT on critical business activities within the value chain. Our model incorporates corporate goals for IT and management practices as key determinants of realized IT payoffs. Using survey data from 304 business executives worldwide, we found that corporate goals for IT can be classified into one of four types: unfocused, operations focus, market focus, and dual focus. Our analysis confirms that these goals are useful indicators of payoffs from IT in that executives in firms with more focused goals for IT perceive greater payoffs from IT across the value chain. In addition, we found that management practices such as strategic alignment and IT investment evaluation contribute to higher perceived levels of IT business value.",fullPaper,jv268
Economics,p2305,d11,d28a63bfe54d607e419f389e6ad8c4000c6a803e,j268,Journal of Management Information Systems,Executives’ Perceptions of the Business Value of Information Technology: A Process-Oriented Approach,"Abstract: Despite significant progress in evaluating the productivity payoffs from information technology (IT), the inability of traditional firm-level economic analysis to account fully for the intangible impacts of IT has led to calls for a more inclusive and comprehensive approach to measuring IT business value. In response to this call, we develop a process-oriented model to assess the impacts of IT on critical business activities within the value chain. Our model incorporates corporate goals for IT and management practices as key determinants of realized IT payoffs. Using survey data from 304 business executives worldwide, we found that corporate goals for IT can be classified into one of four types: unfocused, operations focus, market focus, and dual focus. Our analysis confirms that these goals are useful indicators of payoffs from IT in that executives in firms with more focused goals for IT perceive greater payoffs from IT across the value chain. In addition, we found that management practices such as strategic alignment and IT investment evaluation contribute to higher perceived levels of IT business value.",fullPaper,jv268
Computer Science,p2308,d3,2e936679ce176b3a8307d7691c4e06e51e71e923,j423,European Journal of Information Systems,The business model concept: theoretical underpinnings and empirical illustrations,Abstract,fullPaper,jv423
Computer Science,p2309,d3,0ee47ca8e90f3dd2107b6791c0da42357c56f5bc,j206,Computer,Agile Software Development: The Business of Innovation,"The rise and fall of the dotcom-driven Internet economy shouldn't distract us from seeing that the business environment continues to change at a dramatically increasing pace. To thrive in this turbulent environment, we must confront the business need for relentless innovation and forge the future workforce culture. Agile software development approaches, such as extreme programming, Crystal methods, lean development, Scrum, adaptive software development (ASD) and others, view change from a perspective that mirrors today's turbulent business and technology environment.",fullPaper,jv206
Computer Science,p2318,d3,f6d63536e760d53e87b101ca260117c11a312d10,c7,International Symposium on Intelligent Data Analysis,Business process modelling: Review and framework,Abstract,poster,cp7
Computer Science,p2323,d3,e2dc457ae29877a1136026248bbf707567932ae1,c17,International Conference on Statistical and Scientific Database Management,ARIS - Business Process Modeling,Abstract,poster,cp17
Computer Science,p2329,d3,003568903b89c006ebec4646d8f7b8ffefe294cd,c65,International Symposium on Empirical Software Engineering and Measurement,Measuring the Linkage Between Business and Information Technology Objectives,"The establishment of linkage between business and information technology objectives has consistently been reported as one of the key concerns of information systems (IS) managers. The two objectives of this paper are: (1) to clarify the nature of the linkage construct, and (2) to report on a project that developed and tested measures of the social dimension of linkage. According to our research, the linkage construct has two dimensions:1. Intellectual: the content of information technology and business plans are internally consistent and externally valid.2. Social: the IS and business executives understand each others' objectives and plans.We conducted a study of measurement issues associated with the social dimension of linkage. The following candidate measures of linkage were examined:1. Cross references between written business and information technology plans;2. S and business executives' mutual understanding of each other's current objectives;3. Congruence between IS and business executives' long-term visions for information technology deployment;4. Executives' self-reported rating of linkage. Data were collected from 10 business units in three large Canadian life insurance companies. In addition to examining written documents such as strategic plans and minutes of steering committee meetings, extensive interviews were conducted with information systems and business unit executives. Based on this data, understanding of current objectives and shared vision for the utilization of information technology are proposed as the most promising potential measures for short- and long-term aspects of the social dimension of linkage, respectively. With some precautions, self-reports may also be used as a surrogate measure for short-term linkage.",poster,cp65
Business,p2329,d9,003568903b89c006ebec4646d8f7b8ffefe294cd,c65,International Symposium on Empirical Software Engineering and Measurement,Measuring the Linkage Between Business and Information Technology Objectives,"The establishment of linkage between business and information technology objectives has consistently been reported as one of the key concerns of information systems (IS) managers. The two objectives of this paper are: (1) to clarify the nature of the linkage construct, and (2) to report on a project that developed and tested measures of the social dimension of linkage. According to our research, the linkage construct has two dimensions:1. Intellectual: the content of information technology and business plans are internally consistent and externally valid.2. Social: the IS and business executives understand each others' objectives and plans.We conducted a study of measurement issues associated with the social dimension of linkage. The following candidate measures of linkage were examined:1. Cross references between written business and information technology plans;2. S and business executives' mutual understanding of each other's current objectives;3. Congruence between IS and business executives' long-term visions for information technology deployment;4. Executives' self-reported rating of linkage. Data were collected from 10 business units in three large Canadian life insurance companies. In addition to examining written documents such as strategic plans and minutes of steering committee meetings, extensive interviews were conducted with information systems and business unit executives. Based on this data, understanding of current objectives and shared vision for the utilization of information technology are proposed as the most promising potential measures for short- and long-term aspects of the social dimension of linkage, respectively. With some precautions, self-reports may also be used as a surrogate measure for short-term linkage.",poster,cp65
Computer Science,p2339,d3,efe91430fa2e83590441084b7b2996cc547e60a5,c102,ACM SIGMOD Conference,Business Model Dynamics and Innovation: (Re)establishing the Missing Linkages,"Purpose – This paper aims to discuss the need to dynamize the existing conceptualization of business model, and proposes a new typology to distinguish different types of business model change.Design/methodology/approach – The paper integrates basic insights of innovation, business process and routine research into the concept of business model. The main focus of the paper is on strategic and terminological issues.Findings – The paper offers a new, process‐based conceptualization of business model, which recognizes and integrates the role of individual agency. Based on this, it distinguishes and specifies four different types of business model change: business model creation, extension, revision, and termination. Each type of business model change is associated with specific challenges.Practical implications – The proposed typology can serve as a basis for developing a management tool to evaluate the impact of specific changes to a firm's business model. Such a tool would be particularly useful in identify...",poster,cp102
Computer Science,p2343,d3,4462e6c004949d1ed46b59f1e959a1b35e55e190,j268,Journal of Management Information Systems,Investment in Enterprise Resource Planning: Business Impact and Productivity Measures,"Enterprise Resource Planning (ERP)software systems integrate key business and management processes within and beyond a firm's boundary.Although the business value of ERP implementations has been extensively debated in trade periodicals in the form of qualitative discussion or detailed case studies, there is little large-sample statistical evidence on whether the benefits of ERP implementation exceed the costs and risks. With multiyear multi-firm ERP implementation and financial data, we find that firms that invest in ERP tend to show higher performance across a wide variety of financial metrics. Even though there is a slowdown in business performance and productivity shortly after the implementation, financial markets consistently reward the adopters with higher market valuation (as measured by Tobin's q). Due to the lack of mid- and long-term post-implementation data, future research on the long-run impact of ERP is proposed.",fullPaper,jv268
Business,p2343,d9,4462e6c004949d1ed46b59f1e959a1b35e55e190,j268,Journal of Management Information Systems,Investment in Enterprise Resource Planning: Business Impact and Productivity Measures,"Enterprise Resource Planning (ERP)software systems integrate key business and management processes within and beyond a firm's boundary.Although the business value of ERP implementations has been extensively debated in trade periodicals in the form of qualitative discussion or detailed case studies, there is little large-sample statistical evidence on whether the benefits of ERP implementation exceed the costs and risks. With multiyear multi-firm ERP implementation and financial data, we find that firms that invest in ERP tend to show higher performance across a wide variety of financial metrics. Even though there is a slowdown in business performance and productivity shortly after the implementation, financial markets consistently reward the adopters with higher market valuation (as measured by Tobin's q). Due to the lack of mid- and long-term post-implementation data, future research on the long-run impact of ERP is proposed.",fullPaper,jv268
Computer Science,p2347,d3,a8aafdf4eb2987c808ee268d055bd440648bf60b,c49,ACM/SIGCOMM Internet Measurement Conference,Managing Business Complexity: Discovering Strategic Solutions with Agent-Based Modeling and Simulation,"Agent-based modeling and simulation (ABMS), a way to simulate a large number of choices by individual actors, is one of the most exciting practical developments in business modeling since the invention of relational databases. It represents a new way to understand data and generate information that has never been available before-a way for businesses to view the future and to understand and anticipate the likely effects of their decisions on their markets and industries. It thus promises to have far-reaching effects on the way that businesses in many areas use computers to support practical decision-making. Managing Business Complexity is the first complete business-oriented agent-based modeling and simulation resource. It has three purposes: first, to teach readers how to think about ABMS, that is, about agents and their interactions; second, to teach readers how to explain the features and advantages of ABMS to other people and third, to teach readers how to actually implement ABMS by building agent-based simulations. It is intended to be a complete ABMS resource, accessible to readers who haven't had any previous experience in building agent-based simulations, or any other kinds of models, for that matter. It is also a collection of ABMS business applications resources, all assembled in one place for the first time. In short, Managing Business Complexity addresses who needs ABMS and why, where and when ABMS can be applied to the everyday business problems that surround us, and how specifically to build these powerful agent-based models.",poster,cp49
Computer Science,p2352,d3,804a7f596ad5f04216dcf60aec3c2d548f534edc,j226,Information Systems,Business process mining: An industrial application,Abstract,fullPaper,jv226
Computer Science,p2356,d3,b6cabc7a9bedbf0af662a639a3baf74ad01f6b82,c61,"International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems","Enterprise, Business-Process and Information Systems Modeling",Abstract,poster,cp61
Computer Science,p2362,d3,8e7caabb5ab6e205188ab6987b6f70c7687658e2,j428,European Journal of Operational Research,Simulation in manufacturing and business: A review,Abstract,fullPaper,jv428
Computer Science,p2368,d3,3a3c694410c50a8b6bbf16b2dbe4b80a980248d7,c110,Biometrics and Identity Management,Decision Support and Business Intelligence Systems (8th Edition),"Decision Support and Business Intelligence Systems 9e provides the only comprehensive, up-to-date guide to today's revolutionary management support system technologies, and showcases how they can be used for better decision-making. KEY TOPICS: Decision Support Systems and Business Intelligence. Decision Making, Systems, Modeling, and Support. Decision Support Systems Concepts, Methodologies, and Technologies: An Overview. Modeling and Analysis. Data Mining for Business Intelligence. Artificial Neural Networks for Data Mining. Text and Web Mining. Data Warehousing. Collaborative Computer-Supported Technologies and Group Support Systems. Knowledge Management. Artificial Intelligence and Expert Systems. Advanced Intelligent Systems. Management Support Systems: Emerging Trends and Impacts. Ideal for practicing managers interested in the foundations and applications of BI, group support systems (GSS), knowledge management, ES, data mining, intelligent agents, and other intelligent systems.",poster,cp110
Computer Science,p2370,d3,c3392106a981bdb6f393e061efe0fec9989e15d7,c108,IEEE International Conference on Multimedia and Expo,Business Intelligence: Data Mining and Optimization for Decision Making,"Business intelligence is a broad category of applications and technologies for gathering, providing access to, and analyzing data for the purpose of helping enterprise users make better business decisions. The term implies having a comprehensive knowledge of all factors that affect a business, such as customers, competitors, business partners, economic environment, and internal operations, therefore enabling optimal decisions to be made. Business Intelligence provides readers with an introduction and practical guide to the mathematical models and analysis methodologies vital to business intelligence. This book: Combines detailed coverage with a practical guide to the mathematical models and analysis methodologies of business intelligence. Covers all the hot topics such as data warehousing, data mining and its applications, machine learning, classification, supply optimization models, decision support systems, and analytical methods for performance evaluation. Is made accessible to readers through the careful definition and introduction of each concept, followed by the extensive use of examples and numerous real-life case studies. Explains how to utilise mathematical models and analysis models to make effective and good quality business decisions. This book is aimed at postgraduate students following data analysis and data mining courses. Researchers looking for a systematic and broad coverage of topics in operations research and mathematical models for decision-making will find this an invaluable guide.",poster,cp108
Computer Science,p2374,d3,fe3c2556303014c9e2d7c42528ce08ebfb90a02d,c98,Vision,Compliance Aware Business Process Design,Abstract,poster,cp98
Computer Science,p2378,d3,0fa0c7404dcd98f03bebefed15f023926594e8ed,j268,Journal of Management Information Systems,Information Technology Payoff in E-Business Environments: An International Perspective on Value Creation of E-Business in the Financial Services Industry,"Grounded in the technology-organization-environment (TOE) framework, we develop a research model for assessing the value of e-business at the firm level. Based on this framework, we formulate six hypotheses and identify six factors (technology readiness, firm size, global scope, financial resources, competition intensity, and regulatory environment) that may affect value creation of e-business. Survey data from 612 firms across 10 countries in the financial services industry were collected and used to test the theoretical model. To examine how e-business value is influenced by economic environments, we compare two subsamples from developed and developing countries. Based on structural equation modeling, our empirical analysis demonstrates several key findings: (1) Within the TOE framework, technology readiness emerges as the strongest factor for e-business value, while financial resources, global scope, and regulatory environment also significantly contribute to e-business value. (2) Firm size is negatively related to e-business value, suggesting that structural inertia associated with large firms tends to retard e-business value. (3) Competitive pressure often drives firms to adopt e-business, but e-business value is associated more with internal organizational resources (e.g., technological readiness) than with external pressure to adopt. (4) While financial resources are an important factor in developing countries, technological capabilities become far more important in developed countries. This suggests that as firms move into deeper stages of e-business transformation, the key determinant of e-business value shifts from monetary spending to higher dimensions of organizational capabilities. (5) Government regulation plays a much more important role in developing countries than in developed countries. These findings indicate the usefulness of the proposed research model and theoretical framework for studying e-business value. They also provide insights for both business managers and policy-makers.",fullPaper,jv268
Economics,p2378,d11,0fa0c7404dcd98f03bebefed15f023926594e8ed,j268,Journal of Management Information Systems,Information Technology Payoff in E-Business Environments: An International Perspective on Value Creation of E-Business in the Financial Services Industry,"Grounded in the technology-organization-environment (TOE) framework, we develop a research model for assessing the value of e-business at the firm level. Based on this framework, we formulate six hypotheses and identify six factors (technology readiness, firm size, global scope, financial resources, competition intensity, and regulatory environment) that may affect value creation of e-business. Survey data from 612 firms across 10 countries in the financial services industry were collected and used to test the theoretical model. To examine how e-business value is influenced by economic environments, we compare two subsamples from developed and developing countries. Based on structural equation modeling, our empirical analysis demonstrates several key findings: (1) Within the TOE framework, technology readiness emerges as the strongest factor for e-business value, while financial resources, global scope, and regulatory environment also significantly contribute to e-business value. (2) Firm size is negatively related to e-business value, suggesting that structural inertia associated with large firms tends to retard e-business value. (3) Competitive pressure often drives firms to adopt e-business, but e-business value is associated more with internal organizational resources (e.g., technological readiness) than with external pressure to adopt. (4) While financial resources are an important factor in developing countries, technological capabilities become far more important in developed countries. This suggests that as firms move into deeper stages of e-business transformation, the key determinant of e-business value shifts from monetary spending to higher dimensions of organizational capabilities. (5) Government regulation plays a much more important role in developing countries than in developed countries. These findings indicate the usefulness of the proposed research model and theoretical framework for studying e-business value. They also provide insights for both business managers and policy-makers.",fullPaper,jv268
Computer Science,p2379,d3,0d72079450fba5b6c10fdf1238a11859c9e74555,j206,Computer,The Current State of Business Intelligence,"Business intelligence (BI) is now widely used, especially in the world of practice, to describe analytic applications. BI is currently the top-most priority of many chief information officers. BI has become a strategic initiative and is now recognized by CIOs and business leaders as instrumental in driving business effectiveness and innovation. BI is a process that includes two primary activities: getting data in and getting data out. Getting data in, traditionally referred to as data warehousing, involves moving data from a set of source systems into an integrated data warehouse. Getting data in delivers limited value to an enterprise; only when users and applications access the data and use it to make decisions does the organization realize the full value from its data warehouse. Thus, getting data out receives most attention from organizations. This second activity, which is commonly referred to as BI, consists of business users and applications accessing data from the data warehouse to perform enterprise reporting, OLAP, querying, and predictive analytics.",fullPaper,jv206
Computer Science,p2382,d3,f3c9e1abd1608fb5bf5e853379333da96662c183,c76,Group,Digital capital: harnessing the power of business Webs,"From the Publisher: 
When Transmeta unveiled its remarkable new microprocessors earlier this year, the company's founder, David Ditzel, told the media: ""The Internet changes everything. In the future you will no more want to leave your home without your Internet connection than you do without your cell phone today."" 
 
The Transmeta chips are designed from scratch to facilitate wireless Internet access from Web appliances and ultra-light laptops. Scores of other corporations are scrambling to come up with similar devices. 
 
In Japan, the revolutionary ""i-mode"" mobile phone is soaring in popularity. These svelte phones are constantly connected to the Internet. You don't have to 'logon' to the Web as you do in North America. The display screen is the size of a business card. More than 350 companies have built a vast array of Web sites for the gadget. Users can receive email, chat, buy and sell securities, download video and music, swap photos, read train schedules, look up their horoscopes, check movie listings, and on and on. The Japanese are hooked. 
 
This constant connectivity to the Net will profoundly effect how we go about our lives. At the early stages we will download straightforward content like music, newspaper clipping and ebooks. 
 
Soon wireless devices that pinpoint your location will be able to answer any questions on services and amenities in the neighbourhood. As you drive through a neighbourhood that you would like to move to, you will be alerted to any houses for sale with the right number of bedrooms and bathrooms. Naturally the device will tell you how to get to each house. 
 
These gadgets will be our constant companions and co-pilots as we go about our work and play. Their ability to extract useful information from the blizzard of digital data will be key. We will insist these devices intimately understand our needs and wants. 
 
As we explore in Digital Capital: Harnessing the Power of Business Webs, corporations are reinventing their business models around the ubiquitous, deep, rich and increasingly functional Internet. Large and diverse sets of people scattered around the world can now, easily and cheaply, gain near real-time access to the information they need to make safe decisions and coordinate complex activities. 
 
Different companies can add knowledge value to a product or service - through innovation, enhancement, cost reduction, or customization - at each step in its lifecycle. Often, specialists do a better value-adding job than vertically integrated firms. In the digital economy, the notion of a separate, electronically negotiated deal at each step of the value cycle becomes a reasonable, often compelling, proposition. 
 
Successful corporations are now distinguished by their ability to identify and accumulate digital capital. They use the Net to blend their intellectual acumen with other companies and leverage the combined insight. They use the Web to develop much deeper relationships with their customers. And they jettison the business models of the industrial age to reinvent their corporations for success in the digital economy. 
 
To repeat: The Internet changes everything. Digital Capital shows how.",poster,cp76
Computer Science,p2385,d3,0db2f6dffb8fb84dede11e82b7d3765a43cb2d54,c48,"Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing",Statistics for Business and Economics,"An introductory business statistics text, providing a range of computer applications related to five different software packages. It presents business-orientated examples after each new topic. A variety of case studies also reinforce the real-world applications of statistics.",poster,cp48
Computer Science,p2386,d3,aa8b08c68652971d81579e8642190ed23dfeeb73,j156,Lecture Notes in Computer Science,Business Process Management,"Information System is one of the key domain in which lot of research has been done in the past few years. Applying Information systems to heterogeneous and distributed environments is one of the current research areas. Business Process Management Systems is the information systems which deal with the administration of tasks in the business processes, organizational structures, or in the related context. The Workflow Management system, the early idea of BPM, controls the workflow in an organization, data transfer, and integration of legacy information systems with existing programs and program modules, delegation of business tasks. Offering task management services especially modeling of business processes and underlying organizations, BPM serves it meaning and incorporates the WFM system. BPM supporting the use of knowledge concerning the awareness/unawareness of integrated software, and analysis of processes and organizational structure in terms of verification, modification, evaluation are key management in BPM system I.INTRODUCTION TO BPM: Combination of corporate restructuring and mergers urged the department and processes to be automated and linked together. Unfortunately it created largely disconnected spaghetti of systems and applications. Business Process Management (BPM) was born from a strong need to streamline internal processes and connections between both internal and external functions. It provides ability to model, monitor, manage and manipulate processes quickly in response to changes in strategy and market forces. BPM is an extension of classical Work Flow Management (WFM) systems and approaches. BPM delivers the maximum utility of legacy system. It is a method for revolutionizing the way the work moves throughout an organization. II.COMPONENTS OF BPM: The four key elements in Business Process Management are: Process modeling, Process rule, Process management, and Process performance analytics. Process modeling: Process modeling describes the flow of information (business process).It helps in building a model of process and mapping into an implementation frame work. Modeling requires a consistent analysis and representation of the processes. It forms the basis for the userspecific process environment. Process Rule: Process Rule is a heart of the BPM. It involves execution of discrete steps & integration of information. A process rules engine is used for this purpose. The engine is to monitor and manage the given steps by providing a flow control over the business process. If a process fails or delayed, a compensated action can be done to rectify. Process Management: Process Management is to correlate business/application events to fundamental business metrics .It enables the organization to monitor and optimize the business process. Once the process has been analyzed, modeled and the associated rules are formed. The next stage involves the visualization and management. Visualization means transaction data ranging from holistic to process-state or client state specific rules. Process Performance Analytics: Process Performance Analytics is a transition data source available for sophisticated management analytics .This data should be available on a retrospective or real-time basis. It should answer to one of the most difficult question like “how much does it cost to process a specific transaction?” By this way BPM mainly addresses four components. They are Work Flow Management, Content and records management Enterprise Application Integration (EAI) and Business Activity monitoring (BAM). III.RELEVENCE TO SYSTEM INTEGRATION: BPM can be fitted easily on the top of existing IT system. It is the Process rules that governs all the happenings and optimizes the process. BPM will not be able to work without the connectivity provided by the EAI layer. EAI layer provides BPM to gain and measure the value from the investment. On the other hand, BPM provides value to the EAI technology layer of Information technology. BPM layer offers business management the view of the message processing layer. It is used to unlock the value of the information held in the integration layer. It is also used to gather data of every transaction in the lifecycle. IV.APPLICATION OF BPM: BPM can be applied to any application which needs these advantages. BPM reduces unnecessary high losses in time critical transactions, helps to provide any critical information, practical application yield fast ROI ,better control over the information, identifying and maximizing the benefits faster, better analysis and tracking, provides enterprise-wide visibility of business processes, improve the effectiveness of the core operations, provides ability to coordinate interactions between information systems, business process and the people, flexible and easily changed by non-technical business people.",fullPaper,jv156
Business,p2386,d9,aa8b08c68652971d81579e8642190ed23dfeeb73,j156,Lecture Notes in Computer Science,Business Process Management,"Information System is one of the key domain in which lot of research has been done in the past few years. Applying Information systems to heterogeneous and distributed environments is one of the current research areas. Business Process Management Systems is the information systems which deal with the administration of tasks in the business processes, organizational structures, or in the related context. The Workflow Management system, the early idea of BPM, controls the workflow in an organization, data transfer, and integration of legacy information systems with existing programs and program modules, delegation of business tasks. Offering task management services especially modeling of business processes and underlying organizations, BPM serves it meaning and incorporates the WFM system. BPM supporting the use of knowledge concerning the awareness/unawareness of integrated software, and analysis of processes and organizational structure in terms of verification, modification, evaluation are key management in BPM system I.INTRODUCTION TO BPM: Combination of corporate restructuring and mergers urged the department and processes to be automated and linked together. Unfortunately it created largely disconnected spaghetti of systems and applications. Business Process Management (BPM) was born from a strong need to streamline internal processes and connections between both internal and external functions. It provides ability to model, monitor, manage and manipulate processes quickly in response to changes in strategy and market forces. BPM is an extension of classical Work Flow Management (WFM) systems and approaches. BPM delivers the maximum utility of legacy system. It is a method for revolutionizing the way the work moves throughout an organization. II.COMPONENTS OF BPM: The four key elements in Business Process Management are: Process modeling, Process rule, Process management, and Process performance analytics. Process modeling: Process modeling describes the flow of information (business process).It helps in building a model of process and mapping into an implementation frame work. Modeling requires a consistent analysis and representation of the processes. It forms the basis for the userspecific process environment. Process Rule: Process Rule is a heart of the BPM. It involves execution of discrete steps & integration of information. A process rules engine is used for this purpose. The engine is to monitor and manage the given steps by providing a flow control over the business process. If a process fails or delayed, a compensated action can be done to rectify. Process Management: Process Management is to correlate business/application events to fundamental business metrics .It enables the organization to monitor and optimize the business process. Once the process has been analyzed, modeled and the associated rules are formed. The next stage involves the visualization and management. Visualization means transaction data ranging from holistic to process-state or client state specific rules. Process Performance Analytics: Process Performance Analytics is a transition data source available for sophisticated management analytics .This data should be available on a retrospective or real-time basis. It should answer to one of the most difficult question like “how much does it cost to process a specific transaction?” By this way BPM mainly addresses four components. They are Work Flow Management, Content and records management Enterprise Application Integration (EAI) and Business Activity monitoring (BAM). III.RELEVENCE TO SYSTEM INTEGRATION: BPM can be fitted easily on the top of existing IT system. It is the Process rules that governs all the happenings and optimizes the process. BPM will not be able to work without the connectivity provided by the EAI layer. EAI layer provides BPM to gain and measure the value from the investment. On the other hand, BPM provides value to the EAI technology layer of Information technology. BPM layer offers business management the view of the message processing layer. It is used to unlock the value of the information held in the integration layer. It is also used to gather data of every transaction in the lifecycle. IV.APPLICATION OF BPM: BPM can be applied to any application which needs these advantages. BPM reduces unnecessary high losses in time critical transactions, helps to provide any critical information, practical application yield fast ROI ,better control over the information, identifying and maximizing the benefits faster, better analysis and tracking, provides enterprise-wide visibility of business processes, improve the effectiveness of the core operations, provides ability to coordinate interactions between information systems, business process and the people, flexible and easily changed by non-technical business people.",fullPaper,jv156
Computer Science,p2390,d3,b16b639b53da44de43dd79c63faedc38106e7c07,c28,International Conference on Contemporary Computing,The Business Model: An Integrative Framework for Strategy Execution,"We have many useful frameworks for formulating business strategy, i.e., devising a theory of how to compete. Frameworks for strategy execution are comparatively fragmented and idiosyncratic. This paper proposes a business model framework to link the firm's theory about how to compete to its execution. The framework captures previous ideas about business models in a simple logical structure that reflects current thinking in strategy. The business model framework provides a consistent logical picture of the firm that is a useful tool for the strategist, for teaching, and potentially for research on business models in strategy.",poster,cp28
Computer Science,p2395,d3,5cabae6a12c89a49fe9ad70c41f820487cd05c9b,c74,International Conference on Computational Linguistics,Designing and evaluating e-business models,This article presents an e-business modeling approach that combines the rigorous approach of IT systems analysis with an economic value perspective from business sciences.,poster,cp74
Computer Science,p2401,d3,b32ca7a3b395ef89ff01ca6929462d8c0ab66185,c1,International Conference on Human Factors in Computing Systems,Social Network Analysis and Mining for Business Applications,"Social network analysis has gained significant attention in recent years, largely due to the success of online social networking and media-sharing sites, and the consequent availability of a wealth of social network data. In spite of the growing interest, however, there is little understanding of the potential business applications of mining social networks. While there is a large body of research on different problems and methods for social network mining, there is a gap between the techniques developed by the research community and their deployment in real-world applications. Therefore the potential business impact of these techniques is still largely unexplored.
 In this article we use a business process classification framework to put the research topics in a business context and provide an overview of what we consider key problems and techniques in social network analysis and mining from the perspective of business applications. In particular, we discuss data acquisition and preparation, trust, expertise, community structure, network dynamics, and information propagation. In each case we present a brief overview of the problem, describe state-of-the art approaches, discuss business application examples, and map each of the topics to a business process classification framework. In addition, we provide insights on prospective business applications, challenges, and future research directions. The main contribution of this article is to provide a state-of-the-art overview of current techniques while providing a critical perspective on business applications of social network analysis and mining.",poster,cp1
Computer Science,p2405,d3,15646d6234e04a4797a8df2f74f62f7aa2306de1,c16,International Conference on Data Science and Advanced Analytics,Groupware: Computer Support for Business Teams,Abstract,poster,cp16
Computer Science,p2406,d3,2c8b5431de3a6404f964d5c16121358031f0a181,c121,International Conference on Interaction Sciences,How Does Business Analytics Contribute to Business Value?,"This paper presents a model, synthesized from the literature, of factors that explain how business analytics contributes to business value. It also reports results from a preliminary test of that model. The model consists of two parts: a process and a variance model. The process model depicts the analyze-insight-decision-action process through which use of an organization’s business-analytic capabilities create business value. The variance model proposes that the five factors in Davenport et al.’s (2010) DELTA model of BA success factors, six from Watson and Wixom (2007), and three from Seddon et al.’s (2010) model of organizational benefits from enterprise systems, assist a firm to gain business value from business analytics. A preliminary test of the model was conducted using data from 100 customer-success stories from vendors such as IBM, SAP, and Teradata. Our conclusion is that the model is likely to be a useful basis for future research.",fullPaper,cp121
Business,p2406,d9,2c8b5431de3a6404f964d5c16121358031f0a181,c121,International Conference on Interaction Sciences,How Does Business Analytics Contribute to Business Value?,"This paper presents a model, synthesized from the literature, of factors that explain how business analytics contributes to business value. It also reports results from a preliminary test of that model. The model consists of two parts: a process and a variance model. The process model depicts the analyze-insight-decision-action process through which use of an organization’s business-analytic capabilities create business value. The variance model proposes that the five factors in Davenport et al.’s (2010) DELTA model of BA success factors, six from Watson and Wixom (2007), and three from Seddon et al.’s (2010) model of organizational benefits from enterprise systems, assist a firm to gain business value from business analytics. A preliminary test of the model was conducted using data from 100 customer-success stories from vendors such as IBM, SAP, and Teradata. Our conclusion is that the model is likely to be a useful basis for future research.",fullPaper,cp121
Computer Science,p2408,d3,2915daa6626fa02db3d00da1398d54a45ed0f922,c34,International Conference on Data Warehousing and Knowledge Discovery,Multilevel models in international business research,Abstract,poster,cp34
Economics,p2408,d11,2915daa6626fa02db3d00da1398d54a45ed0f922,c34,International Conference on Data Warehousing and Knowledge Discovery,Multilevel models in international business research,Abstract,poster,cp34
Computer Science,p2414,d3,bc60d2cc12b5844fc7622931daf441d459202c75,j424,Business Process Management Journal,Business process management (BPM) standards: a survey,"Purpose – In the last two decades, a proliferation of business process management (BPM) modeling languages, standards and software systems has given rise to much confusion and obstacles to adoption. Since new BPM languages and notation terminologies were not well defined, duplicate features are common. This paper seeks to make sense of the myriad BPM standards, organising them in a classification framework, and to identify key industry trends.Design/methodology/approach – An extensive literature review is conducted and relevant BPM notations, languages and standards are referenced against the proposed BPM Standards Classification Framework, which lists each standard's distinct features, strengths and weaknesses.Findings – The paper is unaware of any classification of BPM languages. An attempt is made to classify BPM languages, standards and notations into four main groups: execution, interchange, graphical, and diagnosis standards. At the present time, there is a lack of established diagnosis standards. I...",fullPaper,jv424
Computer Science,p2417,d3,5ee9528953bf5d7b54b554daeaf7c6718ee7f1d3,j62,Business & Information Systems Engineering,"Cloud Computing – A Classification, Business Models, and Research Directions",Abstract,fullPaper,jv62
Computer Science,p2436,d3,6622ab4a32e6deb6fda5104041d2c7c2f85e808a,c44,Italian National Conference on Sensors,A Declarative Approach for Flexible Business Processes Management,Abstract,poster,cp44
Computer Science,p2437,d3,5090d2b6f82339d0b821f1ada996d2ee104a95ae,j430,IEEE Data Engineering Bulletin,Business Artifacts: A Data-centric Approach to Modeling Business Operations and Processes,"Traditional approaches to business process modeling and wo rkflow are based on activity flows (with data often an afterthought) or documents (with processing o ften an afterthought). In contrast, an emerging approach uses(business) artifacts, that combine data and process in an holistic manner as the ba sic building block. These correspond to key business entities w hich evolve as they pass through the business’s operation. This short paper motivates the approach,surveys research and its applications, and discusses how principles and techniques from database mana gement research can further develop the artifact-centric paradigm.",fullPaper,jv430
Computer Science,p2452,d3,9dc60273c9651a378d4463e038a2d33e5cf6d3b7,c104,North American Chapter of the Association for Computational Linguistics,Enablers and Inhibitors of Business-IT Alignment,"This paper provides insight into identifying areas that help or hinder business-IT alignment. Alignment focuses on the activities that management performs to achieve cohesive goals across the organization. The aim of this paper is to determine the most important enablers and inhibitors to alignment. The paper presents and analyzes the results of a multi-year study of strategic alignment. Data were obtained from business and information technology executives from over 500 firms representing 15 industries who attended classes addressing alignment at IBM’s Advanced Business Institute. The executives were asked to describe those activities that assist in achieving alignment and those which seem to hinder it. These enablers and inhibitors to alignment were then analyzed with respect to industry, to time, and executive position. The results indicate that certain activities can assist in the achievement of this state of alignment while others are clearly barriers. Achieving alignment is evolutionary and dynamic. It requires strong support from senior management, good working relationships, strong leadership, appropriate prioritization, trust, and effective communication, as well as a thorough understanding of the",poster,cp104
Business,p2452,d9,9dc60273c9651a378d4463e038a2d33e5cf6d3b7,c104,North American Chapter of the Association for Computational Linguistics,Enablers and Inhibitors of Business-IT Alignment,"This paper provides insight into identifying areas that help or hinder business-IT alignment. Alignment focuses on the activities that management performs to achieve cohesive goals across the organization. The aim of this paper is to determine the most important enablers and inhibitors to alignment. The paper presents and analyzes the results of a multi-year study of strategic alignment. Data were obtained from business and information technology executives from over 500 firms representing 15 industries who attended classes addressing alignment at IBM’s Advanced Business Institute. The executives were asked to describe those activities that assist in achieving alignment and those which seem to hinder it. These enablers and inhibitors to alignment were then analyzed with respect to industry, to time, and executive position. The results indicate that certain activities can assist in the achievement of this state of alignment while others are clearly barriers. Achieving alignment is evolutionary and dynamic. It requires strong support from senior management, good working relationships, strong leadership, appropriate prioritization, trust, and effective communication, as well as a thorough understanding of the",poster,cp104
Computer Science,p2461,d3,8f0f416a4cec36d43ecbfb7aee1aa57ae6d71e47,c23,International Conference on Open and Big Data,Business Competence of Information Technology Professionals: Conceptual Development and Influence on IT-Business Partnerships,"This research aims at improving our understanding of the concept of business competence of information technology professionals and at exploring the contribution of this competence to the development of partnerships between IT professionals and their business clients. Business competence focuses on the areas of knowledge that are not specifically IT-related. At a broad level, it comprises the organization-specific knowledge and the interpersonal and management knowledge possessed by IT professionals. Each of these categories is in turn inclusive of more specific areas of knowledge. Organizational overview, organizational unit, organizational responsibility, and IT-business integration form the organization-specific knowledge, while interpersonal communication, leadership, and knowledge networking form the interpersonal and management knowledge. Such competence is hypothesized to be instrumental in increasing the intentions of IT professionals to develop and strengthen the relationship with their clients. 
 
The first step in the study was to develop a scale to measure business competence of IT professionals. The scale was validated, and then used to test the model that relates competence to intentions to form IT-business partnerships. The results support the suggested structure for business competence and indicate that business competence significantly influences the intentions of IT professionals to develop partnerships with their business clients.",poster,cp23
Business,p2461,d9,8f0f416a4cec36d43ecbfb7aee1aa57ae6d71e47,c23,International Conference on Open and Big Data,Business Competence of Information Technology Professionals: Conceptual Development and Influence on IT-Business Partnerships,"This research aims at improving our understanding of the concept of business competence of information technology professionals and at exploring the contribution of this competence to the development of partnerships between IT professionals and their business clients. Business competence focuses on the areas of knowledge that are not specifically IT-related. At a broad level, it comprises the organization-specific knowledge and the interpersonal and management knowledge possessed by IT professionals. Each of these categories is in turn inclusive of more specific areas of knowledge. Organizational overview, organizational unit, organizational responsibility, and IT-business integration form the organization-specific knowledge, while interpersonal communication, leadership, and knowledge networking form the interpersonal and management knowledge. Such competence is hypothesized to be instrumental in increasing the intentions of IT professionals to develop and strengthen the relationship with their clients. 
 
The first step in the study was to develop a scale to measure business competence of IT professionals. The scale was validated, and then used to test the model that relates competence to intentions to form IT-business partnerships. The results support the suggested structure for business competence and indicate that business competence significantly influences the intentions of IT professionals to develop partnerships with their business clients.",poster,cp23
Computer Science,p2463,d3,9f03d1d1642d050ee86d79dffb390b86a6d9a7ae,c99,Symposium on the Theory of Computing,Business Statistics,Abstract,poster,cp99
Political Science,p2463,d15,9f03d1d1642d050ee86d79dffb390b86a6d9a7ae,c99,Symposium on the Theory of Computing,Business Statistics,Abstract,poster,cp99
Computer Science,p2464,d3,be0e6f115a05acd44055c134fc4749cbcde0382c,j424,Business Process Management Journal,Business process management: a missing link in business education,"Purpose – The purpose of this paper is to analyse the inadequacies of current business education in the context of “process”. It presents an analysis of the background to business processes in historical perspective and posits the significance of business management for today's business education. It argues the importance of business processes and business process management (BPM) in the context of the current and emerging information technologies (IT) and business education and highlights its ability to offer a missing link between business, IT and strategy.Design/methodology/approach – The approach involves analysis and review of the literature and analysis of secondary data.Findings – Even though business processes have been the subject of formal study from multiple perspectives for a long time, since the start of industrial age, processes still are not well understood, left unmanaged and poorly executed. With business schools teaching primarily function specific and narrow and IT schools focused on na...",fullPaper,jv424
Business,p2464,d9,be0e6f115a05acd44055c134fc4749cbcde0382c,j424,Business Process Management Journal,Business process management: a missing link in business education,"Purpose – The purpose of this paper is to analyse the inadequacies of current business education in the context of “process”. It presents an analysis of the background to business processes in historical perspective and posits the significance of business management for today's business education. It argues the importance of business processes and business process management (BPM) in the context of the current and emerging information technologies (IT) and business education and highlights its ability to offer a missing link between business, IT and strategy.Design/methodology/approach – The approach involves analysis and review of the literature and analysis of secondary data.Findings – Even though business processes have been the subject of formal study from multiple perspectives for a long time, since the start of industrial age, processes still are not well understood, left unmanaged and poorly executed. With business schools teaching primarily function specific and narrow and IT schools focused on na...",fullPaper,jv424
Computer Science,p2468,d3,d74f9f02cbe0ca10c93715804722eef322bf60bb,j268,Journal of Management Information Systems,"Strategic Alignment Between Business and Information Technology: A Knowledge-Based View of Behaviors, Outcome, and Consequences","Senior executives continue to be concerned about factors influencing the business effect of information technology (IT). Prior research has argued that business-IT strategic alignment facilitates business effect of IT and that contextual factors affect business-IT alignment. However, the role of knowledge considerations in the relationship between contextual factors and alignment, and the role of IT projects in the relationship between alignment and business effects of IT, have not been explicitly examined. Therefore, this paper pursues the following two research questions: (1) Based on knowledge considerations, how do planning behaviors (specifically, IT managers' participation in business planning and business managers' participation in IT planning) and top management knowledge of IT mediate the effects of two contextual factors—organizational emphasis on knowledge management and centralization of IT decisions—on business-IT strategic alignment? (2) How do aspects of IT projects (specifically, quality of IT project planning and implementation problems in IT projects) mediate the relationship between business-IT strategic alignment and business effects of IT? Results from a survey of 274 senior information officers indicate that organizational emphasis on knowledge management and centralization of IT decisions affect top managers' knowledge of IT, which facilitates business managers' participation in strategic IT planning and IT managers' participation in business planning, and both of these planning behaviors affect business-IT strategic alignment. Moreover, the results indicate that quality of IT project planning and implementation problems in IT projects mediate the relationship between business-IT strategic alignment and business effect of IT. These findings highlight the importance of considering the planning and implementation of IT projects when examining the effects of business-IT strategic alignment, and highlight the importance of considering shared domain knowledge (i.e., top managers' knowledge of IT) and planning behaviors when examining the effects of contextual factors on business-IT strategic alignment. Managers can use these results to develop more comprehensive action plans for achieving greater business-IT strategic alignment, and for translating alignment into enhanced IT effects on business performance.",fullPaper,jv268
Business,p2468,d9,d74f9f02cbe0ca10c93715804722eef322bf60bb,j268,Journal of Management Information Systems,"Strategic Alignment Between Business and Information Technology: A Knowledge-Based View of Behaviors, Outcome, and Consequences","Senior executives continue to be concerned about factors influencing the business effect of information technology (IT). Prior research has argued that business-IT strategic alignment facilitates business effect of IT and that contextual factors affect business-IT alignment. However, the role of knowledge considerations in the relationship between contextual factors and alignment, and the role of IT projects in the relationship between alignment and business effects of IT, have not been explicitly examined. Therefore, this paper pursues the following two research questions: (1) Based on knowledge considerations, how do planning behaviors (specifically, IT managers' participation in business planning and business managers' participation in IT planning) and top management knowledge of IT mediate the effects of two contextual factors—organizational emphasis on knowledge management and centralization of IT decisions—on business-IT strategic alignment? (2) How do aspects of IT projects (specifically, quality of IT project planning and implementation problems in IT projects) mediate the relationship between business-IT strategic alignment and business effects of IT? Results from a survey of 274 senior information officers indicate that organizational emphasis on knowledge management and centralization of IT decisions affect top managers' knowledge of IT, which facilitates business managers' participation in strategic IT planning and IT managers' participation in business planning, and both of these planning behaviors affect business-IT strategic alignment. Moreover, the results indicate that quality of IT project planning and implementation problems in IT projects mediate the relationship between business-IT strategic alignment and business effect of IT. These findings highlight the importance of considering the planning and implementation of IT projects when examining the effects of business-IT strategic alignment, and highlight the importance of considering shared domain knowledge (i.e., top managers' knowledge of IT) and planning behaviors when examining the effects of contextual factors on business-IT strategic alignment. Managers can use these results to develop more comprehensive action plans for achieving greater business-IT strategic alignment, and for translating alignment into enhanced IT effects on business performance.",fullPaper,jv268
Computer Science,p2479,d3,844b8b27bca8ddd6128bd1211906d2a6f8282881,c67,The Sea,The Implications of Information Technology Infrastructure for Business Process Redesign,"Business process redesign (BPR) is a pervasive but challenging tool for transforming organizations. Information technology plays an important role by either enabling or constraining successful BPR. This paper explores the links between firm-wide IT infrastructure and business process change. IT infrastructure is the base foundation of the IT portfolio, which is shared throughout the firm in the form of reliable services, and is usually coordinated by the IS group. IT infrastructure capability includes both the technical and managerial expertise required to provide reliable physical services and extensive electronic connectivity within and outside the firm.Exploratory case analysis of four firms (two in retail and two in petroleum) was used to understand the ways IT infrastructure contributes to success in implementing BPR. The finding was that all firms needed a basic level of IT infrastructure capability to implement BPR. The firms that had developed a higher level of IT infrastructure capabilities, before or concurrent with undertaking business process redesign, were able to implement extensive changes to their business processes over relatively short time frames. The higher level of infrastructure capability was provided in the form of (1) a set of infrastructure services that spanned organizational boundaries such as those between functions, business units, or firms, and (2) the ability of the infrastructure to reach particular constituencies inside and outside the firm to transfer information and process complex transactions.The more extensive business process changes were more innovative and radical, crossing business and functional unit boundaries, and resulted in more significant business impact. The practical implication of the study is that before embarking on any form of BPR, managers should complete a business audit of their IT infrastructure capabilities, as these capabilities have an important impact on the speed and nature of business process change.",poster,cp67
Computer Science,p2482,d3,ee2ecc0cad3ca66bd9b694883929726871ba4981,j424,Business Process Management Journal,Culture in business process management: a literature review,"Purpose – Business process management (BPM) is a management approach that developed with a strong focus on the adoption of information technology (IT). However, there is a growing awareness that BPM requires a holistic organizational perspective especially since culture is often considered a key element in BPM practice. Therefore, the purpose of this paper is to provide an overview of existing research on culture in BPM.Design/methodology/approach – This literature review builds on major sources of the BPM community including the BPM Journal, the BPM Conference and central journal/conference databases. Forward and backward searches additionally deepen the analysis. Based on the results, a model of culture's role in BPM is developed.Findings – The results of the literature review provide evidence that culture is still a widely under‐researched topic in BPM. Furthermore, a framework on culture's role in BPM is developed and areas for future research are revealed.Research limitations/implications – The analy...",fullPaper,jv424
Computer Science,p2483,d3,5e23ba80d737e3f5aacccd9e32fc3de4887accec,j432,IBM Systems Journal,Business artifacts: An approach to operational specification,"Any business, no matter what physical goods or services it produces, relies on business records. It needs to record details of what it produces in terms of concrete information. Business artifacts are a mechanism to record this information in units that are concrete, identifiable, self-describing, and indivisible. We developed the concept of artifacts, or semantic objects, in the context of a technique for constructing formal yet intuitive operational descriptions of a business. This technique, called OpS (Operational Specification), was developed over the course of many business-transformation and business-process-integration engagements for use in IBM's internal processes as well as for use with customers. Business artifacts (or business records) are the basis for the factorization of knowledge that enables the OpS technique. In this paper we present a comprehensive discussion of business artifacts--what they are, how they are represented, and the role they play in operational business modeling. Unlike the more familiar and popular concept of business objects, business artifacts are pure instances rather than instances of a taxonomy of types. Consequently, the key operation on business artifacts is recognition rather than classification.",fullPaper,jv432
Computer Science,p2484,d3,2e16ae4c5c3b32451a65f83b279c69b51ab2452b,j424,Business Process Management Journal,A structured evaluation of business process improvement approaches,"Purpose – The purpose of this paper is to provide a structured overview of so‐called business process improvement (BPI) approaches and their contribution to the actual act of improving. Even though a lot is said about BPI, there is still a lack of supporting the act of improving the process. Most approaches concentrate on what needs to be done before and after the improvement act, but the act of improving itself still seems to be a black box.Design/methodology/approach – This paper is mainly based on a review of literature that deals with the term “Business Process Improvement”. The analysis of the literature is supported by qualitative content analysis. The structure of the evaluation follows the mandatory elements of a method (MEM).Findings – A lot of literature and consulting approaches deal with the restructuring and improvement of business processes. The author finds that even so‐called BPI approaches do not describe the act of improvement itself. And if they do, they lack a methodological structure ...",fullPaper,jv424
Business,p2484,d9,2e16ae4c5c3b32451a65f83b279c69b51ab2452b,j424,Business Process Management Journal,A structured evaluation of business process improvement approaches,"Purpose – The purpose of this paper is to provide a structured overview of so‐called business process improvement (BPI) approaches and their contribution to the actual act of improving. Even though a lot is said about BPI, there is still a lack of supporting the act of improving the process. Most approaches concentrate on what needs to be done before and after the improvement act, but the act of improving itself still seems to be a black box.Design/methodology/approach – This paper is mainly based on a review of literature that deals with the term “Business Process Improvement”. The analysis of the literature is supported by qualitative content analysis. The structure of the evaluation follows the mandatory elements of a method (MEM).Findings – A lot of literature and consulting approaches deal with the restructuring and improvement of business processes. The author finds that even so‐called BPI approaches do not describe the act of improvement itself. And if they do, they lack a methodological structure ...",fullPaper,jv424
Computer Science,p2489,d3,776605e26791b6114a66febdb86ab6595c6cdbd3,j62,Business & Information Systems Engineering,The Impact of Business Process Complexity on Business Process Standardization,Abstract,fullPaper,jv62
Computer Science,p2499,d3,b99d6c9872f1aa24c07da49fbf49147d9135117e,j268,Journal of Management Information Systems,The Implementation of Business Process Reengineering,"As more organizations undertake business process reengineering (BPR), issues in implementing BPR projects become a major concern. This field research seeks empirically to explore the problems of implementing reengineering projects and how the severity of these problems relates to BPR project success. Based on past theories and research related to the implementation of organizational change as well as field experience of reengineering experts, a comprehensive list of sixty-four BPR implementation problems was identified. The severity of each problem was then rated by those who have participated in reengineering in 105 organizations. Analysis of the results clearly demonstrates the central importance of change management in BPR implementation success. Resolutions of problems in other areas such as technological competence and project planning were also determined to be necessary, but not sufficient, conditions for reengineering success. Further, problems that are more directly related to the conduct of a project such as process delineation, project management, and tactical planning were perceived as less difficult, yet highly related to project success. This situation was also true for human resource problems such as training personnel for the redesigned process. These findings suggest that reengineering project implementation is complex, involving many factors. To succeed, it is essential that change be managed and that balanced attention be paid to all identified factors, including those that are more contextual (e.g., management support and technological competence) as well as factors that pertain directly to the conduct of the project (e.g., project management and process delineation). As one of the first pieces of empirical evidence based on a field study, this research emphasizes the importance of addressing BPR implementation within the broader context of organizational change in a complex sociotechnical environment.",fullPaper,jv268
Business,p2499,d9,b99d6c9872f1aa24c07da49fbf49147d9135117e,j268,Journal of Management Information Systems,The Implementation of Business Process Reengineering,"As more organizations undertake business process reengineering (BPR), issues in implementing BPR projects become a major concern. This field research seeks empirically to explore the problems of implementing reengineering projects and how the severity of these problems relates to BPR project success. Based on past theories and research related to the implementation of organizational change as well as field experience of reengineering experts, a comprehensive list of sixty-four BPR implementation problems was identified. The severity of each problem was then rated by those who have participated in reengineering in 105 organizations. Analysis of the results clearly demonstrates the central importance of change management in BPR implementation success. Resolutions of problems in other areas such as technological competence and project planning were also determined to be necessary, but not sufficient, conditions for reengineering success. Further, problems that are more directly related to the conduct of a project such as process delineation, project management, and tactical planning were perceived as less difficult, yet highly related to project success. This situation was also true for human resource problems such as training personnel for the redesigned process. These findings suggest that reengineering project implementation is complex, involving many factors. To succeed, it is essential that change be managed and that balanced attention be paid to all identified factors, including those that are more contextual (e.g., management support and technological competence) as well as factors that pertain directly to the conduct of the project (e.g., project management and process delineation). As one of the first pieces of empirical evidence based on a field study, this research emphasizes the importance of addressing BPR implementation within the broader context of organizational change in a complex sociotechnical environment.",fullPaper,jv268
Sociology,p55,d4,d9a984e15b1a86a66ecbac9e66d458dae4cb616c,c76,Group,What Is Data Science,Abstract,poster,cp76
Sociology,p79,d4,2ab2796390ac12df283e218907ed0ffef232dbc7,j35,The Journal of the Learning Sciences,Situating Data Science: Exploring How Relationships to Data Shape Learning,"The emerging field of Data Science has had a large impact on science and society. This has led to over a decade of calls to establish a corresponding field of Data Science Education. There is still a need, however, to more deeply conceptualize what a field of Data Science Education might entail in terms of scope, responsibility, and execution. This special issue explores how one distinguishing feature of Data Science—its focus on data collected from social and environmental contexts within which learners often find themselves deeply embedded—suggests serious implications for learning and education. The learning sciences is uniquely positioned to investigate how such contextual embeddings impact learners’ engagement with data including conceptual, experiential, communal, racialized, spatial, and political dimensions. This special issue demonstrates the richly layered relationships learners build with data and reveals them to be not merely utilitarian mechanisms for learning about data, but a critical part of navigating data as social text and understanding Data Science as a discipline. Together, the contributions offer a vision of how the learning sciences can contribute to a more expansive, agentive and socially aware Data Science Education.",fullPaper,jv35
Sociology,p143,d4,1901a26945ecb5445a9d58b4c32a0dc6dbd12f1a,j56,"Science, Technology and Human Values","STS, Meet Data Science, Once Again","Science and technology studies (STS) and the emerging field of data science share surprising elective affinities. At the growing intersections of these fields, there will be many opportunities and not a few thorny difficulties for STS scholars. First, I discuss how both fields frame the rollout of data science as a simultaneously social and technical endeavor, even if in distinct ways and for diverging purposes. Second, I discuss the logic of domains in contemporary computer, information, and data science circles. While STS is often agnostic about the borders between the sciences or with industry and state—occasionally taking those boundaries as an object of study—data science takes those boundaries as its target to overcome. These two elective affinities present analytic and practical challenges for STS but also opportunities for engagement. Overall, in addition to these typifications, I urge STS scholars to strategically position themselves to investigate and contribute to the breadth of transformations that seek to touch virtually every science and newly bind spheres of academy, industry, and state.",fullPaper,jv56
Sociology,p150,d4,e78be911203960b3b2a417465d726734367f8e30,c54,ACM SIGCOMM Symposium on Software Defined Networking Research,Counter‐mapping data science,"Counter-mapping is a combination of critical ideas and practices for social change that offers a productive and promising approach for grassroots data science initiatives. Current information technologies collect, store, and analyze data with new degrees of size, speed, heterogeneity, and detail. While much work utilizing data science technologies is dedicated to generating profit or to national security, some data science projects explicitly attempt to facilitate new social relations, though with inconsistent results and consequences. This paper reviews counter-mapping's particular combination of theory and practice as a potential point of reference for such initiatives. Counter-mapping takes the tools of institutional map-making at government agencies and corporations and applies them in situated, bottom-up ways. Moreover, counter-mapping's multiple theoretical approaches and polyglot practices offer a variety of inspirations and avenues for future work in identifying and realizing alternative, ideally better, possibilities. This paper defines counter-mapping; outlines its multiple theorizations; briefly describes three relevant case studies, The Detroit Geographical Expedition and Institute, Mapping Police Violence, and the Counter-Cartographies Collective; and concludes with a few hard-learned considerations from counter-mapping that are directly pertinent for data-oriented projects focused on change.",poster,cp54
Sociology,p154,d4,6e8d94181832771bc5dca8d288c52b6ad5914029,j58,Philosophy & Technology,Data Science as Machinic Neoplatonism,Abstract,fullPaper,jv58
Sociology,p222,d4,8ea48934b6f6a0717efb4e5355be3b008fc5b1bd,c37,International Workshop on the Semantic Web,Coding the biodigital child: the biopolitics and pedagogic strategies of educational data science,"Abstract Educational data science is an emerging transdisciplinary field formed from an amalgamation of data science and elements of biological, psychological and neuroscientific knowledge about learning, or learning science. This article conceptualises educational data science as a biopolitical strategy focused on the evaluation and management of the corporeal, emotional and embrained lives of children. Such strategies are enacted through the development of new kinds of digitally-mediated ‘biopedagogies’ of body optimisation, ‘psychopedagogies’ of emotional maximisation, and ‘neuropedagogies’ of brain empowerment. The data practices, scientific knowledges, digital devices and pedagogies that constitute educational data science produce new systems of knowledge about the child that are consequential to their formation as ‘biodigital’ subjects, whose assumed qualities and capacities are defined through expert practices of biosensing, emotion analytics, and neurocomputation, combined with associated scientific knowledges. The article develops the concept of transcoding to account for the processes involved in the formation of the biodigital child.",poster,cp37
Sociology,p254,d4,0040c830969302a8c88c0c083aee5051e405bfe5,c109,Computer Vision and Pattern Recognition,"Big Data, Big Problems: Emerging Issues in the Ethics of Data Science and Journalism","As big data techniques become widespread in journalism, both as the subject of reporting and as newsgathering tools, the ethics of data science must inform and be informed by media ethics. This article explores emerging problems in ethical research using big data techniques. It does so using the duty-based framework advanced by W.D. Ross, who has significantly influenced both research science and media ethics. A successful framework must provide stability and flexibility. Without stability, ethical precommitments will vanish as technology rapidly shifts costs. Without flexibility, traditional approaches will rapidly become obsolete in the face of technological change. The article concludes that Ross's duty-based approach both provides stability in the face of rapid technological change and flexibility to innovate to achieve the original purpose of basic ethical principles.",poster,cp109
Sociology,p328,d4,33aeb033401ec748633bdd5b806db4f58288ee69,c41,IEEE International Conference on Data Engineering,The Accuracy of Citizen Science Data: A Quantitative Review,"Author(s): Aceves-Bueno, Erendira; Adeleye, Adeyemi S; Feraud, Marina; Huang, Yuxiong; Tao, Mengya; Yang, Yi; Anderson, Sarah E",poster,cp41
Sociology,p332,d4,08a2ef1648fa5ea539ebe1718da577dc79124a21,j30,Frontiers in Environmental Science,Prospects and challenges for social media data in conservation science,"Social media data have been extensively used in numerous fields of science, but examples of their use in conservation science are still very limited. In this paper, we propose a framework on how social media data could be useful for conservation science and practice. We present the commonly used social media platforms and discuss how their content could be providing new data and information for conservation science. Based on this, we discuss how future work in conservation science and practice would benefit from social media data.",fullPaper,jv30
Sociology,p373,d4,ac8db14cbc7ad0119d0130e88f98ccb3ec61780f,c6,Annual Conference on Genetic and Evolutionary Computation,"Big Data, Digital Media, and Computational Social Science",forecasts and misrepresent,poster,cp6
Sociology,p395,d4,06ba782753bad19254db5d28ad4155556f286ee0,c72,Workshop on Research on Enterprise Networking,Data Management and Analysis Methods,"This chapter is about methods for managing and analyzing qualitative data. By qualitative data the authors mean text: newspapers, movies, sitcoms, e-mail traffic, folktales, life histories. They also mean narratives--narratives about getting divorced, about being sick, about surviving hand-to-hand combat, about selling sex, about trying to quit smoking. In fact, most of the archaeologically recoverable information about human thought and human behavior is text, the good stuff of social science.",poster,cp72
Sociology,p403,d4,d62126bfe0e1b299c9383bb30ee099c77aee5222,c117,Very Large Data Bases Conference,"Interpreting Qualitative Data: Methods for Analysing Talk, Text and Interaction","This a much expanded and updated version of David Silvermans best-selling introductory textbook for the beginning qualitative researcher. 
 
Features of the New Edition: 
• Takes account of the flood of qualitative work since the 1990s 
• All chapters have been substantially rewritten with the aim of greater clarity 
• A new chapter on Visual Images and a considerably expanded treatment of discourse analysis are provided 
• The number of student exercises has been considerably increased and are now present at the end of every chapter 
• An even greater degree of student accessibility: Key Points and Recommended Readings appear at the end of each chapter and technical terms are highlighted and appear in a Glossary 
• A more inter-disciplinary social science text which takes account of the growing interest in qualitative research outside sociology and anthropology from psychology to geography, information systems, health promotion, management and many other disciplines 
• Expanded coverage 50% longer than the First Edition 
This book has a more recent edition (2006)",poster,cp117
Sociology,p452,d4,d90f276316589f503690d541392989031f9d046b,c88,International Conference on Big Data Computing and Communications,Online Citizen Science: A Systematic Review of Effects on Learning and Scientific Literacy,"Participation in online citizen science is increasingly popular, yet studies that examine the impact on participants’ learning are limited. The aims of this paper are to identify the learning impact on volunteers who participate in online citizen science projects and to explore the methods used to study the impact. The ten empirical studies, examined in this systematic review, report learning impacts on citizens’ attitudes towards science, on their understanding of the nature of science, on topic-specific knowledge, on science knowledge, and on generic knowledge. These impacts were measured using self-reports, content analysis of contributed data and of forum posts, accuracy checks of contributed data, science and project-specific quizzes, and instruments for measuring scientific attitudes and beliefs. The findings highlight that certain technological affordances in online citizen science projects can cultivate citizens’ knowledge and skills, and they point to unexplored areas, including the lack of experimental and long-term studies, and studies in formal education settings.",poster,cp88
Sociology,p459,d4,917943472ec4a00443d78bb696ed4d8f8d8c7f0a,c46,Ideal,Understanding the Science Experiences of Successful Women of Color: Science Identity as an Analytic Lens.,"In this study, we develop a model of science identity to make sense of the science experiences of 15 successful women of color over the course of their undergraduate and graduate studies in science and into science-related careers. In our view, science identity accounts both for how women make meaning of science experiences and how society structures possible meanings. Primary data included ethnographic interviews during students' undergraduate careers, follow-up interviews 6 years later, and ongoing member-checking. Our results highlight the importance of recognition by others for women in the three science identity trajectories: research scientist; altruistic scientist; and disrupted scientist. The women with research scientist identities were passionate about science and recognized themselves and were recognized by science faculty as science people. The women with altruistic scientist identities regarded science as a vehicle for altruism and created innovative meanings of ''science,'' ''recognition by others,'' and ''woman of color in science.'' The women with disrupted scientist identities sought, but did not often receive, recognition by meaningful scientific others. Although they were ultimately successful, their trajectories were more difficult because, in part, their bids for recognition were disrupted by the interaction with gendered, ethnic, and racial factors. This study clarifies theoretical conceptions of science identity, promotes a rethinking of recruitment and retention efforts, and illuminates various ways women of color experience, make meaning of, and negotiate the culture of science. 2007 Wiley Periodicals, Inc. J Res Sci Teach 44: 1187-1218, 2007.",poster,cp46
Sociology,p678,d4,7980cf24fca6dd9314c11537ff1596c36ffc01dd,c44,Italian National Conference on Sensors,Culture of Disengagement in Engineering Education?,"Much has been made of the importance of training ethical, socially conscious engineers, but does US engineering education actually encourage neophytes to take seriously their professional responsibility to public welfare? Counter to such ideals of engagement, I argue that students’ interest in public welfare concerns may actually decline over the course of their engineering education. Using unique longitudinal survey data of students at four colleges, this article examines (a) how students’ public welfare beliefs change during their engineering education, (b) whether engineering programs emphasize engagement, and (c) whether these program emphases are related to students’ public welfare beliefs. I track four specific public welfare considerations: the importance to students of professional/ethical responsibilities, understanding the consequences of technology, understanding how people use machines, and social consciousness. Suggesting a culture of disengagement, I find that the cultural emphases of students’ engineering programs are directly related to their public welfare commitments and students’ public welfare concerns decline significantly over the course of their engineering education. However, these findings also suggest that if engineering programs can dismantle the ideological pillars of disengagement in their local climates, they may foster more engaged engineers.",poster,cp44
Sociology,p830,d4,801adcdeaed1c965fdb542ec4dc83d9ccdb71fdf,c24,International Conference on Data Technologies and Applications,Design Experiments in Educational Research,"In this article, the authors first indicate the range of purposes and the variety of settings in which design experiments have been conducted and then delineate five crosscutting features that collectively differentiate design experiments from other methodologies. Design experiments have both a pragmatic bent—“engineering” particular forms of learning—and a theoretical orientation—developing domain-specific theories by systematically studying those forms of learning and the means of supporting them. The authors clarify what is involved in preparing for and carrying out a design experiment, and in conducting a retrospective analysis of the extensive, longitudinal data sets generated during an experiment. Logistical issues, issues of measure, the importance of working through the data systematically, and the need to be explicit about the criteria for making inferences are discussed.",poster,cp24
Sociology,p878,d4,ceb57e2c12c97b430351abf90bfc7538c2b3b9f6,c31,Information Security Solutions Europe,Engineering Identity: Gender and Professional Identity Negotiation among Women Engineers,"This article considers how women in a gendered profession, engineering, construct their professional identity in response to workplace interpersonal interactions that marginalize it. Using data from interviews with women engineers, it also explores how these interactions influence the engineers' sense of self and belonging in engineering. The interpersonal interactions place professional identity on the periphery and can overly validate gender identity. I discuss two types of identity construction strategies employed by the participants in response to these marginalizing interactions: impression management tactics and coping strategies. Although the data demonstrate that participants may be left feeling devalued or ambivalent towards their identity or fit in engineering, some interactions are more validating and offer a sense of belonging. This article also reflects on how the engineers' actions may, in fact, represent forces for change in the gendered culture of engineering.",poster,cp31
Sociology,p890,d4,cab63c026728017d6dc6ebbb5abdfe3d110c7e5b,c76,Group,Why do academics engage with industry? The entrepreneurial university and individual motivations,Abstract,poster,cp76
Economics,p890,d11,cab63c026728017d6dc6ebbb5abdfe3d110c7e5b,c76,Group,Why do academics engage with industry? The entrepreneurial university and individual motivations,Abstract,poster,cp76
Political Science,p890,d15,cab63c026728017d6dc6ebbb5abdfe3d110c7e5b,c76,Group,Why do academics engage with industry? The entrepreneurial university and individual motivations,Abstract,poster,cp76
Sociology,p1010,d4,bf69c98fca9a9f6c1cde871beddbcdc668b77771,c39,Online World Conference on Soft Computing in Industrial Applications,"Big Data: A Revolution That Will Transform How We Live, Work, and Think","Since Aristotle, we have fought to understand the causes behind everything. But this ideology is fading. The world of big data can crunch However the indirect implication of a raw material in cdc data processing. He says most common search terms think we'll have lost. At the damnation profoundly surprising conclusions, from make it seems more recently I think. Less as a fascinatingand sometimes profoundly surprising ways not knowing why only one to find knowledge.",poster,cp39
Sociology,p1012,d4,4e6bba65f7636a655c778a3e54cc58e148468963,c98,Vision,CRITICAL QUESTIONS FOR BIG DATA,"The era of Big Data has begun. Computer scientists, physicists, economists, mathematicians, political scientists, bio-informaticists, sociologists, and other scholars are clamoring for access to the massive quantities of information produced by and about people, things, and their interactions. Diverse groups argue about the potential benefits and costs of analyzing genetic sequences, social media interactions, health records, phone logs, government records, and other digital traces left by people. Significant questions emerge. Will large-scale search data help us create better tools, services, and public goods? Or will it usher in a new wave of privacy incursions and invasive marketing? Will data analytics help us understand online communities and political movements? Or will it be used to track protesters and suppress speech? Will it transform how we study human communication and culture, or narrow the palette of research options and alter what ‘research’ means? Given the rise of Big Data as a socio-technical phenomenon, we argue that it is necessary to critically interrogate its assumptions and biases. In this article, we offer six provocations to spark conversations about the issues of Big Data: a cultural, technological, and scholarly phenomenon that rests on the interplay of technology, analysis, and mythology that provokes extensive utopian and dystopian rhetoric.",poster,cp98
Sociology,p1014,d4,1d174f0e3c391368d0f3384a144a6c7487f2a143,c13,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,Big Data's Disparate Impact,"Advocates of algorithmic techniques like data mining argue that these techniques eliminate human biases from the decision-making process. But an algorithm is only as good as the data it works with. Data is frequently imperfect in ways that allow these algorithms to inherit the prejudices of prior decision makers. In other cases, data may simply reflect the widespread biases that persist in society at large. In still others, data mining can discover surprisingly useful regularities that are really just preexisting patterns of exclusion and inequality. Unthinking reliance on data mining can deny historically disadvantaged and vulnerable groups full participation in society. Worse still, because the resulting discrimination is almost always an unintentional emergent property of the algorithm’s use rather than a conscious choice by its programmers, it can be unusually hard to identify the source of the problem or to explain it to a court.This Essay examines these concerns through the lens of American antidiscrimination law — more particularly, through Title VII’s prohibition of discrimination in employment. In the absence of a demonstrable intent to discriminate, the best doctrinal hope for data mining’s victims would seem to lie in disparate impact doctrine. Case law and the Equal Employment Opportunity Commission’s Uniform Guidelines, though, hold that a practice can be justified as a business necessity when its outcomes are predictive of future employment outcomes, and data mining is specifically designed to find such statistical correlations. Unless there is a reasonably practical way to demonstrate that these discoveries are spurious, Title VII would appear to bless its use, even though the correlations it discovers will often reflect historic patterns of prejudice, others’ discrimination against members of protected groups, or flaws in the underlying dataAddressing the sources of this unintentional discrimination and remedying the corresponding deficiencies in the law will be difficult technically, difficult legally, and difficult politically. There are a number of practical limits to what can be accomplished computationally. For example, when discrimination occurs because the data being mined is itself a result of past intentional discrimination, there is frequently no obvious method to adjust historical data to rid it of this taint. Corrective measures that alter the results of the data mining after it is complete would tread on legally and politically disputed terrain. These challenges for reform throw into stark relief the tension between the two major theories underlying antidiscrimination law: anticlassification and antisubordination. Finding a solution to big data’s disparate impact will require more than best efforts to stamp out prejudice and bias; it will require a wholesale reexamination of the meanings of “discrimination” and “fairness.”",poster,cp13
Sociology,p1072,d4,370db1355cc605b6c36f6f8f1b0d8f53db9b4466,j263,Television & New Media,Big Data from the South(s): Beyond Data Universalism,"This article introduces the tenets of a theory of datafication of and in the Souths. It calls for a de-Westernization of critical data studies, in view of promoting a reparation to the cognitive injustice that fails to recognize non-mainstream ways of knowing the world through data. It situates the “Big Data from the South” research agenda as an epistemological, ontological, and ethical program and outlines five conceptual operations to shape this agenda. First, it suggests moving past the “universalism” associated with our interpretations of datafication. Second, it advocates understanding the South as a composite and plural entity, beyond the geographical connotation (i.e., “global South”). Third, it postulates a critical engagement with the decolonial approach. Fourth, it argues for the need to bring agency to the core of our analyses. Finally, it suggests embracing the imaginaries of datafication emerging from the Souths, foregrounding empowering ways of thinking data from the margins.",fullPaper,jv263
Sociology,p1138,d4,6e737a0e5ef29303760a565ba5e9d98510ab0976,j283,GeoJournal,The real-time city? Big data and smart urbanism,Abstract,fullPaper,jv283
Sociology,p1170,d4,5766f245105247b4ce3f580ddf2c8213c80f7969,c38,IEEE Global Engineering Education Conference,The role of Big Data in explaining disaster resilience in supply chains for sustainability,Abstract,poster,cp38
Sociology,p1171,d4,ace8d8f019cdd1c9f6a775263d0552bb5c51f8a2,c59,Australian Software Engineering Conference,Situating methods in the magic of Big Data and AI,"ABSTRACT “Big Data” and “artificial intelligence” have captured the public imagination and are profoundly shaping social, economic, and political spheres. Through an interrogation of the histories, perceptions, and practices that shape these technologies, we problematize the myths that animate the supposed “magic” of these systems. In the face of an increasingly widespread blind faith in data-driven technologies, we argue for grounding machine learning-based practices and untethering them from hype and fear cycles. One path forward is to develop a rich methodological framework for addressing the strengths and weaknesses of doing data analysis. Through provocatively reimagining machine learning as computational ethnography, we invite practitioners to prioritize methodological reflection and recognize that all knowledge work is situated practice.",poster,cp59
Sociology,p1229,d4,6d1c5eb45a50d62c031088bc4b82828d582068a4,c22,Grid Computing Environments,"Datafication, dataism and dataveillance: Big Data between scientific paradigm and ideology","Metadata and data have become a regular currency for citizens to pay for their communication services and security—a trade-off that has nestled into the comfort zone of most people. This article deconstructs the ideological grounds of datafication. Datafication is rooted in problematic ontological and epistemological claims. As part of a larger social media logic, it shows characteristics of a widespread secular belief. Dataism, as this conviction is called, is so successful because masses of people — naively or unwittingly — trust their personal information to corporate platforms. The notion of trust becomes more problematic because people’s faith is extended to other public institutions (e.g. academic research and law enforcement) that handle their (meta)data. The interlocking of government, business, and academia in the adaptation of this ideology makes us want to look more critically at the entire ecosystem of connective media.",poster,cp22
Sociology,p1268,d4,b1b2c709b29623d1a8215315e9096f7b93f5c5b6,c83,"International Convention on Information and Communication Technology, Electronics and Microelectronics",Data ex Machina: Introduction to Big Data,"Social life increasingly occurs in digital environments and continues to be mediated by digital systems. Big data represents the data being generated by the digitization of social life, which we break down into three domains: digital life, digital traces, and digitalized life. We argue that there is enormous potential in using big data to study a variety of phenomena that remain difficult to observe. However, there are some recurring vulnerabilities that should be addressed. We also outline the role institutions must play in clarifying the ethical rules of the road. Finally, we conclude by pointing to a few trends that are not yet common in research using big data but will play an increasing role in it. Expected final online publication date for the Annual Review of Sociology Volume 43 is July 30, 2017. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.",poster,cp83
Sociology,p1271,d4,6830f672c1765867a914155d5eb7df140fe7de16,c121,International Conference on Interaction Sciences,CREDIT SCORING IN THE ERA OF BIG DATA,"TABLE OF CONTENTS TABLE OF CONTENTS I. INTRODUCTION II. TRADITIONAL CREDIT-ASSESSMENT TOOLS III. ALGORITHMS, MACHINE LEARNING, AND THE ALTERNATIVE CREDIT-SCORING MARKET A. Introduction to basic terminology and concepts B. How traditional credit-modeling tools compare to alternative, ""big-data"" tools C. Using machine learning to build a big-data credit-scoring model--how it works and potential problems IV. THE INADEQUACIES IN THE EXISTING LEGAL FRAMEWORK FOR CREDIT SCORING A. The Fair Credit Reporting Act (FCRA) B. The Equal Credit Opportunity Act (ECOA) V. THE CHALLENGES OF ALTERNATIVE CREDIT-SCORING AND A LEGISLATIVE FRAMEWORK FOR CHANGE A. Existing transparency rules are inadequate B. The burden of ensuring accuracy should not fall to the consumer C. Better tools are needed to detect and prevent discrimination by proxy D. Credit-assessment tools should not be used to target vulnerable consumers VI. CONCLUSION VII. ANNEXES I. INTRODUCTION One day in late 2008, Atlanta businessman Kevin Johnson returned home from his vacation to find an unpleasant surprise waiting in his mailbox. It was a letter from his credit card company, American Express, informing him that his credit limit had been lowered from $10,800 to a mere $3,800. (1) While Kevin was shocked that American Express would make such a drastic change to his limit, he was even more surprised by the company's reasoning. By any measure, Kevin had been an ideal customer. Kevin, who is black, was running a successful Atlanta public relations firm, was a homeowner, and had always paid his bills on time, rarely carrying a balance on his card. (2) Kevin's father, who had worked in the credit industry, had taught him the importance of responsible spending and, ""because of his father's lessons, [Kevin had] scrupulously maintained his credit since college."" (3) Yet his stellar track record and efforts to maintain ""scrupulous"" credit seemed to matter little, if at all, to American Express. The company had deemed him a risk simply because, as the letter put it, ""[o]ther customers who ha[d] used their card at establishments where [Kevin] recently shopped have a poor repayment history with American Express."" (4) When Kevin sought an explanation, the company was unwilling to share any information on which of businesses--many of them major retailers--contributed to American Express's decision to slash Kevin's limit by more than 65 percent. (5) Kevin Johnson was an early victim of a new form of credit assessment that some experts have labeled ""behavioral analysis"" or ""behavioral scoring,"" (6) but which might also be described as ""creditworthiness by association."" Rather than being judged on their individual merits and actions, consumers may find that access to credit depends on a lender's opaque predictions about a consumer's friends, neighbors, and people with similar interests, income levels, and backgrounds. This data-centric approach to credit is reminiscent of the racially discriminatory and now illegal practice of ""redlining,"" by which lenders classified applicants on the basis their zip codes, and not their individual capacities to borrow responsibly. (7) Since 2008, lenders have only intensified their use of big-data profiling techniques. With increased use of smartphones, social media, and electronic means of payment, every consumer leaves behind a digital trail of data that companies--including lenders and credit scorers--are eagerly scooping up and analyzing as a means to better predict consumer behavior. (8) The credit-scoring industry has experienced a recent explosion of start-ups that take an ""all data is credit data"" approach that combines conventional credit information with thousands of data points mined from consumers' offline and online activities. (9) Many companies also use complex algorithms to detect patterns and signals within a vast sea of information about consumers' daily lives. …",poster,cp121
Sociology,p1295,d4,932c3b5ddef5248a984ce83f7a6dfdd5528c72b2,c7,International Symposium on Intelligent Data Analysis,Will Democracy Survive Big Data and Artificial Intelligence?,Abstract,poster,cp7
Political Science,p1295,d15,932c3b5ddef5248a984ce83f7a6dfdd5528c72b2,c7,International Symposium on Intelligent Data Analysis,Will Democracy Survive Big Data and Artificial Intelligence?,Abstract,poster,cp7
Sociology,p1308,d4,73c57c28f60e59a0bccdae836aa34f90d0dbab15,c66,International Conference on Web and Social Media,Big Data and corporate reporting: impacts and paradoxes,"Purpose - The purpose of this paper is to investigate the phenomenon of Big Data and corporate reporting, and to determine the impact of Big Data and the current Big Data state of mind with regard to corporate reporting, what accountant and non-accountant participants’ perceptions are of the phenomenon, what the accountants’ role is and will be in this regard, and what opportunities and risks are associated with Big Data and corporate reporting. Furthermore, this study seeks to identify the inherent technological paradoxes of Big Data and corporate reporting. Design/methodology/approach - The current study is qualitative in nature and assumes an interpretive stance, investigating participants’ perceptions of the phenomenon of Big Data and corporate reporting. To this end, interview data from 25 participants, video and text material, were analysed to enhance and triangulate findings. A four-fold sampling strategy was employed to ensure that any collected data would contribute to the findings. Data were analysed on the basis of open and selective coding stages. Data collection and analysis took place in two stages, in 2014 and in 2016. Findings - Three topics, or categories, emerged from the data analysis, which have sufficient explanatory power to illustrate the phenomenon of Big Data and corporate reporting, namely the Big Data state of mind and corporate reporting, accountants’ role and future related to Big Data, and perceived opportunities and risks of Big Data. Features of a new approach to corporate reporting were identified and discussed. Furthermore, four paradoxes emerged to express inherent opposing positions of Big Data and corporate reporting, namely empowerment vs enslavement, fulfilling vs creating needs, reliability vs timeliness and simplicity vs complexity. Originality/value - The original contribution of the study lies in the empirical investigation of the phenomenon of Big Data and corporate reporting as one of the most recent and praised developments in the accounting context. The dual communication flows of corporate reporting with Big Data is an important element of the findings, which can enhance the prospective financial statements significantly. Finally, technological paradoxes of Big Data and corporate reporting are discussed for the first time, two of which are based on the literature and the remaining two are inherent in the phenomenon of Big Data and corporate reporting.",poster,cp66
Sociology,p1328,d4,dd4015e51085e24a2a213c7e2efe35c4b10ae781,c92,International Symposium on Computer Architecture,Speaking Sociologically with Big Data: Symphonic Social Science and the Future for Big Data Research,"Recent years have seen persistent tension between proponents of big data analytics, using new forms of digital data to make computational and statistical claims about ‘the social’, and many sociologists sceptical about the value of big data, its associated methods and claims to knowledge. We seek to move beyond this, taking inspiration from a mode of argumentation pursued by Piketty, Putnam and Wilkinson and Pickett that we label ‘symphonic social science’. This bears both striking similarities and significant differences to the big data paradigm and – as such – offers the potential to do big data analytics differently. This offers value to those already working with big data – for whom the difficulties of making useful and sustainable claims about the social are increasingly apparent – and to sociologists, offering a mode of practice that might shape big data analytics for the future.",poster,cp92
Sociology,p1459,d4,174f5142dc4709da03f6c3bd7b6955b08c9762f3,c106,International Conference on Biometrics,Bottom of the Data Pyramid: Big Data and the Global South,"To date, little attention has been given to the impact of big data in the Global South, about 60% of whose residents are below the poverty line. Big data manifests in novel and unprecedented ways in these neglected contexts. For instance, India has created biometric national identities for her 1.2 billion people, linking them to welfare schemes, and social entrepreneurial initiatives like the Ushahidi project that leveraged crowdsourcing to provide real-time crisis maps for humanitarian relief. While these projects are indeed inspirational, this article argues that in the context of the Global South there is a bias in the framing of big data as an instrument of empowerment. Here, the poor, or the “bottom of the pyramid” populace are the new consumer base, agents of social change instead of passive beneficiaries. This neoliberal outlook of big data facilitating inclusive capitalism for the common good sidelines critical perspectives urgently needed if we are to channel big data as a positive social force in emerging economies. This article proposes to assess these new technological developments through the lens of databased democracies, databased identities, and databased geographies to make evident normative assumptions and perspectives in this under-examined context.",poster,cp106
Sociology,p1489,d4,17c2cda2cb039eddb6751696e7c079b1b15f1138,c77,Visualization for Computer Security,Life beyond big data: governing with little analytics,"Abstract The twenty-first-century rise of big data marks a significant break with statistical notions of what is of interest or concern. The vast expansion of digital data has been closely intertwined with the development of advanced analytical algorithms with which to make sense of the data. The advent of techniques of knowledge discovery affords some capacity for the analytics to derive the object or subject of interest from clusters and patterns in large volumes of data, otherwise imperceptible to human reading. Thus, the scale of the big in big data is of less significance to contemporary forms of knowing and governing than what we will call the little analytics. Following Henri Bergson's analysis of forms of perception which ‘cut out’ a series of figures detached from the whole, we propose that analytical algorithms are instruments of perception without which the extensity of big data would not be comprehensible. The technologies of analytics focus human attention and decision on particular persons and things of interest, whilst annulling or discarding much of the material context from which they are extracted. Following the algorithmic processes of ingestion, partitioning and memory, we illuminate how the use of analytics engines has transformed the nature of analysis and knowledge and, thus, the nature of the governing of economic, social and political life.",poster,cp77
Sociology,p1499,d4,ca8f5eac152531426396e1e0b7e6b62658c72060,c11,European Conference on Modelling and Simulation,Big Data and Journalism,"Big data is a social, cultural, and technological phenomenon—a complex amalgamation of digital data abundance, emerging analytic techniques, mythology about data-driven insights, and growing critique about the overall consequences of big-data practices for democracy and society. While media and communication scholars have begun to examine and theorize about big data in the context of media and public life broadly, what are the particular implications for journalism? This article introduces and applies four conceptual lenses—epistemology, expertise, economics, and ethics—to explore both contemporary and potential applications of big data for the professional logic and industrial production of journalism. These distinct yet inter-related conceptual approaches reveal how journalists and news media organizations are seeking to make sense of, act upon, and derive value from big data during a time of exploration in algorithms, computation, and quantification. In all, the developments of big data potentially have great meaning for journalism’s ways of knowing (epistemology) and doing (expertise), as well as its negotiation of value (economics) and values (ethics). Ultimately, this article outlines future directions for journalism studies research in the context of big data.",poster,cp11
Sociology,p2033,d4,a461f27a272e9f0a7b0b9cc4f6f8c7f8e80601ad,c60,Network and Distributed System Security Symposium,Misery Loves Companies: Rethinking Social Initiatives by Business,"Companies are increasingly asked to provide innovative solutions to deep-seated problems of human misery, even as economic theory instructs managers to focus on maximizing their shareholders' wealth. In this paper, we assess how organization theory and empirical research have thus far responded to this tension over corporate involvement in wider social life. Organizational scholarship has typically sought to reconcile corporate social initiatives with seemingly inhospitable economic logic. Depicting the hold that economics has had on how the relationship between the firm and society is conceived, we examine the consequences for organizational research and theory by appraising both the 30-year quest for an empirical relationship between a corporation's social initiatives and its financial performance, as well as the development of stakeholder theory. We propose an alternative approach, embracing the tension between economic and broader social objectives as a starting point for systematic organizational inquiry. Adopting a pragmatic stance, we introduce a series of research questions whose answers will reveal the descriptive and normative dimensions of organizational responses to misery.",poster,cp60
Sociology,p2041,d4,e644e09b60a9419ada86b2901cafefb8e1eaafa9,c34,International Conference on Data Warehousing and Knowledge Discovery,Business Research Methods,PART 1 1. Business research strategies 2. Research designs 3. Planning a research project and formulating research questions 4. Getting Started: reviewing the literature 5. Ethics in business research PART 2 6. The nature of quantitative research 7. Sampling 8. Structured interviewing 9. Self-completion questionnaires 10. Asking questions 11. Structured observation 12. Content analysis 13. Secondary analysis and official statistics 14. Quantitative data analysis 15. Using SPSS for Windows PART 3 16. The nature of qualitative research 17. Ethnography and participant observation 18. Interviewing in qualitative research 19. Focus groups 20. Language in qualitative research 21. Documents as sources of data 22. Qualitative data analysis 23. Computer-assisted qualitative data analysis: Using NVivo PART 4 24. Breaking down the quantitative/qualitative divide 25. Combining quantitative and qualitative research 26. Internet research methods 27. Writing up business research,poster,cp34
Sociology,p2073,d4,9642c6188a66893f12dd7bcedb53527080638135,c87,International Conference on Big Data Research,Qualitative methods in business research,"For a student or practitioner beginning their qualitative research journey in business, there are few dedicated texts. Business schools have tended to recommend the routinely revised and increasing...",poster,cp87
Sociology,p2138,d4,35122b629b19d7c61c977e652a2c2cf59ccf4d7f,c20,ACM Conference on Economics and Computation,Springer Science+Business Media,Abstract,poster,cp20
Sociology,p2153,d4,dbaefb56716326f42302064b078e73e4c5a4fa82,c6,Annual Conference on Genetic and Evolutionary Computation,Toward a Political Conception of Corporate Responsibility - Business and Society Seen from a Habermasian Perspective,"We review two important schools within business and society research, which we label positivist and post-positivist corporate social responsibility (CSR). The former is criticized because of its instrumentalism and normative vacuity, and the latter because of its relativism, foundationalism, and utopianism. We propose a new approach, based on J?rgen Habermas's theory of democracy, and define the new role of the business firm as a political actor in a globalizing society.",poster,cp6
Sociology,p2155,d4,b550162d7f267f48f83099afc29133d2f551eede,c97,International Conference on Computational Logic,Crowdsourcing: Why the Power of the Crowd is Driving the Future of Business,"By Jeff Howe, Published by the Crown Publishing Group, a division of Random House, Inc., 1745 Broadway, New York, NY 10019, 2008. vii + 311 p. Price $27.

A new concept has emerged that is changing the way the business world operates. Many research and development (R&D) problems in a particular",poster,cp97
Sociology,p2165,d4,d79c22e0702fe119ff0f3f0d64369a0a4452e273,c7,International Symposium on Intelligent Data Analysis,Qualitative Research in Business & Management,PART ONE: INTRODUCTION How to Use This Book Overview of Qualitative Research PART TWO: FUNDAMENTAL CONCEPTS OF RESEARCH Research Design Philosophical Perspectives Ethics PART THREE: QUALITATIVE RESEARCH METHODS Action Research Case Study Research Ethnographic Research Grounded Theory PART FOUR: DATA COLLECTION TECHNIQUES Interviews Participant Observation and Fieldwork Using Documents PART FIVE: ANALYSING QUALITATIVE DATA Analysing Qualitative Data: An Overview Hermeneutics Semiotics Narrative Analysis PART SIX: WRITING UP AND PUBLISHING Writing up Getting Published PART SEVEN: CONCLUSION Qualitative Research in Perspective,poster,cp7
Sociology,p2190,d4,4b997f4dd0101f0fac69bce44c7ba08af5b8ab4c,c82,Symposium on Networked Systems Design and Implementation,Theory Z: How American Business Can Meet the Japanese Challenge.,"Proposes a new form of business management that focuses on long-range planning, strong corporate philosophy, and concensus decision-making to help American corporations meet the challenge of Japan.",poster,cp82
Business,p2190,d9,4b997f4dd0101f0fac69bce44c7ba08af5b8ab4c,c82,Symposium on Networked Systems Design and Implementation,Theory Z: How American Business Can Meet the Japanese Challenge.,"Proposes a new form of business management that focuses on long-range planning, strong corporate philosophy, and concensus decision-making to help American corporations meet the challenge of Japan.",poster,cp82
Sociology,p2192,d4,28fd8d035b5250f553af4ef377dea5d05a782815,j419,Business Ethics Quarterly,Managing Social-Business Tensions: A Review and Research Agenda for Social Enterprise,"ABSTRACT: In a world filled with poverty, environmental degradation, and moral injustice, social enterprises offer a ray of hope. These organizations seek to achieve social missions through business ventures. Yet social missions and business ventures are associated with divergent goals, values, norms, and identities. Attending to them simultaneously creates tensions, competing demands, and ethical dilemmas. Effectively understanding social enterprises therefore depends on insight into the nature and management of these tensions. While existing research recognizes tensions between social missions and business ventures, we lack any systematic analysis. Our paper addresses this issue. We first categorize the types of tensions that arise between social missions and business ventures, emphasizing their prevalence and variety. We then explore how four different organizational theories offer insight into these tensions, and we develop an agenda for future research. We end by arguing that a focus on social-business tensions not only expands insight into social enterprises, but also provides an opportunity for research on social enterprises to inform traditional organizational theories. Taken together, our analysis of tensions in social enterprises integrates and seeks to energize research on this expanding phenomenon.",fullPaper,jv419
Political Science,p2192,d15,28fd8d035b5250f553af4ef377dea5d05a782815,j419,Business Ethics Quarterly,Managing Social-Business Tensions: A Review and Research Agenda for Social Enterprise,"ABSTRACT: In a world filled with poverty, environmental degradation, and moral injustice, social enterprises offer a ray of hope. These organizations seek to achieve social missions through business ventures. Yet social missions and business ventures are associated with divergent goals, values, norms, and identities. Attending to them simultaneously creates tensions, competing demands, and ethical dilemmas. Effectively understanding social enterprises therefore depends on insight into the nature and management of these tensions. While existing research recognizes tensions between social missions and business ventures, we lack any systematic analysis. Our paper addresses this issue. We first categorize the types of tensions that arise between social missions and business ventures, emphasizing their prevalence and variety. We then explore how four different organizational theories offer insight into these tensions, and we develop an agenda for future research. We end by arguing that a focus on social-business tensions not only expands insight into social enterprises, but also provides an opportunity for research on social enterprises to inform traditional organizational theories. Taken together, our analysis of tensions in social enterprises integrates and seeks to energize research on this expanding phenomenon.",fullPaper,jv419
Sociology,p2195,d4,c8b8b01062f7e48df1171f64622dfbbdf2df1567,c94,International Conferences on Contemporary Computing and Informatics,Global Business Regulation,"Across an amazing sweep of the critical areas of business regulation - from contract, intellectual property and corporations law, to trade, telecommunications, labour standards, drugs, food, transport and environment - this book confronts the question of how the regulation of business has shifted from national to global institutions. Based on interviews with 500 international leaders in business and government, this book examines the role played by global institutions such as the WTO, the OECD, IMF, Moody's and the World Bank, as well as various NGOs and significant individuals. The authors argue that effective and decent global regulation depends on the determination of individuals to engage with powerful agendas and decision-making bodies that would otherwise be dominated by concentrated economic interests. This book will become a standard reference for readers in business, law, politics and international relations.",poster,cp94
Political Science,p2195,d15,c8b8b01062f7e48df1171f64622dfbbdf2df1567,c94,International Conferences on Contemporary Computing and Informatics,Global Business Regulation,"Across an amazing sweep of the critical areas of business regulation - from contract, intellectual property and corporations law, to trade, telecommunications, labour standards, drugs, food, transport and environment - this book confronts the question of how the regulation of business has shifted from national to global institutions. Based on interviews with 500 international leaders in business and government, this book examines the role played by global institutions such as the WTO, the OECD, IMF, Moody's and the World Bank, as well as various NGOs and significant individuals. The authors argue that effective and decent global regulation depends on the determination of individuals to engage with powerful agendas and decision-making bodies that would otherwise be dominated by concentrated economic interests. This book will become a standard reference for readers in business, law, politics and international relations.",poster,cp94
Sociology,p2216,d4,40e74d3936c3ef767348063323a2741c2e02c3b6,c64,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",No business is an island: The network concept of business strategy,Abstract,poster,cp64
Sociology,p2234,d4,721c5511b3a2967c3fd0169224bb13f91da0aa15,c36,International Conference on Information Technology Based Higher Education and Training,Strategic Management and Business Policy,"I. INTRODUCTION TO STRATEGIC MANAGEMENT AND BUSINESS POLICY. 1. Basic Concepts in Strategic Management. 2. Corporate Governance and Social Responsibility. II. SCANNING THE ENVIRONMENT. 3. Environmental Scanning and Industry Analysis. 4. Internal Scanning and Organizational Analysis. III. STRATEGY FORMULATION. 5. Strategy Formulation: Situation Analysis and Business Strategy. 6. Strategy Formulation: Corporate Strategy. 7. Strategy Formulation: Functional Strategy and Strategic Choice. IV. STRATEGY IMPLEMENTATION AMD CONTROL. 8. Strategy Implementation: Organizing for Action. 9. Strategy Implementation: Staffing and Directing. 10. Evaluation and Control. V. OTHER STRATEGIC ISSUES. 11. Strategic Issues in Managing Technology and Innovation. 12. Strategic Issues in Entrepreneurial Ventures and Small Businesses. 13. Strategic Issues in Not-for-Profit Organizations. VI. INTRODUCTION TO CASE ANALYSIS. 14. Suggestions for Case Analysis. VII. CASES IN STRATEGIC MANAGEMENT. Section A. Corporate Governance, Social Responsibility and Ethics. The Recalcitrant Director at Byte Products, Inc.: Corporate Legality versus Corporate Responsibility, Dan R. Dalton, Richard A. Cosier, Cathy A. Enz. The Wallace Group, Laurence J. Stybel. The Audit, John A. Kilpatrick, Gamewell D. Gantt, George A. Johnson. McKesson Makes a Deal, Phyllis G. Holland, John E. Oliver, Peter M. Bergevin, Kenneth L. Stanley. Section B. International Issues in Strategic Management. Singapore Telecom: Strategic Challenges in a Turbulent Environment, Loizos Heracleous, Kulwant Singh. Hewlett-Packard Company in Vietnam, Geok Theng Lau. The Body Shop International PLC: Anita Roddick, OBE, Ellie A. Forgart, Joyce P. Vincelette, Thomas L. Wheelen. Waterford Wedgwood, PLC. (2000): The Millennium, Thomas L. Wheelen, Edward Kosobov, Philip H. Anderson, Kathryn E. Wheelen. Section C. General Issues in Strategic Management. Industry One: Internet/Software. Larry J. Ellison-Entrepreneurial Spirit at Oracle Corporation (2000), Joyce P. Vincelette, Ellie A. Fogarty, Thomas L. Wheelen. Palm Computing, Inc. 2002: How to Survive in the Crowded PDA Market? Cynthia Tonucci, Alan N. Hoffman. Handspring, Inc. 2002, Lisa-Marie Mulkern, Alan N. Hoffman. Apple Computer Inc. (2000): Here We Go Again, David B. Croll, Gordon P. Croll, Andrew J. Croll. Industry Two: Internet Companies. WingspanBank.com, Laura Cooke, Liza Hovey, Hyung Kim, Paul Rakowski. drkoop.com, Nicole Herskowitz, Fred Howard, Michael Iverson, Janet Mehlhop, Pilar Speer. Industry Three: Recreation and Leisure. Harley-Davidson, Inc. 2002: The 100th Anniversary, Patricia A. Ryan, Thomas L. Wheelen. Carnival Corporation (2002), Michael J. Keeffe, John K. Ross, III, Bill J. Middlebrook. Reebok International, Ltd. 2002 Thomas L. Wheelen, Moustafa H. Abdelsamad, Richard D. Wheelen, Thomas L. Wheelen II. Industry Four: Major Home Appliances. U.S. Major Home Appliance Industry in 2002: Competition Becomes Global, J. David Hunger. Maytag Corporation (2002), J. David Hunger. Industry Five: Mass Merchandising. Kmart Corporation: Seeking Survival in a Changing Marketplace 2002, James W. Camerius. Wal-Mart Stores, Inc.: On Becoming the World's Largest Company 2002, James W. Camerius. Industry Six: Specialty Retailing/Distribution. The Home Depot, Inc.: Growing the Professional Market (Revised), Thomas L. Wheelen, Hitesh (John) P. Adhia, Thomas H. Cangley, Paul M. Sweircz. Gardner Distribution Co.-Providing Products for Plants and Pets, Tom Hinthorne. Industry Seven: Entrepreneurial Ventures. Adrenaline Air Sports, Larry Alexander, Billy Cockrell, Jonathan Charlton. 25. Inner-City Paint Corporation Revised, Donald K. Kuratko, Norman J. Gierlasinski. Guajilote Cooperativo Forestal: Honduras, Nathan Nebbe, J. David Hunger. Industry Eight: Manufacturing. The Vermont Teddy Bear Co., Inc. (2002): Challenges Facing a New CEO (Revised), Joyce P. Vincelette, Ellie A. Fogarty, Thomas M. Patrick, Thomas L. Wheelen. The Carey Plant, Thomas L. Wheelen, J. David Hunger. Industry Nine: Beverage/Food. Arm & Hammer 2002: Church & Dwight Grows Up, Roy A. Cook. Redhock Ale Brewery, Stephen E. Barndt. Industry Ten: Transportation. American Airlines' Dilemma: 2002, Richard C. Scamehorn. The Boeing Commercial Group: Decision 2001, Richard C. Scamehorn. Mercedes-Benz and ""Swatch"": Inventing the Smart and Networked Organization, Eric Pfaffmann, Ben M. Bensaou. Section D. Issues in Not-for-Profit Organizations. A.W.A.R.E. Always Wanted a Riding Experience, John K. Ross, III, Eric G. Kirby. Section E. Internet Research Mini-Cases. eBay, Maryanne Rouse. Hershey Foods Corporation, Maryanne Rouse. AirTran Holdings, Inc. Maryanne Rouse. Tyson Foods, Inc. Maryanne Rouse. Eli Lily & Company, Maryanne Rouse. Section F. Additional internet Research Mini-Cases Available on Web Site. Southwest Airlines Company, Maryanne Rouse. Stryker Corporation, Maryanne Rouse. H.J. Heinz Company, Maryanne Rouse. Williams-Sonoma, Maryanne Rouse. Pfizer, Inc. Maryanne Rouse.",poster,cp36
Sociology,p2235,d4,66e5f69ec37acb717bc5a646507a210e42e3ed1f,c106,International Conference on Biometrics,The Business Model in Practice and its Implications for Entrepreneurship Research,"While the term “business model” has gained widespread use in the practice community, the academic literature on this topic is fragmented and confounded by inconsistent definitions and construct boundaries. In this study, we review prior research and reframe the business model with an entrepreneurial lens. We report on a discourse analysis of 151 surveys of practicing managers to better understand their conceptualization of a business model. We find that the underlying dimensions of the business model are resource structure, transactive structure, and value structure, and discuss the nature and implications of dimensional dominance for firm characteristics and behavior. These findings provide new directions for theory development and empirical studies in entrepreneurship by linking the business model to entrepreneurial cognition, opportunity co–creation, and organizational outcomes.",poster,cp106
Sociology,p2256,d4,3e03a20dd9944b9a96825d1686700f97044790e5,c101,Interspeech,The Sicilian Mafia: The Business of Private Protection,Abstract,poster,cp101
Sociology,p2286,d4,ef1d3314910efd626510396293feeef5997710fd,c112,British Machine Vision Conference,The business model: A theoretically anchored robust construct for strategic analysis,"Anchored in our research on business models, we delineate in this article a future research agenda. We establish that the theoretical and empirical advancements in business model research provide solid conceptual and empirical foundations on which scholars can build in order to explore a range of important, yet unanswered research questions. We draw inspiration on the direction of the business model research agenda by briefly reviewing several distinct bodies of literature adjacent to the business model literature including new organizational forms, ecosystems, activity systems, and value chain. In doing so, we also distinguish the business model concept from seemingly similar concepts that have been proposed by researchers.",poster,cp112
Sociology,p2297,d4,64b1557928ae8edf376b44b0b746ecad10ed743d,j406,Journal of Business Ethics,Meaningful Work: Connecting Business Ethics and Organization Studies,Abstract,fullPaper,jv406
Sociology,p2310,d4,f4d6c5db5585f4fc3a01c4c6d8f283f7538a9d05,c21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,"Some Research Perspectives on Entrepreneurship Education, Enterprise Education and Education for Small Business Management: A Ten-Year Literature Review","GARY GORMAN IS AN ASSOCIATE DEAN AND associate professor and Dennis Hanlon an assistant professor at the Faculty of Business Administration, Memorial University ofNewfoundland, Canada, and Wayne King is director of the P. J. Gardiner Institute for Small Business Studies as well as an assistant professor at Memorial University of Newfoundland. This paper reviews the literature in the areas of entrepreneurship education, enterprise education and education for small business management. The review covers the period from 1985 to 1994 inclusive and is limited to mainstream journals that focus on entrepreneurship and small business. Theoretical and empirical papers are examined from the perspective of content and market focus. The paper also suggests directions for future research.",poster,cp21
Sociology,p2367,d4,cb5aca734abff75d1cacf980926c89387a6dc3e4,c3,Knowledge Discovery and Data Mining,Theory of Business Enterprise,"Veblen has been claimed and rejected both by sociologists and economists as being one of theirs. He enriched and attacked both disciplines, as he did so many others: philosophy, history, social psychology, politics, and linguistics. Because he took all knowledge as necessary and relevant to adequate understanding, Veblen was a holistic analyst of the social process. First published in 1904, this classic analysis of the U.S. economy has enduring value today. In it, Veblen posited a theory of business fluctuations and economic growth which included chronic depression and inflation. He predicted the socioeconomic changes that would occur as a result: militarism, imperialism, fascism, consumerism, and the development of the mass media as well as the corporate bureaucracy. Douglas Dowd's introduction places the volume within the traditions of both macroeconomics and microeconomics, tracing Veblen's place among social thinkers, and the place of this volume in the body of his work.",poster,cp3
Sociology,p2391,d4,42ec03d7615d518f225cf8ebd2ef74927577328b,c46,Ideal,"Small-Business Owner-Managers’ Perceptions of Business Ethics 
and CSR-Related Concepts",Abstract,poster,cp46
Economics,p2391,d11,42ec03d7615d518f225cf8ebd2ef74927577328b,c46,Ideal,"Small-Business Owner-Managers’ Perceptions of Business Ethics 
and CSR-Related Concepts",Abstract,poster,cp46
Sociology,p2398,d4,c16251aaf6f369cebb843bb64bb8c1b8ecb5b55a,c111,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,Introduction to From Higher Aims to Hired Hands The Social Transformation of American Business Schools and the Unfulfilled Promise of Management as a Profession,"Is management a profession? Should it be? Can it be? This major work of social and intellectual history reveals how such questions have driven business education and shaped American management and society for more than a century. The book is also a call for reform. Rakesh Khurana shows that university-based business schools were founded to train a professional class of managers in the mold of doctors and lawyers but have effectively retreated from that goal, leaving a gaping moral hole at the center of business education and perhaps in management itself. Khurana begins in the late nineteenth century, when members of an emerging managerial elite, seeking social status to match the wealth and power they had accrued, began working with major universities to establish graduate business education programs paralleling those for medicine and law. Constituting business as a profession, however, required codifying the knowledge relevant for practitioners and developing enforceable standards of conduct. Khurana, drawing on a rich set of archival material from business schools, foundations, and academic associations, traces how business educators confronted these challenges with varying strategies during the Progressive era and the Depression, the postwar boom years, and recent decades of freewheeling capitalism. Today, Khurana argues, business schools have largely capitulated in the battle for professionalism and have become merely purveyors of a product, the MBA, with students treated as consumers. Professional and moral ideals that once animated and inspired business schools have been conquered by a perspective that managers are merely agents of shareholders, beholden only to the cause of share profits. According to Khurana, we should not thus be surprised at the rise of corporate malfeasance. The time has come, he concludes, to rejuvenate intellectually and morally the training of our future business leaders.",poster,cp111
Political Science,p2398,d15,c16251aaf6f369cebb843bb64bb8c1b8ecb5b55a,c111,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,Introduction to From Higher Aims to Hired Hands The Social Transformation of American Business Schools and the Unfulfilled Promise of Management as a Profession,"Is management a profession? Should it be? Can it be? This major work of social and intellectual history reveals how such questions have driven business education and shaped American management and society for more than a century. The book is also a call for reform. Rakesh Khurana shows that university-based business schools were founded to train a professional class of managers in the mold of doctors and lawyers but have effectively retreated from that goal, leaving a gaping moral hole at the center of business education and perhaps in management itself. Khurana begins in the late nineteenth century, when members of an emerging managerial elite, seeking social status to match the wealth and power they had accrued, began working with major universities to establish graduate business education programs paralleling those for medicine and law. Constituting business as a profession, however, required codifying the knowledge relevant for practitioners and developing enforceable standards of conduct. Khurana, drawing on a rich set of archival material from business schools, foundations, and academic associations, traces how business educators confronted these challenges with varying strategies during the Progressive era and the Depression, the postwar boom years, and recent decades of freewheeling capitalism. Today, Khurana argues, business schools have largely capitulated in the battle for professionalism and have become merely purveyors of a product, the MBA, with students treated as consumers. Professional and moral ideals that once animated and inspired business schools have been conquered by a perspective that managers are merely agents of shareholders, beholden only to the cause of share profits. According to Khurana, we should not thus be surprised at the rise of corporate malfeasance. The time has come, he concludes, to rejuvenate intellectually and morally the training of our future business leaders.",poster,cp111
Sociology,p2402,d4,c7064a44cb1b4fe0d3b6ff9ae834f13e8c00935d,c102,ACM SIGMOD Conference,The Oxford Handbook of International Business,"The Oxford Handbook of International Business comprises twenty-eight original chapters from the world's most distinguished scholars in the field of international business. United as a whole, these reflect both the present structure of the field and the major intellectual issues of current and likely future interest. Demonstrating analytical insight and critical thinking, the authors are all authorities on their chosen topics and have been active as leaders in the Academy of International Business. Their chapters survey and synthesize relevant literature of recent years, ensuring that the volume will be a primary reference for students and scholars throughout the social sciences. The book is split into five major sections, providing comprehensive coverage of the following areas: the history and theory of the multinational enterprise; the political and policy environment of international business; strategies of multinational enterprises; the financial areas of the multinational enterprise (marketing, finance and accounting, HRM, and innovation); and business systems in Asia, South America, and the transitional economies. Contributors to this volume - David M. Berg (Assistant Professor, School of Management, University of Texas at Dallas) Julian Birkinshaw (Associate Professor, London Business School) Michael Bowe (Senior Lecturer in Economics and International Finance, Manchester School of Management, UMIST) Thomas L. Brewer (Professor and Editor of Journal of International Business Studies, Georgetown University) Peter Buckley (Professor and Director, Centre for International Business, University of Leeds) John Cantwell (Professor of Economics, University of Reading) Mark Casson (Professor of Economics, University of Reading) John Child (Chair of Commerce, University of Birmingham and Visiting Professor of Management, University of Hong Kong) James W. Dean (Professor of Economics, Simon Fraser University and Kaiser Professor of International Business, Western Washington University) John Dunning (Emeritus Professor of International Business, University of Reading and Emeritus State of New Jersey Professor of International Business, Rutgers University) Lorraine Eden (Associate Professor, Department of Management, Texas A&M University) John L Graham (Professor, Graduate School of Management, University of California, Irvine) Robert Grosse (Professor of World Business and Director of Research, Thunderbird, The American Graduate School of International Management) Stephen Guisinger (Professor of International Management, University of Texas at Dallas) Jean-Francois Hennart (Professor, Department of Organization and Strategy, Tilburg University) Andrew C. Inkpen (Associate Professor, Thunderbird, American Graduate School of International Management) Stephen J. Kobrin (William Wurster Professor of Multinational Management, The Wharton School, University of Pennsylvania) Bruce Kogut (The Dr. Felix Zandman Professor of International Management, The Wharton School, University of Pennsylvania) Masaaki (Mike) Kotabe (The Washburn Chair of International Business and Marketing, Temple University) James R. Markusen (Professor of Economics, University of Colorado, Boulder, NBER and CEPR) Klaus E. Meyer (Associate Professor, Center for East European Studies, Copenhagen Business School) Sylvia Ostry (Distinguished Research Fellow, Munk Centre for International Studies, University of Toronto) Gordon Redding (Affiliate Professor of Asian Business, INSEAD) Alan M. Rugman (Professor of Management, Professor of Business Economics and Public Policy, Indiana University) Debora L. Spar (Professor, Harvard Business School) Stephen B. Tallman (Professor of Management, David Eccles School of Business, University of Utah) Alain Verbeke (Professor of International Business, University of Brussels and Associate Fellow, Templeton College, University of Oxford) D. Eleanor Westney (Sloan Fellows Professor, Sloan School of Management, Massachusetts Institute of Technology) Mira Wilkins (Professor of Economics, Florida International University) George S. Yip (Professor of Strategic and International Management, London Business School) Stephen Young (Professor and Co-Director, Strathclyde International Business Unit, University of Strathclyde) Srilata Zaheer (Professor, Carlson School of Management, University of Minnesota)",poster,cp102
Sociology,p2424,d4,95118ba8358cc4b06e64eb2bcad2dee3db19f280,c89,Conference on Uncertainty in Artificial Intelligence,Business on a handshake,Abstract,poster,cp89
Sociology,p2434,d4,8e20810a0d4ad1cb482b8a0ee7ba037aaa4c3e75,j419,Business Ethics Quarterly,Virtue Ethics as a Resource in Business,"ABSTRACT: This article provides an account of virtues as praiseworthy traits of character with a far-reaching capacity to influence conduct. Virtues supply their possessors both with good reasons that indicate, for diverse contexts, what sort of thing should be done and with motivation to do them. This motivational power of virtue is crucial for the question of what kind of person, or businessperson, one wants to be. The article shows how the contrast between virtue ethics and rule ethics is often drawn too sharply and indicates how virtue theories can incorporate both theoretical and practical uses of rules. More generally, it shows how a virtue orientation affects attitudes in management practices and how an understanding of certain virtues can help in making better decisions, both ethically and in relation to success in business.",fullPaper,jv419
Sociology,p2494,d4,f81249e49b18115e0041ba9f8aeb8e28b5299284,c83,"International Convention on Information and Communication Technology, Electronics and Microelectronics",Business strategies in transition economies,"Business Strategies An Overview Economies in Transition Institutions, Organizations and Strategic Choices Strategies of State-Owned Enterprises Strategies of Privatized and Reformed Firms Strategies of Entrepreneurial Start-Ups Strategies of Foreign Companies Retrospect and Road Ahead",poster,cp83
Biology,p337,d5,54d9fc3ed4937ee546ed45aee7bef16b4ae3775d,c94,International Conferences on Contemporary Computing and Informatics,Statistics for citizen science: extracting signals of change from noisy ecological data,"Policy‐makers increasingly demand robust measures of biodiversity change over short time periods. Long‐term monitoring schemes provide high‐quality data, often on an annual basis, but are taxonomically and geographically restricted. By contrast, opportunistic biological records are relatively unstructured but vast in quantity. Recently, these data have been applied to increasingly elaborate science and policy questions, using a range of methods. At present, we lack a firm understanding of which methods, if any, are capable of delivering unbiased trend estimates on policy‐relevant time‐scales. We identified a set of candidate methods that employ data filtering criteria and/or correction factors to deal with variation in recorder activity. We designed a computer simulation to compare the statistical properties of these methods under a suite of realistic data collection scenarios. We measured the Type I error rates of each method–scenario combination, as well as the power to detect genuine trends. We found that simple methods produce biased trend estimates, and/or had low power. Most methods are robust to variation in sampling effort, but biases in spatial coverage, sampling effort per visit, and detectability, as well as turnover in community composition, all induced some methods to fail. No method was wholly unaffected by all forms of variation in recorder activity, although some performed well enough to be useful. We warn against the use of simple methods. Sophisticated methods that model the data collection process offer the greatest potential to estimate timely trends, notably Frescalo and occupancy–detection models. The potential of these methods and the value of opportunistic data would be further enhanced by assessing the validity of model assumptions and by capturing small amounts of information about sampling intensity at the point of data collection.",poster,cp94
Biology,p443,d5,2e888654c68524163fbf7a54396488249e73a702,c98,Vision,Citizen Science: A Developing Tool for Expanding Science Knowledge and Scientific Literacy,"Citizen science enlists the public in collecting large quantities of data across an array of habitats and locations over long spans of time. Citizen science projects have been remarkably successful in advancing scientific knowledge, and contributions from citizen scientists now provide a vast quantity of data about species occurrence and distribution around the world. Most citizen science projects also strive to help participants learn about the organisms they are observing and to experience the process by which scientific investigations are conducted. Developing and implementing public data-collection projects that yield both scientific and educational outcomes requires significant effort. This article describes the model for building and operating citizen science projects that has evolved at the Cornell Lab of Ornithology over the past two decades. We hope that our model will inform the fields of biodiversity monitoring, biological research, and science education while providing a window into the culture of citizen science.",poster,cp98
Biology,p1510,d5,93d5369a0be3134c6018373d5290923f3d718815,c85,IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering,The Molecular Signatures Database Hallmark Gene Set Collection,Abstract,poster,cp85
Biology,p1536,d5,80e394ee3e1834091596e8b55c9ad9bf11456e09,j6,Genome Biology,"DAVID: Database for Annotation, Visualization, and Integrated Discovery",Abstract,fullPaper,jv6
Biology,p1637,d5,683bc72f18d2eede7d4c46c55537cbce63a47f62,c21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,Tmbase-A database of membrane spanning protein segments,Abstract,poster,cp21
Biology,p1641,d5,2ea5d8555fe5efb7b35bc40e55ead122cfac03b4,j374,Global Change Biology,TRY – a global database of plant traits,"Plant traits – the morphological, anatomical, physiological, biochemical and phenological characteristics of plants and their organs – determine how primary producers respond to environmental factors, affect other trophic levels, influence ecosystem processes and services and provide a link from species richness to ecosystem functional diversity. Trait data thus represent the raw material for a wide range of research from evolutionary biology, community and functional ecology to biogeography. Here we present the global database initiative named TRY, which has united a wide range of the plant trait research community worldwide and gained an unprecedented buy‐in of trait data: so far 93 trait databases have been contributed. The data repository currently contains almost three million trait entries for 69 000 out of the world's 300 000 plant species, with a focus on 52 groups of traits characterizing the vegetative and regeneration stages of the plant life cycle, including growth, dispersal, establishment and persistence. A first data analysis shows that most plant traits are approximately log‐normally distributed, with widely differing ranges of variation across traits. Most trait variation is between species (interspecific), but significant intraspecific variation is also documented, up to 40% of the overall variation. Plant functional types (PFTs), as commonly used in vegetation models, capture a substantial fraction of the observed variation – but for several traits most variation occurs within PFTs, up to 75% of the overall variation. In the context of vegetation models these traits would better be represented by state variables rather than fixed parameter values. The improved availability of plant trait data in the unified global database is expected to support a paradigm shift from species to trait‐based ecology, offer new opportunities for synthetic plant trait research and enable a more realistic and empirically grounded representation of terrestrial vegetation in Earth system models.",fullPaper,jv374
Biology,p1652,d5,429c25227c2225447fd3bf3d17582a19671cc872,c95,Cyber ..,The PLANTS Database,Abstract,poster,cp95
Biology,p1674,d5,9162a7e9434022c2ed6f249b129d6e50b90eb1a3,j375,Biocontrol News and Information,Biological control of insect pests by insect parasitoids and predators: the BIOCAT database.,"The structure of the BIOCAT database, which contains records of the introductions of insect natural enemies for the control of insect pests worldwide, and is now available online, is explained. It is a useful summary of biological control effort and a guide to factors which may influence the success of introduction programmes, but is not detailed enough for making firm predictions.",fullPaper,jv375
Biology,p1756,d5,66c0e24d97786f0382ce9f7acde37de9a349537c,c44,Italian National Conference on Sensors,The Reptile Database,Abstract,poster,cp44
Biology,p1999,d5,54a54371577545b529a8ab53b421d14a58b33ba6,j214,Nature reviews genetics,Genetic association database,Abstract,fullPaper,jv214
Mathematics,p257,d6,b9111489ec08b50bc573982ede11f5bc2d7a4e88,c61,"International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems",Sjplot - Data Visualization For Statistics In Social Science.,"New functions


 tab_model() as replacement for sjt.lm() , sjt.glm() , sjt.lmer() and sjt.glmer() . Furthermore, tab_model() is designed to work with the same model-objects as plot_model() .
 New colour scales for ggplot-objects: scale_fill_sjplot() and scale_color_sjplot() . These provide predifined colour palettes from this package.
 show_sjplot_pals() to show all predefined colour palettes provided by this package.
 sjplot_pal() to return colour values of a specific palette.


Deprecated

Following functions are now deprecated:


 sjp.lm() , sjp.glm() , sjp.lmer() , sjp.glmer() and sjp.int() . Please use plot_model() instead.
 sjt.frq() . Please use sjmisc::frq(out = ""v"") instead.


Removed / Defunct

Following functions are now defunct:


 sjt.grpmean() , sjt.mwu() and sjt.df() . The replacements are sjstats::grpmean() , sjstats::mwu() and tab_df() resp. tab_dfs() .


Changes to functions


 plot_model() and plot_models() get a prefix.labels -argument, to prefix automatically retrieved term labels with either the related variable name or label.
 plot_model() gets a show.zeroinf -argument to show or hide the zero-inflation-part of models in the plot.
 plot_model() gets a jitter -argument to add some random variation to data points for those plot types that accept show.data = TRUE .
 plot_model() gets a legend.title -argument to define the legend title for plots that display a legend.
 plot_model() now passes more arguments in ... down to ggeffects::plot() for marginal effects plots.
 plot_model() now plots the zero-inflated part of the model for brmsfit -objects.
 plot_model() now plots multivariate response models, i.e. models with multiple outcomes.
 Diagnostic plots in plot_model() ( type = ""diag"" ) can now also be used with brmsfit -objects.
 Axis limits of diagnostic plots in plot_model() ( type = ""diag"" ) for Stan-models ( brmsfit or stanreg resp. stanfit ) can now be set with the axis.lim -argument.
 The grid.breaks -argument for plot_model() and plot_models() now also takes a vector of values to directly define the grid breaks for the plot.
 Better default calculation for grid breaks in plot_model() and plot_models() when the grid.breaks -argument is of length one.
 The terms -argument for plot_model() now also allows the specification of a range of numeric values in square brackets for marginal effects plots, e.g. terms = ""age [30:50]"" or terms = ""age [pretty]"" .
 For coefficient-plots, the terms - and rm.terms -arguments for plot_model() now also allows specification of factor levels for categorical terms. Coefficients for the indicted factor levels are kept resp. removed (see ?plot_model for details).
 plot_model() now supports clmm -objects (package ordinal).
 plot_model(type = ""diag"") now also shows random-effects QQ-plots for glmmTMB -models, and also plots random-effects QQ-plots for all random effects (if model has more than one random effect term).


Bug fixes


 plot_model(type = ""re"") now supports standard errors and confidence intervals for glmmTMB -objects.
 Fixed typo for glmmTMB -tidier, which may have returned wrong data for zero-inflation part of model.
 Multiple random intercepts for multilevel models fitted with brms area now shown in each own facet per intercept.
 Remove unnecessary warning in sjp.likert() for uneven category count when neutral category is specified.
 plot_model(type = ""int"") could not automatically select mdrt.values properly for non-integer variables.
 sjp.grpfrq() now correctly uses the complete space in facets when facet.grid = TRUE .
 sjp.grpfrq(type = ""boxplot"") did not correctly label the x-axis when one category had no elements in a vector.
 Problems with German umlauts when printing HTML tables were fixed.",poster,cp61
Mathematics,p355,d6,b7d5dda24d0c540929cd58b0226eac8a85e9769b,j118,Review of Economics and Statistics,Consistent Covariance Matrix Estimation with Spatially Dependent Panel Data,"Many panel data sets encountered in macroeconomics, international economics, regional science, and finance are characterized by cross-sectional or spatial dependence. Standard techniques that fail to account for this dependence will result in inconsistently estimated standard errors. In this paper we present conditions under which a simple extension of common nonparametric covariance matrix estimation techniques yields standard error estimates that are robust to very general forms of spatial and temporal dependence as the time dimension becomes large. We illustrate the relevance of this approach using Monte Carlo simulations and a number of empirical examples.",fullPaper,jv118
Mathematics,p358,d6,7fc70d4cc5118fdbc8e8807979eae8b61948ff91,c21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,"The elements of statistical learning: data mining, inference and prediction",Abstract,poster,cp21
Mathematics,p380,d6,fe5bb5d8d6b7ac251d87bc16e75ea5889cc92425,j128,Political Science Research and Methods,Explaining Fixed Effects: Random Effects Modeling of Time-Series Cross-Sectional and Panel Data*,"This article challenges Fixed Effects (FE) modeling as the ‘default’ for time-series-cross-sectional and panel data. Understanding different within and between effects is crucial when choosing modeling strategies. The downside of Random Effects (RE) modeling—correlated lower-level covariates and higher-level residuals—is omitted-variable bias, solvable with Mundlak's (1978a) formulation. Consequently, RE can provide everything that FE promises and more, as confirmed by Monte-Carlo simulations, which additionally show problems with Plümper and Troeger's FE Vector Decomposition method when data are unbalanced. As well as incorporating time-invariant variables, RE models are readily extendable, with random coefficients, cross-level interactions and complex variance functions. We argue not simply for technical solutions to endogeneity, but for the substantive importance of context/heterogeneity, modeled using RE. The implications extend beyond political science to all multilevel datasets. However, omitted variables could still bias estimated higher-level variable effects; as with any model, care is required in interpretation.",fullPaper,jv128
Mathematics,p389,d6,bfcf1ed94050a4c60d459cd02456dfd9f08fdb4c,c121,International Conference on Interaction Sciences,"Statistics for experimenters : an introduction to design, data analysis, and model building","Science and Statistics. COMPARING TWO TREATMENTS. Use of External Reference Distribution to Compare Two Means. Random Sampling and the Declaration of Independence. Randomization and Blocking with Paired Comparisons. Significance Tests and Confidence Intervals for Means, Variances, Proportions and Frequences. COMPARING MORE THAN TWO TREATMENTS. Experiments to Compare k Treatment Means. Randomized Block and Two--Way Factorial Designs. Designs with More Than One Blocking Variable. MEASURING THE EFFECTS OF VARIABLES. Empirical Modeling. Factorial Designs at Two Levels. More Applications of Factorial Designs. Fractional Factorial Designs at Two Levels. More Applications of Fractional Factorial Designs. BUILDING MODELS AND USING THEM. Simple Modeling with Least Squares (Regression Analysis). Response Surface Methods. Mechanistic Model Building. Study of Variation. Modeling Dependence: Times Series. Appendix Tables. Index.",poster,cp121
Business,p389,d9,bfcf1ed94050a4c60d459cd02456dfd9f08fdb4c,c121,International Conference on Interaction Sciences,"Statistics for experimenters : an introduction to design, data analysis, and model building","Science and Statistics. COMPARING TWO TREATMENTS. Use of External Reference Distribution to Compare Two Means. Random Sampling and the Declaration of Independence. Randomization and Blocking with Paired Comparisons. Significance Tests and Confidence Intervals for Means, Variances, Proportions and Frequences. COMPARING MORE THAN TWO TREATMENTS. Experiments to Compare k Treatment Means. Randomized Block and Two--Way Factorial Designs. Designs with More Than One Blocking Variable. MEASURING THE EFFECTS OF VARIABLES. Empirical Modeling. Factorial Designs at Two Levels. More Applications of Factorial Designs. Fractional Factorial Designs at Two Levels. More Applications of Fractional Factorial Designs. BUILDING MODELS AND USING THEM. Simple Modeling with Least Squares (Regression Analysis). Response Surface Methods. Mechanistic Model Building. Study of Variation. Modeling Dependence: Times Series. Appendix Tables. Index.",poster,cp121
Mathematics,p573,d6,1e97929230cdcfc13584c3c66f0ecf5cff9be5e2,c43,European Conference on Machine Learning,Influence of Data Splitting on Performance of Machine Learning Models in Prediction of Shear Strength of Soil,"The main objective of this study is to evaluate and compare the performance of different machine learning (ML) algorithms, namely, Artificial Neural Network (ANN), Extreme Learning Machine (ELM), and Boosting Trees (Boosted) algorithms, considering the influence of various training to testing ratios in predicting the soil shear strength, one of the most critical geotechnical engineering properties in civil engineering design and construction. For this aim, a database of 538 soil samples collected from the Long Phu 1 power plant project, Vietnam, was utilized to generate the datasets for the modeling process. Different ratios (i.e., 10/90, 20/80, 30/70, 40/60, 50/50, 60/40, 70/30, 80/20, and 90/10) were used to divide the datasets into the training and testing datasets for the performance assessment of models. Popular statistical indicators, such as Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and Correlation Coefficient (R), were employed to evaluate the predictive capability of the models under different training and testing ratios. Besides, Monte Carlo simulation was simultaneously carried out to evaluate the performance of the proposed models, taking into account the random sampling effect. The results showed that although all three ML models performed well, the ANN was the most accurate and statistically stable model after 1000 Monte Carlo simulations (Mean R = 0.9348) compared with other models such as Boosted (Mean R = 0.9192) and ELM (Mean R = 0.8703). Investigation on the performance of the models showed that the predictive capability of the ML models was greatly affected by the training/testing ratios, where the 70/30 one presented the best performance of the models. Concisely, the results presented herein showed an effective manner in selecting the appropriate ratios of datasets and the best ML model to predict the soil shear strength accurately, which would be helpful in the design and engineering phases of construction projects.",poster,cp43
Mathematics,p696,d6,51a66bf3e76cd0457f7533a9449c3410fc72bf4f,c99,Symposium on the Theory of Computing,Foundations for microwave engineering,Chapter 1: Introduction Chapter 2: Electromagnetic Theory Chapter 3: Transmission Line and Waveguides Chapter 4: Circuit Theory for Waveguiding Systems Chapter 5: Impedence Transformations and Matching Chapter 6: Passive Microwave Devices Chapter 7: Electromagnetic Resonators Chapter 8: Periodic Structures and Filters Chapter 9: Microwave Tubes Chapter 10: Solid State Amplifiers Chapter 11: Parametric Amplifiers Chapter 12: Oscillators and Mixers Appendix One: Useful Relations from Vector Analysis Appendix Two: Bessel Functions Appendix Three: Conformal Mapping Techniques Appendix Four: Physical Constants and Other Data,poster,cp99
Mathematics,p794,d6,445ccb9c6bf47fdf71bc9f14ee1b775b08096da8,c1,International Conference on Human Factors in Computing Systems,Some applications of statistical methods to the analysis of physical and engineering data,"Whenever we measure any physical quantity we customarily obtain as many different values as there are observations. From a consideration of these measurements we must determine the most probable value; we must find out how much an observation may be expected to vary from this most probable value; and we must learn as much as possible of the reasons why it varies in the particular way that it does. In other words, the real value of physical measurements lies in the fact that from them it is possible to determine something of the nature of the results to be expected if the series of observations is repeated. The best use can be made of the data if we can find from them the most probable frequency or occurrence of any observed magnitude of the physical quantity or, in other words, the most probable law of distribution. It is customary practice in connection with physical and engineering measurements to assume that the arithmetic mean of the observations is the most probable value and that the frequency of occurrence of deviations from this mean is in accord with the Gaussian or normal law of error which lies at the foundation of the theory of errors. In most of those cases where the observed distributions of deviations have been compared with the theoretical ones based on the assumption of this law, it has been found highly improbable that the groups of observations could have arisen from systems of causes consistent with the normal law. Furthermore, even upon an a priori basis the normal law is a very limited case of a more generalized one. Therefore, in order to find the probability of the occurrence of a deviation of a given magnitude, it is necessary in most instances to find the theoretical distribution which is more probable than that given by the normal law. The present paper deals with the application of elementary statistical methods for finding this best frequency distribution of the deviations. In other words, the present paper points out some of the limitations of the theory of errors, based upon the normal law, in the analysis of physical and engineering data; it suggests methods for overcoming these difficulties by basing the analysis upon a more generalized law of error; it reviews the methods for finding the best theoretical distribution and closes with a discussion of the magnitude of the advantages to be gained by either the physicist or the engineer from an application of the methods reviewed herein.",poster,cp1
Mathematics,p817,d6,85ed1f128c65f00e7ab3c41f3af904e0fd3dae2f,c115,International Conference on Information Integration and Web-based Applications & Services,Reliability and Statistics in Geotechnical Engineering,"Preface. Part I. 1 Introduction - uncertainty and risk in geotechnical engineering. 1.1 Offshore platforms. 1.2 Pit mine slopes. 1.3 Balancing risk and reliability in a geotechnical design. 1.4 Historical development of reliability methods in civil engineering. 1.5 Some terminological and philosophical issues. 1.6 The organization of this book. 1.7 A comment on notation and nomenclature. 2 Uncertainty. 2.1 Randomness, uncertainty, and the world. 2.2 Modeling uncertainties in risk and reliability analysis. 2.3 Probability. 3 Probability. 3.1 Histograms and frequency diagrams. 3.2 Summary statistics. 3.3 Probability theory. 3.4 Random variables. 3.5 Random process models. 3.6 Fitting mathematical pdf models to data. 3.7 Covariance among variables. 4 Inference. 4.1 Frequentist theory. 4.2 Bayesian theory. 4.3 Prior probabilities. 4.4 Inferences from sampling. 4.5 Regression analysis. 4.6 Hypothesis tests. 4.7 Choice among models. 5 Risk, decisions and judgment. 5.1 Risk. 5.2 Optimizing decisions. 5.3 Non-optimizing decisions. 5.4 Engineering judgment. Part II. 6 Site characterization. 6.1 Developments in site characterization. 6.2 Analytical approaches to site characterization. 6.3 Modeling site characterization activities. 6.4 Some pitfalls of intuitive data evaluation. 6.5 Organization of Part II. 7 Classification and mapping. 7.1 Mapping discrete variables. 7.2 Classification. 7.3 Discriminant analysis. 7.4 Mapping. 7.5 Carrying out a discriminant or logistic analysis. 8 Soil variability. 8.1 Soil properties. 8.2 Index tests and classification of soils. 8.3 Consolidation properties. 8.4 Permeability. 8.5 Strength properties. 8.6 Distributional properties. 8.7 Measurement error. 9 Spatial variability within homogeneous deposits. 9.1 Trends and variations about trends. 9.2 Residual variations. 9.3 Estimating autocorrelation and autocovariance. 9.4 Variograms and geostatistics. Appendix: algorithm for maximizing log-likelihood of autocovariance. 10 Random field theory. 10.1 Stationary processes. 10.2 Mathematical properties of autocovariance functions. 10.3 Multivariate (vector) random fields. 10.4 Gaussian random fields. 10.5 Functions of random fields. 11 Spatial sampling. 11.1 Concepts of sampling. 11.2 Common spatial sampling plans. 11.3 Interpolating random fields. 11.4 Sampling for autocorrelation. 12 Search theory. 12.1 Brief history of search theory. 12.2 Logic of a search process. 12.3 Single stage search. 12.4 Grid search. 12.5 Inferring target characteristics. 12.6 Optimal search. 12.7 Sequential search. Part III. 13 Reliability analysis and error propagation. 13.1 Loads, resistances and reliability. 13.2 Results for different distributions of the performance function. 13.3 Steps and approximations in reliability analysis. 13.4 Error propagation - statistical moments of the performance function. 13.5 Solution techniques for practical cases. 13.6 A simple conceptual model of practical significance. 14 First order second moment (FOSM) methods. 14.1 The James Bay dikes. 14.2 Uncertainty in geotechnical parameters. 14.3 FOSM calculations. 14.4 Extrapolations and consequences. 14.5 Conclusions from the James Bay study. 14.6 Final comments. 15 Point estimate methods. 15.1 Mathematical background. 15.2 Rosenblueth's cases and notation. 15.3 Numerical results for simple cases. 15.4 Relation to orthogonal polynomial quadrature. 15.5 Relation with 'Gauss points' in the finite element method. 15.6 Limitations of orthogonal polynomial quadrature. 15.7 Accuracy, or when to use the point-estimate method. 15.8 The problem of the number of computation points. 15.9 Final comments and conclusions. 16 The Hasofer-Lind approach (FORM). 16.1 Justification for improvement - vertical cut in cohesive soil. 16.2 The Hasofer-Lind formulation. 16.3 Linear or non-linear failure criteria and uncorrelated variables. 16.4 Higher order reliability. 16.5 Correlated variables. 16.6 Non-normal variables. 17 Monte Carlo simulation methods. 17.1 Basic considerations. 17.2 Computer programming considerations. 17.3 Simulation of random processes. 17.4 Variance reduction methods. 17.5 Summary. 18 Load and resistance factor design. 18.1 Limit state design and code development. 18.2 Load and resistance factor design. 18.3 Foundation design based on LRFD. 18.4 Concluding remarks. 19 Stochastic finite elements. 19.1 Elementary finite element issues. 19.2 Correlated properties. 19.3 Explicit formulation. 19.4 Monte Carlo study of differential settlement. 19.5 Summary and conclusions. Part IV. 20 Event tree analysis. 20.1 Systems failure. 20.2 Influence diagrams. 20.3 Constructing event trees. 20.4 Branch probabilities. 20.5 Levee example revisited. 21 Expert opinion. 21.1 Expert opinion in geotechnical practice. 21.2 How do people estimate subjective probabilities? 21.3 How well do people estimate subjective probabilities? 21.4 Can people learn to be well-calibrated? 21.5 Protocol for assessing subjective probabilities. 21.6 Conducting a process to elicit quantified judgment. 21.7 Practical suggestions and techniques. 21.8 Summary. 22 System reliability assessment. 22.1 Concepts of system reliability. 22.2 Dependencies among component failures. 22.3 Event tree representations. 22.4 Fault tree representations. 22.5 Simulation approach to system reliability. 22.6 Combined approaches. 22.7 Summary. Appendix A: A primer on probability theory. A.1 Notation and axioms. A.2 Elementary results. A.3 Total probability and Bayes' theorem. A.4 Discrete distributions. A.5 Continuous distributions. A.6 Multiple variables. A.7 Functions of random variables. References. Index.",poster,cp115
Mathematics,p852,d6,53dc97756369cd1f9300116d6aabdffb7072f2ed,c117,Very Large Data Bases Conference,A Statistical View of Some Chemometrics Regression Tools,"Chemometrics is a field of chemistry that studies the application of statistical methods to chemical data analysis. In addition to borrowing many techniques from the statistics and engineering literatures, chemometrics itself has given rise to several new data-analytical methods. This article examines two methods commonly used in chemometrics for predictive modeling—partial least squares and principal components regression—from a statistical perspective. The goal is to try to understand their apparent successes and in what situations they can be expected to work well and to compare them with other statistical methods intended for those situations. These methods include ordinary least squares, variable subset selection, and ridge regression.",poster,cp117
Mathematics,p897,d6,a68dabe8715a68440821eaf2c54a79f33bb3109b,j215,Optimization and Engineering,Data fitting with geometric-programming-compatible softmax functions,Abstract,fullPaper,jv215
Mathematics,p931,d6,15b43cf0c415aa428b1c7913786cbfc4aded36be,c55,Design Automation Conference,Engineering Design via Surrogate Modelling: A Practical Guide,"Preface. About the Authors. Foreword. Prologue. Part I: Fundamentals. 1. Sampling Plans. 1.1 The 'Curse of Dimensionality' and How to Avoid It. 1.2 Physical versus Computational Experiments. 1.3 Designing Preliminary Experiments (Screening). 1.3.1 Estimating the Distribution of Elementary Effects. 1.4 Designing a Sampling Plan. 1.4.1 Stratification. 1.4.2 Latin Squares and Random Latin Hypercubes. 1.4.3 Space-filling Latin Hypercubes. 1.4.4 Space-filling Subsets. 1.5 A Note on Harmonic Responses. 1.6 Some Pointers for Further Reading. References. 2. Constructing a Surrogate. 2.1 The Modelling Process. 2.1.1 Stage One: Preparing the Data and Choosing a Modelling Approach. 2.1.2 Stage Two: Parameter Estimation and Training. 2.1.3 Stage Three: Model Testing. 2.2 Polynomial Models. 2.2.1 Example One: Aerofoil Drag. 2.2.2 Example Two: a Multimodal Testcase. 2.2.3 What About the k -variable Case? 2.3 Radial Basis Function Models. 2.3.1 Fitting Noise-Free Data. 2.3.2 Radial Basis Function Models of Noisy Data. 2.4 Kriging. 2.4.1 Building the Kriging Model. 2.4.2 Kriging Prediction. 2.5 Support Vector Regression. 2.5.1 The Support Vector Predictor. 2.5.2 The Kernel Trick. 2.5.3 Finding the Support Vectors. 2.5.4 Finding . 2.5.5 Choosing C and epsilon. 2.5.6 Computing epsilon : v -SVR 71. 2.6 The Big(ger) Picture. References. 3. Exploring and Exploiting a Surrogate. 3.1 Searching the Surrogate. 3.2 Infill Criteria. 3.2.1 Prediction Based Exploitation. 3.2.2 Error Based Exploration. 3.2.3 Balanced Exploitation and Exploration. 3.2.4 Conditional Likelihood Approaches. 3.2.5 Other Methods. 3.3 Managing a Surrogate Based Optimization Process. 3.3.1 Which Surrogate for What Use? 3.3.2 How Many Sample Plan and Infill Points? 3.3.3 Convergence Criteria. 3.3.4 Search of the Vibration Isolator Geometry Feasibility Using Kriging Goal Seeking. References. Part II: Advanced Concepts. 4. Visualization. 4.1 Matrices of Contour Plots. 4.2 Nested Dimensions. Reference. 5. Constraints. 5.1 Satisfaction of Constraints by Construction. 5.2 Penalty Functions. 5.3 Example Constrained Problem. 5.3.1 Using a Kriging Model of the Constraint Function. 5.3.2 Using a Kriging Model of the Objective Function. 5.4 Expected Improvement Based Approaches. 5.4.1 Expected Improvement With Simple Penalty Function. 5.4.2 Constrained Expected Improvement. 5.5 Missing Data. 5.5.1 Imputing Data for Infeasible Designs. 5.6 Design of a Helical Compression Spring Using Constrained Expected Improvement. 5.7 Summary. References. 6. Infill Criteria With Noisy Data. 6.1 Regressing Kriging. 6.2 Searching the Regression Model. 6.2.1 Re-Interpolation. 6.2.2 Re-Interpolation With Conditional Likelihood Approaches. 6.3 A Note on Matrix Ill-Conditioning. 6.4 Summary. References. 7. Exploiting Gradient Information. 7.1 Obtaining Gradients. 7.1.1 Finite Differencing. 7.1.2 Complex Step Approximation. 7.1.3 Adjoint Methods and Algorithmic Differentiation. 7.2 Gradient-enhanced Modelling. 7.3 Hessian-enhanced Modelling. 7.4 Summary. References. 8. Multi-fidelity Analysis. 8.1 Co-Kriging. 8.2 One-variable Demonstration. 8.3 Choosing X c and X e . 8.4 Summary. References. 9. Multiple Design Objectives. 9.1 Pareto Optimization. 9.2 Multi-objective Expected Improvement. 9.3 Design of the Nowacki Cantilever Beam Using Multi-objective, Constrained Expected Improvement. 9.4 Design of a Helical Compression Spring Using Multi-objective, Constrained Expected Improvement. 9.5 Summary. References. Appendix: Example Problems. A.1 One-Variable Test Function. A.2 Branin Test Function. A.3 Aerofoil Design. A.4 The Nowacki Beam. A.5 Multi-objective, Constrained Optimal Design of a Helical Compression Spring. A.6 Novel Passive Vibration Isolator Feasibility. References. Index.",poster,cp55
Mathematics,p976,d6,440d228c4ee2a52a1564738105867f50c8bea733,j235,Journal of Applied Probability,Multivariate Hawkes processes: an application to financial data,"A Hawkes process is also known under the name of a self-exciting point process and has numerous applications throughout science and engineering. We derive the statistical estimation (maximum likelihood estimation) and goodness-of-fit (mainly graphical) for multivariate Hawkes processes with possibly dependent marks. As an application, we analyze two data sets from finance.",fullPaper,jv235
Mathematics,p1923,d6,369f62edea6e6ace6f68c7ebe9bdde046b9514ad,c77,Visualization for Computer Security,The asteroid lightcurve database,Abstract,poster,cp77
Mathematics,p2136,d6,5c33cf35e4ac502583cbd1b3f5c32ef4dedc3d42,c112,British Machine Vision Conference,Measuring Business Cycles.,Abstract,poster,cp112
Business,p2136,d9,5c33cf35e4ac502583cbd1b3f5c32ef4dedc3d42,c112,British Machine Vision Conference,Measuring Business Cycles.,Abstract,poster,cp112
Economics,p2136,d11,5c33cf35e4ac502583cbd1b3f5c32ef4dedc3d42,c112,British Machine Vision Conference,Measuring Business Cycles.,Abstract,poster,cp112
Mathematics,p2317,d6,ca32bbb75b23b04029512337cc1021c60f4d8b69,c57,IEEE International Geoscience and Remote Sensing Symposium,Models of business cycles,Preface. Section I. Section II. Section II. Section III. Section IV. Section V. Section VI. Section VII. Section VIII. Index.,poster,cp57
Mathematics,p2411,d6,5abfa9ce53a3c3dfa0e56a0a2aac1ab38c91f55f,c95,Cyber ..,Business-Cycle Phases and Their Transitional Dynamics,"This article examines differences in expansionary and contractionary phases of the business cycle. By extending the nonlinear Markov-switching estimation method of J. D. Hamilton to incorporate time-varying probabilities of transitions between the phases, the marginal benefits of the time-varying transition probability Markov-switching model are highlighted. Using this technique, the author documents the high correlation between the evolution of the phases inferred from the model and traditional reference cycles for monthly output data. Many of the economic variables that determine the time-varying probabilities help to predict turning points. The predictive power of standard leading indicators is evaluated and compared.",poster,cp95
Materials Science,p271,d7,cbf6a6d8fff87b74f36c5e4ede09f55e7a71506c,c92,International Symposium on Computer Architecture,Numerical data and functional relationships in science and technology,Abstract,poster,cp92
Materials Science,p290,d7,00a4bdc5158945a0b9463a29da4810838e474875,c53,International Conference on Learning Representations,Perspective: Materials informatics and big data: Realization of the “fourth paradigm” of science in materials science,"Our ability to collect “big data” has greatly surpassed our capability to analyze it, underscoring the emergence of the fourth paradigm of science, which is data-driven discovery. The need for data informatics is also emphasized by the Materials Genome Initiative (MGI), further boosting the emerging field of materials informatics. In this article, we look at how data-driven techniques are playing a big role in deciphering processing-structure-property-performance relationships in materials, with illustrative examples of both forward models (property prediction) and inverse models (materials discovery). Such analytics can significantly reduce time-to-insight and accelerate cost-effective materials discovery, which is the goal of MGI.",poster,cp53
Materials Science,p668,d7,b88fd0a029c4cc229c1331d8d85444418d0b80c8,c76,Group,Polymer Data Handbook,"Polymers are the compounds that includes platics, artificial fibers, rubber, cellulose, and many other materials, including coatings and adhesives. This book presents key data on approximately 200 important polymers currently in industrial use or under study in industrial or academic research. No other single source covers so many polymers or offers such a depth of data. The book standardizes and makes accessible a wealth of essential data for students, teachers, researchers, and other professionals in chemistry and chemical engineering.",poster,cp76
Materials Science,p704,d7,b579ffbd6bf615be06d97f954710ad7be34abb53,c106,International Conference on Biometrics,State of the Art Manufacturing and Engineering of Nanocellulose: A Review of Available Data and Industrial Applications,"This review provides a critical overview 
of the recent methods and processes developed for the production of cellulose nanoparticles 
with controlled morphology, structure and properties, and also sums up (1) the processes for the chemical modifications 
of these particles in order to prevent their re-aggregation during spray-drying 
procedures and to increase their reactivity, (2) the recent processes involved in the 
production of nanostructured biomaterials and composites. The structural and physical 
properties of those nanocelluloses, combined with their biodegradability, make them 
materials of choice in the very promising area of nanotechnology, likely subject 
to major commercial successes in the context of green chemistry. With a prospective 
and pioneering approach to the subject matter, various laboratories involved in 
this domain have developed bio-products now almost suitable to industrial applications; 
although some important steps remain to be overcome, those are worth been reviewed and supplemented. At this stage, several pilot 
units and demonstration 
plants have been built to improve, optimize and scale-up the processes developed 
at laboratory scale. Industrial reactors with suitable environment and modern control 
equipment are to be expected within that context. This review shall bring the suitable 
processing dimension that may be needed now, given the numerous reviews outlining 
the product potential attributes. 
An abundant literature database, close to 250 publications and patents, is provided, 
consolidating the various research 
and more practical angles.",poster,cp106
Materials Science,p744,d7,d7385eb4c0482f4f224b591419b18da094f3c729,c13,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,Engineering the singlet–triplet energy splitting in a TADF molecule,"The key to engineering an efficient TADF emitter is to achieve a small energy splitting between a pair of molecular singlet and triplet states. This work makes important contributions towards achieving this goal. By studying the new TADF emitter 2,7-bis(phenoxazin-10-yl)-9,9-dimethylthioxanthene-S,S-dioxide (DPO-TXO2) and the donor and acceptor units separately, the available radiative and non-radiative pathways of DPO-TXO2 have been identified. The energy splitting between singlet and triplet states was clearly identified in four different environments, in solutions and solid state. The results show that DPO-TXO2 is a promising TADF emitter, having ΔEST = 0.01 eV in zeonex matrix. We further show how the environment plays a key role in the fine tuning of the energy levels of the 1CT state with respect to the donor 3LED triplet state, which can then be used to control the ΔEST energy value. We elucidate the TADF mechanism dynamics when the 1CT state is located below the 3LE triplet state which it spin orbit couples to, and we also discuss the OLED device performance with this new emitter, which shows maximum external quantum efficiency (E.Q.E.) of 13.5% at 166 cd m−2.",poster,cp13
Materials Science,p841,d7,371ed298dba16172a6e36920f41d4188a852b375,j204,Journal of microelectromechanical systems,What is the Young's Modulus of Silicon?,"The Young's modulus (E) of a material is a key parameter for mechanical engineering design. Silicon, the most common single material used in microelectromechanical systems (MEMS), is an anisotropic crystalline material whose material properties depend on orientation relative to the crystal lattice. This fact means that the correct value of E for analyzing two different designs in silicon may differ by up to 45%. However, perhaps, because of the perceived complexity of the subject, many researchers oversimplify silicon elastic behavior and use inaccurate values for design and analysis. This paper presents the best known elasticity data for silicon, both in depth and in a summary form, so that it may be readily accessible to MEMS designers.",fullPaper,jv204
Materials Science,p842,d7,80fd8b366a25977d44a23efc75f20222b4e46ee9,c103,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,"Adsorption by Powders and Porous Solids: Principles, Methodology and Applications","The declared objective of this book is to provide an introductory review of the various theoretical and practical aspects of adsorption by powders and porous solids with particular reference to materials of technological importance. The primary aim is to meet the needs of students and non-specialists, who are new to surface science or who wish to use the advanced techniques now available for the determination of surface area, pore size and surface characterization. In addition, a critical account is given of recent work on the adsorptive properties of activated carbons, oxides, clays and zeolites. Key Features * Provides a comprehensive treatment of adsorption at both the gas/solid interface and the liquid/solid interface * Includes chapters dealing with experimental methodology and the interpretation of adsorption data obtained with porous oxides, carbons and zeolites * Techniques capture the importance of heterogeneous catalysis, chemical engineering and the production of pigments, cements, agrochemicals, and pharmaceuticals",poster,cp103
Materials Science,p860,d7,decbd6ebdf8c0e008f7f2a74ff164910f5597c81,c41,IEEE International Conference on Data Engineering,Smithells metals reference book,"General physical and chemical constants X-ray analysis of metallic material Crystallography Crystal chemistry Metallurgically important minerals Thermochemical data Physical properties of molton salts Metallography Equilibrium diagrams Gas-metal systems Diffusion in metals General physical properties Elastic properties, damping capacity and shape memory alloys Temperature measurement and thermoelectric properties Radiating properties of metals Electron emission Electrical properties Magnetic materials and their properties Mechanical testing Mechanical properties of metals and alloys Sintered materials Lubricants Friction and wear Casting alloys and foundry data Engineering ceramics and refractory materials Fuels Heat treatment Metal cutting and forming Corrosion Electroplating and metal finishing Welding Soldering and brazing Vapour deposited coatings and thermal spraying Superplasticity Metal-matrix composites Non-conventional and emerging metallic minerals modelling and simulation supporting technologies for the processing of metals and alloys.",poster,cp41
Materials Science,p889,d7,0ae8159db7bf5ad1fbc606e6fea0be7a0e2ea3c9,c74,International Conference on Computational Linguistics,Titanium: A Technical Guide,"Designed to support the need of engineering, management, and other professionals for information on titanium by providing an overview of the major topics, this book provides a concise summary of the most useful information required to understand titanium and its alloys. The author provides a review of the significant features of the metallurgy and application of titanium and its alloys. All technical aspects of the use of titanium are covered, with sufficient metals property data for most users. Because of its unique density, corrosion resistance, and relative strength advantages over competing materials such as aluminum, steels, and superalloys, titanium has found a niche in many industries. Much of this use has occurred through military research, and subsequent applications in aircraft, of gas turbine engines, although more recent use features replacement joints, golf clubs, and bicycles. Contents include: A primer on titanium and its alloys, Introduction to selection of titanium alloys, Understanding titanium's metallurgy and mill products, Forging and forming, Castings, Powder metallurgy, Heat treating, Joining technology and practice, Machining, Cleaning and finishing, Structure/processing/property relationships, Corrosion resistance, Advanced alloys and future directions, Appendices: Summary table of titanium alloys, Titanium alloy datasheets, Cross-reference to titanium alloys, Listing of selected specification and standardization organizations, Selected manufacturers, suppliers, services, Corrosion data, Machining data.",poster,cp74
Materials Science,p901,d7,6be6f71ced97d6e4a2cd52a628a035535e324aaa,c70,Annual Meeting of the Association for Computational Linguistics,High-performance Ge-on-Si photodetectors,Abstract,poster,cp70
Materials Science,p1216,d7,00a4bdc5158945a0b9463a29da4810838e474875,c89,Conference on Uncertainty in Artificial Intelligence,Perspective: Materials informatics and big data: Realization of the “fourth paradigm” of science in materials science,"Our ability to collect “big data” has greatly surpassed our capability to analyze it, underscoring the emergence of the fourth paradigm of science, which is data-driven discovery. The need for data informatics is also emphasized by the Materials Genome Initiative (MGI), further boosting the emerging field of materials informatics. In this article, we look at how data-driven techniques are playing a big role in deciphering processing-structure-property-performance relationships in materials, with illustrative examples of both forward models (property prediction) and inverse models (materials discovery). Such analytics can significantly reduce time-to-insight and accelerate cost-effective materials discovery, which is the goal of MGI.",poster,cp89
Materials Science,p1665,d7,789ae8da65d2b26be12a8a2dcba51073cea41182,c90,International Conference on Collaboration Technologies and Systems,Materials Design and Discovery with High-Throughput Density Functional Theory: The Open Quantum Materials Database (OQMD),Abstract,poster,cp90
Materials Science,p1678,d7,1c3588499908417312d172f1b5102e76a90fbb2c,c20,ACM Conference on Economics and Computation,"HITEMP, the high-temperature molecular spectroscopic database",Abstract,poster,cp20
Materials Science,p1704,d7,fd2d9588818d4bd6d5cc59a26642c3d3c05ab915,c25,IEEE International Parallel and Distributed Processing Symposium,IT’IS Database for Thermal and Electromagnetic Parameters of Biological Tissues,Abstract,poster,cp25
Materials Science,p1779,d7,c378df148d9e42376e4f47888b04a1f679d2e25f,c13,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,XCOM : Photon Cross Sections Database,Abstract,poster,cp13
Materials Science,p1911,d7,5db8051dd2ee99996484bf1c48795c5ca13e04c5,c23,International Conference on Open and Big Data,XCOM: Photon Cross Section Database (version 1.2),Abstract,poster,cp23
Materials Science,p1913,d7,40691c4ba3c2eb44849273eb92dca66da6b634ad,c10,Americas Conference on Information Systems,NIST Atomic Spectra Database (version 2.0),Abstract,poster,cp10
Materials Science,p2475,d7,3e6b36166d0fe356e206eeb49067eaf5e90b9933,c46,Ideal,Business Groups and Risk Sharing Around the World,"We examine the hypothesis that business groups facilitate mutual insurance among affiliated firms and find substantial evidence of risk sharing by Japanese, Korean, and Thai groups but little evidence of it elsewhere. We also find no correlation between measures of capital market development and the nature of the legal system, on the one hand, and the extent of risk sharing provided by business groups, on the other. The popular view that risk sharing in business groups is important is not validated by our analysis; other reasons are more likely to explain the ubiquity of business groups around the world.",poster,cp46
Economics,p2475,d11,3e6b36166d0fe356e206eeb49067eaf5e90b9933,c46,Ideal,Business Groups and Risk Sharing Around the World,"We examine the hypothesis that business groups facilitate mutual insurance among affiliated firms and find substantial evidence of risk sharing by Japanese, Korean, and Thai groups but little evidence of it elsewhere. We also find no correlation between measures of capital market development and the nature of the legal system, on the one hand, and the extent of risk sharing provided by business groups, on the other. The popular view that risk sharing in business groups is important is not validated by our analysis; other reasons are more likely to explain the ubiquity of business groups around the world.",poster,cp46
Chemistry,p463,d8,62e0c6cf57bc345026d56fd654e80beaf9315c92,c119,International Conference on Business Process Management,JENDL-4.0: A New Library for Nuclear Science and Engineering,"The fourth version of the Japanese Evaluated Nuclear Data Library has been produced in cooperation with the Japanese Nuclear Data Committee. In the new library, much emphasis is placed on the improvements of fission product and minor actinoid data. Two nuclear model codes were developed in order to evaluate the cross sections of fission products and minor actinoids. Coupled-channel optical model parameters, which can be applied to wide mass and energy regions, were obtained for nuclear model calculations. Thermal cross sections of actinoids were carefully examined by considering experimental data or by the systematics of neighboring nuclei. Most of the fission cross sections were derived from experimental data. A simultaneous evaluation was performed for the fission cross sections of important uranium and plutonium isotopes above 10 keV. New evaluations were performed for the thirty fissionproduct nuclides that had not been contained in the previous library JENDL-3.3. The data for light elements and structural materials were partly reevaluated. Moreover, covariances were estimated mainly for actinoids. The new library was released as JENDL-4.0, and the data can be retrieved from the Web site of the JAEA Nuclear Data Center.",poster,cp119
Chemistry,p643,d8,9b3508e82af76f3da1b233f57756ae9e6328e6f5,c22,Grid Computing Environments,Elements of Chemical Reaction Engineering,"1. Mole Balances. The Rate of Reaction The General Mole Balance Equation Batch Reactors Continuous-Flow Reactors Industrial Reactors Summary CD-ROM Material Questions and Problems Supplementary Reading 2. Conversion and Reactor Sizing. Definition of Conversion Batch Reactor Design Equations Design Equations for Flow Reactors Applications of the Design Equations for Continuous-Flow Reactors Reactors in Series Some Further Definitions Summary CD-ROM Materials Questions and Problems Supplementary Reading 3. Rate Laws and Stoichiometry. Part 1. Rate Laws Basic Definitions The Reaction Order and the Rate Law The Reaction Rate Constant Present Status of Our Approach to Reactor Sizing and Design Part 2. Stoichiometry Batch Systems Flow Systems Summary CD-ROM Material Questions and Problems Supplementary Reading 4. Isothermal Reactor Design. Part 1. Mole Balances in Terms of Conversion Design Structure for Isothermal Reactors Scale-Up of Liquid-Phase Batch Reactor Data to the Design of a CSTR Design of Continuous Stirred Tank Reactors (CSTRs) Tubular Reactors Pressure Drop in Reactors Synthesizing the Design of a Chemical Plant Part 2. Mole Balances Written in Terms of Concentration and Molar Flow Rate Mole Balances on CSTRs, PFRs, PBRs, and Batch Reactors Microreactors Membrane Reactors Unsteady-State Operation of Stirred Reactors The Practical Side Summary ODE Solver Algorithm CD-ROM Material Questions and Problems Some Thoughts on Critiquing What You read Journal Critique Problems Supplementary Reading 5. Collection and Analysis of Rate Data. The Algorithm for Data Analysis Batch Reactor Data Method of Initial Rates Method of Half-Lives Differential Reactors Experimental Planning Evaluation of Laboratory Reactors Summary CD-ROM Material Questions and Problems Journal Critique Problems Supplementary Reading 6. Multiple Reactions. Definitions Parallel Reactions Maximizing the Desired Product in Series Reactions Algorithm for Solution of Complex Reactions Multiple Reactions in a PFR/PBR Multiple Reactions in a CSTR Membrane Reactors to Improve Selectivity in Multiple Reactions Complex Reactions of Ammonia Oxidation Sorting It All Out The Fun Part Summary CD-ROM Material Questions and Problems Journal Critique Problems Supplementary Reading 7. Reaction Mechanisms, Pathways, Bioreactions, and Bioreactors. Active Intermediates and Nonelementary Rate Laws Enzymatic Reaction Fundamentals Inhibition of Enzyme Reactions Bioreactors Physiologically Based Pharmacokinetic (PBPK) Models Summary CD-ROM Material Questions and Problems Journal Critique Problems Supplementary Reading 8. Steady-State Nonisothermal Reactor Design. Rationale The Energy Balance Adiabatic Operation Steady-State Tubular Reactor with Heat Exchange Equilibrium Conversion CSTR with Heat Effects Multiple Steady States Nonisothermal Multiple Chemical Reactions Radial and Axial Variations in a Tubular Reactor The Practical Side Summary CD-ROM Material Questions and Problems Journal Critique Problems Supplementary Reading 9. Unsteady-State Nonisothermal Reactor Design. The Unsteady-State Energy Balance Energy Balance on Batch Reactors Semibatch Reactors with a Heat Exchanger Unsteady Operation of a CSTR Nonisothermal Multiple Reactions Unsteady Operation of Plug-Flow Reactors Summary CD-ROM Material Questions and Problems Supplementary Reading 10. Catalysis and Catalytic Reactors. Catalysts Steps in a Catalytic Reaction Synthesizing a Rate Law, Mechanism, and Rate-Limiting Step Heterogeneous Data Analysis for Reactor Design Reaction Engineering in Microelectronic Fabrication Model Discrimination Catalyst Deactivation Summary ODE Solver Algorithm CD-ROM Material Questions and Problems Journal Critique Problems Supplementary Reading 11. External Diffusion Effects on Heterogeneous Reactions. Diffusion Fundamentals Binary Diffusion External Resistance to Mass Transfer What If ... ? (Parameter Sensitivity) The Shrinking Core Model Summary CD-ROM Material Questions and Problems Supplementary Reading 12. Diffusion and Reaction. Diffusion and Reaction in Spherical Catalyst Pellets Internal Effectiveness Factor Falsified Kinetics Overall Effectiveness Factor Estimation of Diffusion- and Reaction-Limited Regimes Mass Transfer and Reaction in a Packed Bed Determination of Limiting Situations from Reaction Data Multiphase Reactors Fluidized Bed Reactors Chemical Vapor Deposition (CVD) Summary CD-ROM Material Questions and Problems Journal Article Problems Journal Critique Problems Supplementary Reading 13. Distributions of Residence Times for Chemical Reactors. General Characteristics Part 1. Characteristics and Diagnostics Measurement of the RTD Characteristics of the RTD RTD in Ideal Reactors Diagnostics and Troubleshooting Part 2. Predicting Conversion and Exit Concentration Reactor Modeling Using the RTD Zero-Parameter Models Using Software Packages RTD and Multiple Reactions Summary CD-ROM Material Questions and Problems Supplementary Reading 14. Models for Nonideal Reactors. Some Guidelines Tanks-in-Series (T-I-S) Model Dispersion Model Flow, Reaction, and Dispersion Tanks-in-Series Model Versus Dispersion Model Numerical Solutions to Flows with Dispersion and Reaction Two-Parameter Models-Modeling Real Reactors with Combinations of Ideal Reactors Use of Software Packages to Determine the Model Parameters Other Models of Nonideal Reactors Using CSTRs and PFRs Applications to Pharmacokinetic Modeling Summary CD-ROM Material Questions and Problems Supplementary Reading Appendix A: Numerical Techniques. Appendix B: Ideal Gas Constant and Conversion Factors. Appendix C: Thermodynamic Relationships Involving the Equilibrium Constant. Appendix D: Measurement of Slopes on Semilog Paper. Appendix E: Software Packages. Appendix F: Nomenclature. Appendix G: Rate Law Data. Appendix H: Open-Ended Problems. Appendix I: How to Use the CD-ROM. Appendix J: Use of Computational Chemistry Software Packages. Index. About the CD-ROM.",poster,cp22
Chemistry,p677,d8,62e0c6cf57bc345026d56fd654e80beaf9315c92,c9,Big Data,JENDL-4.0: A New Library for Nuclear Science and Engineering,"The fourth version of the Japanese Evaluated Nuclear Data Library has been produced in cooperation with the Japanese Nuclear Data Committee. In the new library, much emphasis is placed on the improvements of fission product and minor actinoid data. Two nuclear model codes were developed in order to evaluate the cross sections of fission products and minor actinoids. Coupled-channel optical model parameters, which can be applied to wide mass and energy regions, were obtained for nuclear model calculations. Thermal cross sections of actinoids were carefully examined by considering experimental data or by the systematics of neighboring nuclei. Most of the fission cross sections were derived from experimental data. A simultaneous evaluation was performed for the fission cross sections of important uranium and plutonium isotopes above 10 keV. New evaluations were performed for the thirty fissionproduct nuclides that had not been contained in the previous library JENDL-3.3. The data for light elements and structural materials were partly reevaluated. Moreover, covariances were estimated mainly for actinoids. The new library was released as JENDL-4.0, and the data can be retrieved from the Web site of the JAEA Nuclear Data Center.",poster,cp9
Chemistry,p691,d8,b42d14dab8e5b172d99b6eee11581f8bdbb8b0a3,c96,Human Language Technology - The Baltic Perspectiv,"Chemical, Biochemical, and Engineering Thermodynamics","Notation. Chapter 1. Introduction. Chapter 2. Conservation of Mass. Chapter 3. Conservation of Energy. Chapter 4. Entropy: An Additional Balance Equation. Chapter 5. Liquefaction, Power Cycles, and Explosions. Chapter 6. The Thermodynamic Properties of Real Substances. Chapter 7. Equilibrium and Stability in One-Component Systems. Chapter 8. The Thermodynamics of Multicomponent Mixtures. Chapter 9. The Estimation of the Gibbs Free Energy and Fugacity of a Component in a Mixture. Chapter 10. Vapor-liquid Equilibrium in Mixtures. Chapter 11. Other types of Phase Equilibria in Fluid Mixtures. Chapter 12. Mixture Phase Equilibria Involving Solids. Chapter 13. Chemical Equilibrium. Chapter 14. The Balance Equations for Chemical Reactors and Electrochemistry. Chapter 15. Some Biochemical Applications of Thermodynamics. Appendix A: Thermodynamic Data. Appendix B: Computer Programs.",poster,cp96
Chemistry,p990,d8,0a92d8c6c358ae729877e97c782641cb58f06a38,c17,International Conference on Statistical and Scientific Database Management,An Engineering Data Book,Abstract,poster,cp17
Chemistry,p1671,d8,31dbfb575ef802a89a5c08e632f42265bcf30684,c38,IEEE Global Engineering Education Conference,High Resolution XPS of Organic Polymers: The Scienta ESCA300 Database,Description of the spectrometer x-ray source monochromator electron lens hemispherical analyser multichannel detector sample analysis chamber charge compensation performance on conducting samples performance on insulating samples performance on testing of the spectrometer experimental protocol sample mounting data acquisition correction of binding energy scale for sample charging curve fitting lineshapes shake-up structure valence bands impurities x-ray degradation organization of the database list of polymers and acronyms the database appendix 1 - primary C 1s shifts appendix 2 - secondary C 1s shifts appendix 3.1 - 0 1s binding energies in CHO polymers appendix 3.2 - 0 1s binding energies in other polymers appendix 4 - N 1s binding energies appendix 5 - F 1s binding energies appendix 6 - binding energies and spin-orbit constants for core-line doublets apendix 7 - binding energies of peaks appearing in the valence band region.,poster,cp38
Chemistry,p1802,d8,ef50e6878d6addcbd5d1ca96e08eef51b9ddec9e,c25,IEEE International Parallel and Distributed Processing Symposium,Reference database of Raman spectra of biological molecules,"Raman spectra of biological materials are very complex, because they consist of signals from all molecules present in cells. In order to obtain chemical information from these spectra, it is necessary to know the Raman patterns of the possible components of a cell. In this paper, we present a collection of Raman spectra of biomolecules that can serve as references for the interpretation of Raman spectra of biological materials. We included the most important components present in a cell: (1) DNA and RNA bases (adenine, cytosine, guanine, thymine and uracil), (2) amino acids (glycine, L-alanine, L-valine, L-serine, L-glutamic acid, L-arginine, L-phenylalanine, L-tyrosine, L-tryptophan, L-histidine, L-proline), (3) fatty acids and fats (lauric acid, myristic acid, palmitic acid, stearic acid, 12-methyltetradecanoic acid, 13-methylmyristic acid, 14-methylpentadecanoic acid, 14-methylhexadecanoic acid, 15-methylpalmitic acid, oleic acid, vaccenic acid, glycerol, triolein, trilinolein, trilinolenin), (4) saccharides (β-D-glucose, lactose, cellulose, D-(+)-dextrose, D-(+)-trehalose, amylose, amylopectine, D-(+)-mannose, D-(+)-fucose, D-(−)-arabinose, D-(+)-xylose, D-(−)-fructose, D-(+)-galactosamine, N-acetyl-D-glucosamine, chitin), (5) primary metabolites (citric acid, succinic acid, fumarate, malic acid, pyruvate, phosphoenolpyruvate, coenzyme A, acetyl coenzyme A, acetoacetate, D-fructose-6-phosphate) and (6) others (β-carotene, ascorbic acid, riboflavin, glutathione). Examples of Raman spectra of bacteria and fungal spores are shown, together with band assignments to the reference products. Copyright © 2007 John Wiley & Sons, Ltd.",poster,cp25
Chemistry,p1842,d8,311c0501c68f8cbe0d2e3a161de1ab12d49cb9ce,c107,Annual Haifa Experimental Systems Conference,Physical Properties of Ionic Liquids: Database and Evaluation,"A comprehensive database on physical properties of ionic liquids (ILs), which was collected from 109 kinds of literature sources in the period from 1984 through 2004, has been presented. There are 1680 pieces of data on the physical properties for 588 available ILs, from which 276 kinds of cations and 55 kinds of anions were extracted. In terms of the collected database, the structure-property relationship was evaluated. The correlation of melting points of two most common systems, disubstituted imidazolium tetrafluoroborate and disubstituted imidazolium hexafluorophosphate, was carried out using a quantitative structure-property relationship method.",poster,cp107
Chemistry,p1882,d8,7fbb81ee7df0f48b3e55649cf41bcc506d19b314,c92,International Symposium on Computer Architecture,The American Mineralogist crystal structure database,"A database has been constructed that contains all the crystal structures previously published in the American Mineralogist. The database is called “The American Mineralogist Crystal Structure Database” and is freely accessible from the websites of the Mineralogical Society of America at http://www.minsocam.org/MSA/Crystal_Database.html and the University of Arizona. In addition to the database, a suite of interactive software is provided that can be used to view and manipulate the crystal structures and compute different properties of a crystal such as geometry, diffraction patterns, and procrystal electron densities. The database is set up so that the data can be easily incorporated into other software packages. Included at the website is an evolving set of guides to instruct the user and help with classroom education. parameters; (5) incorporating comments from either the original authors or ourselves when changes are made to the originally published data. Each record in the database consists of a bibliographic reference, cell parameters, symmetry, atomic positions, displacement parameters, and site occupancies. An example of a data set is provided in Figure 1. The first part of each data set contains identifying information, bibliography and notes, while the second part contains the crystallographic parameters. The first line of a data file contains an identifier, such as the name of the mineral or formula of the chemical species. The next line(s) contain the names of the authors, each separated by a comma. This is followed by the journal reference, title of the paper, and additional notes. The crystallographic data begins with a listing of the cell parameters and space group. If the data is given with respect to a non-standard space group origin then an asterisk precedes the space group symbol and the next line contains the translation vector from the standard origin. The 1952 edition of the International Tables for X-ray Crystallography are used to define the standard origin. The rest of the data set is a fixed-formatted listing of the atoms, their positional and displacement parameters, and occupancies. A header is provided that defines rightjustified columns. The name of each atom identifies the occupying elements, with additional identifiers added when appropriate. For instance, “Oco” identifies a particular oxygen atom in the albite structure. Some data sets report a crystallographic site occupied by molecular species rather than elemental, such as OH, water or methane. In most of these cases the atom name is denoted by molecular formula. For example, “CH4” denotes methane, and “Wat” denotes water. The displacement factors are tabulated in one of two formats, U’s or b’s",poster,cp92
Chemistry,p1941,d8,b349449e4a3bf70882ec07d333fb4ec99a87ae62,c110,Biometrics and Identity Management,NIST Computational Chemistry Comparison and Benchmark Database,Abstract,poster,cp110
Chemistry,p1966,d8,86dc58305d4cb2766eb2746bbe578e9a58a6f238,c93,ASE BigData & SocialInformatics,Carbohydrate-active enzymes : an integrated database approach,Abstract,poster,cp93
Chemistry,p1971,d8,d4fe4d4f62b8f14a0475bc8f8028a0d3565650f9,c7,International Symposium on Intelligent Data Analysis,The Harmonized World Soil Database,"For more than 30 years the FAO/Unesco Soil map of the World has been the only harmonized source of global soil information. Recent updates and release of new soil information in all regions of the globe was an incentive to tackle the harmonization and integration of the new soil data. The task was undertaken by a consortium of institutes and organizations and resulted in a product with 30 arc second resolution that includes for each soil unit estimates for fifteen top- and subsoil properties. The data come with a viewer, are GIS compatible and are freely available on-line.",poster,cp7
Geography,p1971,d13,d4fe4d4f62b8f14a0475bc8f8028a0d3565650f9,c7,International Symposium on Intelligent Data Analysis,The Harmonized World Soil Database,"For more than 30 years the FAO/Unesco Soil map of the World has been the only harmonized source of global soil information. Recent updates and release of new soil information in all regions of the globe was an incentive to tackle the harmonization and integration of the new soil data. The task was undertaken by a consortium of institutes and organizations and resulted in a product with 30 arc second resolution that includes for each soil unit estimates for fifteen top- and subsoil properties. The data come with a viewer, are GIS compatible and are freely available on-line.",poster,cp7
Environmental Science,p1971,d14,d4fe4d4f62b8f14a0475bc8f8028a0d3565650f9,c7,International Symposium on Intelligent Data Analysis,The Harmonized World Soil Database,"For more than 30 years the FAO/Unesco Soil map of the World has been the only harmonized source of global soil information. Recent updates and release of new soil information in all regions of the globe was an incentive to tackle the harmonization and integration of the new soil data. The task was undertaken by a consortium of institutes and organizations and resulted in a product with 30 arc second resolution that includes for each soil unit estimates for fifteen top- and subsoil properties. The data come with a viewer, are GIS compatible and are freely available on-line.",poster,cp7
Business,p72,d9,1ac524c713423bc50822b34e0aa1bbfab42d2b00,j29,Small Business Economics,Data science for entrepreneurship research: studying demand dynamics for entrepreneurial skills in the Netherlands,Abstract,fullPaper,jv29
Business,p180,d9,f6705e68c71bc0b51ddb8d1e4f986c894ba8f34f,c78,ACM/IEEE International Conference on Model Driven Engineering Languages and Systems,"Data Science, Predictive Analytics, and Big Data in Supply Chain Management: Current State and Future Potential","While data science, predictive analytics, and big data have been frequently used buzzwords, rigorous academic investigations into these areas are just emerging. In this forward thinking article, we discuss the results of a recent large-scale survey on these topics among supply chain management (SCM) professionals, complemented with our experiences in developing, implementing, and administering one of the first master's degree programs in predictive analytics. As such, we effectively provide an assessment of the current state of the field via a large-scale survey, and offer insight into its future potential via the discussion of how a research university is training next-generation data scientists. Specifically, we report on the current use of predictive analytics in SCM and the underlying motivations, as well as perceived benefits and barriers. In addition, we highlight skills desired for successful data scientists, and provide illustrations of how predictive analytics can be implemented in the curriculum. Relying on one of the largest data sets of predictive analytics users in SCM collected to date and our experiences with one of the first master's degree programs in predictive analytics, it is our intent to provide a timely assessment of the field, illustrate its future potential, and motivate additional research and pedagogical advancements in this domain.",poster,cp78
Business,p208,d9,9c1b9598f82f9ed7d75ef1a9e627496759aa2387,c97,International Conference on Computational Logic,"Data Science, Predictive Analytics, and Big Data: A Revolution that Will Transform Supply Chain Design and Management","We illuminate the myriad of opportunities for research where supply chain management intersects with data science, predictive analytics, and big data, collectively referred to as DPB. We show that these terms are not only becoming popular but are also relevant to supply chain research and education. Data science requires both domain knowledge and a broad set of quantitative skills, but there is a dearth of literature on the topic and many questions. We call for research on skills that are needed by SCM data scientists and discuss how such skills and domain knowledge affect the effectiveness of a SCM data scientist. Such knowledge is crucial to developing future supply chain leaders. We propose definitions of data science and predictive analytics as applied to supply chain management. We examine possible applications of DPB in practice and provide examples of research questions from these applications, as well as examples of research questions employing DPB that stem from management theories. Finally, we propose specific steps interested researchers can take to respond to our call for research on the intersection of supply chain management and DPB.",poster,cp97
Business,p319,d9,9445423239efb633f5c15791a7abe352199ce678,c107,Annual Haifa Experimental Systems Conference,General Data Protection Regulation,"Presentacio sobre l'Oficina de Proteccio de Dades Personals de la UAB i la politica Open Science. Va formar part de la conferencia ""Les politiques d'Open Data / Open Acces: Implicacions a la recerca"" orientada a investigadors i gestors de projectes europeus que va tenir lloc el 20 de setembre de 2018 a la Universitat Autonoma de Barcelona",poster,cp107
Political Science,p319,d15,9445423239efb633f5c15791a7abe352199ce678,c107,Annual Haifa Experimental Systems Conference,General Data Protection Regulation,"Presentacio sobre l'Oficina de Proteccio de Dades Personals de la UAB i la politica Open Science. Va formar part de la conferencia ""Les politiques d'Open Data / Open Acces: Implicacions a la recerca"" orientada a investigadors i gestors de projectes europeus que va tenir lloc el 20 de setembre de 2018 a la Universitat Autonoma de Barcelona",poster,cp107
Business,p385,d9,28aecd08b2488c5300abf399feeb83a1f9c19890,c95,Cyber ..,Open Government Data,"Story Slides Slide 1 W3C eGovernment Community: Data Science Slide 2 Agenda Slide 3 The Changing Landscape of Federal Information Technology Slide 4 Cloud: SOA, Semantic, & Data Science: September 10-11th Slide 5 Opportunities for Data Science Slide 6 Discussion 1 Slide 7 Discussion 2 Slide 8 Discussion 3 Slide 9 Discussion 4 Slide 10 Discussion 5 Spotfire Dashboard Research Notes Joshua Tauberer’s Blog Open Government Data 1 Big Data Meets Open Government Figure 1 The New Federal Register 2.0 Figure 2 This animated visualization of live wind speeds and directions Figure 3 Data is like refrigerator poetry Figure 4 http://www.GovTrack.us Figure 5 John Oliver parodies Schoolhouse Rock’s “I’m Just a Bill” References 1 2 3 4 5 6 7",poster,cp95
Business,p432,d9,c278f3e91bf11c72be6808972f00810f15d877a4,j142,Sustainability Science,Mapping citizen science contributions to the UN sustainable development goals,Abstract,fullPaper,jv142
Business,p920,d9,8ad7b7d55331d708055217797d3ff080dd3c2836,c22,Grid Computing Environments,Why are companies offshoring innovation? The emerging global race for talent,Abstract,poster,cp22
Economics,p920,d11,8ad7b7d55331d708055217797d3ff080dd3c2836,c22,Grid Computing Environments,Why are companies offshoring innovation? The emerging global race for talent,Abstract,poster,cp22
Business,p954,d9,5677cfc24058078ef513c7d6c99be2d1eb8bf11e,c6,Annual Conference on Genetic and Evolutionary Computation,Toyota's Principles of Set-Based Concurrent Engineering,"Not well documented to date, the design and development system of Toyota Motor Corporation contributes greatly to the firm's remarkably consistent growth in market share and its enviable profit per vehicle. This article, which extends the authors' previous study of the Toyota product development system, reports on further data collection in Japan and at the Toyota Technical Center in Michigan. Findings substantiate the authors' previous claims about the product development system and lead them to conclude that Toyota is ""set-based"" in its approaches. 

Set-based concurrent engineering (SBCE) begins by broadly considering sets of possible solutions (in parallel and relatively independently) and gradually narrowing the set of possibilities to converge on a final solution. Gradually eliminating weaker solutions increases the likelihood of finding the best or better solutions. In this way, Toyota can move more quickly toward convergence and production than their traditional, ""point-based"" counterparts. 

The authors develop the SBCE idea by describing three principles that guide Toyota's decision making in design: (1) simultaneous mapping of the design space according to functional expertise, (2) ""integration by intersection"" of mutually acceptable functional refinements introduced by the design and manufacturing engineering groups, and (3) establishment of feasibility before commitment. The authors also present a conceptual framework tied to the Toyota development system and discuss why the SBCE principles lead to highly effective product development.

Findings suggest that a change to a distributed, concurrent engineering environment should involve a corresponding change in design method to a set-based process. Product development organizations able to master and apply SBCE principles and Toyota's principles for integrating systems and cultivating organizational knowledge may be able to radically improve their design and development processes.",poster,cp6
Business,p1001,d9,91b63db746becca15090963a8990dfe2b5103799,c10,Americas Conference on Information Systems,"Big data: The next frontier for innovation, competition, and productivity","The amount of data in our world has been exploding, and analyzing large data sets—so-called big data— will become a key basis of competition, underpinning new waves of productivity growth, innovation, and consumer surplus, according to research by MGI and McKinsey's Business Technology Office. Leaders in every sector will have to grapple with the implications of big data, not just a few data-oriented managers. The increasing volume and detail of information captured by enterprises, the rise of multimedia, social media, and the Internet of Things will fuel exponential growth in data for the foreseeable future.",poster,cp10
Business,p1038,d9,5bc511aa30f72720260d792e57537379fb04c395,c73,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,Sentiment Analysis in Tourism: Capitalizing on Big Data,"Advances in technology have fundamentally changed how information is produced and consumed by all actors involved in tourism. Tourists can now access different sources of information, and they can generate their own content and share their views and experiences. Tourism content shared through social media has become a very influential information source that impacts tourism in terms of both reputation and performance. However, the volume of data on the Internet has reached a level that makes manual processing almost impossible, demanding new analytical approaches. Sentiment analysis is rapidly emerging as an automated process of examining semantic relationships and meaning in reviews. In this article, different sentiment analysis approaches applied in tourism are reviewed and assessed in terms of the datasets used and performances on key evaluation metrics. The article concludes by outlining future research avenues to further advance sentiment analysis in tourism as part of a broader Big Data approach.",poster,cp73
Business,p1054,d9,8e6d0ed32aaa5e3d7c598d5a2ace76eab8485801,j256,Cities,"On big data, artificial intelligence and smart cities",Abstract,fullPaper,jv256
Business,p1056,d9,e8b7a9be9f2d0578a95319ed5841978e10429967,j257,"International Journal of Minerals, Metallurgy, and Materials",Big data management in the mining industry,Abstract,fullPaper,jv257
Business,p1075,d9,78e40584f0d149bf6f98beb5561b7b83cb68e1b1,j264,Journal of business research,Assessing the impact of big data on firm innovation performance: Big data is not always better data,Abstract,fullPaper,jv264
Business,p1082,d9,e3d74f13374063a86909c7b0a8105be94902b7f4,j269,Journal of Cleaner Production,Role of big data analytics in developing sustainable capabilities,Abstract,fullPaper,jv269
Business,p1102,d9,5cbd591298595fb575f1d01e3db910ee1f4573fb,j275,Technological forecasting & social change,Circular economy and big data analytics: A stakeholder perspective,Abstract,fullPaper,jv275
Business,p1148,d9,335848cc7da51a26126c1fa255bd776a6cff8c03,c53,International Conference on Learning Representations,Big Data and Firm Dynamics,"We study a model where firms accumulate data as a valuable intangible asset. Data accumulation affects firms' dynamics. It increases the skewness of the firm size distribution as large firms generate more data and invest more in active experimentation. On the other hand, small data-savvy firms can overtake more traditional incumbents, provided they can finance their initial money-losing growth. Our model can be used to estimate the market and social value of data.",poster,cp53
Business,p1153,d9,ca68d86366c38eef8451917197191093d18865a6,c47,EUROMICRO Conference on Software Engineering and Advanced Applications,Big data and predictive analytics for supply chain and organizational performance,Abstract,poster,cp47
Business,p1168,d9,5a765bef6d657595066d66c173ff228bd01a3627,c14,Hawaii International Conference on System Sciences,Big Data in Natural Disaster Management: A Review,"Undoubtedly, the age of big data has opened new options for natural disaster management, primarily because of the varied possibilities it provides in visualizing, analyzing, and predicting natural disasters. From this perspective, big data has radically changed the ways through which human societies adopt natural disaster management strategies to reduce human suffering and economic losses. In a world that is now heavily dependent on information technology, the prime objective of computer experts and policy makers is to make the best of big data by sourcing information from varied formats and storing it in ways that it can be effectively used during different stages of natural disaster management. This paper aimed at making a systematic review of the literature in analyzing the role of big data in natural disaster management and highlighting the present status of the technology in providing meaningful and effective solutions in natural disaster management. The paper has presented the findings of several researchers on varied scientific and technological perspectives that have a bearing on the efficacy of big data in facilitating natural disaster management. In this context, this paper reviews the major big data sources, the associated achievements in different disaster management phases, and emerging technological topics associated with leveraging this new ecosystem of Big Data to monitor and detect natural hazards, mitigate their effects, assist in relief efforts, and contribute to the recovery and reconstruction processes.",poster,cp14
Business,p1169,d9,d2bed4fe58f38dfdda55ac530521ed442bfc6ae7,c61,"International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems",Factors influencing big data decision-making quality,Abstract,poster,cp61
Business,p1180,d9,b58a48f0aba6ccf4ea4ee5c5155ddad75a9617f9,j299,Annual Review of Resource Economics,Opportunities and Challenges for Big Data in Agricultural and Environmental Analysis,"Agriculture stands on the cusp of a digital revolution, and the same technologies that created the Internet and are transforming medicine are now being applied in our farms and on our fields. Overall, this digital agricultural revolution is being driven by the low cost of collecting data on everything from soil conditions to animal health and crop development along with weather station data and data collected by drones and satellites. The promise of these technologies is more food, produced on less land, with fewer inputs and a smaller environmental footprint. At present, however, barriers to realizing this potential include a lack of ability to aggregate and interpret data in such a way that it results in useful decision support tools for farmers and the need to train farmers in how to use new tools. This article reviews the state of the literature on the promise and barriers to realizing the potential for Big Data to revolutionize agriculture.",fullPaper,jv299
Business,p1184,d9,a51034d6cda97671b987990c43bfe5b263b2205d,c22,Grid Computing Environments,Big Data in Agriculture: A Challenge for the Future,"This article examines the challenge and opportunities of Big Data, and concludes that these technologies will lead to relevant analysis at every stage of the agricultural value chain. Big Data is defined by several characteristics beyond size, particularly, the volume, velocity, variety, and veracity of the data. We discuss a set of analytical techniques that are increasingly relevant to our profession as one addresses these issues. Ultimately, we resolve that agricultural and applied economists are uniquely positioned to contribute to the research and outreach agenda on Big Data. We believe there are relevant policy, farm management, supply chain, consumer demand, and sustainability issues where our profession can make major contributions. The authors are thankful to the anonymous reviewers and editor Craig Gundersen for helpful comments. Support was provided by the Mississippi Agricultural and Forestry Experiment Station Special Research Initiative.",poster,cp22
Business,p1186,d9,e9e5549b839e16384300229e2b93c46a9635040f,c115,International Conference on Information Integration and Web-based Applications & Services,"Big Data for Open Innovation in SMEs and Large Corporations: Trends, Opportunities, and Challenges","The notion of ‘Big Data’ has recently been attracting an increasing degree of attention from scholars and practitioners in an attempt to identify how it may be leveraged to create innovative solutions and business opportunities. Specifically, Big Data may come from a variety of sources, especially sources outside the usual boundaries of organizations, and it represents an interesting and emerging opportunity for sustaining and enhancing the effectiveness of the so-called open innovation paradigm. However, to the best of our knowledge, no prior works have provided a broad overview of the use of Big Data for open innovation strategies. We aim to fill this gap. In particular, we have focused our investigation on two types of companies: small and medium-sized enterprises (SMEs) and big corporations, reviewing the major academic works published so far and analysing the main industrial applications on this topic. As a result, we provide a relevant list of the main trends, opportunities, and challenges faced by SMEs and large corporations when dealing with Big Data for open innovation strategies.",poster,cp115
Business,p1187,d9,404c8bd8d1497a341d25889d714ce1d41507c2b0,j301,Journal of Management Analytics,Banking with blockchain-ed big data,"Blockchain is disrupting the banking industry and contributing to the increased big data in banking. However, there exists a gap in research and development into blockchain-ed big data in banking f...",fullPaper,jv301
Business,p1197,d9,7df5a0412ea3888df35c779f72cdc53dd0c3efb3,c51,International Conference on Engineering Education,Big Data and Service Operations,"This study discusses how the tremendous volume of available data collected by firms has been transforming the service industry. The focus is primarily on services in the following sectors: finance/banking, transportation and hospitality, and online platforms (e.g., subscription services, online advertising, and online dating). We report anecdotal evidence borrowed from various collaborations and discussions with executives and data analysts who work in management consulting or finance, or for technology/startup companies. Our main goals are (i) to present an overview of how big data is shaping the service industry, (ii) to describe several mechanisms used in the service industry that leverage the potential information hidden in big data, and (iii) to point out some of the pitfalls and risks incurred. On one hand, collecting and storing large amounts of data on customers and on past transactions can help firms improve the quality of their services. For example, firms can now customize their services to unprecedented levels of granularity, which enables the firms to offer targeted personalized offers (sometimes, even in real‐time). On the other hand, collecting this data may allow some firms to utilize the data against their customers by charging them higher prices. Furthermore, data‐driven algorithms may often be biased toward illicit discrimination. The availability of data on sensitive personal information may also attract hackers and gives rise to important cybersecurity concerns (e.g., information leakage, fraud, and identity theft).",poster,cp51
Business,p1238,d9,8065484c20a0b972f2c73531513361550ee9c194,j245,The Review of financial studies,Big Data as a Governance Mechanism,"This study empirically investigates two effects of alternative data availability: stock price informativeness and its disciplining effect on managers’ actions. Recent computing advancements have enabled technology companies to collect real-time, granular indicators of fundamentals to sell to investment professionals. These data include consumer transactions and satellite images. The introduction of these data increases price informativeness through decreased information acquisition costs, particularly in firms in which sophisticated investors have higher incentives to uncover information. I document two effects on managers. First, managers reduce their opportunistic trading. Second, investment efficiency increases, consistent with price informativeness improving managers’ incentives to invest and divest efficiently.Received June 1, 2017; editorial decision June 1, 2018 by Editor Wei Jiang. The Author has furnished an Internet Appendix, which is available on the Oxford University Press Web site next to the link to the final published paper online.",fullPaper,jv245
Business,p1241,d9,c0f28236a0e351281ba6ec92cb33d8e5feef569e,c60,Network and Distributed System Security Symposium,How Sustainable Is Big Data?,"The rapid growth of “big data” provides tremendous opportunities for making better decisions, where “better” can be defined using any combination of economic, environmental, or social metrics. This essay provides a few examples of how the use of big data can precipitate more sustainable decision‐making. However, as with any technology, the use of big data on a large scale will have some undesirable consequences. Some of these are foreseeable, while others are entirely unpredictable. This essay highlights some of the sustainability‐related challenges posed by the use of big data. It does not intend to suggest that the advent of big data is an undesirable development. However, it is not too early to start asking what the unwanted repercussions of the big data revolution might be.",poster,cp60
Business,p1244,d9,6ef71db293160a0e0a1a66d787f9242d99555370,c5,Technical Symposium on Computer Science Education,Big Data and Analytics in the Modern Audit Engagement: Research Needs,"SUMMARY: Modern audit engagements often involve examination of clients that are using Big Data and analytics to remain competitive and relevant in today's business environment. Client systems now are integrated with the cloud, the Internet of Things, and external data sources such as social media. Furthermore, many engagement clients are now integrating this Big Data with new and complex business analytical approaches to generate intelligence for decision making. This scenario provides almost limitless opportunities and the urgency for the external auditor to utilize advanced analytics. This paper first positions the need for the external audit profession to move toward Big Data and audit analytics. It then reviews the regulations regarding audit evidence and analytical procedures, in contrast to the emerging environment of Big Data and advanced analytics. In a Big Data environment, the audit profession has the potential to undertake more advanced predictive and prescriptive-oriented analytics. The next s...",poster,cp5
Business,p1249,d9,02e0982171b3eed379f4dea393e1aa61d4f0a9da,j316,Journal of Accounting Literature,Big Data Techniques in Auditing Research and Practice: Current Trends and Future Opportunities,"Abstract This paper analyses the use of big data techniques in auditing, and finds that the practice is not as widespread as it is in other related fields. We first introduce contemporary big data techniques to promote understanding of their potential application. Next, we review existing research on big data in accounting and finance. In addition to auditing, our analysis shows that existing research extends across three other genealogies: financial distress modelling, financial fraud modelling, and stock market prediction and quantitative modelling. Auditing is lagging behind the other research streams in the use of valuable big data techniques. A possible explanation is that auditors are reluctant to use techniques that are far ahead of those adopted by their clients, but we refute this argument. We call for more research and a greater alignment to practice. We also outline future opportunities for auditing in the context of real-time information and in collaborative platforms and peer-to-peer marketplaces.",fullPaper,jv316
Business,p1259,d9,58525d47d85d38bc48dbfbcb17020c57a7242ebf,c27,International Conference Geographic Information Science,"Big Data Facilitation, Utilization, and Monetization: Exploring the 3Vs in a New Product Development Process","Big data is transforming the new product development (NPD) process. Organizations are investing heavily in big data capabilities to capitalize on the ongoing analytics movement. Yet there is a lack of understanding of how firms can leverage big data as a capability to generate innovation success in dynamic marketplaces. To address this need for improved insights, the authors operationalize and analyze the 3Vs of big data usage—volume, variety, and velocity—in an NPD model. Drawing on the results of a survey of 261 managers reporting on their business unit's NPD processes and big data usage, this study identifies the antecedents of the multidimensional usage of big data. Empirically assessing the effects of firm orientations, the authors show that an exploration orientation has a positive effect on all three dimensions of a firm's big data usage while an exploitation orientation has no effect. Moving downstream, the results also reveal that the environmental factor of customer turbulence interacts differentially with the big data usage dimensions' impact on new product revenue (NPR). Specifically, customer turbulence accentuates the relationship between big data velocity and NPR but attenuates the relationship between big data volume and NPR.",poster,cp27
Business,p1280,d9,33a0d4f2d280a7f1080aa1da11e8f05f06263cc6,c82,Symposium on Networked Systems Design and Implementation,Big Data Security Intelligence for Healthcare Industry 4.0,Abstract,poster,cp82
Business,p1284,d9,5b5130060aee7c96efae18101bc8084f52380306,c94,International Conferences on Contemporary Computing and Informatics,Incompatible: The GDPR in the Age of Big Data,"After years of drafting and negotiations, the EU finally passed the General Data Protection Regulation (GDPR). The GDPR’s impact will, most likely, be profound. Among the challenges data protection law faces in the digital age, the emergence of Big Data is perhaps the greatest. Indeed, Big Data analysis carries both hope and potential harm to the individuals whose data is analyzed, as well as other individuals indirectly affected by such analyses. These novel developments call for both conceptual and practical changes in the current legal setting. 
Unfortunately, the GDPR fails to properly address the surge in Big Data practices. The GDPR’s provisions are — to borrow a key term used throughout EU data protection regulation — incompatible with the data environment that the availability of Big Data generates. Such incompatibility is destined to render many of the GDPR’s provisions quickly irrelevant. Alternatively, the GDPR’s enactment could substantially alter the way Big Data analysis is conducted, transferring it to one that is suboptimal and inefficient. It will do so while stalling innovation in Europe and limiting utility to European citizens, while not necessarily providing such citizens with greater privacy protection. 
After a brief introduction (Part I), Part II quickly defines Big Data and its relevance to EU data protection law. Part III addresses four central concepts of EU data protection law as manifested in the GDPR: Purpose Specification, Data Minimization, Automated Decisions and Special Categories. It thereafter proceeds to demonstrate that the treatment of every one of these concepts in the GDPR is lacking and in fact incompatible with the prospects of Big Data analysis. Part IV concludes by discussing the aggregated effect of such incompatibilities on regulated entities, the EU, and society in general.",poster,cp94
Business,p1307,d9,1a8d4b2054782b931840d1a5c9e885da0d3edd0b,c67,The Sea,Intellectual capital in the age of Big Data: establishing a research agenda,"The purpose of this paper is to contribute to the literature on intellectual capital (IC) in light of the emerging paradigm of Big Data. Through a literature review, this paper provides momentum for researchers and scholars to explore the emerging trends and implications of the Big Data movement in the field of IC.,A literature review highlights novel and emerging issues in IC and Big Data research, focussing on: IC for organisational value, the staged evolution of IC research, and Big Data research from the technological to the managerial paradigm. It is expected that identifying these contributions will help establish future research directions.,A conceptual multi-level framework demonstrates how Big Data validates the need to shift the focus of IC research from organisations to ecosystems. The framework is organised into four sections: “why” – the managerial reasons for incorporating Big Data into IC; “what” – the Big Data typologies that enhance IC practice; “who” – the stakeholders involved in and impacted by Big Data IC value creation; and “how” – the Big Data processes suitable for IC management.,The paper provides many avenues for future research in this emerging area of investigation. The key research questions posed aim to advance the contribution of Big Data to research on IC approaches.,The paper outlines the socio-economic value of Big Data generated by and about organisational ecosystems. It identifies opportunities for existing companies to renew their value propositions through Big Data, and discusses new tools for managing Big Data to support disclosing IC value drivers and creating new intangible assets.,This paper investigates the effects and implications Big Data offers for IC management, in support of the fourth stage of IC research. Additionally, it provides an original interpretation of IC research through the lens of Big Data.",poster,cp67
Business,p1321,d9,311e4108de154ac0d60c4b2e9b66ed543ac69e2a,c100,IEEE International Conference on Computer Vision,Addressing barriers to big data,Abstract,poster,cp100
Business,p1329,d9,9883e7c5b130a8cd9656bce0cfe92de27e63b67e,c4,Conference on Innovative Data Systems Research,Predicting Tourist Demand Using Big Data,Abstract,poster,cp4
Business,p1333,d9,7f663e08b32c0d1f0758d6ea02cb261cc3770fbc,c4,Conference on Innovative Data Systems Research,Disparate Impact in Big Data Policing,"Police departments large and small have begun to use data mining techniques to predict the where, when, and who of crime before it occurs. But data mining systems can have a disproportionately adverse impact on vulnerable communities, and predictive policing is no different. Reviewing the technical process of predictive policing, the Article begins by illustrating how use of predictive policing technology will often result in disparate impact on communities of color. After evaluating the possibilities for Fourth Amendment regulation and finding them wanting, the Article turns toward a new regulatory proposal.The Article proposes the use of a rulemaking procedure centered on “discrimination impact assessments.” Predictive policing, like a great deal of data mining solutions, is sold in part as a “neutral” method to counteract unconscious biases. At the moment, however, police departments adopting the technology are not evaluating its potential for a discriminatory impact, which might reproduce or exacerbate the unconscious biases its proponents claim it will cure. Modeled on the environmental impact statements of the National Environmental Policy Act, discrimination impact assessments would require police departments to evaluate the potential discriminatory effects of competing alternative algorithms and to publicly consider mitigation procedures. This regulation balances the need for police expertise in the adoption of new crime control technologies with transparency and public input regarding the potential for harm. Such a public process will also serve to increase trust between police departments and the communities they serve.",poster,cp4
Business,p1335,d9,031cbbc628c6d83e977d24f05097130ee5ad945b,c119,International Conference on Business Process Management,HR and analytics: why HR is set to fail the big data challenge,"The HR world is abuzz with talk of big data and the transformative potential of HR analytics. This article takes issue with optimistic accounts which hail HR analytics as a ‘must have’ capability that will ensure HR’s future as a strategic management function while transforming organisational performance for the better. It argues that unless the HR profession wises up to both the potential and drawbacks of this emerging field, and engages operationally and strategically to develop better methods and approaches, it is unlikely that existing practices of HR analytics will deliver transformational change. Indeed, it is possible that current trends will seal the exclusion of HR from strategic, board level influence while doing little to benefit organisations and actively damaging the interests of employees.",poster,cp119
Business,p1384,d9,211fa2a12c8d55053fbac1f75f41e8d7aaa79f83,c8,Frontiers in Education Conference,Big Data in Accounting: An Overview,"SYNOPSIS: This paper discusses an overall framework of Big Data in accounting, setting the stage for the ensuing collection of essays that presents the ongoing evolution of corporate data into Big ...",poster,cp8
Business,p1398,d9,64a78fb55ac928fc8a902640a8cad0618f42fcee,c39,Online World Conference on Soft Computing in Industrial Applications,How Big Data Will Change Accounting,"SYNOPSIS: Big Data will have increasingly important implications for accounting, even as new types of data become accessible. The video, audio, and textual information made available via Big Data can provide for improved managerial accounting, financial accounting, and financial reporting practices. In managerial accounting, Big Data will contribute to the development and evolution of effective management control systems and budgeting processes. In financial accounting, Big Data will improve the quality and relevance of accounting information, thereby enhancing transparency and stakeholder decision making. In reporting, Big Data can assist with the creation and refinement of accounting standards, helping to ensure that the accounting profession will continue to provide useful information as the dynamic, real-time, global economy evolves.",poster,cp39
Business,p1400,d9,75275edbf8655b404665f176c5c5f76c9a2db8f9,c22,Grid Computing Environments,How to Use Big Data to Drive Your Supply Chain,"Big data analytics has become an imperative for business leaders across every industry sector. Analytics applications that can deliver a competitive advantage appear all along the supply chain decision spectrum—from targeted location-based marketing to optimizing supply chain inventories to enabling supplier risk assessment. While many companies have used it to extract new insights and create new forms of value, other companies have yet to leverage big data to transform their supply chain operations. This article examines how leading companies use big data analytics to drive their supply chains and offers a framework for implementation based on lessons learned.",poster,cp22
Business,p1413,d9,48d9734e70303a6d1af1e7dd455798e29ef78a65,c80,International Symposium on Software Engineering for Adaptive and Self-Managing Systems,A global exploration of Big Data in the supply chain,"Purpose 
 
 
 
 
Journals in business logistics, operations management, supply chain management, and business strategy have initiated ongoing calls for Big Data research and its impact on research and practice. Currently, no extant research has defined the concept fully. The purpose of this paper is to develop an industry grounded definition of Big Data by canvassing supply chain managers across six nations. The supply chain setting defines Big Data as inclusive of four dimensions: volume, velocity, variety, and veracity. The study further extracts multiple concepts that are important to the future of supply chain relationship strategy and performance. These outcomes provide a starting point and extend a call for theoretically grounded and paradigm-breaking research on managing business-to-business relationships in the age of Big Data. 
 
 
 
 
Design/methodology/approach 
 
 
 
 
A native categories qualitative method commonly employed in sociology allows each executive respondent to provide rich, specific data. This approach reduces interviewer bias while examining 27 companies across six industrialized and industrializing nations. This is the first study in supply chain management and logistics (SCMLs) to use the native category approach. 
 
 
 
 
Findings 
 
 
 
 
This study defines Big Data by developing four supporting dimensions that inform and ground future SCMLs research; details ten key success factors/issues; and discusses extensive opportunities for future research. 
 
 
 
 
Research limitations/implications 
 
 
 
 
This study provides a central grounding of the term, dimensions, and issues related to Big Data in supply chain research. 
 
 
 
 
Practical implications 
 
 
 
 
Supply chain managers are provided with a peer-specific definition and unified dimensions of Big Data. The authors detail key success factors for strategic consideration. Finally, this study notes differences in relational priorities concerning these success factors across different markets, and points to future complexity in managing supply chain and logistics relationships. 
 
 
 
 
Originality/value 
 
 
 
 
There is currently no central grounding of the term, dimensions, and issues related to Big Data in supply chain research. For the first time, the authors address subjects related to how supply chain partners employ Big Data across the supply chain, uncover Big Data’s potential to influence supply chain performance, and detail the obstacles to developing Big Data’s potential. In addition, the study introduces the native category qualitative interview approach to SCMLs researchers.",poster,cp80
Business,p1448,d9,7f5e1750f753a3112dde2394d88db842d60f062d,c110,Biometrics and Identity Management,Big Data and Big Cities: The Promises and Limitations of Improved Measures of Urban Life,"New, “big” data sources allow measurement of city characteristics and outcome variables higher frequencies and finer geographic scales than ever before. However, big data will not solve large urban social science questions on its own. Big data has the most value for the study of cities when it allows measurement of the previously opaque, or when it can be coupled with exogenous shocks to people or place. We describe a number of new urban data sources and illustrate how they can be used to improve the study and function of cities. We first show how Google Street View images can be used to predict income in New York City, suggesting that similar image data can be used to map wealth and poverty in previously unmeasured areas of the developing world. We then discuss how survey techniques can be improved to better measure willingness to pay for urban amenities. Finally, we explain how Internet data is being used to improve the quality of city services.",poster,cp110
Economics,p1448,d11,7f5e1750f753a3112dde2394d88db842d60f062d,c110,Biometrics and Identity Management,Big Data and Big Cities: The Promises and Limitations of Improved Measures of Urban Life,"New, “big” data sources allow measurement of city characteristics and outcome variables higher frequencies and finer geographic scales than ever before. However, big data will not solve large urban social science questions on its own. Big data has the most value for the study of cities when it allows measurement of the previously opaque, or when it can be coupled with exogenous shocks to people or place. We describe a number of new urban data sources and illustrate how they can be used to improve the study and function of cities. We first show how Google Street View images can be used to predict income in New York City, suggesting that similar image data can be used to map wealth and poverty in previously unmeasured areas of the developing world. We then discuss how survey techniques can be improved to better measure willingness to pay for urban amenities. Finally, we explain how Internet data is being used to improve the quality of city services.",poster,cp110
Business,p1470,d9,14fd675435f0bc3b76e22efe7a4fe73dcd919ced,c84,EUROCON Conference,"Privacy, Big Data, and the Public Good: Frameworks for Engagement","Big Data is a Big Deal; hence, this Big Book. Before turning to the book itself, I indulge in a bit of background. Most of us have seen claims that Big Data will be our salvation (for instance, by ...",poster,cp84
Business,p1472,d9,e9290576b477b684e40a4a7e2fa1fa314da49b94,c100,IEEE International Conference on Computer Vision,Drivers of the Use and Facilitators and Obstacles of the Evolution of Big Data by the Audit Profession,"SYNOPSIS: Big Data is one of the most important developments in management practice today, with McKinsey Global Institute (2011) arguing that it will fundamentally change business. Forbes (2013) states that “the market for Big Data will reach $16.1 billion in 2014, growing 6 times faster than the overall information technology (IT) market.” Given the growing significance of Big Data as a business tool, this paper considers the extent to which Big Data will be embraced by the audit profession and how that usage will evolve over time. I put forward the hypothesis that auditors cannot stray too far from the practices of their clients since their credibility with and respect of those clients are the basis of the value added that they provide. Hence, if Big Data becomes an essential business tool, then inevitably it will have the same impact on auditing, albeit, perhaps later and with a more muted reaction. Analysis also indicates that American and international auditing standards, technological advances, and ...",poster,cp100
Business,p1495,d9,7d2534e27d25c3e860890852fefe62e85c5e5f42,c84,EUROCON Conference,Customer Engagement in a Big Data World,"This paper aims to propose that the literature on customer engagement has emphasized the benefits of customer engagement to the firm and, to a large extent, ignored the customers’ perspective. By drawing upon co-creation and other literature, this paper attempts to alleviate this gap by proposing a strategic framework that aligns both the customer and firm perspectives in successfully creating engagement that generates value for both the customer and the bottom line.,A strategic framework is proposed that includes the necessary firm resources, data, process, timeline and goals for engagement, and captures customers’ motives, situational factors and preferred engagement styles.,The authors argue that sustainability of data-driven customer engagement requires a dynamic and iterative value generation process involving customers recognizing the value of engagement behaviours and firm’s ability to capture and passing value back to customers.,This paper proposes a dynamic strategic value-creation framework that comprehensively captures both the customer and firm perspectives to data-driven customer engagement.",poster,cp84
Business,p1496,d9,1719c6baf1a95492d47fe6dac0db677045ce2f96,c50,Conference on Emerging Network Experiment and Technology,Big data as complementary audit evidence,"SYNOPSIS In this paper we argue for the use of Big Data as complementary audit evidence. We evaluate the applicability of Big Data using the audit evidence criteria framework and provide cost-benefit analysis for sufficiency, reliability, and relevance considerations. Critical challenges, including integration with traditional audit evidence, information transfer issues, and information privacy protection, are discussed and possible solutions are provided.",poster,cp50
Business,p1604,d9,439a453090e28f0858ad5ba0765cc2eeffb23626,c53,International Conference on Learning Representations,A New Database on Financial Development and Structure,"The authors introduce a new database of indicators of financial development and structure across countries and over time. This database is unique in that it unites a variety of indicators that measure the size, activity, and efficiency of financial intermediaries and markets. It improves on previous efforts by presenting data on the public share of commercial banks, by introducing indicators of the size and activity of non bank financial institutions, and by presenting measures of the size of bond and primary equity markets. The compiled data permit the construction of financial structure indicators to measure whether, for example, a country's banks are larger, more active, and more efficient than its stock markets. These indicators can then be used to investigate the empirical link between the legal, regulatory, and policy environment and indicators of financial structure. They can also be used to analyze the implications of financial structure for economic growth. The authors describe the sources and construction of, and the intuition behind, different indicators and present descriptive statistics.",poster,cp53
Business,p1616,d9,107cbf209b1bd25c7bfc75882347a2655da05118,c2,International Conference on Software Engineering,An international database for pesticide risk assessments and management,"ABSTRACT Despite a changing world in terms of data sharing, availability, and transparency, there are still major resource issues associated with collating datasets that will satisfy the requirements of comprehensive pesticide risk assessments, especially those undertaken at a regional or national scale. In 1996, a long-term project was initiated to begin collating and formatting pesticide data to eventually create a free-to-all repository of data that would provide a comprehensive transparent, harmonized, and managed extensive dataset for all types of pesticide risk assessments. Over the last 20 years, this database has been keeping pace with improving risk assessments, their associated data requirements, and the needs and expectations of database end users. In 2007, the Pesticide Properties DataBase (PPDB) was launched as a free-to-access website. Currently, the PPDB holds data for almost 2300 pesticide active substances and over 700 metabolites. For each substance around 300 parameters are stored, covering human health, environmental quality, and biodiversity risk assessments. With the approach of the twentieth anniversary of the database, this article seeks to elucidate the current data model, data sources, its validation, and quality control processes and describes a number of existing risk assessment applications that depend upon it.",poster,cp2
Business,p1685,d9,cf4dc5ff06fc8c17b05b399a98c164c60a834e09,j44,Social Science Research Network,Systemic Banking Crises: A New Database,"This paper presents a new database on the timing of systemic banking crises and policy responses to resolve them. The database covers the universe of systemic banking crises for the period 1970-2007, with detailed data on crisis containment and resolution policies for 42 crisis episodes, and also includes data on the timing of currency crises and sovereign debt crises. The database extends and builds on the Caprio, Klingebiel, Laeven, and Noguera (2005) banking crisis database, and is the most complete and detailed database on banking crises to date.",fullPaper,jv44
Business,p1715,d9,36698b7863c7d64cf3fb9d17d0f4df1ff6d286dc,c2,International Conference on Software Engineering,Measuring Financial Inclusion: The Global Findex Database,"This paper provides the first analysis of the Global Financial Inclusion (Global Findex) Database, a new set of indicators that measure how adults in 148 economies save, borrow, make payments, and manage risk. The data show that 50 percent of adults worldwide have an account at a formal financial institution, though account penetration varies widely across regions, income groups and individual characteristics. In addition, 22 percent of adults report having saved at a formal financial institution in the past 12 months, and 9 percent report having taken out a new loan from a bank, credit union or microfinance institution in the past year. Although half of adults around the world remain unbanked, at least 35 percent of them report barriers to account use that might be addressed by public policy. Among the most commonly reported barriers are high cost, physical distance, and lack of proper documentation, though there are significant differences across regions and individual characteristics.",poster,cp2
Business,p1725,d9,0afa75ad56cc8ca3cfa176f89443e9a70e09434c,j44,Social Science Research Network,Systemic Banking Crises Database: An Update,"We update the widely used banking crises database by Laeven and Valencia (2008, 2010) with new information on recent and ongoing crises, including updated information on policy responses and outcomes (i.e. fiscal costs, output losses, and increases in public debt). We also update our dating of sovereign debt and currency crises. The database includes all systemic banking, currency, and sovereign debt crises during the period 1970-2011. The data show some striking differences in policy responses between advanced and emerging economies as well as many similarities between past and ongoing crises.",fullPaper,jv44
Business,p1782,d9,3c2d95624ede725cc629cfe63affb57237f009f7,c85,IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering,A New Database of Financial Reforms,Abstract,poster,cp85
Economics,p1782,d11,3c2d95624ede725cc629cfe63affb57237f009f7,c85,IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering,A New Database of Financial Reforms,Abstract,poster,cp85
Business,p1837,d9,1f159d534f2e150577718b92d3ccfd3e23b7e889,c104,North American Chapter of the Association for Computational Linguistics,Constructing a research database of social and environmental reporting by UK companies,"Responds to the widely‐reported methodological problems which have arisen in research into corporate social and environmental reporting. Reports on an attempt to build a database of UK company social and environmental disclosure. The motivation behind the database is an attempt to provide, first, a data set which both refines and develops earlier attempts to capture and interpret such disclosures; second, a data set covering several years to permit longitudinal analysis; and third, a public database for accounting researchers who wish to pursue, in a systematic and comparable way, more focused hypotheses about social and environmental reporting behaviour. Explains the motivation for, the background to, and process of establishing such a database and attempts to expose the difficulties met and the assumptions made in establishing the structure of the data capture. The resultant database has already proved useful to other UK researchers. Aims to help researchers in other countries to develop their own metho...",poster,cp104
Business,p1931,d9,2a92c98a943ae21360d52730f1f2117c7edb2ecb,c88,International Conference on Big Data Computing and Communications,The Regulation and Supervision of Banks around the World: A New Database,"International consultants on bank regulation, and supervision for developing countries, often base their advice on how their home country does things, for lack of information on practice in other countries. Recommendations for reform have tended to be shaped by bias rather than facts. To better inform advice about bank regulation, and supervision, and to lower the marginal cost of empirical research, the authors present, and discuss a new, and comprehensive database on the regulation, and supervision of banks in a hundred and seven countries. The data, based on surveys sent to national bank regulatory, supervisory authorities, are now available to researchers, and policymakers around the world. The data cover such aspects of banking as entry requirements, ownership restrictions, capital requirements, activity restrictions, external auditing requirements, characteristics of deposit insurance schemes, loan classification and provisioning requirements, accounting and disclosure requirements, troubled bank resolution actions, and (uniquely) the quality of supervisory personnel, and their actions. The database permits users to learn how banks are currently regulated, and supervised, and about bank structures, and deposit insurance schemes, for a broad cross-section of countries. In addition to describing the data, the authors show how variables ay be grouped, and aggregated. They also show some simple correlations among selected variables. In a comparison paper (""Bank regulation and supervision: What works best"") studying the relationship between differences in bank regulation and supervision, and bank performance and stability, they conclude that: 1) Countries with policies that promote private monitoring of banks, have better bank performance, and more stability. Countries with more generous deposit insurance schemes tend to have poorer bank performance, and more bank fragility. 2) Diversification of income streams, and loan portfolios - by not restricting bank activities - also tends to improve performance, and stability. (This works best when an active securities market exists). Countries in which banks are encouraged to diversify their portfolios, domestically and internationally, suffer fewer crisis.",poster,cp88
Business,p1961,d9,67f8831944ecc502ce74c761bd7ae0d929b5e2f8,c13,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,An Electronic Lexical Database,"""Natural language processing is essential for dealing efficiently with the large quantities of text now available online: fact extraction and summarization, automated indexing and text categorization, and machine translation. Another essential function is helping the user with query formulation through synonym relationships between words and hierarchical and other relationships between concepts. WordNet supports both of these functions and thus deserves careful study by the digital library community."" By Dagobert Soergel ds52@umail.umd.edu",poster,cp13
Business,p2000,d9,9c3ef6addf7d6eb296a35fd00a8277a6bc2b4e41,j44,Social Science Research Network,BUSINESS RESEARCH METHODS,"This textbook aims to provide a balanced introduction to research methods for today’s undergraduate business students. It does this by synthesising rigorous coverage of methodologies with an accessible ‘real-world’ approach. The text follows course learning objectives for undergraduates in business and provides examples drawn from the full range of business subjects from marketing and strategy to human resource management. The text has unique features: for example, it introduces the four frameworks approach to the research project. The four frameworks approach provides beginner researchers, as well as more advanced researchers, with a simple model that will guide them in the development of their research projects. It facilitates researchers in the task of developing properly focused, fully integrated research projects. The textbook is very sympathetic to the challenges facing a student engaging with the subject for the first time and provides an integrated and balanced approach to quantitative and qualitative research. The writing is simple and direct and the examples and case studies presented were selected particularly with an undergraduate readership in mind. In summary, the text provides a unique and simple yet comprehensive introduction to research methods for business students. Business Research Methods is a valuable resource for all undergraduate business students, particularly for secondand third-year students on business research methods courses. It provides an excellent introduction to the work of undertaking research in an academic environment. This text is essential reading for all business students required to undertake research projects. Postgraduate students, and indeed students from other nonbusiness disciplines, will also find the text very helpful. URI: http://hdl.handle.net/2086/16559 ISBN: 9781473760356 Peer Reviewed: Yes Date: 2019 Creative Commons Licence: N/A Show attachments and full item record This item appears in the following Collection(s) School of Applied Social Sciences [1729] © 2011 De Montfort University Connect with DORA: Business research methods",fullPaper,jv44
Business,p2004,d9,9b60a4fad52e862f1ce144446526315ec1fc6667,c72,Workshop on Research on Enterprise Networking,The Social Responsibility of Business Is to Increase Its Profits,Abstract,poster,cp72
Business,p2007,d9,8f53f3d9b99b00a44b8a1554c01bde274dd1ebe9,c108,IEEE International Conference on Multimedia and Expo,The Effect of a Market Orientation on Business Profitability,"Marketing academicians and practitioners have been observing for more than three decades that business performance is affected by market orientation, yet to date there has been no valid measure of a market orientation and hence no systematic analysis of its effect on a business's performance. The authors report the development of a valid measure of market orientation and analyze its effect on a business's profitability. Using a sample of 140 business units consisting of commodity products businesses and noncommodity businesses, they find a substantial positive effect of a market orientation on the profitability of both types of businesses.",poster,cp108
Business,p2010,d9,c0cd3d1ff165807ef4b755ed2e5cf1e0e4632b18,c16,International Conference on Data Science and Advanced Analytics,The Business Model: Recent Developments and Future Research,"This article provides a broad and multifaceted review of the received literature on business models in which the authors examine the business model concept through multiple subject-matter lenses. The review reveals that scholars do not agree on what a business model is and that the literature is developing largely in silos, according to the phenomena of interest of the respective researchers. However, the authors also found emerging common themes among scholars of business models. Specifically, (1) the business model is emerging as a new unit of analysis; (2) business models emphasize a system-level, holistic approach to explaining how firms “do business”; (3) firm activities play an important role in the various conceptualizations of business models that have been proposed; and (4) business models seek to explain how value is created, not just how it is captured. These emerging themes could serve as catalysts for a more unified study of business models.",poster,cp16
Business,p2019,d9,811c85d23684d771c5f48ff7f74f3a51cba63721,c15,Pacific Symposium on Biocomputing,Research Methods for Business,"Methods for Business: A Skill Building Approach Sekaran, Uma & Bo Business Research Methods 3e. 806 Pages·2016·16.03 MB·5,676 Downloads·New! Business Research Methods 3e Alan Bryman|Emma Bell Business Research Methods. 110 Pages·2012·2.09 MB·5,306 Downloads. place e.g. full text or abstract or keywords rather than journal Business Research Methods Bu Research Methods for Graduate Business and S... Research Methods for Graduate Business and. Social Science Students. John Adams. 1.3 The nature of business and management research Management research is different from other kinds of research because it is transdisciplinary (multiple studies are involved with it) and it is a design science. Moreover, it has to be theoretically and methodologically accurate, while at the same time being of practical relevance in the business world. The researcher Michael Gibbons has introduced 3 modes of knowledge creation: Mode 1 – creating fundamental knowledge. Method – This is the longest section and reveals how the research will be conducted. It consists of two parts: Research design and data collection. Research design is an overall overview of the chosen method and provides the reason for choosing this method.     © 2020 Emerald Publishing Limited Citation Hair, J.F., Money, A.H., Samouel, P. and Page, M. (2007), ""Research Methods for Business"", Education + Training, Vol. 49 No. 4, pp. 336-337. https://doi.org/10.1108/et.2007.49.4.336.2  Download as .RIS",poster,cp15
Business,p2024,d9,f21355916b212eb8d71371fd717b490fabf141bb,j264,Journal of business research,Effects of COVID-19 on business and research,Abstract,fullPaper,jv264
Business,p2029,d9,578ae8e6202159101f26227f8b03b249a5913921,c17,International Conference on Statistical and Scientific Database Management,Business Model Innovation: Opportunities and Barriers,Abstract,poster,cp17
Business,p2035,d9,afa327a1a1f8dbad466ebe04628cae70b547f9ec,c8,Frontiers in Education Conference,"The Business Case for Corporate Social Responsibility: A Review of Concepts, Research and Practice","In this review, the primary subject is the ‘business case’ for corporate social responsibility (CSR). The business case refers to the underlying arguments or rationales supporting or documenting why the business community should accept and advance the CSR ‘cause’. The business case is concerned with the primary question: What do the business community and organizations get out of CSR? That is, how do they benefit tangibly from engaging in CSR policies, activities and practices? The business case refers to the bottom-line financial and other reasons for businesses pursuing CSR strategies and policies. In developing this business case, the paper first provides some historical background and perspective. In addition, it provides a brief discussion of the evolving understandings of CSR and some of the long-established, traditional arguments that have been made both for and against the idea of business assuming any responsibility to society beyond profit-seeking and maximizing its own financial well-being. Finally, the paper addresses the business case in more detail. The goal is to describe and summarize what the business case means and to review some of the concepts, research and practice that have come to characterize this developing idea.",poster,cp8
Business,p2036,d9,d23b990b4914ac25b889d0cb02c9ec0213038e40,c21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,Developing relationships in business networks,"This edited collection applies the network approach to the analysis of business relationships in a global context. Drawing on a number of international case studies, a ""network approach"" is developed, giving rise to theoretical and practical managerial insights and a different way of conceptualizing companies within markets. New angles emerge on traditional problems of business management, with some novel implications which should challenge established ways to analyze business markets. Previous publications by these authors include, ""Corporate Technological Behaviour"", ""Professional Purchasing"" and ""Managing Innovation Within Networks"".",poster,cp21
Business,p2037,d9,4608b67619c975aec1c94b96f6f6fd47df8775d1,c10,Americas Conference on Information Systems,Product design and business model strategies for a circular economy,"The transition within business from a linear to a circular economy brings with it a range of practical challenges for companies. The following question is addressed: What are the product design and business model strategies for companies that want to move to a circular economy model? This paper develops a framework of strategies to guide designers and business strategists in the move from a linear to a circular economy. Building on Stahel, the terminology of slowing, closing, and narrowing resource loops is introduced. A list of product design strategies, business model strategies, and examples for key decision-makers in businesses is introduced, to facilitate the move to a circular economy. This framework also opens up a future research agenda for the circular economy.",poster,cp10
Business,p2044,d9,91c4779101ca230c8d3730f222ef4df9795c7789,c19,International Conference on Conceptual Structures,Fifteen Years of Research on Business Model Innovation,"Over the last 15 years, business model innovation (BMI) has gained an increasing amount of attention in management research and among practitioners. The emerging BMI literature addresses an important phenomenon but lacks theoretical underpinning, and empirical inquiry is not cumulative. Thus, a concerted research effort seems warranted. Accordingly, we take stock of the extant literature on BMI. We identify and analyze 150 peer-reviewed scholarly articles on BMI published between 2000 and 2015. We provide the first comprehensive systematic review of the BMI literature, include a critical assessment of these research efforts, and offer suggestions for future research. We argue that the literature faces problems with respect to construct clarity and has gaps with respect to the identification of antecedent conditions, contingencies, and outcomes. We identify important avenues for future research and show how the complexity theory, innovation, and other streams of literature can help overcome many of the gaps in the BMI literature.",poster,cp19
Business,p2050,d9,3800d4c8385676df12988c8b614478256368fb08,c7,International Symposium on Intelligent Data Analysis,On the Measurement of Business Performance in Strategy Research: A Comparison of Approaches,"A two-dimensional classificatory scheme highlighting ten different approaches to the measurement of business performance in strategy research is developed. The first dimension concerns the use of financial versus broader operational criteria, while the second focuses on two alternate data sources (primary versus secondary). The scheme permits the classification of an exhaustive coverage of measurement approaches and is useful for discussing their relative merits and demerits. Implications for operationalizing business performance in future strategy research are discussed.",poster,cp7
Business,p2054,d9,70ad73834b38cfc87674ca24678a22db9fe3f4c4,c105,International Conference on Automatic Face and Gesture Recognition,Understanding the small business sector,"This text introduces the key characteristics of the small business sector. Looking at core business functions, it examines the specific problems that face the small business owner. It shows how the business environment for the smaller firm differs from that of larger companies, and studies how far their success or failure depends on the wider economic climate. It also looks at the different locations, whether urban or regional, and the effect of small businesses on the outside community. In addition, the book studies the internal organization of small companies, encompassing discussions of employment, entrepreneurship, management strategies, organizational cultures, finance and the variety of challenges the small business owner faces in each of these questions.",poster,cp105
Business,p2057,d9,cdbf3191bf2b5ba1914c852eb0ceca9b5bdd1327,c5,Technical Symposium on Computer Science Education,Business research methods 12th ed.,Abstract,poster,cp5
Business,p2061,d9,891b788258610ea7761397c89ff2a5e792ca13e7,c4,Conference on Innovative Data Systems Research,REENGINEERING THE CORPORATION: A MANIFESTO FOR BUSINESS REVOLUTION,"This work is on the most important topic in business circles today - the radical redesign of a company's processes, organization and culture to achieve a quantum leap in performance. Adam Smith work broke down into specialized tasks. Hammer and Champy explain that instead of tinkering with - or simply computerizing - an aspect of the work design, the answer is to radically redesign the whole process. Business re-engineering isn't about fixing anything - it's about starting again, about reinventing the nature of work and corporate structures from top to bottom. This book shows how some of the world's premier corporations are re-engineering to save hundreds of millions of dollars a year, achieve unprecedented levels of customer satistfaction, and speed up and make more flexible all aspects of their operations. Re-engineering is the 90s what strategy was for the 70s and quality was for the 80s. After a decade of restructuring and downsizing, of cutting costs and capacity, managers need to rethink boldly to yield the dramatic improvements for companies, both big and small, to compete successfully in a changing world.",poster,cp4
Business,p2065,d9,fdd5b268d2895b7a857ffdd91b6b24a9104cc8c2,c67,The Sea,Mass Customization: The New Frontier in Business Competition,"The article reviews the book “Mass Customization: The New Frontier in Business Competition,” by B. Joseph Pine II.",poster,cp67
Business,p2069,d9,58f238c3c3e75a3e236cacf9e0739d0e9e16fdb6,c39,Online World Conference on Soft Computing in Industrial Applications,Is Group Affiliation Profitable in Emerging Markets? An Analysis of Diversified Indian Business Groups,"Emerging markets like India have poorly functioning institutions, leading to severe agency and information problems. Business groups in these markets have the potential both to offer benefits to member firms, and to destroy value. We analyze the performance of affiliates of diversified Indian business groups relative to unaffiliated firms. We find that accounting and stock market measures of firm performance initially decline with group diversification and subsequently increase once group diversification exceeds a certain level. Unlike U.S. conglomerates' lines of business, and similar to the affiliates of U.S. LBO associations, affiliates of the most diversified business groups outperform unaffiliated firms.",poster,cp39
Economics,p2069,d11,58f238c3c3e75a3e236cacf9e0739d0e9e16fdb6,c39,Online World Conference on Soft Computing in Industrial Applications,Is Group Affiliation Profitable in Emerging Markets? An Analysis of Diversified Indian Business Groups,"Emerging markets like India have poorly functioning institutions, leading to severe agency and information problems. Business groups in these markets have the potential both to offer benefits to member firms, and to destroy value. We analyze the performance of affiliates of diversified Indian business groups relative to unaffiliated firms. We find that accounting and stock market measures of firm performance initially decline with group diversification and subsequently increase once group diversification exceeds a certain level. Unlike U.S. conglomerates' lines of business, and similar to the affiliates of U.S. LBO associations, affiliates of the most diversified business groups outperform unaffiliated firms.",poster,cp39
Business,p2071,d9,b8dc82e6d4f8d28ca4b954505e9d7ea901199549,c77,Visualization for Computer Security,Qualitative research in business and management,Abstract,poster,cp77
Business,p2072,d9,ca9ae1773cf290c4dc83478a1539a2ddaad44544,c73,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,A Review and Typology of Circular Economy Business Model Patterns,"The circular economy (CE) requires companies to rethink their supply chains and business models. Several frameworks found in the academic and practitioner literature propose circular economy business models (CEBMs) to redefine how companies create value while adhering to CE principles. A review of these frameworks shows that some models are frequently discussed, some are framework specific, and some use a different wording to refer to similar CEBMs, pointing to the need to consolidate the current state of the art. We conduct a morphological analysis of 26 current CEBMs from the literature, which includes defining their major business model dimensions and identifying the specific characteristics of these dimensions. Based on this analysis, we identify a broad range of business model design options and propose six major CEBM patterns with the potential to support the closing of resource flows: repair and maintenance; reuse and redistribution; refurbishment and remanufacturing; recycling; cascading and repurposing; and organic feedstock business model patterns. We also discuss different design strategies to support the development of these CEBMs.",poster,cp73
Business,p2076,d9,5811c8e91323ee1ce2cedf0db0b247a4ac0f6359,c37,International Workshop on the Semantic Web,Reinventing Your Business Model,When does an established company need a new business model to capture a game-changing opportunity? Only companies that understand how - and why - their current model works can answer that question.,poster,cp37
Business,p2077,d9,e3e2a70d044207f06f934870b4eef5c694129644,c75,International Conference on Predictive Models in Software Engineering,Defining the Family Business by Behavior,"It is generally accepted that a family's involvement in the business makes the family business unique; but the literature continues to have difficulty defining the family business. We argue for a distinction between theoretical and operational definitions. A theoretical definition must identify the esence that distinguishes the family business from other businesses. It is the standard against which operational definitions must be measured. We propose a theoretical definition based on behavior as the essence of a family business. Our conceptual analysis shows that most of the operational definitions based on the components of family involvement overlap with our theoretical definition. Our empirical results suggest, however, that the components of family involvement typically used in operational definitions are weak predictors of intentions and, therefore, are not always reliable for distinguishing family businesses from non-family ones.",poster,cp75
Business,p2078,d9,0251dd980480e711021806aed54427603c7a9d5e,c17,International Conference on Statistical and Scientific Database Management,Dyadic Business Relationships within a Business Network Context,"In business-to-business settings, dyadic relationships between firms are of paramount interest. Recent developments in business practice strongly suggest that to understand these business relationships, greater attention must be directed to the embedded context within which dyadic business relationships take place. The authors provide a means for understanding the connectedness of these relationships. They then conduct a substantive validity assessment to furnish some empirical support that the constructs they propose are sufficiently well delineated and to generate some suggested measures for them. They conclude with a prospectus for research on business relationships within business networks.",poster,cp17
Business,p2080,d9,88adbd59eb16ad7d11fe02c30bfd7b9a3f408b85,c92,International Symposium on Computer Architecture,Rational Decision Making in Business Organizations,"Lecture to the memory of Alfred Nobel, December 8, 1978(This abstract was borrowed from another version of this item.)",poster,cp92
Economics,p2080,d11,88adbd59eb16ad7d11fe02c30bfd7b9a3f408b85,c92,International Symposium on Computer Architecture,Rational Decision Making in Business Organizations,"Lecture to the memory of Alfred Nobel, December 8, 1978(This abstract was borrowed from another version of this item.)",poster,cp92
Business,p2081,d9,19c44a574e6e6e6b63535cc77b95d88851f33a9d,c42,IEEE Working Conference on Mining Software Repositories,Business Model Innovation for Sustainability: Towards a Unified Perspective for Creation of Sustainable Business Models,"Business model innovation has seen a recent surge in academic research and business practice. Changes to business models are recognized as a fundamental approach to realize innovations for sustainability. However, little is known about the successful adoption of sustainable business models (SBMs). The purpose of this paper is to develop a unified theoretical perspective for understanding business model innovations that lead to better organizational economic, environmental and social performance. The paper examines bodies of literature on business model innovation, sustainability innovation, networks theory, stakeholder theory and product–service systems. We develop five propositions that support the creation of SBMs in a unified perspective, which lays a foundation to support organizations in investigating and experimenting with alternative new business models. This article contributes to the emerging field of SBMs, which embed economic, environmental and social flows of value that are created, delivered and captured in a value network. It highlights gaps for addressing the challenges of business model innovation for sustainability and suggests avenues for future research. © 2017 The Authors. Business Strategy and the Environment published by ERP Environment and John Wiley & Sons Ltd",poster,cp42
Business,p2083,d9,55af0d7fcaa64d295eb76db64e3b522c67b79179,j269,Journal of Cleaner Production,Sustainable Business Model Innovation: A Review,Abstract,fullPaper,jv269
Business,p2089,d9,afcf9bc3efe576d4cb4b980e079ae43f0ac07b46,j264,Journal of business research,Digital servitization business models in ecosystems: A theory of the firm,Abstract,fullPaper,jv264
Business,p2093,d9,d8da3b23cfbdd8fcb6d61b673f779dc9fad73c93,j406,Journal of Business Ethics,A Stakeholder Theory Perspective on Business Models: Value Creation for Sustainability,Abstract,fullPaper,jv406
Business,p2094,d9,036bb5bfd2ba9b4b5165d697767f846deb664f3c,c44,Italian National Conference on Sensors,Business Research Methods,"This is a research methods textbook. The text book is built around a model of the research process and the four frameworks approach to the research project, both models developed by Quinlan in her teaching practice.",poster,cp44
Business,p2095,d9,e10bb8ba70f9976b6a64956f861540a29afd5321,c89,Conference on Uncertainty in Artificial Intelligence,Does Distance Still Matter? The Information Revolution in Small Business Lending,"The distance between small firms and their lenders in the United States is increasing. Not only are firms choosing more distant lenders, they are also communicating with them in more impersonal ways. After documenting these systematic changes, we demonstrate that they do not stem from small firms locating differently, from simple consolidation in the banking industry, or from biases in the sample. Instead, they seem correlated with improvements in bank productivity. We conjecture that greater, and more timely, availability of borrower credit records, as well as the greater ease of processing these may explain the increased lending at a distance. Consistent with such an explanation, distant firms no longer have to be observably the highest quality credits, suggesting that a wider cross-section of firms can now obtain funding from a particular lender. These findings, we believe, are direct evidence that there has been substantial development of the financial sector in the United States, even in areas such as small business lending that have not been directly influenced by the growth in public markets. From a policy perspective, that small firms now obtain wider access to financing suggests the consolidation of banking services may not raise as strong anti-trust concerns as in the past.",poster,cp89
Economics,p2095,d11,e10bb8ba70f9976b6a64956f861540a29afd5321,c89,Conference on Uncertainty in Artificial Intelligence,Does Distance Still Matter? The Information Revolution in Small Business Lending,"The distance between small firms and their lenders in the United States is increasing. Not only are firms choosing more distant lenders, they are also communicating with them in more impersonal ways. After documenting these systematic changes, we demonstrate that they do not stem from small firms locating differently, from simple consolidation in the banking industry, or from biases in the sample. Instead, they seem correlated with improvements in bank productivity. We conjecture that greater, and more timely, availability of borrower credit records, as well as the greater ease of processing these may explain the increased lending at a distance. Consistent with such an explanation, distant firms no longer have to be observably the highest quality credits, suggesting that a wider cross-section of firms can now obtain funding from a particular lender. These findings, we believe, are direct evidence that there has been substantial development of the financial sector in the United States, even in areas such as small business lending that have not been directly influenced by the growth in public markets. From a policy perspective, that small firms now obtain wider access to financing suggests the consolidation of banking services may not raise as strong anti-trust concerns as in the past.",poster,cp89
Business,p2096,d9,a8a2e5db4f5276ac0d475636dffda690b7d4ddd9,j44,Social Science Research Network,Sustainable Business Models: A Review,"During the past two decades of e-commerce growth, the concept of a business model has become increasingly popular. More recently, the research on this realm has grown rapidly, with diverse research activity covering a wide range of application areas. Considering the sustainable development goals, the innovative business models have brought a competitive advantage to improve the sustainability performance of organizations. The concept of the sustainable business model describes the rationale of how an organization creates, delivers, and captures value, in economic, social, cultural, or other contexts, in a sustainable way. The process of sustainable business model construction forms an innovative part of a business strategy. Different industries and businesses have utilized sustainable business models’ concept to satisfy their economic, environmental, and social goals simultaneously. However, the success, popularity, and progress of sustainable business models in different application domains are not clear. To explore this issue, this research provides a comprehensive review of sustainable business models literature in various application areas. Notable sustainable business models are identified and further classified in fourteen unique categories, and in every category, the progress -either failure or success- has been reviewed, and the research gaps are discussed. Taxonomy of the applications includes innovation, management and marketing, entrepreneurship, energy, fashion, healthcare, agri-food, supply chain management, circular economy, developing countries, engineering, construction and real estate, mobility and transportation, and hospitality. The key contribution of this study is that it provides an insight into the state of the art of sustainable business models in various application areas and future research directions. This paper concludes that popularity and the success rate of sustainable business models in all application domains have been increased along with the increasing use of advanced technologies.",fullPaper,jv44
Economics,p2096,d11,a8a2e5db4f5276ac0d475636dffda690b7d4ddd9,j44,Social Science Research Network,Sustainable Business Models: A Review,"During the past two decades of e-commerce growth, the concept of a business model has become increasingly popular. More recently, the research on this realm has grown rapidly, with diverse research activity covering a wide range of application areas. Considering the sustainable development goals, the innovative business models have brought a competitive advantage to improve the sustainability performance of organizations. The concept of the sustainable business model describes the rationale of how an organization creates, delivers, and captures value, in economic, social, cultural, or other contexts, in a sustainable way. The process of sustainable business model construction forms an innovative part of a business strategy. Different industries and businesses have utilized sustainable business models’ concept to satisfy their economic, environmental, and social goals simultaneously. However, the success, popularity, and progress of sustainable business models in different application domains are not clear. To explore this issue, this research provides a comprehensive review of sustainable business models literature in various application areas. Notable sustainable business models are identified and further classified in fourteen unique categories, and in every category, the progress -either failure or success- has been reviewed, and the research gaps are discussed. Taxonomy of the applications includes innovation, management and marketing, entrepreneurship, energy, fashion, healthcare, agri-food, supply chain management, circular economy, developing countries, engineering, construction and real estate, mobility and transportation, and hospitality. The key contribution of this study is that it provides an insight into the state of the art of sustainable business models in various application areas and future research directions. This paper concludes that popularity and the success rate of sustainable business models in all application domains have been increased along with the increasing use of advanced technologies.",fullPaper,jv44
Business,p2098,d9,8162968fadfe2407bc8875aa37e47d3ec8fea447,c13,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,Innovativeness: Its antecedents and impact on business performance,Abstract,poster,cp13
Business,p2099,d9,9e6ccf0950936737958f69b8da7a2daed0f82dbf,c120,SIGSAND-Europe Symposium,Clarifying the Meaning of Sustainable Business,"While sustainability management is becoming more widespread among major companies, the impact of their activities does not reflect in studies monitoring the state of the planet. What results from this is a “big disconnect.” With this article, we address two main questions: “How can business make an effective contribution to addressing the sustainability challenges we are facing?” and “When is business truly sustainable?” In a time when more and more corporations claim to manage sustainably, we need to distinguish between those companies that contribute effectively to sustainability and those that do not. We provide an answer by clarifying the meaning of business sustainability. We review established approaches and develop a typology of business sustainability with a focus on effective contributions for sustainable development. This typology ranges from Business Sustainability 1.0 (Refined Shareholder Value Management) to Business Sustainability 2.0 (Managing for the Triple Bottom Line) and to Business Sustainability 3.0 (True Sustainability).",poster,cp120
Business,p2100,d9,8aa54fd441e1c1012d6a252fe5bec4f965d42a53,j407,Industrial Marketing Management,"Digitization capability and the digitalization of business models in business-to-business firms: Past, present, and future",Abstract,fullPaper,jv407
Business,p2102,d9,5670ce7fe0f40c6c06554941cccfe09d75262e1a,j408,Journal of Business Venturing Insights,Blockchain Disruption and Decentralized Finance: The Rise of Decentralized Business Models,"Abstract Blockchain technology can reduce transaction costs, generate distributed trust, and empower decentralized platforms, potentially becoming a new foundation for decentralized business models. In the financial industry, blockchain technology allows for the rise of decentralized financial services, which tend to be more decentralized, innovative, interoperable, borderless, and transparent. Empowered by blockchain technology, decentralized financial services have the potential to broaden financial inclusion, facilitate open access, encourage permissionless innovation, and create new opportunities for entrepreneurs and innovators. In this article, we assess the benefits of decentralized finance, identify existing business models, and evaluate potential challenges and limits. As a new area of financial technology, decentralized finance may reshape the structure of modern finance and create a new landscape for entrepreneurship and innovation, showcasing the promises and challenges of decentralized business models.",fullPaper,jv408
Business,p2103,d9,e565bc2ca7767eb7d5c2282a03946d27c6b12294,c32,International Conference on Smart Data Services,Business Model Innovation,"Purpose – The purpose of this paper is to develop a framework that expands business model innovation literature by including a social goal, the emerging markets (EMs) environmental characteristics and adopting a bottom-up perspective. Design/methodology/approach – This paper draws on a single-case study. Sistema Ser/CEGIN (SER–CEGIN) is an Argentine social business that offers high-quality medical healthcare to BOP users. Findings – The paper presents a new conceptualization on business model innovation that includes three dimensions: firm-centric, environment and customer-centric. The framework incorporates to the traditional framework on business model innovation, the social profit equation, the general and task environment and the end-user, as well as the dynamics between them. Research limitations/implications – While the authors acknowledge the importance of studying the components of the business model operating levels (economic, operational and strategic) to determine the type of business model inn...",poster,cp32
Business,p2104,d9,491d381f959c8274035d15b1291a72211b501462,j409,Business Horizons,How blockchain technologies impact your business model,Abstract,fullPaper,jv409
Business,p2113,d9,d37be8a91fce4752dbda49adc53ad3cacc53f782,c21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,Business Models for Sustainability,"While a consensus appears to have evolved among many sustainability researchers and practitioners that sustainable development at the societal level is not very likely without the sustainable development of organizations, the business model as a key initiating component of corporate sustainability has only recently moved into the focus of sustainability management research. Apparently, the usual approaches to sustainable development of philanthropy, corporate social responsibility, and technological process and product innovation are insufficient to create the necessary radical transformation of organizations, industries, and societies toward genuine, substantive sustainable development. More in-depth research is needed on whether both modified and completely new business models can help develop integrative and competitive solutions by either radically reducing negative and/or creating positive external effects for the natural environment and society (cf. Boons & Lüdeke-Freund, 2013; Hansen, Große-Dunker, & Reichwald, 2009; Schaltegger, Lüdeke-Freund, & Hansen, 2012; Stubbs & Cocklin, 2008). One of the first articles in this field was published in Organization & Environment. The Stubbs and Cocklin (2008) article titled “Conceptualizing a ‘Sustainability Business Model” was a seminal study published a few years before the currently emerging wave of academic business model publications. The Stubbs and Cocklin study revealed a set of normative principles of organizational development that together form an “ideal type” of sustainability-oriented business model. These authors pioneered the field of case-based theory building for sustainability-oriented business models, using Interface Inc. and Bendigo Bank as examples of sustainability-driven organizations. Their ideal type comprised different structural and cultural attributes of an organization, such as developing community spirit, investing in employees’ trust and loyalty, and engaging in sustainability assessment and reporting. They also advanced propositions about sustainabilityoriented business models dealing with an organization’s purpose and goals, its performance measurement approach, the need to consider all stakeholders, how nature should be treated, whether the organization’s leaders drive the necessary cultural and structural changes to implement sustainability, and whether a systems-level, as well as a firm-level, perspective should be employed.",poster,cp21
Business,p2115,d9,32f1608a17a3bc26fe14df36ce7fe0361816ac08,c14,Hawaii International Conference on System Sciences,"Customer value, satisfaction, loyalty, and switching costs: An illustration from a business-to-business service context","Although researchers and managers pay increasing attention to customer value, satisfaction, loyalty, and switching costs, not much is known about their interrelationships. Prior research has examined the relationships within subsets of these constructs, mainly in the business-to-consumer (B2C) environment. The authors extend prior research by developing a conceptual framework linking all of these constructs in a business-to-business (B2B) service setting. On the basis of the cognition-affect-behavior model, the authors hypothesize that customer satisfaction mediates the relationship between customer value and customer loyalty, and that customer satisfaction and loyalty have significant reciprocal effects on each other. Furthermore, the potential interaction effect of satisfaction and switching costs, and the quadratic effect of satisfaction, on loyalty are explored. The authors test the hypotheses on data obtained from a courier service provider in a B2B context. The results support most of the hypotheses and, in particular, confirm the mediating role of customer satisfaction.",poster,cp14
Business,p2126,d9,c713e118a1d122a382b86e7c5712eccbf31c49f3,c102,ACM SIGMOD Conference,Small Business Management,Abstract,poster,cp102
Business,p2129,d9,c8d03aa866863308931f7d40ea285efa07745a9a,j412,Journal of Economics and Business,The Impact of Covid-19 Pandemic Crisis on Micro-Enterprises: Entrepreneurs’ Perspective on Business Continuity and Recovery Strategy,"COVID-19 pandemic outbreaks have led many countries to impose travel restrictions and movement controls. In Malaysia, the small business sector is one of the most directly affected by the movement's control order. In fact, the impact is more significant among micro-enterprises than its larger counterparts. Entrepreneurs experience business cancellation or closure and reduced income due to the closure of several supporting sectors such as retails and transportation. There is still a lack of study on the impact of a pandemic outbreak on micro-enterprises in developing countries, especially in relating to business continuity and recovery strategy. It is crucial to explore how micro-entrepreneurs experience crisis and what decision they make for business survival. This study represents the perspectives of two micro-entrepreneurs in the rural area of Sabah, about their business continuity strategy during movement control order. The results of unstructured phone interviews provide insights on business survival approach and recovery plan of micro-enterprises during and after a crisis. This study will hopefully contribute towards the creation of effective support mechanisms through associated entrepreneurial development organizations for micro-entrepreneurs to thrive during and after a crisis.",fullPaper,jv412
Business,p2133,d9,f442d89c79863ecabc8eae3d297ae5c153a1bf7c,c46,Ideal,The Visible Hand: The Managerial Revolution in American Business,Abstract,poster,cp46
Economics,p2133,d11,f442d89c79863ecabc8eae3d297ae5c153a1bf7c,c46,Ideal,The Visible Hand: The Managerial Revolution in American Business,Abstract,poster,cp46
Business,p2137,d9,aa835e92e6eae6dceed29b9204d08449749b7b38,c34,International Conference on Data Warehousing and Knowledge Discovery,Business Research Methods,Abstract,poster,cp34
Business,p2139,d9,c6ac80a7d7a65fc5f1d8bc8e0123515f7b104018,j415,Thunderbird International Business Review,Coronavirus and international business: An entrepreneurial ecosystem perspective,Abstract Covid‐19 (Coronavirus) is a health pandemic that has significantly affected the global economy and fundamentally changed society. The impacts of coronavirus are most apparent at the international business level due to the restrictions on travel and mobility of labor. This has necessitated an entrepreneurial outlook for businesses in order to survive in the current market environment. The aim of this article is to discuss the implications of the coronavirus for international business by taking an entrepreneurial ecosystem point of view. This enables a reflection on how the pandemic has impacted various entities of the ecosystem in terms of stakeholder engagement. Suggestions for acting in a more dynamic and innovative manner are provided that highlight the importance of utilizing entrepreneurial ecosystems in times of crises.,fullPaper,jv415
Business,p2140,d9,148f734d2965f42bc6e0808fe2da5e8aea412800,c40,European Conference on Computer Vision,Generation to Generation: Life Cycles of the Family Business,The business development axis thae ownership developmental axis the family developmental axis three classic family business plans managing the developing family business.,poster,cp40
Business,p2141,d9,658ed0485dc09257a8d2d63a0a4198983ab70aa3,c115,International Conference on Information Integration and Web-based Applications & Services,”Coopetition” in Business Networks—to Cooperate and Compete Simultaneously,Abstract,poster,cp115
Business,p2143,d9,59d2efd0c763c7f02c709646b2686844d32f29be,j416,Journal of Small Business and Entrepreneurship,Cutting-edge technologies for small business and innovation in the era of COVID-19 global health pandemic,"Abstract The adoption of cutting-edge technologies to steer business activities during community lockdown to contain the spread of the COVID-19 pandemic, even if involuntarily, provides evidence that technologies not only offer competitive advantages but also provides a means for survival, by improvising existing business models. In June 2019, we issued a call for papers to address the awareness, adoption, and implementation challenges of technologies that can drive businesses of all sizes in the fourth industrial revolution. We intended to identify as critical elements the “must-have” and a “nice to have” technologies for small businesses and innovation. Then the ongoing COVID-19 global health pandemic struck in December 2019, forcing the need for digitization of business activities and remote operations, which was considered a “nice to have” to immediately become a “critical to have” to survive in the ever increasingly uncertain business environment. This paper identifies the technologies, evaluates disruptive software platforms, and strategies needed for creating and managing small business innovation and highlighting the complexity of that process and the context within which this process takes place. We integrate this discussion alongside a summary of the articles included in the Special Issue. The current realities show that technologies that enable social business creation, customer relationship management systems, new communications channels, virtual reality technologies for remote operations, and the Internet of Things (IoT) are crucial to lowering the costs of doing business. Big data and predictive and visual analytics are critical enablers to aiding complex business decisions in the current challenging business climate.",fullPaper,jv416
Business,p2146,d9,4edaa8500b7ca161a82c6c03786d8bea05b7d86d,c81,ACM Symposium on Applied Computing,Business Model Generation,"Few books in the business literature can truly be called breakthroughs. This may be one. Alexander Osterwalder and Yves Pigneur, professors of information technology at the University of Lausanne, have produced what may be the first book on business models written for the general public. It relies on the more technical work that Osterwalder did in his doctoral dissertation in 2004, submitted under Pigneur, and that both researchers have since conducted individually and jointly.",poster,cp81
Business,p2152,d9,8f7432f9311eedb3fec9651aa956058de13e3d10,c16,International Conference on Data Science and Advanced Analytics,Business Model: What it is and What it is Not,"The term “business model” has been misinterpreted and misused over the years, resulting in it being inadequately understood and applied by both practitioners and scholars. It is frequently confused with other popular terms in the management literature such as strategy, business concept, revenue model, economic model or even business process modeling. Our findings suggest that while business model describes what an organization currently is, it needs to be complemented with a strategy and capabilities in order to face upcoming changes. Besides clarifying the meaning and use of the business model terminology, we theorize about its roots through a combination of the resource-based view and transaction cost economics. Finally, we identify new avenues for further research such as the investigation of path dependency in a business model and the meaning of business model innovation. “The definition of a business model is murky at best. Most often, it seems to refer to a loose conception of how a company does business and generates revenue... The business model approach to management becomes an invitation for faulty thinking and self-delusion” (Porter, 2001, p. 73).“While the term ‘business model’ has gained widespread use in the practice community, the academic literature on this topic is fragmented and confounded by inconsistent definitions and construct boundaries” (George & Bock, 2011, p. 83).",poster,cp16
Economics,p2152,d11,8f7432f9311eedb3fec9651aa956058de13e3d10,c16,International Conference on Data Science and Advanced Analytics,Business Model: What it is and What it is Not,"The term “business model” has been misinterpreted and misused over the years, resulting in it being inadequately understood and applied by both practitioners and scholars. It is frequently confused with other popular terms in the management literature such as strategy, business concept, revenue model, economic model or even business process modeling. Our findings suggest that while business model describes what an organization currently is, it needs to be complemented with a strategy and capabilities in order to face upcoming changes. Besides clarifying the meaning and use of the business model terminology, we theorize about its roots through a combination of the resource-based view and transaction cost economics. Finally, we identify new avenues for further research such as the investigation of path dependency in a business model and the meaning of business model innovation. “The definition of a business model is murky at best. Most often, it seems to refer to a loose conception of how a company does business and generates revenue... The business model approach to management becomes an invitation for faulty thinking and self-delusion” (Porter, 2001, p. 73).“While the term ‘business model’ has gained widespread use in the practice community, the academic literature on this topic is fragmented and confounded by inconsistent definitions and construct boundaries” (George & Bock, 2011, p. 83).",poster,cp16
Business,p2156,d9,e4aab748df1e0cbc20b943c47bd1f3a7cc6c5f7f,c74,International Conference on Computational Linguistics,Ferreting Out Tunneling: An Application to Indian Business Groups,"In many countries, controlling shareholders are accused of tunneling, transferring resources from companies where they have few cash flow rights to ones where they have more cash flow rights. Quantifying the extent of such tunneling, however, has proven difficult because of its illicit nature. This paper develops a general empirical technique for quantifying tunneling. We use the responses of different firms to performance shocks to map out the flow of resources within a group of firms and to quantify the extent to which the marginal dollar is tunneled. We apply our technique to data on Indian business groups. The results suggest a significant amount of tunneling between firms in these groups.",poster,cp74
Economics,p2156,d11,e4aab748df1e0cbc20b943c47bd1f3a7cc6c5f7f,c74,International Conference on Computational Linguistics,Ferreting Out Tunneling: An Application to Indian Business Groups,"In many countries, controlling shareholders are accused of tunneling, transferring resources from companies where they have few cash flow rights to ones where they have more cash flow rights. Quantifying the extent of such tunneling, however, has proven difficult because of its illicit nature. This paper develops a general empirical technique for quantifying tunneling. We use the responses of different firms to performance shocks to map out the flow of resources within a group of firms and to quantify the extent to which the marginal dollar is tunneled. We apply our technique to data on Indian business groups. The results suggest a significant amount of tunneling between firms in these groups.",poster,cp74
Business,p2161,d9,dfeee3f58cc016b70619273db43e0d2b94c47089,c70,Annual Meeting of the Association for Computational Linguistics,Guanxi and Business,"Definition, principles, and philosophy of ""guanxi"" economic perspective of ""guanxi"" ""guanxi"" and firm performance organizational dynamics and ""guanxi"" ""guanxi""-based business strategies foreign businesses and ""guanxi"" business implications of corruption practical guidelines to ""guanxi"" cultivation.",poster,cp70
Business,p2162,d9,1a68a0c46e06b791a3deefbea6e6a1220cb610fa,c19,International Conference on Conceptual Structures,"Research on Women Business Owners: Past Trends, a New Perspective and Future Directions","The number of women starting and owning their own businesses has grown dramatically over the past decade. Concurrent with this trend, there has been an increase in the number of research studies focusing on or including women business owners in their samples. This paper reviews empirical research studies on women business owners and their ventures, classifies the studies in a framework, and summarizes trends emerging from this research. To guide future research, a new perspective on women-owned businesses is proposed and research questions, methods, and implications are discussed.",poster,cp19
Business,p2163,d9,ae62806cb193ac6473d5d13f423ae1351d833158,j264,Journal of business research,Business analytics and firm performance: The mediating role of business process performance,Abstract,fullPaper,jv264
Business,p2164,d9,eff256fa0974cf47075b38444c47c0da42f9504d,j264,Journal of business research,Skills for disruptive digital business,Abstract,fullPaper,jv264
Business,p2166,d9,aadbb11d6821a27cec403c9cb263a8800ff2e0c1,j411,Business Strategy and the Environment,"Sustainable entrepreneurship, innovation, and business models: Integrative framework and propositions for future research","This article introduces the business models for sustainability innovation (BMfSI) framework to study how business models mediate between sustainability innovations and business cases for sustainability. The BMfSI framework integrates two major perspectives (implicitly) found in the sustainable business model literature. The first is the agency perspective. It takes into consideration that some form of agency is needed, that is, “someone” who takes decisions and acts. Sustainable entrepreneurs are discussed as those agents who align their new or existing business models with sustainability innovations in order to be successful in business and to create value with and for stakeholders. The second perspective is the systems perspective, which acknowledges that business models are always embedded within sociotechnical contexts through which, for example, public policies, private financing, or stakeholder interests influence whether and how business models can be developed. The agency and systems perspectives are integrated in the so‐called business model mediation space. This theoretical notion embraces the decisions and activities pursued by sustainable entrepreneurs as they align their business models with sustainability innovations on the one hand and the influence of environmental contingencies, barriers, and stakeholders from the sociotechnical context on the other hand. The paper concludes with propositions for future research derived from the BMfSI framework.",fullPaper,jv411
Business,p2170,d9,d32d5a7e1702437e95555fdd0799c3220a58f517,c45,IEEE Symposium on Security and Privacy,The End of Business Schools? Less Success Than Meets the Eye,Presents an update on the state of business schools in the U.S. as of September 2002. Effects of business schools on careers concentrates on the Master of Business Administration (MBA) degree; Sign...,poster,cp45
Business,p2172,d9,30141ae0f045779ed2d6a932e5c51d1ece1df4a3,c59,Australian Software Engineering Conference,Buyer–Seller Relationships in Business Markets,"During the past decade, marketing managers and scholars have focused increased attention on buyer–seller relationships in business markets. This article contributes to the emerging body of knowledge in this important arena. Building from theories of relationships and empirical research across several disciplines, the authors specify six key underlying dimensions (connectors) that characterize the manner in which buyers and sellers relate and conduct relationships. Measures for these relationship connectors (information exchange, operational linkages, legal bonds, cooperation, and relationship-specific adaptations by buyers and sellers) are developed in a series of pretests. Then, on the basis of relationship profiles for more than 400 buyer–seller relationships sampled from a wide array of industries and market situations, the authors apply numerical taxonomy to develop an empirically based classification of different types of business relationships. Contrary to approaches used in much of the extant literature, taxonomic methods do not rely on an assumption that the connectors are highly intercorrelated or that they combine in some linear fashion to form a single underlying dimension. Furthermore, the research specifies antecedent market and purchase situations and shows that they affect when specific types of relationships are used. The research also shows how customer satisfaction and evaluations of supplier performance vary across different types of relationships. The results provide new insights about the nature of relationships in business markets.",poster,cp59
Business,p2174,d9,4a9a4e3d0231cfc6b71b99e070bdba73575955cf,c97,International Conference on Computational Logic,Business Models for Sustainability,"The relevance of business models for corporate performance in general and corporate sustainability in particular has been widely acknowledged in the literature while sustainable entrepreneurship research has started to explore contributions to the sustainability transformation of markets and society. Particularities of the business models of sustainable niche market pioneers have been identified in earlier research, but little is known about the dynamic role of business models for sustainable entrepreneurship processes aiming at upscaling ecologically and socially beneficial niche models or sustainability upgrading of conventional mass market players. Informed by evolutionary economics, we develop a theoretical framework to analyze co-evolutionary business model development for sustainable niche pioneers and conventional mass market players aiming at the sustainability transformation of markets. Core evolutionary processes of business model variation, selection and retention, and evolutionary pathways are identified to support structured analyses of the dynamics between business model innovation and sustainability transformation of markets.",poster,cp97
Business,p2179,d9,da29b2b97ee3ad80946926522abdeb5cc6191570,j411,Business Strategy and the Environment,Dynamic business modeling for sustainability: Exploring a system dynamics perspective to develop sustainable business models,"In the last decade, business models for sustainability have gained increasing attraction by corporate sustainability scholars with international conferences and scientific journals encouraging the development of the debate on their design, use and innovation processes. Capitalizing on the basic principles, requirements, and methodological limitations found in the literature on sustainability‐oriented business model design, this paper aims to conceptualize a dynamic business modeling for sustainability approach, which combines an adapted sustainable business model canvas and system dynamics modeling. To this end, the paper also illustrates the key operating principles of the proposed approach through an exemplary application to Patagonia's business model. Findings suggest that dynamic business modeling for sustainability may contribute to sustainable business model research and practice by introducing a systemic design tool, which frames environmental, social, and economic drivers of value generation into a dynamic business model causal feedback structure, thus overcoming methodological gaps of the extant business model design tools.",fullPaper,jv411
Business,p2180,d9,979e2624a448cb9efb0c469cb54915ba2361d60a,c32,International Conference on Smart Data Services,Good to great and the social sectors : why business thinking is not the answer : a monograph to accompany good to great : why some companies make the leap--and others don't,"We must reject the idea - well-intentioned, but dead wrong - that the primary path to greatness in the social sectors is to become ""more like a business"".' So begins this astonishingly blunt and timely manifesto by leading business thinker Jim Collins. Rejecting the belief, common among politicians, that all would be well in society if only the public sector operated more like the private sector, he sets out a radically new approach to creating successful hospitals, police forces, universities, charities, and other non-profit-making organisations. In the process he rejects many deep-rooted assumptions: that somehow it's possible to measure social bodies in purely financial terms; that they can be managed like traditional businesses; that they can be transformed simply by throwing money at them. Instead he argues for radical new attitudes and strategies, using the analytical approach and clear thinking that lie at the heart of Good to Great.",poster,cp32
Economics,p2180,d11,979e2624a448cb9efb0c469cb54915ba2361d60a,c32,International Conference on Smart Data Services,Good to great and the social sectors : why business thinking is not the answer : a monograph to accompany good to great : why some companies make the leap--and others don't,"We must reject the idea - well-intentioned, but dead wrong - that the primary path to greatness in the social sectors is to become ""more like a business"".' So begins this astonishingly blunt and timely manifesto by leading business thinker Jim Collins. Rejecting the belief, common among politicians, that all would be well in society if only the public sector operated more like the private sector, he sets out a radically new approach to creating successful hospitals, police forces, universities, charities, and other non-profit-making organisations. In the process he rejects many deep-rooted assumptions: that somehow it's possible to measure social bodies in purely financial terms; that they can be managed like traditional businesses; that they can be transformed simply by throwing money at them. Instead he argues for radical new attitudes and strategies, using the analytical approach and clear thinking that lie at the heart of Good to Great.",poster,cp32
Political Science,p2180,d15,979e2624a448cb9efb0c469cb54915ba2361d60a,c32,International Conference on Smart Data Services,Good to great and the social sectors : why business thinking is not the answer : a monograph to accompany good to great : why some companies make the leap--and others don't,"We must reject the idea - well-intentioned, but dead wrong - that the primary path to greatness in the social sectors is to become ""more like a business"".' So begins this astonishingly blunt and timely manifesto by leading business thinker Jim Collins. Rejecting the belief, common among politicians, that all would be well in society if only the public sector operated more like the private sector, he sets out a radically new approach to creating successful hospitals, police forces, universities, charities, and other non-profit-making organisations. In the process he rejects many deep-rooted assumptions: that somehow it's possible to measure social bodies in purely financial terms; that they can be managed like traditional businesses; that they can be transformed simply by throwing money at them. Instead he argues for radical new attitudes and strategies, using the analytical approach and clear thinking that lie at the heart of Good to Great.",poster,cp32
Business,p2182,d9,a5a6b25809e92e34297d02fb4effff721359f46a,c9,Big Data,The strategic fit between innovation strategies and business environment in delivering business performance,Abstract,poster,cp9
Business,p2183,d9,884536c2142990316b757a033fa8572eda81168b,c25,IEEE International Parallel and Distributed Processing Symposium,Business Dynamics,Abstract,poster,cp25
Business,p2185,d9,75dbcf3838b0be718118b32eff820f906219f37c,c102,ACM SIGMOD Conference,Business Models and Technological Innovation,Abstract,poster,cp102
Business,p2186,d9,28bc484ce2b41d0cb09b4acbd006ac4dfb8e9f2c,c45,IEEE Symposium on Security and Privacy,Managing Business Relationships,Abstract,poster,cp45
Business,p2187,d9,5baeff8324f54eee02ce4887bbb29e61a3f2ede8,j406,Journal of Business Ethics,The Effect of Environmental Corporate Social Responsibility on Environmental Performance and Business Competitiveness: The Mediation of Green Information Technology Capital,Abstract,fullPaper,jv406
Business,p2189,d9,22b4d20369a281ea3970397102a75dc798040ba7,c8,Frontiers in Education Conference,An Ontology for Strongly Sustainable Business Models,"Business is increasingly employing sustainability practices, aiming to improve environmental and social responsibility while maintaining and improving profitability. For many organizations, profit-oriented business models are a major constraint impeding progress in sustainability. A formally defined ontology, a model definition, for profit-oriented business models has been employed globally for several years. However, no equivalent ontology is available in research or practice that enables the description of strongly sustainable business models, as validated by ecological economics and derived from natural, social, and system sciences. We present a framework of strongly sustainable business model propositions and principles as findings from a transdisciplinary review of the literature. A comparative analysis was performed between the framework and the Osterwalder profit-oriented ontology for business models. We introduce an ontology that enables the description of successful strongly sustainable business models that resolves weaknesses and includes functionally necessary relationships.",poster,cp8
Business,p2193,d9,d649201a42327a504f566b2d2270a8bbabbd09fd,c9,Big Data,Riding the Waves of Culture: Understanding Cultural Diversity in Business,Abstract,poster,cp9
Business,p2194,d9,fe50184e4df8b2f318516214631970aaf020ede8,j411,Business Strategy and the Environment,Circular business models for sustainable development: A “waste is food” restorative ecosystem,"Our aim is to provide a better understanding of a business model based on circular principles. In particular, we focus on two issues that support the development of a circular business model: (a) the focal actor as orchestrator of the circular network and (b) the circular ecosystem encompassing suppliers, customers, research centers, and public authorities, in which each actor/stakeholder plays a specific role, based on effective interorganizational relationships. The research method applied is an in‐depth nested single case study of a circular project. Our results highlight an exemplar case of an ecosystemic business model in agriculture, involving different types of innovation and strong collaboration among network members, orchestrated by a focal firm. The abductive approach used led to the formulation of some research propositions and to the identification of some adoption factors and barriers to growth in circular business models.",fullPaper,jv411
Business,p2196,d9,c8fdfa7ad287fa2c8508f911d1c2f226a95b163b,c72,Workshop on Research on Enterprise Networking,The Five Stages of Small Business Growth,"Develops a model relevant to small and growing businesses that delineates five stages of firm development. These stages are: (1) existence--concerned with garnering customers and delivering the product or service contracted for; (2) survival--firms have demonstrated that they are workable business entities, but the key question becomes whether there is enough money for the firm to break even and stay in business; (3) success--here the decision facing owners is whether to exploit the company's accomplishments and expand or keep the company stable and profitable, providing a base for alternative owner activities; (4) take-off--concerned with how to make the firm grow rapidly and how to finance this growth; and (5) resource maturity--companies have the advantages of size, financial resources, and managerial talent and will be a formidable force in the market if they retain their entrepreneurial spirit. Each stage is characterized by an index of size, diversity, and complexity and described by five management factors: managerial style, organizational structure, extent of formal systems, major strategic goals, and the owner's involvement in the business. In addition, this research identifies eight factors prominent in determining firm success or failure. They include: financial, personnel, systems and business resources and the owner's goals for him/herself, operational abilities in doing important jobs, managerial ability and willingness to delegate, and strategic ability for looking to the future. Knowing which development stage the firm is in will help managers, consultants, and investors make more informed choices and prepare the company for later challenges. (SFL)",poster,cp72
Business,p2199,d9,59c8424dbf41288dd33f1cd6e07e34f376312ca9,c56,International Conference on Automated Software Engineering,Business and Society: Ethics and Stakeholder Management,Abstract,poster,cp56
Business,p2200,d9,4f1d339aad93d1ba66ef0466c1420e5ee8cc78af,c100,IEEE International Conference on Computer Vision,Differentiating Entrepreneurs from Small Business Owners: A Conceptualization,"The literature of small business and entrepreneurship is explored. It is established that, although there is an overlap between entrepreneurial firms and small business firms, they are different entities. Using the 1934 work of Schumpeter and recognizing the additions to the field of current writers, a conceptual framework is established for the differentiation of entrepreneurs from small business owners. Keywords: entrepreneurial, Small business",poster,cp100
Business,p2201,d9,15714812656ad729f6fb32b6a27a4f0d9e0c12de,c69,Neural Information Processing Systems,How should companies interact in business networks,Abstract,poster,cp69
Business,p2202,d9,8301d554184e5779444f78849aa2cb0d8d11ade0,j420,Family Business Review,Advancing Family Business Research: The Promise of Microfoundations,"Family business research has been flourishing in the past two decades (De Massis, Sharma, Chua, & Chrisman, 2012; Neubaum, 2018). This is manifest in an increasing number of family business studies published in top-tier management journals (e.g., Kotlar, Signori, De Massis, & Vismara, 2018; Neckebrouck, Schulze, & Zellweger, 2018) and an accumulating body of knowledge about the distinctive characteristics and behaviors of family enterprises and the outcomes of such behaviors. However, many of the mechanisms driving family business phenomena involve the micro-level of analysis, and family business researchers have yet to fully embrace this level of analysis in order to build more sophisticated and more robust theory. In this essay, we highlight and explain the need to apply a microfoundational lens in order to build and test theory that allows scholars to extend, enrich, and refine current knowledge on family enterprises. We also offer specific suggestions concerning how scholars can draw on microfoundations to advance family business research. While microfoundations has swept across (macro-) management research over the past decade or so (Barney & Felin, 2013; Felin & Foss, 2005; Felin, Foss, & Ployhart, 2015; Gavetti, 2005), it has so far had little impact on family business research. Our main point is that claims about “macro”-level phenomena require hypotheses about the underlying local circumstances of purposive agents that are embedded within social structures and make choices that bring about the macro-outcome—a general point that applies to family business research. In general, microfoundations are about explaining macro-level phenomena in terms of the actions and interactions of lower level entities. Thus, microfoundational explanations are reductionist and mechanism based. In social science, the micro typically (but not necessarily) involves individuals and their interactions, the macro may be, for example, firm-level outcomes, and furnishing microfoundations then means providing a theoretically informed explanation of how firm-level outcomes emerge from the characteristics, behaviors, and interactions of individuals (Felin et al., 2015). Although family business research has substantially increased in rigor over the years, much research on family business phenomena still does not specify theoretically and empirically the mechanisms of choice and action at the levels of analysis lower than that of the phenomenon itself. Ignoring the micro level is problematic because the predictions of our theory for macro-level phenomena may be wrong or incomplete (Coleman, 1990). With this essay, we encourage scholars to reflect on the importance of applying a microfoundational lens to study family business phenomena, reflect on possible ways to account for microfoundations in future research on different family business topics, and discuss a microfoundational agenda for future family business research.",fullPaper,jv420
Business,p2203,d9,2c92bfd47c2a50d8de17085f07d40b952f645e0e,c87,International Conference on Big Data Research,Crafting Business Architecture: the Antecedents of Business Model Design,"Anchored in the broad design literature, we derive four antecedents of business model design: goals, templates, stakeholder activities, and environmental constraints. These business model design antecedents are illustrated using interview data from nine new ventures in the peer-to-peer lending space. We proceed with the theoretical development to link the design antecedents to the design themes of business models and conclude with implications for business model research and entrepreneurial leaders. Copyright © 2015 Strategic Management Society.",poster,cp87
Business,p2204,d9,a4b42139084d14a7cf8a0937b67965050e2fd74b,c30,PS,Business Groups in Emerging Markets: Paragons or Parasites?,"Diversified business groups, consisting of legally independent firms operating across diverse industries, are ubiquitous in emerging markets. Groups around the world share certain attributes but also vary substantially in structure, ownership, and other dimensions. This paper proposes a business group taxonomy, which is used to formulate hypotheses and present evidence about the reasons for the formation, prevalence, and evolution of groups in different environments. In interpreting the evidence, the authors pay particular attention to two aspects neglected in much of the literature: the circumstances under which groups emerge and the historical evidence on some of the questions addressed by recent studies. They argue that business groups are responses to different economic conditions and that, from a welfare standpoint, they can sometimes be ""paragons"" and, at other times, ""parasites."" The authors conclude with an agenda for future research.",poster,cp30
Economics,p2204,d11,a4b42139084d14a7cf8a0937b67965050e2fd74b,c30,PS,Business Groups in Emerging Markets: Paragons or Parasites?,"Diversified business groups, consisting of legally independent firms operating across diverse industries, are ubiquitous in emerging markets. Groups around the world share certain attributes but also vary substantially in structure, ownership, and other dimensions. This paper proposes a business group taxonomy, which is used to formulate hypotheses and present evidence about the reasons for the formation, prevalence, and evolution of groups in different environments. In interpreting the evidence, the authors pay particular attention to two aspects neglected in much of the literature: the circumstances under which groups emerge and the historical evidence on some of the questions addressed by recent studies. They argue that business groups are responses to different economic conditions and that, from a welfare standpoint, they can sometimes be ""paragons"" and, at other times, ""parasites."" The authors conclude with an agenda for future research.",poster,cp30
Business,p2207,d9,489be1ef40bc6500c0b7c3ba65cbfa7660d1c89f,j406,Journal of Business Ethics,Business Strategy and Corporate Social Responsibility,Abstract,fullPaper,jv406
Business,p2214,d9,60c86fbeea8ab269b9c7c35e6dac1373f166fb98,j406,Journal of Business Ethics,"Exploring the Relationship Between Business Model Innovation, Corporate Sustainability, and Organisational Values within the Fashion Industry",Abstract,fullPaper,jv406
Business,p2215,d9,7f16160e7551ea5f0e689bb8af99349fba11b4a0,c108,IEEE International Conference on Multimedia and Expo,Customer perceived value: a substitute for satisfaction in business markets?,"In recent years, there has been a resurgence of interest in the value construct among both marketing researchers and practitioners. Despite a growing body of research, it is still not clear how value interacts with related marketing constructs. Researchers have called for an investigation of the interrelationship between customer satisfaction and customer value to reduce the ambiguities surrounding both concepts. Investigates whether customer value and satisfaction represent two theoretically and empirically distinct concepts. Also addresses whether value is a better predictor of behavioral outcomes than satisfaction in a business marketing context. Two alternative models are developed and empirically tested in a cross-sectional survey with purchasing managers in Germany. The first model suggests a direct impact of perceived value on the purchasing managers' intentions. In the second model, perceived value is mediated by satisfaction. This research suggests that value and satisfaction can be conceptualized and measured as two distinct, yet complementary constructs.",poster,cp108
Business,p2221,d9,7477643ab40570ea516a6ac46b403f5324cd9411,c43,European Conference on Machine Learning,Business Model Innovation – State of the Art and Future Challenges for the Field,"Business model innovation is receiving increased attention in corporate practice and research alike. We propose in this article a role‐based approach to categorize the literature and argue that the respective roles of explaining the business, running the business, and developing the business can serve as three interrelated perspectives to present an overview of the current business model innovation field and to accommodate the selected contributions of this special issue. We refer to contributions from entrepreneurship, innovation and technology management, and corporate strategy to explicate the three elaborated perspectives and to summarize the main contents of the special issue articles. We conclude by reflecting on main theoretical challenges for studies on business model innovation which stem from the uncertain boundaries of the phenomenon, and we propose some theoretical stances and analytic levels to develop future avenues for research.",poster,cp43
Business,p2222,d9,e8484ed8253f79d6a40ab7637da9a1f3125fcc44,c59,Australian Software Engineering Conference,The Future of Business Groups in Emerging Markets: Long Run Evidence from Chile,"We demonstrate variation in the extent to which firms benefit from their affiliation with Chilean business groups in the 1988-1996 period. The net benefits of unrelated diversification are positive if group diversification exceeds a threshold level, though this threshold increases with time. We find evidence of non-diversification related group benefits, which atrophy over time. We conjecture that the evolution of institutional context alters the value creating potential of business groups, though it does so slowly.",poster,cp59
Economics,p2222,d11,e8484ed8253f79d6a40ab7637da9a1f3125fcc44,c59,Australian Software Engineering Conference,The Future of Business Groups in Emerging Markets: Long Run Evidence from Chile,"We demonstrate variation in the extent to which firms benefit from their affiliation with Chilean business groups in the 1988-1996 period. The net benefits of unrelated diversification are positive if group diversification exceeds a threshold level, though this threshold increases with time. We find evidence of non-diversification related group benefits, which atrophy over time. We conjecture that the evolution of institutional context alters the value creating potential of business groups, though it does so slowly.",poster,cp59
Business,p2225,d9,95d87821739a62a06ae054eca5b77b0e0e81e9de,c7,International Symposium on Intelligent Data Analysis,Business to business digital content marketing: Marketers’ perceptions of best practice,"Purpose – This paper aims to draw attention to the emerging phenomenon of business to business (B2B) digital content marketing, offers a range of insights and reflections on good practice and contributes to theoretical understanding of the role of digital content in marketing. B2B digital content marketing is an inbound marketing technique and hence offers a solution to the declining effectiveness of traditional interruptive marketing techniques. Design/methodology/approach – Semi-structured interviews were conducted with 15 key informants involved in B2B content marketing in the USA, UK and France, in five industry sectors. Findings – B2B digital content marketing is an inbound marketing technique, effected through web page, social media and value-add content, and is perceived to be a useful tool for achieving and sustaining trusted brand status. Creating content that is valuable to B2B audiences requires brands to take a “publishing” approach, which involves developing an understanding of the audience’s...",poster,cp7
Business,p2229,d9,68f963eff2cf78b9f3197e88c34f4bf150b4df4b,j422,Journal of African Business,Challenges of Doing Business in Africa: A Systematic Review,"ABSTRACT This paper provides a systematic review of challenges to doing business in Africa. It complements the extant literature by answering two critical questions: what are the linkages between the ease of doing business and economic development; and what are the challenges to doing business in Africa? In providing answers to these questions, the nexus between the ease of doing business and economic development is discussed in six main strands, namely: wealth creation and sharing; opportunities of employment; balanced regional and economic development; Gross Domestic Product (GDP) and GDP per capita; standards of living and exports. Moreover, challenges to doing business are articulated along the following lines: (i) issues related to the cost of starting a business and doing business; (ii) shortage of energy and electricity; (iii) lack of access to finance; and (v) high taxes and low cross-border trade.",fullPaper,jv422
Business,p2230,d9,78da8dd470c057dd3a786ec66ea4a56494616371,c47,EUROMICRO Conference on Software Engineering and Advanced Applications,"International Business Cycles: World, Region, and Country-Specific Factors","The paper investigates the common dynamic properties of business-cycle fluctuations across countries, regions, and the world. We employ a Bayesian dynamic latent factor model to estimate common components in macroeconomic aggregates (output, consumption, and investment) in a 60-country sample covering seven regions of the world. The results indicate that a common world factor is an important source of volatility for aggregates in most countries, providing evidence for a world business cycle. We find that region-specific factors play only a minor role in explaining fluctuations in economic activity. We also document similarities and differences across regions, countries, and aggregates. (JEL F41, E32, C11, C32)",poster,cp47
Economics,p2230,d11,78da8dd470c057dd3a786ec66ea4a56494616371,c47,EUROMICRO Conference on Software Engineering and Advanced Applications,"International Business Cycles: World, Region, and Country-Specific Factors","The paper investigates the common dynamic properties of business-cycle fluctuations across countries, regions, and the world. We employ a Bayesian dynamic latent factor model to estimate common components in macroeconomic aggregates (output, consumption, and investment) in a 60-country sample covering seven regions of the world. The results indicate that a common world factor is an important source of volatility for aggregates in most countries, providing evidence for a world business cycle. We find that region-specific factors play only a minor role in explaining fluctuations in economic activity. We also document similarities and differences across regions, countries, and aggregates. (JEL F41, E32, C11, C32)",poster,cp47
Business,p2237,d9,b2ad59a653e1e24e55b75cde19eb977abf33a51e,c62,International Conference on Advanced Data and Information Engineering,BUSINESS MODEL INNOVATION: TOWARDS AN INTEGRATED FUTURE RESEARCH AGENDA,"Motivated by the high ubiquity of the term ""business models"" and its increasing proliferation in terms of the transition from a measure to commercialise innovations to the subject of innovations, this paper provides a systematic review of extant academic literature on business model innovation. The particular characteristics of business model innovation are discussed and three distinct research streams addressing prerequisites, process and elements, and effects of business model innovation are identified. A tentative theoretical framework emphasising the need to distinguish between developing and innovating business models as well as to apply an entrepreneurial perspective for further research on business model innovation is proposed. An integrated research agenda emphasising the need to further enhance our understanding of the process and elements of business model innovation as well as its enablers and effects in anticipation and response to increasing environmental volatility is suggested.",poster,cp62
Business,p2245,d9,201c45f3f6fd5d9c390396aa5a16f5a50a4b10af,c74,International Conference on Computational Linguistics,Real Business Cycles in a Small Open Economy,"This paper analyzes a real-business-cycle model of a small open economy. The model is parameterized, calibrated, and simulated to explore its ability to rationalize the observed pattern of postwar Canadian business fluctuations. The results show that the model mimics many of the stylized facts using moderate adjustment costs and minimal variability and persistence in the technological disturbances. In particular, the model is consistent with the observed positive correlation between savings and investment, even though financial capital is perfectly mobile, and with countercyclical fluctuations in external trade. Copyright 1991 by American Economic Association.",poster,cp74
Economics,p2245,d11,201c45f3f6fd5d9c390396aa5a16f5a50a4b10af,c74,International Conference on Computational Linguistics,Real Business Cycles in a Small Open Economy,"This paper analyzes a real-business-cycle model of a small open economy. The model is parameterized, calibrated, and simulated to explore its ability to rationalize the observed pattern of postwar Canadian business fluctuations. The results show that the model mimics many of the stylized facts using moderate adjustment costs and minimal variability and persistence in the technological disturbances. In particular, the model is consistent with the observed positive correlation between savings and investment, even though financial capital is perfectly mobile, and with countercyclical fluctuations in external trade. Copyright 1991 by American Economic Association.",poster,cp74
Business,p2254,d9,9827a4b34eb652d4c1e995139c252834c2727493,c105,International Conference on Automatic Face and Gesture Recognition,"Modelling the relationship between perceived value, satisfaction and repurchase intentions in a business‐to‐business, services context: an empirical examination","Examines the relationship between four key post‐purchase constructs: perceived performance, satisfaction, perceived value, and repurchase intentions, in a causal path framework in an empirical study of business‐to‐business professional services. Attempts to disaggregate performance into its component multiple dimensions, and assess the individual impact of each on post‐purchase evaluation processes. Shows that the effect of perceived value on repurchase intentions is completely mediated through satisfaction. Confirms six performance dimensions, each having a significant impact on both value and satisfaction and adds new insight to our understanding of the respective roles of perceived value, satisfaction and post‐purchase intentions.",poster,cp105
Business,p2255,d9,c8a5b5fdf0da179822d15b57259cdd1cb5cf9058,c105,International Conference on Automatic Face and Gesture Recognition,It-enabled business transformation: from automation to business scope redefinition,Abstract,poster,cp105
Business,p2258,d9,8dc10de314ea0608db66daacbb865ca5ad45366f,c49,ACM/SIGCOMM Internet Measurement Conference,Marketing and business performance,Abstract,poster,cp49
Business,p2264,d9,5ef31b3692e3eb332e4dd99f702daca182e25cb3,c41,IEEE International Conference on Data Engineering,Value-Based Differentiation in Business Relationships: Gaining and Sustaining Key Supplier Status,"Many business customers today consolidate their supply bases and implement preferred supplier programs. Consequently, vendors increasingly face the alternative of either gaining a key supplier status with their customers or being pushed into the role of a backup supplier. As product and price become less important differentiators, suppliers of routinely purchased products search for new ways to differentiate themselves in a buyer–seller relationship. This research investigates avenues for differentiation through value creation in business-to-business relationships. The results suggest that relationship benefits display a stronger potential for differentiation in key supplier relationships than cost considerations. The authors identify service support and personal interaction as core differentiators, followed by a supplier's know-how and its ability to improve a customer's time to market. Product quality and delivery performance, along with acquisition costs and operation costs, display a moderate potential to help a firm gain and maintain key supplier status. Finally, price shows the weakest potential for differentiation.",poster,cp41
Business,p2266,d9,259a450c374b03309f0a9fce2cbd910fa8699ae0,c35,"International Conference on Internet of Things, Big Data and Security","Business Unit Strategy, Managerial Characteristics, and Business Unit Effectiveness at Strategy Implementation","Data from 58 strategic business units (SBUs) reveal that greater marketing/sales experience, greater willingness to take risk, and greater tolerance for ambiguity on the part of the SBU general manager contribute to effectiveness in the case of “build” SBUs but hamper it in the case of “harvest” SBUs.",poster,cp35
Business,p2268,d9,bb9765a4fd955f247dc150424a47c7be33833511,c43,European Conference on Machine Learning,International Business: Competing in the Global Marketplace,"Part One-Introduction and Overview Chapter 1: Globalization Case: Who Makes the Apple iPhone? Part Two-Country Differences Chapter 2: National Differences in Political Economy Chapter 3: Political Economy and Economic Development Chapter 4: Differences in Culture Chapter 5: Ethics in International Business Case: Siemens Bribery Scandal Case: Disaster in Bangladesh Case: Knights Apparel Case: Japan's Economic Malaise Case: Indonesia: The Next Asian Giant? Part Three-The Global Trade and Investment Environment Chapter 6: International Trade Theory Chapter 7: The Political Economy of International Trade Chapter 8: Foreign Direct Investment Chapter 9: Regional Economic Integration Case: Legal Outsourcing Case: The Global Financial Crisis and Protectionism Case: NAFTA and Mexican Trucking Case: The Rise of the Indian Automobile Industry Case: Logitech Part Four-The Global Monetary System Chapter 10: The Foreign Exchange Market Chapter 11: The International Monetary System Chapter 12: The Global Capital Market Case: South Korean Currency Crisis Case: Russian Ruble Crisis Case: Caterpillar: Competing in a World of Fluctuating Currencies Part Five-The Strategy and Structure of International Business Chapter 13: The Strategy of International Business Chapter 14: The Organization of International Business Chapter 15: Entry Strategy and Strategic Alliances Case: The Evolving Strategy of IBM Case: IKEA in 2013 Case: General Electric's Joint Ventures Case: The Globalization of Starbucks Case: Coca-Cola's Strategy Part Six-Business Operations Chapter 16: Exporting, Importing, and Countertrade Chapter 17: Production, Outsourcing, and Logistics Chapter 18: Global Marketing and R&D Chapter 19: Global Human Resource Management Chapter 20: Accounting and Finance in the International Business Case: Brazil's Gol Airlines Case: Staffing Policy at AstraZeneca",poster,cp43
Economics,p2268,d11,bb9765a4fd955f247dc150424a47c7be33833511,c43,European Conference on Machine Learning,International Business: Competing in the Global Marketplace,"Part One-Introduction and Overview Chapter 1: Globalization Case: Who Makes the Apple iPhone? Part Two-Country Differences Chapter 2: National Differences in Political Economy Chapter 3: Political Economy and Economic Development Chapter 4: Differences in Culture Chapter 5: Ethics in International Business Case: Siemens Bribery Scandal Case: Disaster in Bangladesh Case: Knights Apparel Case: Japan's Economic Malaise Case: Indonesia: The Next Asian Giant? Part Three-The Global Trade and Investment Environment Chapter 6: International Trade Theory Chapter 7: The Political Economy of International Trade Chapter 8: Foreign Direct Investment Chapter 9: Regional Economic Integration Case: Legal Outsourcing Case: The Global Financial Crisis and Protectionism Case: NAFTA and Mexican Trucking Case: The Rise of the Indian Automobile Industry Case: Logitech Part Four-The Global Monetary System Chapter 10: The Foreign Exchange Market Chapter 11: The International Monetary System Chapter 12: The Global Capital Market Case: South Korean Currency Crisis Case: Russian Ruble Crisis Case: Caterpillar: Competing in a World of Fluctuating Currencies Part Five-The Strategy and Structure of International Business Chapter 13: The Strategy of International Business Chapter 14: The Organization of International Business Chapter 15: Entry Strategy and Strategic Alliances Case: The Evolving Strategy of IBM Case: IKEA in 2013 Case: General Electric's Joint Ventures Case: The Globalization of Starbucks Case: Coca-Cola's Strategy Part Six-Business Operations Chapter 16: Exporting, Importing, and Countertrade Chapter 17: Production, Outsourcing, and Logistics Chapter 18: Global Marketing and R&D Chapter 19: Global Human Resource Management Chapter 20: Accounting and Finance in the International Business Case: Brazil's Gol Airlines Case: Staffing Policy at AstraZeneca",poster,cp43
Business,p2269,d9,ba82bdbf76c6e31dcfac8a4e1a71faecfbce5c16,c14,Hawaii International Conference on System Sciences,Life After Business Failure,"Where there is uncertainty, there is bound to be failure. It is not surprising, therefore, that many new ventures fail. What happens to entrepreneurs when their business fails? People hear of highly successful entrepreneurs extolling the virtues of failure as a valuable teacher. Yet the aftermath of failure is often fraught with psychological, social, and financial turmoil. The purpose of this article is to review research on life after business failure for entrepreneurs, from the immediate aftermath through to recovery and re-emergence. First, the authors examine the financial, social, and psychological costs of failure, highlighting factors that may influence the magnitude of these costs (including individual responses to managing these costs). Second, they review research that explains how entrepreneurs make sense of and learn from failure. Finally, the authors present research on the outcomes of business failure, including recovery as well as cognitive and behavioral outcomes. They develop a schema to organize extant work and use this as a platform for developing an agenda for future research.",poster,cp14
Business,p2272,d9,5cb4764ea0914fcb3aa5cdc115cdba1cb38d6249,c13,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,Corporate citizenship: Cultural antecedents and business benefits,"The article explores the nature of corporate citizenship and its relevance for marketing practitioners and academic researchers. Specifically, a conceptualization and operationalization of corporate citizenship are first proposed. Then, an empirical investigation conducted in two independent samples examines whether components of an organization’s culture affect the level of commitment to corporate citizenship and whether corporate citizenship is conducive to business benefits. Survey results suggest that market-oriented cultures as well as humanistic cultures lead to proactive corporate citizenship, which in turn is associated with improved levels of employee commitment, customer loyalty, and business performance. The results point to corporate citizenship as a potentially fruitful business practice both in terms of internal and external marketing.",poster,cp13
Business,p2276,d9,f2e4c2b221c26916d83ebc89edb2e48c44cc9ddd,c32,International Conference on Smart Data Services,"Marketing, Business Processes, and Shareholder Value: An Organizationally Embedded View of Marketing Activities and the Discipline of Marketing","The authors develop a framework for understanding the integration of marketing with business processes and shareholder value. The framework redefines marketing phenomena as embedded in three core business processes that generate value for customers—product development management, supply chain management, and customer relationship management—which in turn creates shareholder value. Such a conceptualization of marketing has the potential to introduce dramatic shifts in the scope, content, and influence of marketing in the organization. The authors highlight the implications of an organizationally embedded view of marketing for the future of marketing theory and practice.",poster,cp32
Business,p2282,d9,9aec5150d7e5a4d3f72494c88f97f9371fa548d9,c87,International Conference on Big Data Research,Scanning the business environment,Abstract,poster,cp87
Business,p2287,d9,ba24e2a88c2464e3270a9d916fa7b965aa072075,c110,Biometrics and Identity Management,The Effects of business and Political Ties on Firm Performance: Evidence from China.,"Abstract Despite increasing attention to the role of social ties in emerging economies, few studies have explicitly distinguished the differential roles of business versus political ties. Drawing on relational governance and institutional theories, this study offers a contingent view of business and political ties in China. The findings from a survey of 241 Chinese firms indicate that business ties have a stronger positive effect on performance than political ties, and both effects depend on institutional and market environments. Business ties are more beneficial when legal enforcement is inefficient and technology is changing rapidly, whereas political ties lead to greater performance when general government support is weak and technological turbulence is low. These findings indicate that firms operating in China should be cautious in their use of business and political ties and adapt their tie utilization to changing institutional and market environments.",poster,cp110
Business,p2288,d9,ceec8168c5dd00871086adbfc5cbcc9208bc0f16,c0,International Conference on Machine Learning,Relating Porter's Business Strategies to Environment and Structure: Analysis and Performance Implications,This study investigated the relationships of Porter's business strategies to the structures and environments of undiversified firms. It was shown that strategies must be matched with complementary ...,poster,cp0
Business,p2289,d9,e8f86c204a969da6eeee488a8e24c10bf609be05,c64,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",A Systematic Review of Business Incubation Research,Abstract,poster,cp64
Business,p2292,d9,cf84db0ef03bad50ff3c617acdaab5131679dce8,c36,International Conference on Information Technology Based Higher Education and Training,Business Research Method,Abstract,poster,cp36
Business,p2296,d9,32c740cf2355251922c8bc4e063b650e3661bdb2,c73,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,Business Relationship Learning and Commitment in the Internationalization Process,Abstract,poster,cp73
Business,p2298,d9,6612e813d65b0ef854a09145faa121bbaa28ff1a,c24,International Conference on Data Technologies and Applications,Business Model Innovation: What Can the Ambidexterity Literature Teach US?,"One of the issues addressed in the growing literature on business model innovation is how to compete with two business models simultaneously. Unfortunately, this body of work lacks a theoretical foundation. I propose that the challenge of managing two different and conflicting business models simultaneously can be framed as an ambidexterity challenge. This implies that ideas and theoretical concepts from the ambidexterity literature can be used to explore issues pertinent to the business model literature. I apply this idea to explore four specific areas where the ambidexterity literature could guide research on the challenge of managing two business models simultaneously and identify several insights that can guide future research on business model innovation.",poster,cp24
Business,p2304,d9,0fd54530dabbb01afa0bdba5799378ad5a3e260d,c92,International Symposium on Computer Architecture,"Service quality, relationship satisfaction, trust, commitment and business‐to‐business loyalty","– The purpose of this study is to establish a theoretical basis for evaluating a strategic increase in customers' perceptions of service/product quality – specifically in terms of an increase in relationship quality and customer loyalty in a B2B environment – and to test this theoretical basis empirically., – Drawing on the relationship‐marketing literature, the authors empirically test a model of business loyalty in a sample of 234 advertising agencies' clients., – Using the Gronroos conceptualisation, a clear pattern of service‐quality dimensions is established and several important findings are reported – including empirical verification of the mediating role of overall relationship satisfaction in the formation of loyalty attributes. The effects of trust and commitment are also verified., – Studies that model attitudinal as well as behavioural relationship outcomes have strong precedence in the relationship marketing area. Although in this study the “intentions” approach is followed rather than a behavioural one, the measurement of the real behaviour of industrial clients proves to be very difficult from a practical point‐of‐view., – In this service continuum, managers need to clearly define relationship development strategies, service provision policies and develop homogeneous service provision. Towards this direction, it is essential that firms communicate the service and product quality standards to partners so that differences in service provision can be avoided., – The study integrates the concepts of service/product quality, relationship satisfaction, trust, and commitment in a business‐loyalty model, demonstrating the benefits of investing in relationships based on trust and commitment.",poster,cp92
Business,p2315,d9,4d8aa3df4af1dc96177924c0f8d6538e6ccaed77,c60,Network and Distributed System Security Symposium,The Effects of Entrepreneurial Proclivity and Market Orientation on Business Performance,"The recent literature suggests a potential tension between market orientation and entrepreneurial proclivity in achieving superior business performance. This is unsettling for marketers, because it could mean that being market oriented is detrimental to a firm that is also trying to be entrepreneurial and successful. To examine this unnerving potential, the authors investigate structural influences (both direct and indirect) of entrepreneurial proclivity and market orientation on business performance. The results indicate that entrepreneurial proclivity has not only a positive and direct relationship on market orientation but also an indirect and positive effect on market orientation through the reduction of departmentalization. The results also suggest that entrepreneurial proclivity's performance influence is positive when mediated by market orientation but negative or nonsignificant when not mediated by market orientation. The authors also provide a discussion and future research implications.",poster,cp60
Business,p2316,d9,e8e299f936fb24ddf75653ff7eb3638d0c7333d9,c24,International Conference on Data Technologies and Applications,Why Can’t a Family Business Be More Like a Nonfamily Business?,"The authors survey arguments that family firms should behave more like nonfamily firms and “professionalize.” Despite the apparent advantages of this transition, many family firms fail to do so or do so only partially. The authors reflect on why this might be so, and the range of possible modes of professionalization. They derive six ideal types: (a) minimally professional family firms; (b) wealth dispensing, private family firms; (c) entrepreneurially operated family firms; (d) entrepreneurial family business groups; (e) pseudoprofessional, public family firms; and (f) hybrid professional family firms. The authors conclude with suggestions for further research that is attentive to such variation.",poster,cp24
Business,p2322,d9,c35dee0109cc395ce85c16dbc170f0fdbc0f2bc7,c72,Workshop on Research on Enterprise Networking,Building an integrative model of small business growth,Abstract,poster,cp72
Business,p2324,d9,7a4696e4084a3a1b03ba486764eea0a0fe643d6d,c2,International Conference on Software Engineering,Investigating the Impact of Firm Size on Small Business Social Responsibility: A Critical Review,Abstract,poster,cp2
Business,p2325,d9,87c0d7c68b3832e1b39b869ab39c82f3ee57bc1c,j118,Review of Economics and Statistics,Teaching Entrepreneurship: Impact of Business Training on Microfinance Clients and Institutions,"Abstract Most academic and development policy discussions about microentrepreneurs focus on credit constraints and assume that subject to those constraints, the entrepreneurs manage their business optimally. Yet the self-employed poor rarely have any formal training in business skills. A growing number of microfinance organizations are attempting to build the human capital of microentrepreneurs in order to improve the livelihood of their clients and help further their mission of poverty alleviation. Using a randomized control trial, we measure the marginal impact of adding business training to a Peruvian group lending program for female microentrepreneurs. Treatment groups received thirty- to sixty-minute entrepreneurship training sessions during their normal weekly or monthly banking meeting over a period of one to two years. Control groups remained as they were before, meeting at the same frequency but solely for making loan and savings payments. We find little or no evidence of changes in key outcomes such as business revenue, profits, or employment. We nevertheless observed business knowledge improvements and increased client retention rates for the microfinance institution.",fullPaper,jv118
Economics,p2325,d11,87c0d7c68b3832e1b39b869ab39c82f3ee57bc1c,j118,Review of Economics and Statistics,Teaching Entrepreneurship: Impact of Business Training on Microfinance Clients and Institutions,"Abstract Most academic and development policy discussions about microentrepreneurs focus on credit constraints and assume that subject to those constraints, the entrepreneurs manage their business optimally. Yet the self-employed poor rarely have any formal training in business skills. A growing number of microfinance organizations are attempting to build the human capital of microentrepreneurs in order to improve the livelihood of their clients and help further their mission of poverty alleviation. Using a randomized control trial, we measure the marginal impact of adding business training to a Peruvian group lending program for female microentrepreneurs. Treatment groups received thirty- to sixty-minute entrepreneurship training sessions during their normal weekly or monthly banking meeting over a period of one to two years. Control groups remained as they were before, meeting at the same frequency but solely for making loan and savings payments. We find little or no evidence of changes in key outcomes such as business revenue, profits, or employment. We nevertheless observed business knowledge improvements and increased client retention rates for the microfinance institution.",fullPaper,jv118
Business,p2330,d9,b1c604ccfbcf28ec0faf22afe8f24f5374fb479e,c22,Grid Computing Environments,Intellectual Capital: Navigating in the New Business Landscape,"From the Publisher: 
Providing an organizational system that helps managers maximize the flow of intellectual capital in their companies, Intellectual Capital presents the insights gained by leading experts from the practice, research, and consulting side of business management. Starting with a definition of intellectual capital and its main components, the book offers a step-by-step ""process model"" to rooting out corporate inefficiency at all levels. Illustrated with vivid case studies, the book also covers the latest thinking and practices of the ""second generation"" of intellectual capital practices, from consolidating intellectual capital measures into a single ""IC Index"" to linking them with shareholder value creation systems.",poster,cp22
Business,p2331,d9,c96a5a0bcb2e7b1d7301bb55e898b41aa4c89dea,c118,International Conference on Image Analysis and Processing,Innovations in Retail Business Models,Abstract,poster,cp118
Business,p2332,d9,af3e84d062506e82f8b1bdc600fd7ab49d0d868b,c66,International Conference on Web and Social Media,"Internationalization, innovation and entrepreneurship: business models for new technology-based firms",Abstract,poster,cp66
Business,p2335,d9,11cacc926ce547dc71c98b724ffd1624acd53e67,c19,International Conference on Conceptual Structures,Interfirm Adaptation in Business Relationships,"On the basis of social exchange theory and the resource-dependence model, a structural model of interfirm adaptation is formulated. The model accounts for mutual adaptation as a consequence of trust-building as well as for unilateral adaptation due to imbalanced dependence between the parties. The view that interfirm adaptations are elements in a social exchange process is supported.",poster,cp19
Business,p2337,d9,89d6bc098a59b4c627ffc2710210dc1a9b4b384b,j419,Business Ethics Quarterly,Business Ethics and Stakeholder Analysis,"Much has been written about stakeholder analysis as a process by which to introduce ethical values into management decision-making. This paper takes a critical look at the assumptions behind this idea, in an effort to understand better the meaning of ethical management decisions. A distinction is made between stakeholder analysis and stakeholder synthesis. The two most natural kinds of stakeholder synthesis are then defined and discussed: strategic and multi-fiduciary. Paradoxically, the former appears to yield business without ethics and the latter appears to yield ethics without business. The paper concludes by suggesting that a third approach to stakeholder thinking needs to be developed, one that avoids the paradox just mentioned and that clarifies for managers (and directors) the legitimate role of ethical considerations in decision-making. So we must think through what management should be accountable for; and how and through whom its accountability can be discharged. The stockholders’ interest, both short- and long-term, is one of the areas. But it is only one.Peter Drucker, 1988Harvard Business Review",fullPaper,jv419
Business,p2338,d9,fbc4afa09cbcff0026040983c738584326312d32,c80,International Symposium on Software Engineering for Adaptive and Self-Managing Systems,Strategic Management of the Family Business: Past Research and Future Challenges,"This article reviews the literature on family business from a strategic management perspective. In general, this literature is dominated by descriptive articles that typically focus on family relationships. However, the literature does not usually address how these relationships affect the performance of a family business. Taking a strategic management perspective, we outline a new set of objectives for family-business research. We also identify some of the key issues and gaps that should be explored in future studies if research is to contribute to improving the management practices and performance of family firms.",poster,cp80
Business,p2341,d9,779d7c2b423589106f8e1aed5816a2a6435f1f63,c39,Online World Conference on Soft Computing in Industrial Applications,Can Business Afford to Ignore Social Responsibilities?,"Business for business' sake? Or must businessmen act as ""social godfathers?"" This article answers these questions and suggests ways that social responsibilities can be appraised objectively and met squarely.",poster,cp39
Business,p2344,d9,76d6397414e8d37e6aa73440c3dce008d0c7aab2,c73,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,Business Models for Internet-Based E-Commerce: An Anatomy,"The success of Internet-based businesses in the business-to-customer segment in recent years has been impressive. It is widely projected that the business-to-business segment is poised for a spectacular growth as well. However, a consistent definition and a framework for a business model for Internet-based business is still non-existent. This article proposes a three-dimensional framework for defining a business model and applies it to the emerging market structure. It also identifies certain factors that can guide organizations in their choice of an appropriate business model.",poster,cp73
Economics,p2344,d11,76d6397414e8d37e6aa73440c3dce008d0c7aab2,c73,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,Business Models for Internet-Based E-Commerce: An Anatomy,"The success of Internet-based businesses in the business-to-customer segment in recent years has been impressive. It is widely projected that the business-to-business segment is poised for a spectacular growth as well. However, a consistent definition and a framework for a business model for Internet-based business is still non-existent. This article proposes a three-dimensional framework for defining a business model and applies it to the emerging market structure. It also identifies certain factors that can guide organizations in their choice of an appropriate business model.",poster,cp73
Business,p2349,d9,3f47391a5ac2c85ae00d4a2057fe2a71f562b972,c18,International Conference on Exploring Services Science,Business Groups in Emerging Economies: A Resource-Based View,"Business groups in emerging economies result when entrepreneurs and firms accumulate the capability for repeated industry entry. Such a capability, however, can be maintained as a valuable, rare, a...",poster,cp18
Business,p2354,d9,88cc010afdd95ab67d732357e930dfb7a09a2b17,c109,Computer Vision and Pattern Recognition,Business Modeling With UML: Business Patterns at Work,"From the Publisher: 
Eriksson and Magnus Penker now provide guidance on how to use UML to model your business systems. In this book, key business modeling concepts are presented, including how to define Business Rules with UML's Object Constraint Language (OCL) and how to use business models with use cases. The authors then provide 26 valuable Business Patterns along with an e-business case study that utilizes the techniques and patterns discussed in the book.",poster,cp109
Business,p2355,d9,9f95ac6766e91be8635bdab0ddf29e8646722911,c1,International Conference on Human Factors in Computing Systems,Economic Performance of Group-Affiliated Companies in Korea: Intragroup Resource Sharing and Internal Business Transactions,This study examined the economic performance of the firms associated with Korean business groups by explicitly addressing groupwide resource sharing and internal business transactions. The results ...,poster,cp1
Business,p2361,d9,a95f23a31a837ca5d18199dc3d0ea9cdd6397710,c72,Workshop on Research on Enterprise Networking,Business Research Methods (9th edition),"It is possible to obtain Business Research Methods 4th Edition at our internet site without registration and without any charge. Download free open textbooks at the open textbook library for the classes Business Research Methods 4th Edition. On the web acquire for Publications Business Research Methods 4th Edition: amazon offers free kindle book section from where you can quickly get free ebooks. Books can be found in many forms, and you may also check out rankings and reviews from different users, one of typically the most popular Business Research Methods 4th Edition Download PDF. Search here for bestsellers, beloved classics, and more including new top-rated concept Business Research Methods 4th Edition Download PDF. Do visit this website for Business Research Methods 4th Edition Download PDF! Trust I really could be of support to you! Give publications away. Get Business Research Methods 4th Edition you want. No annoying ads, no download restricts, relish it and do not overlook to save and share Business Research Methods 4th Edition Download PDF. The sites is definitely an open, editable library directory to see free Business Research Methods 4th Edition Download PDF online. The free Business Research Methods 4th Edition Download PDF download pdf without enrollment site has a enormous variety of new fiction, non-fiction, and even audiobooks at your fingertips, atlanta divorce attorneys type you could hope for. Many thanks entirely significantly for getting Business Research Methods 4th Edition Download PDF. Probably you have information that folks have experienced numerous period for their favorite books considering that Business Research Methods 4th Edition, but conclusion getting devote harmful downloads. The following, we've a countless book Business Research Methods 4th Edition Download PDF and collections to check on out. We moreover purchase variant types and more over type of books to browse. The suitable guide, fiction, record, novel, medical study, as properly as numerous more kinds of publications are quickly useful here. Yes, researching an e-book Business Research Methods 4th Edition Download PDF could add your shut connections listings. That is just one of the solutions for you to be successful. As recognized, job doesn't recommend that you've amazing points. Comprehending as without difficulty as a guarantee even more than the others can pay for each success. close to, the information as skillfully whilst the keenness with this Business Research Methods 4th Edition Download PDF may be studied as with ease as picked to act. Nevertheless below, subsequent for you visit this web site, it will be therefore decided simple to get as skillfully as obtain information Business Research Methods 4th Edition Download PDF. Yes, researching an book Business Research Methods 4th Edition can increase your shut associations listings. That is one among the answers for you to be successful. As recognized, achievement does not claim that you have amazing points.",poster,cp72
Business,p2366,d9,9b803466c57f290d5ff443b5cccfc39d97d72f58,c2,International Conference on Software Engineering,Relationship value and relationship quality: Broadening the nomological network of business‐to‐business relationships,"Purpose – Established models of buyer-seller relationships do not reflect managerial emphasis on supplier performance evaluation when modelling business relationships. Proposes that relationship value should be included as a key constituent in such models. Aims to explore the construct's links with key constituents of relationship quality, i.e. commitment, satisfaction, and trust. Design/methodology/approach – A two-stage research design was used. First, depth-interviews were conducted with ten senior-level purchasing managers in US manufacturing companies. Second, data were gathered in a nation-wide mail survey among 400 purchasing professionals. Findings – The findings suggest that relationship value is an antecedent to relationship quality and behavioural outcomes in the nomological network of relationship marketing. Value displays a stronger impact on satisfaction than on commitment and trust. Value also directly impacts a customer's intention to expand business with a supplier. In turn, its impact on the propensity to leave a relationship is mediated by relationship quality. Contrary to previous research, trust does not appear in this study as an antecedent of behavioural outcomes, but as a mediator of the satisfaction-commitment link. Research limitations/implications – Confirms the role of value as a key relationship building-block. Researchers should integrate this cognitive performance-based construct in models of business relationships. Limitations and research directions refer to the sampling procedure, the need to include the supplier's value perceptions, the possibility of conducting longitudinal research, and the opportunity to assess additional moderating variables. Practical implications – When the goal is to increase business with an existing customer, managers should focus on relationship value. In turn, when managers are concerned with the risk of customers leaving a relationship, they should focus on relationship quality. Trust appears as an important ingredient in stabilising existing business relationships. Originality/value – Stresses the pivotal role of relationship value in marketing. Contributes to a better fit between relationship marketing models and managerial practice in business markets.",poster,cp2
Business,p2371,d9,65b95a441efff0e099b6b8837a3ddb294ec8cc9a,c74,International Conference on Computational Linguistics,"Business process improvement : the breakthrough strategy for total quality, productivity, and competitiveness","Why Focus on Business Processes? Setting the Stage for Business Process Improvement.Organizing for Process Improvement.Flowcharting: Drawing a Process Picture.Understanding the Process Characteristics.Streamlining the Process.Measurements, Feedback, and Action (Load, Aim, and Fire).Process Qualification.Benchmarking Process.The Beginning.Appendix: Interview Guidelines.",poster,cp74
Business,p2372,d9,96d204033af987ff494c6aa1ac18b09c54c4e117,c21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,Internet Business Models and Strategies: Text and Cases,"From the Publisher: 
Despite the Internet's phenomenal impact on business and its reach across all sectors,no model has emerged for thoughtfully valuing companies' Internet efforts. In addition,strategies for effectively competing in this environment are just beginning to materialize. This book addresses both of these critical aspects of the Internet and offers business models and strategies for better understanding this important phenomenon. In the words of the authors,""The framework (presented here) allows users of the text to make more informed theory-based arguments as to how successful an Internet-based firm is likely to be,how much it might be worth,and the relative merits of formulating and implementing an Internet strategy for an established firm. """,poster,cp21
Business,p2380,d9,3d064e3f5c65855064fba7092d49e76530b33013,c75,International Conference on Predictive Models in Software Engineering,Factors Affecting Business Success of Small & Medium Enterprises (SMEs) in Thailand,"This study attempted to identify factors that are affecting business success of small and medium enterprises (SMEs) in Thailand. The intention of this study is to provide the understanding on how people should start their business by looking at all the factors affecting business success hence help to reduce the risk of failure and increase chances of success. The study examined eight factors that influence the SMEs business success. These factors are: SMEs characteristic, management and know-how, products and services, Customer and Market, the way of doing business and cooperation, resources and finance, Strategy, and external environment. The theoretical framework has been drawn out and questionnaire was designed based on the factors chosen. Eight hypotheses were developed to find out factors that are affecting Business Success of SMEs in Thailand. The entire hypotheses were successfully tested with SPSS and five hypotheses were accepted. The regression analysis result shown that the most significant factors affecting business success of SMEs in Thailand were SMEs characteristics, customer and market, the way of doing business, resources and finance, and external environment.",poster,cp75
Business,p2383,d9,ee1f42658d133b12dd93dfe177007790c12ff306,c91,International Symposium on High-Performance Computer Architecture,"The Effect of Business Regulations on Nascent and Young Business 
Entrepreneurship",Abstract,poster,cp91
Economics,p2383,d11,ee1f42658d133b12dd93dfe177007790c12ff306,c91,International Symposium on High-Performance Computer Architecture,"The Effect of Business Regulations on Nascent and Young Business 
Entrepreneurship",Abstract,poster,cp91
Business,p2384,d9,be42f9c8be8509f138537286c9abbeee6389fd18,c61,"International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems",Exploring the Phenomenon of Customers' Desired Value Change in a Business-to-Business Context,"Increasingly, organizations are pushed to adopt customer value strategies in order to grow profits and ensure long-term survival. Yet little is known about the dynamic nature of how customers perceive value from suppliers. The authors present findings from a grounded theory study conducted in a business-to-business context that sheds light on the nature of customers' desired value change and related contextual conditions. The authors discover that the phenomenon of customers' desired value change typically occurs in an emotional context, as managers try to cope with feelings of tension. The phenomenon extends well past the change itself into strategies customers use to motivate suppliers to meet their changed needs. Customers' value change provides a reason for customers to seek, maintain, or move away from relationships with suppliers.",poster,cp61
Business,p2387,d9,8246f5505e0aa4db7cb5dcd06efd5365c404aafc,c30,PS,Managing in complex business networks,Abstract,poster,cp30
Business,p2388,d9,ce74f30dd1a6c1154d6b40b6164dacfaa1d0375c,c43,European Conference on Machine Learning,The Link between Business Strategy and Industrial Relations Systems in American Steel Minimills,"This study tests the “strategic choice” proposition that variation in workplace industrial relations policies and practices is related to differences in business strategy. A cluster analysis of data from a 1988–89 questionnaire examining workplace industrial relations and business strategies in U.S. steel minimills suggests that the industrial relations systems of these mills can be broadly categorized as emphasizing either cost reduction or employee commitment; similarly, the business strategies of the mills appear to stress either the manufacture of a few products in large quantities at the lowest possible cost, or more flexible manufacturing, with products marketed on some basis other than cost. Further investigation shows a significant association between the type of workplace industrial relations system and the business strategy choices in these mills.",poster,cp43
Economics,p2388,d11,ce74f30dd1a6c1154d6b40b6164dacfaa1d0375c,c43,European Conference on Machine Learning,The Link between Business Strategy and Industrial Relations Systems in American Steel Minimills,"This study tests the “strategic choice” proposition that variation in workplace industrial relations policies and practices is related to differences in business strategy. A cluster analysis of data from a 1988–89 questionnaire examining workplace industrial relations and business strategies in U.S. steel minimills suggests that the industrial relations systems of these mills can be broadly categorized as emphasizing either cost reduction or employee commitment; similarly, the business strategies of the mills appear to stress either the manufacture of a few products in large quantities at the lowest possible cost, or more flexible manufacturing, with products marketed on some basis other than cost. Further investigation shows a significant association between the type of workplace industrial relations system and the business strategy choices in these mills.",poster,cp43
Business,p2392,d9,0ec7e55d7611e8d1854ca8c6140c3f803ab9881c,c73,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,Trust and commitment influences on customer retention: insights from business-to-business services,Abstract,poster,cp73
Business,p2404,d9,3d0116a5027609974982659af99d2793725ebb0e,c1,International Conference on Human Factors in Computing Systems,Banking panics and business cycles,"Using a century of newly constructed data, competing theories explaining banking panics are tested. The evidence shows that banking panics during the U.S. national banking era (1865-1914) were the products of revisions in the perceived risk of the banking system based on the arrival of new information. Panics were triggered by a leading indicator of recession. Whenever this variable reached a threshold level, there was a panic. Thus, panics were not random events. The panics of the 1930s, however, did not result from the same pre-Federal Reserve System pattern of behavior. Copyright 1988 by Royal Economic Society.",poster,cp1
Business,p2407,d9,01d185ed4bdcf7dc0df8ddc53a21f0abc443f7ec,c26,Decision Support Systems,Differentiating Entrepreneurs from Small Business Owners: A Conceptualization,"The literature of small business and entrepreneurship is explored. It is established that, although there is an overlap between entrepreneurial firms and small business firms, they are different entities. Using the 1934 work of Schumpeter and recognizing the additions to the field of current writers, a conceptual framework is established for the differentiation of entrepreneurs from small business owners.",poster,cp26
Business,p2413,d9,af6237cb1e07aeb905c06a3c6ef2fcda8e1dcb75,c62,International Conference on Advanced Data and Information Engineering,Modeling the determinants of customer satisfaction for business-to-business professional services,"This research empirically examines for the first time the determinants of customer satisfaction or dissatisfaction (CS/D) in the context of business professional services. The simultaneous effect of key CS/D constructs (expectations, performance, and disconfirmation) and several variables—fairness (equity), purchase situation (novelty, importance, and complexity)—and individual-level variables (decision uncertainty and stakeholding) are examined in a causal path framework. Data were obtained from a two-stage longitudinal survey of client organizations. The results indicated substantial support for the hypothesized model. The effect of purchase situation and individual-level variables (via their indirect affects) rivals that of disconfirmation and expectations in explaining CS/D. Performance was found to affect CS/D directly but not as powerfully as disconfirmation.",poster,cp62
Business,p2415,d9,8083dda27e1a60fa47702ef17d917dba725c28d1,c52,Workshop on Applied Computational Geometry,Achieving and Sustaining Business-IT Alignment,"This article identifies the major enablers and inhibitors in the achievement of business-Information Technology (IT) alignment. Alignment involves the activities that management performs to achieve cohesive goals across the IT and functional (e.g., finance, marketing, manufacturing) organizations. Therefore, alignment addresses both how IT is in harmony with the business, and how the business should or could be in harmony with IT. IT requires strong support from senior management, good working relationships, strong leadership, appropriate prioritization, trust, and effective communication, as well as a thorough understanding of the business environment. This article develops a methodology that leverages the most important enablers and inhibitors to business-IT alignment.",poster,cp52
Business,p2418,d9,d520df727c5ac7f5c5c573e31dbe8fa77ad8bc9f,c93,ASE BigData & SocialInformatics,Marketing's Role in the Implementation of Business Strategies: A Critical Review and Conceptual Framework,"The authors review and integrate various theoretical perspectives, normative statements, and pieces of empirical evidence about the organizational structures and processes best suited for implementing different types of business strategies. Particular emphasis is given to the relationship of different types of structure, processes, and policies involved in the performance of marketing activities to the overall performance of different business strategies. Several specific research propositions are developed.",poster,cp93
Business,p2420,d9,3b3a15da8a3eaf1ad3c9a48bd423d4e9a746df2d,c7,International Symposium on Intelligent Data Analysis,Business Cycles and the Asset Structure of Foreign Trade,"International financial market linkages are widely believed to be important for the international transmission of business cycles, since these govern the extent to which individuals can smooth consumption in the presence of country-specific shocks to income. This paper develops a two-country, general equilibrium model with restricted asset trade and provides a detailed analysis of the channels through which these financial linkages affect international business cycles. Our central finding is that the absence of complete financial integration may not be important if the shocks to national economies are of low persistence, or are transmitted rapidly across countries over time. However, if shocks are highly persistent or are not transmitted internationally, the extent of financial integration is central to the international transmission of business cycles.",poster,cp7
Economics,p2420,d11,3b3a15da8a3eaf1ad3c9a48bd423d4e9a746df2d,c7,International Symposium on Intelligent Data Analysis,Business Cycles and the Asset Structure of Foreign Trade,"International financial market linkages are widely believed to be important for the international transmission of business cycles, since these govern the extent to which individuals can smooth consumption in the presence of country-specific shocks to income. This paper develops a two-country, general equilibrium model with restricted asset trade and provides a detailed analysis of the channels through which these financial linkages affect international business cycles. Our central finding is that the absence of complete financial integration may not be important if the shocks to national economies are of low persistence, or are transmitted rapidly across countries over time. However, if shocks are highly persistent or are not transmitted internationally, the extent of financial integration is central to the international transmission of business cycles.",poster,cp7
Business,p2426,d9,a25bdceb157d2973172fbd860eda8bb9bb7cdcab,c80,International Symposium on Software Engineering for Adaptive and Self-Managing Systems,Mixing Family with Business: A Study of Thai Business Groups and the Families Behind Them,"Families run a large fraction of business groups around the world. In this paper, we analyze how the structure of the families behind these business groups affects the groups' organization, governance and performance. To address this question, we constructed a unique data set of family trees and business groups for nearly 100 of the largest business families in Thailand. We find a strong positive association between family size and family involvement in the ownership and control of the family business. The sons of the founders play a central role in both ownership and board membership, especially when the founder of the group is gone. The availability of more sons is also associated with lower firm-level performance, especially when the founder is no longer present. We identify a possible governance channel for this performance effect. Excess control by sons, but not other family members, is associated with lower firm performance. In addition, excess control by sons increases with the number of sons and with the death of the founder. One hypothesis that emerges from our analysis is that part of the decay of family-run groups over time may be due to a dilution of ownership and control across a set of equally powerful descendants of the founder, which creates a race to the bottom in tunneling resources out of the group firms.",poster,cp80
Economics,p2426,d11,a25bdceb157d2973172fbd860eda8bb9bb7cdcab,c80,International Symposium on Software Engineering for Adaptive and Self-Managing Systems,Mixing Family with Business: A Study of Thai Business Groups and the Families Behind Them,"Families run a large fraction of business groups around the world. In this paper, we analyze how the structure of the families behind these business groups affects the groups' organization, governance and performance. To address this question, we constructed a unique data set of family trees and business groups for nearly 100 of the largest business families in Thailand. We find a strong positive association between family size and family involvement in the ownership and control of the family business. The sons of the founders play a central role in both ownership and board membership, especially when the founder of the group is gone. The availability of more sons is also associated with lower firm-level performance, especially when the founder is no longer present. We identify a possible governance channel for this performance effect. Excess control by sons, but not other family members, is associated with lower firm performance. In addition, excess control by sons increases with the number of sons and with the death of the founder. One hypothesis that emerges from our analysis is that part of the decay of family-run groups over time may be due to a dilution of ownership and control across a set of equally powerful descendants of the founder, which creates a race to the bottom in tunneling resources out of the group firms.",poster,cp80
Business,p2427,d9,9d2a58f684955afc6eac5fec992027bcf2c9afc8,c27,International Conference Geographic Information Science,The business of international business is culture,Abstract,poster,cp27
Business,p2429,d9,0f692fe5b0030073be84fec9b5a4d92359eee0ff,c78,ACM/IEEE International Conference on Model Driven Engineering Languages and Systems,Keeping The Family Business Healthy,Abstract,poster,cp78
Business,p2430,d9,fb7efa2e11da7bff8f51114e595ab85798b5a837,c73,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,Five stages of growth in small business,Abstract,poster,cp73
Business,p2431,d9,87b5f8b4dcc26426f3b9ac169add15e84206f433,c104,North American Chapter of the Association for Computational Linguistics,Business Ethics: Managing Corporate Citizenship and Sustainability in the Age of Globalization,"PART A: UNDERSTANDING BUSINESS ETHICS 1. Introducing Business Ethics 2. Framing Business Ethics: Corporate Responsibility, Stakeholders, and Citizenship 3. Evaluating Business Ethics: Normative Ethical Theories 4. Making Decisions in Business Ethics: Descriptive Ethical Theories 5. Managing Business Ethics: Tools and Techniques of Business Ethics Management PART B: CONTEXTUALIZING BUSINESS ETHICS 6. Shareholders and Business Ethics 7. Employees and Business Ethics 8. Consumers and Business Ethics 9. Suppliers, Competitors, and Business Ethics 10. Civil Society and Business Ethics 11. Government, Regulation, and Business Ethics 12. Conclusions and Future Perspectives",poster,cp104
Business,p2432,d9,a06b4074e74fe700611a0a80c62465476c09683d,c26,Decision Support Systems,"Evolution and Implementation: A Study of Values, Business Ethics and Corporate Social Responsibility",Abstract,poster,cp26
Business,p2433,d9,ee723c1664ad95cc2cfe65e17c01fde7806f0265,c88,International Conference on Big Data Computing and Communications,Succession in Family Business: A Review of the Research,"This paper reviews the research to date on succession in the field of family business management. Five streams of research are highlighted: (1) succession as a process, (2) the role of the founder, (3) the perspective of the next generation, (4) multiple levels of analysis, and (5) characteristics of effective successions. Gaps in the literature and future research directions are also presented.",poster,cp88
Business,p2442,d9,a62d7ed608c0f4dabade5ceab933330f08c0c446,c66,International Conference on Web and Social Media,An analysis of e-business adoption and its impact on business performance,"Across industries, firms have adopted e-business initiatives to better manage their internal business processes as well as their interfaces with the environment. In this study, a unified framework that captures the antecedents of e-business adoption, adoption intensity, and performance outcomes is proposed and empirically tested using data collected from senior managers in four technology-intensive industries. Applying a framework that captures the intensity of e-business adoption across four business process domains, the authors find that the antecedents and performance outcomes of e-business adoption are best studied in a process-specific context. They find, for example, that while the communication and internal administration aspects of e-business positively affect performance outcomes, the more high-profile activities related to online order taking and e-procurement do not. The authors' findings provide the foundation for a more rigorous study of e-business.",poster,cp66
Business,p2445,d9,6023f394d236dd54ee49c98021b659f17b585c60,c77,Visualization for Computer Security,Business Ecosystems and the View from the Firm,"For more than sixty years, markets and hierarchies have dominated our thinking about economic organization.' This article suggests that a third form, the ecosystem organizational form, has now become so important in practice that it should be accorded equal recognition in theory and in policymaking. Markets, hierarchies, and ecosystems are the three pillars of modern business thinking and should provide the foundation for competition policy, regulation, and antitrust actions. I am pleased to contribute to this issue of The Antitrust Bulletin as a member of the American Antitrust",poster,cp77
Business,p2446,d9,5dcdeb6d9ebedd399243d2c79219b61af2428def,j431,The Journal of Entrepreneurial Finance,Small Business Uniqueness and the Theory of Financial Management,"Small businesses do not share the same financial management problems with large businesses. This paper shows that the source of the differences could be traced to several characteristics unique to small businesses. This uniqueness in turn creates a whole new set of financial management issues. The major implication is that, yes, there are new and interesting topics in small business financial management research.",fullPaper,jv431
Business,p2448,d9,a5c7ebb4d2768b2ffa6d2256ef53ef68c0f84bd3,c62,International Conference on Advanced Data and Information Engineering,Auditors' Perceived Business Risk and Audit Fees: Analysis and Evidence,"type=""main"" xml:lang=""en""> This study analyzes the relation between auditors' perceived business risk and audit fees to determine whether audit firms or their clients bear the expected legal costs of business risk. We predict that hourly audit fees and the number of audit hours are increasing in business risk. Using confidential survey data collected by a large international accounting firm for 422 audits, we find that high business risk increases the number of audit hours, but not the fee per hour. This implies that firms perceive firm-level differences in business risk and obtain compensation through billing additional hours, not by raising the hourly charge.",poster,cp62
Business,p2450,d9,04a29d42a2e5135f32dd28dac93652316a8d3a11,c49,ACM/SIGCOMM Internet Measurement Conference,Business policy: text and cases,Abstract,poster,cp49
Business,p2457,d9,d265360ce8ab875ce5c378a108b27f998c5b066a,c9,Big Data,Innovating Business Models with Co-Development Partnerships,"OVERVIEW: Business model innovation is vital to sustaining open innovation. External technology partnerships allow open business models to accomplish even more. One important mechanism for innovating one's business model is through establishing co-development relationships. The proper character of these relationships varies, depending on the context for the relationship. To sustain co-development relationships, one must carefully define the business objectives and align the business models of each firm. One should also determine whether the various R&D capabilities are core, critical or contextual. The decision to partner externally will have different implications for each of these.",poster,cp9
Business,p2460,d9,73713699dbd6ac09e151f634e2bcb2c3b4be33be,c121,International Conference on Interaction Sciences,Social Exchange Theory and Research on Business-to-Business Relational Exchange,"ABSTRACT Social exchange theory (SET) has been used extensively by marketing scholars to explain business-to-business relational exchange. Despite its popularity as a theoretical explanatory mechanism, there is no recent literature review that delineates SET's foundational premises, how it has been used in the marketing literature, and its theoretical limitations. This article provides such a review and is intended to assist researchers who wish to use SET to examine business-to-business relational exchange.",poster,cp121
Business,p2462,d9,504d6b49bb97efc59bb772b885d15391b3ad6bc2,c63,International Conference on Evaluation & Assessment in Software Engineering,What Success Factors are Important to Small Business Owners?,"Financial criteria are usually considered to be the most appropriate measure of business success, yet many small business owners are motivated to start a business on the basis of lifestyle or personal factors. Non-financial goals could lead to alternative measures of success, particularly in the small business sector. To explore the significance of these two dimensions of success, 290 small business owner-managers in Western Australia were surveyed. Respondents rated the importance of items relating to lifestyle and financial measures, which they used to judge their business success. Findings suggest that both financial and non-financial lifestyle criteria are used to judge business success, with the latter being more important. Personal satisfaction and achievement, pride in the job and a flexible lifestyle are generally valued higher than wealth creation. Personal factors such as age and also business characteristics influenced perceptions on the importance of these factors.",poster,cp63
Business,p2466,d9,3107bff5703b0f43416bc459984f84f2bb845c18,c60,Network and Distributed System Security Symposium,How strong is the business-to-business brand in the workforce? An empirically-tested model of ‘internal brand equity’ in a business-to-business setting,Abstract,poster,cp60
Business,p2469,d9,0136ae2eaeaf40028aacc0fa445eef9ba2188683,c97,International Conference on Computational Logic,Sustainability and business-to-business marketing: A framework and implications,Abstract,poster,cp97
Business,p2472,d9,77179b34320aba47efabbc158e284e2daec2692c,c61,"International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems",“Living the brand”: brand orientation in the business‐to‐business sector,"Purpose – The purpose of this paper is to design and test a model for the internal anchorage of a business‐to‐business brand via corporate brand orientation.Design/methodology/approach – Data from 261 usable responses to a questionnaire distributed in the German business‐to‐business sector, were applied to model estimation by the “soft modelling” partial‐least‐squares regression technique.Findings – The structure of the brand orientation model is supported by the results. The findings demonstrate the positive influence of brand orientation on market and economic performance. Smaller business‐to‐business companies exhibit lower levels of brand orientation than larger counterparts, to their strategic disadvantage. Line of business and management type had no influence on brand orientation in this survey.Practical implications – Business‐to‐business brand managers now have empirically‐based evidence for the benefits of intensive implementation of brand orientation. The associated four‐level conceptual model o...",poster,cp61
Business,p2477,d9,b6f05cc2a4544fb1d00431358600c6e7cfab4c35,c10,Americas Conference on Information Systems,Matching corporate culture and business strategy,Abstract,poster,cp10
Business,p2488,d9,dd62bd1c7237a2c0822c92d712db968d28aa279c,c108,IEEE International Conference on Multimedia and Expo,Emotional Returns and Emotional Costs in Privately Held Family Businesses: Advancing Traditional Business Valuation,"This article introduces a formula to assess the total value of privately held family businesses from the owner's perspective. It is argued that the total value of a business is not only composed of its financial worth and private benefits, as is usually assumed by traditional financial theory, but that emotional components also have an impact on valuation. In particular, it is assumed that emotional returns (ER) positively affect total value, whereas emotional costs (EC) negatively affect total value. Even though every stakeholder faces emotional costs and returns, it is solely the family business owner who ultimately decides on the worth of a business and consequently factors ER-EC into his or her valuation. The presented formula provides a better understanding of investment decisions in family businesses and a more accurate valuation of these businesses.",poster,cp108
Business,p2491,d9,07fd0bb8639b78df100b6d3303d8f8e486d129fe,c71,International Joint Conference on Artificial Intelligence,Ties That Bind: A Social Contracts Approach to Business Ethics,"This text offers a method for untangling the ethical dilemmas that arise through business transactions, regardless of culture or context. It also demonstrates how empirical descriptions and normative evaluations of business policies must cooperate to inform sound business decisions. Examples are used to support the author's points, featuring companies such as AT&T, Levi-Strauss and Royal Dutch/Shell.",poster,cp71
Business,p2492,d9,9c8c3ba9bfd78d54942e3611bf932066aa0d371e,c121,International Conference on Interaction Sciences,Growing the Family Business: Special Challenges and Best Practices,Most family businesses simply don't grow. This paper explores the reasons for and theories behind business stagnation and proposes a set of “best practices” that can revitalize a firm and enhance its performance.,poster,cp121
Business,p2495,d9,e4609d22d6b4d824a91edf865476bddd6f7d6aa3,c120,SIGSAND-Europe Symposium,Myths and Realities: Family Businesses' Contribution to the US Economy— A Framework for Assessing Family Business Statistics,"This article presents a framework for assessing commonly accepted family business statistics, based on the criteria used to define a family business. Using existing research from multiple fields and sources, a range is extrapolated for the total number of family businesses in the US, their contribution to Gross Domestic Product (GDP) and employment.",poster,cp120
Business,p2496,d9,4dca9b82468d26fd70384dd94515447cb3253ae3,c102,ACM SIGMOD Conference,"E‐business model design, classification, and measurements","“Business model” is one of the latest buzzwords in the Internet and electronic business world. This paper has the ambition to give this term a more rigorous content. The objective is threefold. The first one is to propose a theoretical e-business model framework for doing business in the Internet era. The second one is to propose a multi-dimensional classification-scheme for eBusiness Models, as opposed to the actual tendency in academic literature to use two-dimensional classifications. The final objective is to define critical success factors, based on a field study in order to find out and compare the performance indicators used by e-business firms which are competing with similar businesses models.",poster,cp102
Psychology,p101,d10,a9e447d4d6b91f75ac4d8e336c609cfcbcbcfc0a,c117,Very Large Data Bases Conference,Data Science,"The Bachelor of Science in Data Science studies the collection, manipulation, storage, retrieval, and computational analysis of data in its various forms, including numeric, textual, image, and video data from small to large volumes. The program combines computer science, information science, mathematics, statistics, and probability theory into an integrated curriculum that prepares students for careers or graduate studies in big data analysis, data science, and data analytics. The coursework covers exploratory data analysis, data manipulation in a variety of programming languages, large-scale data storage, predictive analytics, machine learning, data mining, and information visualization and presentation. Data science has emerged as a discipline due to the confluence of two major events:",poster,cp117
Psychology,p207,d10,c694c6a685a067393204f36e21c8917a49f02a9b,c54,ACM SIGCOMM Symposium on Software Defined Networking Research,Big data at work : the data science revolution and organizational psychology,"Foreword by Richard Klimoski 1. Building Understanding of the Data Science Revolution and IO Psychology Eden B. King, Scott Tonidandel, Jose M. Cortina, & Alexis A. Fink Part I: Big Issues for Big Data Methods 2. Big Data Platform Jacqueline Ryan 3. Statistical Methods for Big Data: A Scenic Tour Frederick L. Oswald & Dan J. Putka 4. Twitter Analysis: Methods for Data Management and a Word Count Dictionary to Measure City-Level Job Satisfaction Ivan Hernandez, Daniel A. Newman, & Gahyun Jeon 5. Data Visualization Evan F. Sinar 6. Sensing Big Data: Multimodal Information Interfaces for Exploration of Large Data Sets Jeffrey Stanton Part II: Big Ideas for Big Data in Organization 7. Implications of the Big Data Movement for the Advancement I-O Science and Practice Dan J. Putka & Frederick L. Oswald 8. Big Data in Talent Selection and Assessment A. James Illingworth, Michael Lippstreu, & Anne-Sophie Deprez-Sims 9. Big Data in Turnover/Retention John P. Hausknecht & Huisi (Jessica) Li 10. Using Big Data to Advance the Science of Team Effectiveness Steve W. J. Kozlowski, Georgia T. Chao, Chu-Hsiang (Daisy) Chang, & Rosemarie Fernandez 11. Using Big Data to Create Diversity and Inclusion in Organizations Whitney Botsford Morgan, Eric Dunleavy, & Peter D. DeVries 12. How Big Data Matters Richard A. Guzzo",poster,cp54
Psychology,p293,d10,7281fb2c44f4d73c86ffefd1fde7e4f8a1f5e75c,j91,Science Education,Engagement in science through citizen science: Moving beyond data collection,"""To date, most studies of citizen science engagement focus on quantifiable measures related to the contribution of data or other output measures. Few studies have attempted to qualitatively characterize citizen science engagement across multiple projects and from the perspective of the participants. Building on pertinent literature and sociocultural learning theories, this study operationalizes engagement in citizen science through an analysis of interviews of 72 participants from six different environmentally based projects. We document engagement in citizen science through an examination of cognitive, affective, social, behavioral, and motivational dimensions. We assert that engagement in citizen science is enhanced by acknowledging these multiple dimensions and creating opportunities for volunteers to find personal relevance in their work with scientists. A Dimensions of Engagement framework is presented that can facilitate the innovation of new questions and methodologies for studying engagement in citizen science and other forms of informal science education.""",fullPaper,jv91
Psychology,p448,d10,25e0d93ca47d86510d6a0f9cda9ae3594f3d05b2,c111,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,"Color Science: Concepts and Methods, Quantitative Data and Formulas","Eventually, you will agreed discover a further experience and achievement by spending more cash. still when? attain you acknowledge that you require to acquire those every needs in imitation of having significantly cash? Why don't you try to acquire something basic in the beginning? That's something that will lead you to comprehend even more in the region of the globe, experience, some places, later than history, amusement, and a lot more?",poster,cp111
Psychology,p482,d10,f6ce14f91b4641942947882062682125369847f7,j44,Social Science Research Network,The V–Dem Measurement Model: Latent Variable Analysis for Cross-National and Cross-Temporal Expert-Coded Data,"This material is based upon work supported by the National Science Foundation (SES-1423944, PI: Daniel Pemstein), Riksbankens Jubileumsfond (Grant M13-0559:1, PI: Staffan I. Lindberg), the Swedish Research Council (2013.0166, PI: Staffan I. Lindberg and Jan Teorell), the Knut and Alice Wallenberg Foundation (PI: Staffan I. Lindberg), and the University of Gothenburg (E 2013/43); as well as internal grants from the Vice-Chancellor’s office, the Dean of the College of Social Sciences, and the Department of Political Science at University of Gothenburg. Marquardt acknowledges research support from the Russian Academic Excellence Project ‘5-100.’ We performed simulations and other computational tasks using resources provided by the Notre Dame Center for Research Computing (CRC) through the High Performance Computing section and the Swedish National Infrastructure for Computing (SNIC) at the National Supercomputer Centre in Sweden (SNIC 2016/1-382, SNIC 2017/1-406 and 2017/1-68). We specifically acknowledge the assistance of In-Saeng Suh at CRC and Johan Raber and Peter Mu nger at SNIC in facilitating our use of their respective systems.",fullPaper,jv44
Psychology,p765,d10,35fd9f4c4289a293e209846717ab28e1eb7fc994,j199,European Journal of Engineering Education,Developing employability in engineering education: a systematic review of the literature,"ABSTRACT In this systematic review of the research literature on engineering employability, curricular and pedagogical arrangements that prepare graduates for work in the twenty-first century were identified. The research question guiding the review was: Which curricular and pedagogical arrangements promote engineering students’ employability? The particular focus of the study was on how authors prioritised engineering knowledge and professional skills. The review drew on a theoretical framework that differentiated between engineering knowledge and professional skills to explain how employability could be included in engineering programmes. Data was obtained from research studies over the period 2007–2017. We found an interdependent relationship between engineering knowledge and professional skills that enabled engineering graduates to attain employability. The com of engineering problems require students to master engineering knowledge, while the ability to work with others across contexts requires professional skills. Both are necessary for deep understanding of engineering principles and a focus on real world problems.",fullPaper,jv199
Psychology,p784,d10,d1ca9accc8e1ca4411aa90e5355924b822bc14af,c60,Network and Distributed System Security Symposium,"Changing Engineering Education: Views of U.S. Faculty, Chairs, and Deans","Many reports present a vision of what engineering education should look like, but few describe how this should happen. An American Society for Engineering Education initiative in 2006 attempted to bridge this gap by engaging faculty, chairs, and deans in discussion of change in engineering education; results were reported in a Phase I report (2009). In a second phase, survey data were integrated into a Phase II report (2012).",poster,cp60
Psychology,p821,d10,abd3c6854f05cef54d1fbe74ead145e77619392f,c27,International Conference Geographic Information Science,Professional Role Confidence and Gendered Persistence in Engineering,"Social psychological research on gendered persistence in science, technology, engineering, and mathematics (STEM) professions is dominated by two explanations: women leave because they perceive their family plans to be at odds with demands of STEM careers, and women leave due to low self-assessment of their skills in STEM’s intellectual tasks, net of their performance. This study uses original panel data to examine behavioral and intentional persistence among students who enter an engineering major in college. Surprisingly, family plans do not contribute to women’s attrition during college but are negatively associated with men’s intentions to pursue an engineering career. Additionally, math self-assessment does not predict behavioral or intentional persistence once students enroll in a STEM major. This study introduces professional role confidence—individuals’ confidence in their ability to successfully fulfill the roles, competencies, and identity features of a profession—and argues that women’s lack of this confidence, compared to men, reduces their likelihood of remaining in engineering majors and careers. We find that professional role confidence predicts behavioral and intentional persistence, and that women’s relative lack of this confidence contributes to their attrition.",poster,cp27
Psychology,p865,d10,ca3d6205ffb8cab6724fb0828634826e2e87ead7,c21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,Spatial ability for STEM domains: Aligning over 50 years of cumulative psychological knowledge solidifies its importance.,"The importance of spatial ability in educational pursuits and the world of work was examined, with particular attention devoted to STEM (science, technology, engineering, and mathematics) domains. Participants were drawn from a stratified random sample of U.S. high schools (Grades 9–12, N 400,000) and were tracked for 11 years; their longitudinal findings were aligned with pre-1957 findings and with contemporary data from the Graduate Record Examination and the Study of Mathematically Precocious Youth. For decades, spatial ability assessed during adolescence has surfaced as a salient psychological attribute among those adolescents who subsequently go on to achieve advanced educational credentials and occupations in STEM. Results solidify the generalization that spatial ability plays a critical role in developing expertise in STEM and suggest, among other things, that including spatial ability in modern talent searches would identify many adolescents with potential for STEM who are currently being missed.",poster,cp21
Psychology,p922,d10,396c6757db39235c57afc0f8612bd9eca1bfaffb,c88,International Conference on Big Data Computing and Communications,Problem‐based Learning: Influence on Students' Learning in an Electrical Engineering Course,"Recently, there has been a shift from using lecture‐based teaching methods in undergraduate engineering courses to using more learner‐centered teaching approaches, such as problem‐based learning. However, research on the impact of these approaches has mainly involved student perceptions of the teaching method and anecdotal and opinion pieces by faculty on their use of the teaching method, rather than empirically collected data on students' learning outcomes.",poster,cp88
Psychology,p979,d10,070eb848983b6447cbc701c8213b7fb021dab527,c121,International Conference on Interaction Sciences,Social Cognitive Predictors of Academic Interests and Goals in Engineering: Utility for Women and Students at Historically Black Universities.,"This study examined the utility of social cognitive career theory (SCCT; R. W. Lent, S. D. Brown, & G. Hackett, 1994) in predicting engineering interests and major choice goals among women and men and among students at historically Black and predominantly White universities. Participants (487 students in introductory engineering courses at 3 universities) completed measures of academic interests, goals, self-efficacy, outcome expectations, and environmental supports and barriers in relation to engineering majors. Findings indicated that the SCCT-based model of interest and choice goals produced good fit to the data across gender and university type. Implications for future research on SCCT's choice hypotheses, and particularly for the role of environmental supports and barriers in the choice of science and engineering fields, are discussed.",poster,cp121
Psychology,p1040,d10,b473e91cbe80c8b46451b49153cd5f93030480ab,c33,Workshop on Python for High-Performance and Scientific Computing,Critical analysis of Big Data challenges and analytical methods,Abstract,poster,cp33
Psychology,p1049,d10,3dfa820702b6181c9964931f0a4d47fd298bf429,j253,Review of Research in Education,Mining Big Data in Education: Affordances and Challenges,"The emergence of big data in educational contexts has led to new data-driven approaches to support informed decision making and efforts to improve educational effectiveness. Digital traces of student behavior promise more scalable and finer-grained understanding and support of learning processes, which were previously too costly to obtain with traditional data sources and methodologies. This synthetic review describes the affordances and applications of microlevel (e.g., clickstream data), mesolevel (e.g., text data), and macrolevel (e.g., institutional data) big data. For instance, clickstream data are often used to operationalize and understand knowledge, cognitive strategies, and behavioral processes in order to personalize and enhance instruction and learning. Corpora of student writing are often analyzed with natural language processing techniques to relate linguistic features to cognitive, social, behavioral, and affective processes. Institutional data are often used to improve student and administrational decision making through course guidance systems and early-warning systems. Furthermore, this chapter outlines current challenges of accessing, analyzing, and using big data. Such challenges include balancing data privacy and protection with data sharing and research, training researchers in educational data science methodologies, and navigating the tensions between explanation and prediction. We argue that addressing these challenges is worthwhile given the potential benefits of mining big data in education.",fullPaper,jv253
Psychology,p1297,d10,d6d0ff535fd7b86ba4d19de11587591e7359a6a0,j264,Journal of business research,"Big data, big decisions: The impact of big data on board level decision-making",Abstract,fullPaper,jv264
Psychology,p1622,d10,3765df816dc5a061bc261e190acc8bdd9d47bec0,c81,ACM Symposium on Applied Computing,Presentation and validation of the Radboud Faces Database,"Many research fields concerned with the processing of information contained in human faces would benefit from face stimulus sets in which specific facial characteristics are systematically varied while other important picture characteristics are kept constant. Specifically, a face database in which displayed expressions, gaze direction, and head orientation are parametrically varied in a complete factorial design would be highly useful in many research domains. Furthermore, these stimuli should be standardised in several important, technical aspects. The present article presents the freely available Radboud Faces Database offering such a stimulus set, containing both Caucasian adult and children images. This face database is described both procedurally and in terms of content, and a validation study concerning its most important characteristics is presented. In the validation study, all frontal images were rated with respect to the shown facial expression, intensity of expression, clarity of expression, genuineness of expression, attractiveness, and valence. The results show very high recognition of the intended facial expressions.",poster,cp81
Psychology,p1711,d10,6d703e6eb1c72241720bafdb42b46b70c3bdd16e,c64,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",MRC Psycholinguistic Database,Abstract,poster,cp64
Psychology,p2002,d10,12be757eabdaf30457b14e06b510a76d7c4e0328,c98,Vision,Research Methods for Business: A Skill-Building Approach,Abstract,poster,cp98
Psychology,p2040,d10,ee5da0454760b9b96e89b43a2bab3e9b1f95f739,c84,EUROCON Conference,Entrepreneurial Orientation and Business Performance: An Assessment of past Research and Suggestions for the Future,"Entrepreneurial orientation (EO) has received substantial conceptual and empirical attention, representing one of the few areas in entrepreneurship research where a cumulative body of knowledge is developing. The time is therefore ripe to document, to review, and to evaluate the cumulative knowledge on the relationship between EO and business performance. Extending beyond qualitative assessment, we undertook a meta–analysis exploring the magnitude of the EO–performance relationship and assessed potential moderators affecting this relationship. Analyses of 53 samples from 51 studies with an N of 14,259 companies indicated that the correlation of EO with performance is moderately large (r = .242) and that this relationship is robust to different operationalizations of key constructs as well as cultural contexts. Internal and environmental moderators were identified, and results suggest that additional moderators should be assessed. Recommendations for future research are developed.",poster,cp84
Psychology,p2120,d10,8389323ff11cedc7efc34c54146313d7cfd4acfd,c21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,"Transformational leadership, transactional leadership, locus of control, and support for innovation: key predictors of consolidated-business-unit performance","The authors used measures of leadership, locus of control, and support for innovation to predict the consolidated-unit performance of 78 managers. Results revealed that 3 transformational-leadership measures were associated with a higher internal locus of control and significantly and positively predicted business-unit performance over a 1-year interval. Transactional measures of leadership, including contingent reward and management by exception (active and passive), were each negatively related to business-unit performance. Causal relationships between the transformational-leadership behaviors and unit performance were moderated by the level of support for innovation in the business unit",poster,cp21
Psychology,p2154,d10,3f19e58518edddd8efd15d832fc3558204bf6ebb,c6,Annual Conference on Genetic and Evolutionary Computation,"Let's put the person back into entrepreneurship research: A meta-analysis on the relationship between business owners' personality traits, business creation, and success","The role of personality traits in the decision to start a business and to maintain it successfully is discussed controversially in entrepreneurship research. Our meta-analysis builds upon and extends earlier meta-analyses by doing a full analysis of personality traits that includes a comparison of different traits from a theoretical perspective and by analysing a full set of personality predictors for both start-up activities as well as success. Theoretically, our article adds to the literature by matching traits to the tasks of entrepreneurs. The results indicate that traits matched to the task of running a business produced higher effect sizes with business creation than traits that were not matched to the task of running an enterprise, corrected r = .247, K = 47, N = 13,280, and corrected r = .124, K = 20, N = 3975, respectively. Moreover, traits matched to the task produced higher correlations with success, corrected r = .250, K = 42, N = 5607, than traits not matched to the task of running a business, corrected r = .028, K = 13, N = 2777. The traits matched to entrepreneurship significantly correlated with entrepreneurial behaviour (business creation, business success) were need for achievement, generalized self-efficacy, innovativeness, stress tolerance, need for autonomy, and proactive personality. These relationships were of moderate size in general and, moreover, heterogeneity suggested that future research should analyse moderator variables.",poster,cp6
Psychology,p2293,d10,3655512037347d8c89beb97c02e84c129e20389f,c81,ACM Symposium on Applied Computing,The effects of diversity on business performance: Report of the diversity research network,"This article summarizes the results and conclusions reached in studies of the relationships between race and gender diversity and business performance carried out in four large firms by a research consortium known as the Diversity Research Network. These researchers were asked by the BOLD Initiative to conduct this research to test arguments regarding the ""business case""for diversity. Few positive or negative direct effects of diversity on performance were observed. Instead a number of different aspects of the organizational context and some group processes moderated diversity-performance relationships. This suggests a more nuanced view of the ""business case"" for diversity may be appropriate. © 2003 Wiley Periodicals, Inc.",poster,cp81
Psychology,p2302,d10,644230a628b5b020723bd32fa46973230234dcb3,c115,International Conference on Information Integration and Web-based Applications & Services,Learning from Business Failure: Propositions of Grief Recovery for the Self-Employed,"In this paper I use the psychological literature on grief to explore the emotion of business failure, suggesting that the loss of a business from failure can cause the self-employed to feel grief—a negative emotional response interfering with the ability to learn from the events surrounding that loss. I discuss how a dual process of grief recovery maximizes the learning from business failure.",poster,cp115
Psychology,p2314,d10,ee685c5addd9a37247cb7115c0dcdf893a8720ef,c116,International Society for Music Information Retrieval Conference,Higher Education: A Critical Business,Part 1 Rethinking critical thinking: conditions of critical thought uncritical theory discourse and critical potential the closing of the critical university? Part 2 Towards critical being: critical being in higher education critical action critical self-reflection a curriculum for critical being. Part 3 critique in society: critical thought in a corporate world critical professionalism academics as intellectuals critical thinking for a learning society a critical space.,poster,cp116
Psychology,p2340,d10,4d0f346c0ed356c596cbfc05e62fdaed07bf7465,c95,Cyber ..,Gender and Organizational Performance: Determinants of Small Business Survival and Success,"This research explores the effect of gender on organizational performance. Data used in the analysis was collected from small businesses in South Central Indiana from 1985-1987. The businesses were from the food and drink, computer sales and software, and health industries. Of the businesses surveyed, 312 were headed by men while 99 where headed by women. On average, these individuals were 44-45 years of age. Organizational performance is examined through two different concepts, survival and success. The results indicate that women were not more likely to go out of business than men, and only prior self-employment had different effects for men and women. Further, there was not a difference in this area among the differing industries. The results also indicate that both genders were equally confident and believed they had the ability to influence business outcomes. As for the success of these businesses, there again was no difference between the males and females with respect to earnings growth. These results are contrary to the traditional thinking that men have an advantage over women with respect to entrepreneurship and organizational performance. Since the dataset used in this analysis was limited in scope, further research is necessary to determine if these results will hold true across other industries. (SRD)",poster,cp95
Psychology,p2369,d10,3550c8970a72aabdc42517e3563beb12ee46f6b7,c118,International Conference on Image Analysis and Processing,Social Media’s Influence on Business-to-Business Sales Performance,"The implementation of social media technology in a firm’s marketing strategy has been adopted by some forward-thinking sales forces. Sharing content and building a network of contacts are the principles behind social media. The utilization of social media (e.g., LinkedIn and Twitter) for reaching business-to-business clients is a relatively new phenomenon with performance outcomes essentially unknown. Data were collected from 1,699 business-to-business salespeople from over 25 different industries. Using structural equation modeling, the findings support that social media has a positive relationship with sales processes (creating opportunities and relationship management) and relationship sales performance.",poster,cp118
Psychology,p2394,d10,fc2003f2173a55d611c8545d0b6fceab4bbb846c,c108,IEEE International Conference on Multimedia and Expo,"Academic Dishonesty in Graduate Business Programs: Prevalence, Causes, and Proposed Action","Little is currently known about cheating among graduate business students. We collected data from more than 5,000 business (mostly MBA) and nonbusiness graduate students at 32 colleges and universities in the United States and Canada during the 2002–2003 and 2003–2004 academic years to test a series of hypotheses regarding the prevalence of graduate business student cheating and reasons why these students cheat. We found that graduate business students cheat more than their nonbusiness-student peers. Correlation results found cheating to be associated with perceived peer behavior, as well as the perceived certainty of being reported by a peer, and the understanding and acceptance of academic integrity policies by students and faculty. But, regression analysis results suggest that perceived peer behavior has the largest effect. Drawing from these findings and past research on undergraduate students, we propose strategies that business schools and faculty can use to promote academic integrity in graduate business programs.",poster,cp108
Political Science,p2394,d15,fc2003f2173a55d611c8545d0b6fceab4bbb846c,c108,IEEE International Conference on Multimedia and Expo,"Academic Dishonesty in Graduate Business Programs: Prevalence, Causes, and Proposed Action","Little is currently known about cheating among graduate business students. We collected data from more than 5,000 business (mostly MBA) and nonbusiness graduate students at 32 colleges and universities in the United States and Canada during the 2002–2003 and 2003–2004 academic years to test a series of hypotheses regarding the prevalence of graduate business student cheating and reasons why these students cheat. We found that graduate business students cheat more than their nonbusiness-student peers. Correlation results found cheating to be associated with perceived peer behavior, as well as the perceived certainty of being reported by a peer, and the understanding and acceptance of academic integrity policies by students and faculty. But, regression analysis results suggest that perceived peer behavior has the largest effect. Drawing from these findings and past research on undergraduate students, we propose strategies that business schools and faculty can use to promote academic integrity in graduate business programs.",poster,cp108
Psychology,p2490,d10,24dc9c7e6f3ae5422180a7b0a0df9d25503a3dd4,c87,International Conference on Big Data Research,"Workplace deviance, organizational citizenship behavior, and business unit performance: the bad apples do spoil the whole barrel","The influences of organizational citizenship behavior (OCB) and workplace deviant behavior (WDB) on business unit performance were investigated using data from branches of a fast food organization. Data included measures of WDB and OCB obtained from staff, ratings of performance provided by supervisors, and objective measures of performance. It was found that WDB was negatively and significantly associated with business unit performance measured both subjectively and objectively. OCB, however, failed to contribute to the prediction of business unit performance beyond the level that was achieved by WDB. It appeared, therefore, that the presence of deviant employees among business units impinges upon the performance of the business unit as a whole, whereas OCBs had comparatively little effect. Copyright © 2004 John Wiley & Sons, Ltd.",poster,cp87
Economics,p418,d11,d65d64c3f6ea322d9e85138fe5c8e85acbf661e3,c25,IEEE International Parallel and Distributed Processing Symposium,A Bibliometric Analysis and Visualization of Medical Big Data Research,"With the rapid development of “Internet plus”, medical care has entered the era of big data. However, there is little research on medical big data (MBD) from the perspectives of bibliometrics and visualization. The substantive research on the basic aspects of MBD itself is also rare. This study aims to explore the current status of medical big data through visualization analysis on the journal papers related to MBD. We analyze a total of 988 references which were downloaded from the Science Citation Index Expanded and the Social Science Citation Index databases from Web of Science and the time span was defined as “all years”. The GraphPad Prism 5, VOSviewer and CiteSpace softwares are used for analysis. Many results concerning the annual trends, the top players in terms of journal and institute levels, the citations and H-index in terms of country level, the keywords distribution, the highly cited papers, the co-authorship status and the most influential journals and authors are presented in this paper. This study points out the development status and trends on MBD. It can help people in the medical profession to get comprehensive understanding on the state of the art of MBD. It also has reference values for the research and application of the MBD visualization methods.",poster,cp25
Economics,p738,d11,f04a9db024afd57bf3cba3e1c9028e8d2f6f3f13,c104,North American Chapter of the Association for Computational Linguistics,Statistics and Data Analysis for Financial Engineering,Abstract,poster,cp104
Economics,p905,d11,4b598fd43705e7e7d52ec911cfd32d22e22b7ae5,c97,International Conference on Computational Logic,Spillover Effects of FDI on Innovation in China: Evidence from the Provincial Data,"Foreign direct investment (FDI) can benefit innovation activity in the host country via spillover channels such as reverse engineering, skilled labor turnovers, demonstration effects, and supplier-customer relationships. Using provincial data from 1995 to 2000, we find positive effects of FDI on the number of domestic patent applications in China. This finding is robust under both pooled time-series and cross-section data estimation and panel data analysis and for different types of patent applications (invention, utility model, and external design). The spillover effect is the strongest for minor innovation such as external design patent, highlighting a ""demonstration effect"" of FDI.",poster,cp97
Economics,p1019,d11,73d4accea441aae2373828a8dc2175aa2759c38f,j245,The Review of financial studies,Big Data in Finance,"
 Big data is revolutionizing the finance industry and has the potential to significantly shape future research in finance. This special issue contains papers following the 2019 NBER-RFS Conference on Big Data. In this introduction to the special issue, we define the “big data” phenomenon as a combination of three features: large size, high dimension, and complex structure. Using the papers in the special issue, we discuss how new research builds on these features to push the frontier on fundamental questions across areas in finance—including corporate finance, market microstructure, and asset pricing. Finally, we offer some thoughts for future research directions.",fullPaper,jv245
Economics,p1032,d11,933baeec555352784848a93284c9dd0e79477759,c103,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,Big Data in Smart Farming – A review,Abstract,poster,cp103
Economics,p1074,d11,ca5931b4c08eb96d40c65e1b9f8e4db46bf0585b,j251,Management Decision,Big data analytics capability in supply chain agility,"
Purpose
The purpose of this paper is to examine when and how organizations build big data analytics capability (BDAC) to improve supply chain agility (SCA) and gain competitive advantage.


Design/methodology/approach
The authors grounded the theoretical framework in two perspectives: the dynamic capabilities view and contingency theory. To test the research hypotheses, the authors gathered 173 usable responses using a pre-tested questionnaire.


Findings
The results suggest that BDAC has a positive and significant effect on SCA and competitive advantage. Further, the results support the hypothesis that organizational flexibility (OF) has a positive and significant moderation effect on the path joining BDAC and SCA. However, contrary to the belief, the authors found no support for the moderation effect of OF on the path joining BDAC and competitive advantage.


Originality/value
The study makes some useful contributions to the literature on BDAC, SCA, OF, and competitive advantage. Moreover, the results may further motivate future scholars to replicate the findings using longitudinal data.
",fullPaper,jv251
Economics,p1090,d11,c7e10109fed73e523bf2a78b00e4b657004d41fc,j251,Management Decision,Big data and dynamic capabilities: a bibliometric analysis and systematic literature review,"
Purpose
Recently, several manuscripts about the effects of big data on organizations used dynamic capabilities as their main theoretical approach. However, these manuscripts still lack systematization. Consequently, the purpose of this paper is to systematize the literature on big data and dynamic capabilities.


Design/methodology/approach
A bibliometric analysis was performed on 170 manuscripts extracted from the Clarivate Analytics Web of Science Core Collection database. The bibliometric analysis was integrated with a literature review.


Findings
The bibliometric analysis revealed four clusters of papers on big data and dynamic capabilities: big data and supply chain management, knowledge management, decision making, business process management and big data analytics. The systematic literature review helped to clarify each clusters’ content.


Originality/value
To the authors’ best knowledge, minimal attention has been paid to systematizing the literature on big data and dynamic capabilities.
",fullPaper,jv251
Economics,p1093,d11,d65d64c3f6ea322d9e85138fe5c8e85acbf661e3,c7,International Symposium on Intelligent Data Analysis,A Bibliometric Analysis and Visualization of Medical Big Data Research,"With the rapid development of “Internet plus”, medical care has entered the era of big data. However, there is little research on medical big data (MBD) from the perspectives of bibliometrics and visualization. The substantive research on the basic aspects of MBD itself is also rare. This study aims to explore the current status of medical big data through visualization analysis on the journal papers related to MBD. We analyze a total of 988 references which were downloaded from the Science Citation Index Expanded and the Social Science Citation Index databases from Web of Science and the time span was defined as “all years”. The GraphPad Prism 5, VOSviewer and CiteSpace softwares are used for analysis. Many results concerning the annual trends, the top players in terms of journal and institute levels, the citations and H-index in terms of country level, the keywords distribution, the highly cited papers, the co-authorship status and the most influential journals and authors are presented in this paper. This study points out the development status and trends on MBD. It can help people in the medical profession to get comprehensive understanding on the state of the art of MBD. It also has reference values for the research and application of the MBD visualization methods.",poster,cp7
Economics,p1125,d11,091929400bd0735c19daa1cc5523e336b354aae9,c68,Symposium on Advances in Databases and Information Systems,Challenges and opportunities of digital information at the intersection of Big Data Analytics and supply chain management,"Purpose 
 
 
 
 
Despite the variety of supply chain management (SCM) research, little attention has been given to the use of Big Data Analytics for increased information exploitation in a supply chain. The purpose of this paper is to contribute to theory development in SCM by investigating the potential impacts of Big Data Analytics on information usage in a corporate and supply chain context. As it is imperative for companies in the supply chain to have access to up-to-date, accurate, and meaningful information, the exploratory research will provide insights into the opportunities and challenges emerging from the adoption of Big Data Analytics in SCM. 
 
 
 
 
Design/methodology/approach 
 
 
 
 
Although Big Data Analytics is gaining increasing attention in management, empirical research on the topic is still scarce. Due to the limited availability of comparable material at the intersection of Big Data Analytics and SCM, the authors apply the Delphi research technique. 
 
 
 
 
Findings 
 
 
 
 
Portraying the emerging transition trend from a digital business environment, the presented Delphi study findings contribute to extant knowledge by identifying 43 opportunities and challenges linked to the emergence of Big Data Analytics from a corporate and supply chain perspective. 
 
 
 
 
Research limitations/implications 
 
 
 
 
These constructs equip the research community with a first collection of aspects, which could provide the basis to tailor further research at the nexus of Big Data Analytics and SCM. 
 
 
 
 
Originality/value 
 
 
 
 
The research adds to the existing knowledge base as no empirical research has been presented so far specifically assessing opportunities and challenges on corporate and supply chain level with a special focus on the implications imposed through Big Data Analytics.",poster,cp68
Economics,p1133,d11,66372bd949105cc705aec7347269a71f82afadfc,j281,Sustainability,"Big Data, Big Data Analytics Capability, and Sustainable Innovation Performance","Literature suggests that big data is a new competitive advantage and that it enhance organizational performance. Yet, previous empirical research has provided conflicting results. Building on the resource-based view and the organizational inertia theory, we develop a model to investigate how big data and big data analytics capability affect innovation success. We show that there is a trade-off between big data and big data analytics capability and that optimal balance of big data depends upon levels of big data analytics capability. We conduct a four-year empirical research project to secure empirical data on 1109 data-driven innovation projects from the United States and China. This research is the first time reporting the empirical results. The study findings reveal several surprising results that challenge traditional views of the importance of big data in innovation. For U.S. innovation projects, big data has an inverted U-shaped relationship with sales growth. Big data analytics capability exerts a positive moderating effect, that is, the stronger this capability is, the greater the impact of big data on sales growth and gross margin. For Chinese innovation projects, when big data resource is low, promoting big data analytics capability increases sales growth and gross margin up to a certain point; developing big data analytics capability beyond that point may actually inhibit innovation performance. Our findings provide guidance to firms on making strategic decisions regarding resource allocations for big data and big data analytics capability.",fullPaper,jv281
Economics,p1158,d11,d345778be736495187b4fd71205fd8125cd32370,j290,International Journal of Contemporary Hospitality Management,Business intelligence and big data in hospitality and tourism: a systematic literature review,"
Purpose
This paper aims to examine the extent to which Business Intelligence and Big Data feature within academic research in hospitality and tourism published until 2016, by identifying research gaps and future developments and designing an agenda for future research.


Design/methodology/approach
The study consists of a systematic quantitative literature review of academic articles indexed on the Scopus and Web of Science databases. The articles were reviewed based on the following features: research topic; conceptual and theoretical characterization; sources of data; type of data and size; data collection methods; data analysis techniques; and data reporting and visualization.


Findings
Findings indicate an increase in hospitality and tourism management literature applying analytical techniques to large quantities of data. However, this research field is fairly fragmented in scope and limited in methodologies and displays several gaps. A conceptual framework that helps to identify critical business problems and links the domains of business intelligence and big data to tourism and hospitality management and development is missing. Moreover, epistemological dilemmas and consequences for theory development of big data-driven knowledge are still a terra incognita. Last, despite calls for more integration of management and data science, cross-disciplinary collaborations with computer and data scientists are rather episodic and related to specific types of work and research.


Research limitations/implications
This work is based on academic articles published before 2017; hence, scientific outputs published after the moment of writing have not been included. A rich research agenda is designed.


Originality/value
This study contributes to explore in depth and systematically to what extent hospitality and tourism scholars are aware of and working intendedly on business intelligence and big data. To the best of the authors’ knowledge, it is the first systematic literature review within hospitality and tourism research dealing with business intelligence and big data.
",fullPaper,jv290
Economics,p1173,d11,8b58f608261132a950a5e83ec00aa3b3836ccab7,j295,Annals of Applied Statistics,"Statistical paradises and paradoxes in big data (I): Law of large populations, big data paradox, and the 2016 US presidential election","Statisticians are increasingly posed with thought-provoking and even paradoxical questions, challenging our qualiﬁcations for entering the statistical paradises created by Big Data. By developing measures for data quality, this article suggests a framework to address such a question: “Which one should I trust more: a 1% survey with 60% response rate or a self-reported administrative dataset covering 80% of the population?” A 5-element Euler-formula-like identity shows that for any dataset of size n , probabilistic or not, the difference between the sample average X n and the population average X N is the product of three terms: (1) a data quality measure, ρ R,X , the correlation between X j and the response/recording indicator R j ; (2) a data quantity measure, √ (N − n)/n , where N is the population size; and (3) a problem difﬁculty measure, σ X , the standard deviation of X . This decompo-sition provides multiple insights: (I) Probabilistic sampling ensures high data quality by controlling ρ R,X at the level of N − 1 / 2 ; (II) When we lose this control, the impact of N is no longer canceled by ρ R,X , leading to a Law of Large Populations (LLP), that is, our estimation error, relative to the benchmarking rate 1 / √ n , increases with √ N ; and (III) the “bigness” of such Big Data (for population inferences) should be measured by the relative size f = n/N , not the absolute size n ; (IV) When combining data sources for population inferences, those relatively tiny but higher quality ones should be given far more weights than suggested by sizes. Estimates obtained from the Cooperative Congressional Election Study (CCES) of the 2016 US presidential election suggest a ρ R,X ≈ − 0 . 005 for self-reporting to vote for Donald Trump. Because of LLP, this seemingly mi-nuscule data defect correlation implies that the simple sample proportion of the self-reported voting preference for Trump from 1% of the US eligible voters, that is, n ≈ 2,300,000, has the same mean squared error as the corresponding sample proportion from a genuine simple random sample of size n ≈ 400, a 99 . 98% reduction of sample size (and hence our conﬁdence). The CCES data demonstrate LLP vividly: on average, the larger the state’s voter populations, the further away the actual Trump vote shares from the usual 95% conﬁdence intervals based on the sample proportions. This should remind us that, without taking data quality into account, population inferences with Big Data are subject to a Big Data Paradox : the more the data, the surer we fool ourselves.",fullPaper,jv295
Economics,p1212,d11,07ef61804c200c4071ad6fe7c1a21cd67ebfd77a,c115,International Conference on Information Integration and Web-based Applications & Services,Big Data for Development: A Review of Promises and Challenges,"type=""main"" xml:id=""dpr12142-abs-0001""> The article uses a conceptual framework to review empirical evidence and some 180 articles related to the opportunities and threats of Big Data Analytics for international development. The advent of Big Data delivers a cost-effective prospect for improved decision-making in critical development areas such as healthcare, economic productivity and security. At the same time, the well-known caveats of the Big Data debate, such as privacy concerns and human resource scarcity, are aggravated in developing countries by long-standing structural shortages in the areas of infrastructure, economic resources and institutions. The result is a new kind of digital divide: a divide in the use of data-based knowledge to inform intelligent decision-making. The article systematically reviews several available policy options in terms of fostering opportunities and minimising risks.",poster,cp115
Economics,p1274,d11,14f4429d868f9c4a06b0ffb1810e20bc064c95f3,j320,Policy sciences,Big data for policymaking: fad or fasttrack?,Abstract,fullPaper,jv320
Economics,p1325,d11,e968e60740164dcd75731892679939904d3551ff,c116,International Society for Music Information Retrieval Conference,Big Data sources and methods for social and economic analyses,Abstract,poster,cp116
Economics,p1475,d11,235af1d1e142cfbecbd0c64ef8126c97c3cd921f,c8,Frontiers in Education Conference,The value of Big Data in servitization,Abstract,poster,cp8
Economics,p1583,d11,f8928221d290a9cdd84d1de52e121373bc836caa,c48,"Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing",New tools in comparative political economy : the database of political institutions,"This article introduces a large new cross-country database, the database of political institutions. It covers 177 countries over 21 years, 1975-95. The article presents the intuition, construction, and definitions of the different variables. Among the novel variables introduced are several measures of checks and balances, tenure and stability, identification of party affiliation with government or opposition, and fragmentation of opposition and government parties in the legislature.",poster,cp48
Economics,p1602,d11,dd31f1439a0b80cb9447a112347836b3325e953e,c110,Biometrics and Identity Management,The Standardized World Income Inequality Database,"Cross-national research on the causes and consequences of income inequality has been hindered by the limitations of existing inequality datasets: greater coverage across countries and over time is available from these sources only at the cost of significantly reduced comparability across observations. The goal of the Standardized World Income Inequality Database (SWIID) is to overcome these limitations. A custom missing-data algorithm was used to standardize the United Nations University's World Income Inequality Database and data from other sources; data collected by the Luxembourg Income Study served as the standard. The SWIID provides comparable Gini indices of gross and net income inequality for 173 countries for as many years as possible from 1960 to the present along with estimates of uncertainty in these statistics. By maximizing comparability for the largest possible sample of countries and years, the SWIID is better suited to broadly cross-national research on income inequality than previously available sources. 
 
In any papers or publications that use the SWIID, authors are asked to cite the article of record for the data set and give the version number as follows: 
 
Solt, Frederick. 2009. ""Standardizing the World Income Inequality Database."" Social Science Quarterly 90(2):231-242. SWIID Version 3.1, December 2011.",poster,cp110
Economics,p1749,d11,148c4770b5fc2f841179f4c4f800f41c2171841a,c65,International Symposium on Empirical Software Engineering and Measurement,A New Database on the Structure and Development of the Financial Sector,"This article introduces a new database of indicators of financial structure and financial development across countries and over time. The database is unique in that it combines a wide variety of indicators that measure the size, activity, and efficiency of financial intermediaries and markets. It improves on previous efforts by presenting data on the public share of commercial banks, introducing indicators of the size and activity of nonbank financial institutions, and constructing measures of the size of bond and primary equity markets. This article introduces a new database, the first to provide comprehensive measures of the development, structure, and performance of the financial sector. This database is the first to define and construct indicators of the size and activity of nonbank financial intermediaries, such as insurance companies, pension funds, and non-deposit money banks. It is also the first to include indicators of the size of primary equity markets and primary and secondary bond markets. In constructing the database, authors carefully deflate measures and match stock and flow variables.",poster,cp65
Economics,p1795,d11,f1d6cbac2f1ad7443c60c035e1c819bef402c5c7,c74,International Conference on Computational Linguistics,Systemic Banking Crises Database II,Abstract,poster,cp74
Economics,p1811,d11,fa7fa53b825228f29ac80e1a4b3d9c10e5870843,c5,Technical Symposium on Computer Science Education,"Output, Input and Productivity Measures at the Industry Level: The EU KLEMS Database","This article describes the contents and the construction of the EU KLEMS Growth and Productivity Accounts. This database contains industry-level measures of output, inputs and productivity for 25 European countries, Japan and the US for the period from 1970 onwards. The article considers the methodology employed in constructing the database and shows how it can be useful in comparing productivity trends. Although growth accounts are the organising principle, it is argued that the database is useful for a wider range of applications. We give some guidance to prudent use and indicate possible extensions. Copyright © The Author(s). Journal compilation © Royal Economic Society 2009.",poster,cp5
Economics,p1878,d11,b0bf5eb499c483b94efd57135cf9572f2bb4bb8f,c55,Design Automation Conference,Systemic Banking Crises Database; An Update,"We update the widely used banking crises database by Laeven and Valencia (2008, 2010) with new information on recent and ongoing crises, including updated information on policy responses and outcomes (i.e. fiscal costs, output losses, and increases in public debt). We also update our dating of sovereign debt and currency crises. The database includes all systemic banking, currency, and sovereign debt crises during the period 1970-2011. The data show some striking differences in policy responses between advanced and emerging economies as well as many similarities between past and ongoing crises.",poster,cp55
Economics,p1977,d11,8bbb5e29bc76675ae3b73185c17e9077742742ee,j44,Social Science Research Network,A Historical Public Debt Database,"This paper describes the compilation of the first truly comprehensive database on gross government debt-to-GDP ratios, covering nearly the entire IMF membership (174 countries) and spanning an exceptionally long time period. The database was constructed by bringing together a number of other datasets and information from original sources. For the most recent years, the data are linked to the IMF World Economic Outlook (WEO) database to facilitate regular updates. The paper discusses the evolution of debt-to-GDP ratios across country groups for several decades, episodes of debt spikes and reversals, and a pattern of negative correlation between debt and growth.",fullPaper,jv44
Economics,p2006,d11,de6046f58a05a769b5aa526d95a09c5fa5e5b42c,c101,Interspeech,A New Approach to the Economic Analysis of Nonstationary Time Series and the Business Cycle,"This paper models occasional, discrete shifts in the growth rate of a nonstationary series. Algorithms for inferring these unobserved shifts are presented, a byproduct of which permits estimation of parameters by maximum likelihood. An empirical application of this technique suggests that the periodic shift from a positive growth rate to a negative growth rate is a recurrent feature of the U.S. business cycle, and indeed could be used as an objective criterion for defining and measuring economic recessions. The estimated parameter values suggest that a typical economic recession is associated with a 3 percent permanent drop in the level of GNP. Copyright 1989 by The Econometric Society.",poster,cp101
Economics,p2008,d11,309f113adecd8b3d8d1f430a1ec0bfcae8a83c39,c116,International Society for Music Information Retrieval Conference,"The Theory of Economic Development: An Inquiry into Profits, Capital, Credit, Interest, and the Business Cycle","Schumpeter first reviews the basic economic concepts that describe the recurring economic processes of a commercially organized state in which private property, division of labor, and free competition prevail. These constitute what Schumpeter calls ""the circular flow of economic life,"" such as consumption, factors and means of production, labor, value, prices, cost, exchange, money as a circulating medium, and exchange value of money. The principal focus of the book is advancing the idea that change (economic development) is the key to explaining the features of a modern economy. Schumpeter emphasizes that his work deals with economic dynamics or economic development, not with theories of equilibrium or ""circular flow"" of a static economy, which have formed the basis of traditional economics. Interest, profit, productive interest, and business fluctuations, capital, credit, and entrepreneurs can better be explained by reference to processes of development. A static economy would know no productive interest, which has its source in the profits that arise from the process of development (successful execution of new combinations). The principal changes in a dynamic economy are due to technical innovations in the production process. Schumpeter elaborates on the role of credit in economic development; credit expansion affects the distribution of income and capital formation. Bank credit detaches productive resources from their place in circular flow to new productive combinations and innovations. Capitalism inherently depends upon economic progress, development, innovation, and expansive activity, which would be suppressed by inflexible monetary policy. The essence of development consists in the introduction of innovations into the system of production. This period of incorporation or adsorption is a period of readjustment, which is the essence of depression. Both profits of booms and losses from depression are part of the process of development. There is a distinction between the processes of creating a new productive apparatus and the process of merely operating it once it is created. Development is effected by the entrepreneur, who guides the diversion of the factors of production into new combinations for better use; by recasting the productive process, including the introduction of new machinery, and producing products at less expense, the entrepreneur creates a surplus, which he claims as profit. The entrepreneur requires capital, which is found in the money market, and for which the entrepreneur pays interest. The entrepreneur creates a model for others to follow, and the appearance of numerous new entrepreneurs causes depressions as the system struggles to achieve a new equilibrium. The entrepreneurial profit then vanishes in the vortex of competition; the stage is set for new combinations. Risk is not part of the entrepreneurial function; risk falls on the provider of capital. (TNM)",poster,cp116
Economics,p2011,d11,7bd9a25227ca4378b86a78994817988863a54b60,c89,Conference on Uncertainty in Artificial Intelligence,"Business Models, Business Strategy and Innovation",Abstract,poster,cp89
Economics,p2012,d11,da6f0c923bc72e2e0cdcf0f16f11ccfdd0dc0955,c9,Big Data,Shocks and Frictions in US Business Cycles: A Bayesian DSGE Approach,"Using a Bayesian likelihood approach, we estimate a dynamic stochastic general equilibrium model for the US economy using seven macro-economic time series. The model incorporates many types of real and nominal frictions and seven types of structural shocks. We show that this model is able to compete with Bayesian Vector Autoregression models in out-of-sample prediction. We investigate the relative empirical importance of the various frictions. Finally, using the estimated model we address a number of key issues in business cycle analysis: What are the sources of business cycle fluctuations? Can the model explain the cross-correlation between output and inflation? What are the effects of productivity on hours worked? What are the sources of the “Great Moderation”?",poster,cp9
Economics,p2013,d11,e87c3be7aaa1c7ce91f94fda0815339ac02af787,c118,International Conference on Image Analysis and Processing,Value creation in E‐business,"We explore the theoretical foundations of value creation in e‐business by examining how 59 American and European e‐businesses that have recently become publicly traded corporations create value. We observe that in e‐business new value can be created by the ways in which transactions are enabled. Grounded in the rich data obtained from case study analyses and in the received theory in entrepreneurship and strategic management, we develop a model of the sources of value creation. The model suggests that the value creation potential of e‐businesses hinges on four interdependent dimensions, namely: efficiency, complementarities, lock‐in, and novelty. Our findings suggest that no single entrepreneurship or strategic management theory can fully explain the value creation potential of e‐business. Rather, an integration of the received theoretical perspectives on value creation is needed. To enable such an integration, we offer the business model construct as a unit of analysis for future research on value creation in e‐business. A business model depicts the design of transaction content, structure, and governance so as to create value through the exploitation of business opportunities. We propose that a firm's business model is an important locus of innovation and a crucial source of value creation for the firm and its suppliers, partners, and customers. Copyright © 2001 John Wiley & Sons, Ltd.",poster,cp118
Economics,p2014,d11,b5b094b0fc24f4e3d282c9282fb2b506164fe873,c1,International Conference on Human Factors in Computing Systems,The Financial Accelerator in a Quantitative Business Cycle Framework,"This paper develops a dynamic general equilibrium model that is intended to help clarify the role of credit market frictions in business fluctuations, from both a qualitative and a quantitative standpoint. The model is a synthesis of the leading approaches in the literature. In particular, the framework exhibits a financial accelerator,' in that endogenous developments in credit markets work to amplify and propagate shocks to the macroeconomy. In addition, we add several features to the model that are designed to enhance the empirical relevance. First, we incorporate money and price stickiness, which allows us to study how credit market frictions may influence the transmission of monetary policy. In addition, we allow for lags in investment which enables the model to generate both hump-shaped output dynamics and a lead-lag relation between asset prices and investment, as is consistent with the data. Finally, we allow for heterogeneity among firms to capture the fact that borrowers have differential access to capital markets. Under reasonable parametrizations of the model, the financial accelerator has a significant influence on business cycle dynamics.",poster,cp1
Economics,p2018,d11,b4f0ac3f2778963fdbb672de5206bf8eef0c3f11,c82,Symposium on Networked Systems Design and Implementation,"Agency Costs, Net Worth, and Business Fluctuations","This paper develops a simple neoclassical model of the business cycle in which the condition of borrowers' balance sheets is a source of output dynamics. The mechanism is that higher borrower net worth reduces the agency costs of financing real capital investments. Business upturns improve net worth, lower agency costs, and increase investment, which amplifies the upturn; vice versa, for downturns. Shocks that affect net worth (as in a debt-deflation) can initiate fluctuations. Copyright 1989 by American Economic Association.",poster,cp82
Economics,p2020,d11,eb7c608c63e71dac1d0c45711ad52989c54ea1fc,c117,Very Large Data Bases Conference,The Benefits of Lending Relationships: Evidence from Small Business Data,"This paper empirically examines how ties between a firm and its creditors affect the availability and cost of funds to the firm. The authors analyze data collected in a survey of small firms by the Small Business Administration. The primary benefit of building close ties with an institutional creditor is that the availability of financing increases. The authors find smaller effects on the price of credit. Attempts to widen the circle of relationships by borrowing from multiple leaders increases the price and reduces the availability of credit. In sum, relationships are valuable and appear to operate more through quantities rather than prices. Copyright 1994 by American Finance Association.",poster,cp117
Economics,p2023,d11,7f507e2c214f2ace084be54aef78b365ac89727c,c78,ACM/IEEE International Conference on Model Driven Engineering Languages and Systems,Postwar U.S. Business Cycles: An Empirical Investigation,The authors propose a procedure for representing a time series of a smoothly varying trend component and a cyclical component. They document the nature of the comovements of the cyclical components of a variety of macroeconomic time series. The authors find that comovements are very different than the corresponding comovements of the slowly varying trend components. Copyright 1997 by Ohio State University Press.,poster,cp78
Economics,p2026,d11,d2d36f50543c65594548953674c26c96295afdd5,c2,International Conference on Software Engineering,"Strategic Factor Markets: Expectations, Luck, and Business Strategy","Much of the current thinking about competitive strategy focuses on ways that firms can create imperfectly competitive product markets in order to obtain greater than normal economic performance. However, the economic performance of firms does not depend simply on whether or not its strategies create such markets, but also on the cost of implementing those strategies. Clearly, if the cost of strategy implementation is greater than returns obtained from creating an imperfectly competitive product market, then firms will not obtain above normal economic performance from their strategizing efforts. To help analyze the cost of implementing strategies, we introduce the concept of a strategic factor market, i.e., a market where the resources necessary to implement a strategy are acquired. If strategic factor markets are perfect, then the cost of acquiring strategic resources will approximately equal the economic value of those resources once they are used to implement product market strategies. Even if such strategies create imperfectly competitive product markets, they will not generate above normal economic performance for a firm, for their full value would have been anticipated when the resources necessary for implementation were acquired. However, strategic factor markets will be imperfectly competitive when different firms have different expectations about the future value of a strategic resource. In these settings, firms may obtain above normal economic performance from acquiring strategic resources and implementing strategies. We show that other apparent strategic factor market imperfections, including when a firm already controls all the resources needed to implement a strategy, when a firm controls unique resources, when only a small number of firms attempt to implement a strategy, and when some firms have access to lower cost capital than others, and so on, are all special cases of differences in expectations held by firms about the future value of a strategic resource. Firms can attempt to develop better expectations about the future value of strategic resources by analyzing their competitive environments or by analyzing skills and capabilities they already control. Environmental analysis cannot be expected to improve the expectations of some firms better than others, and thus cannot be a source of more accurate expectations about the future value of a strategic resource. However, analyzing a firm's skills and capabilities can be a source of more accurate expectations. Thus, from the point of view of firms seeking greater than normal economic performance, our analysis suggests that strategic choices should flow mainly from the analysis of its unique skills and capabilities, rather than from the analysis of its competitive environment.",poster,cp2
Economics,p2027,d11,605f1c5f799c07f3ed71881de3d71dbe8f0d65cf,c20,ACM Conference on Economics and Computation,The Role of the Business Model in Capturing Value from Innovation: Evidence from Xerox Corporation's Technology Spin-Off Companies,"This paper explores the role of the business model in capturing value from early stage technology. A successful business model creates a heuristic logic that connects technical potential with the realization of economic value. The business model unlocks latent value from a technology, but its logic constrains the subsequent search for new, alternative models for other technologies later on--an implicit cognitive dimension overlooked in most discourse on the topic. We explore the intellectual roots of the concept, offer a working definition and show how the Xerox Corporation arose by employing an effective business model to commercialize a technology rejected by other leading companies of the day. We then show the long shadow that this model cast upon Xerox's later management of selected spin-off companies from Xerox PARC. Xerox evaluated the technical potential of these spin-offs through its own business model, while those spin-offs that became successful did so through evolving business models that came to differ substantially from that of Xerox. The search and learning for an effective business model in failed ventures, by contrast, were quite limited. Copyright 2002, Oxford University Press.",poster,cp20
Economics,p2028,d11,3e3418837b4ae0cf83d5789bc279769305a0156b,c76,Group,Socioemotional Wealth and Business Risks in Family-controlled Firms: Evidence from Spanish Olive Oil Mills,"This paper challenges the prevalent notion that family-owned firms are more risk averse than publicly owned firms. Using behavioral theory, we argue that for family firms, the primary reference point is the loss of their socioemotional wealth, and to avoid those losses, family firms are willing to accept a significant risk to their performance; yet at the same time, they avoid risky business decisions that might aggravate that risk. Thus, we propose that the predictions of behavioral theory differ depending on family ownership. We confirm our hypotheses using a population of 1,237 family-owned olive oil mills in Southern Spain who faced the choice during a 54-year period of becoming a member of a cooperative, a decision associated with loss of family control but lower business risk, or remaining independent, which preserves the family's socioemotional wealth but greatly increases its performance hazard. As shown in this study, family firms may be risk willing and risk averse at the same time.",poster,cp76
Economics,p2031,d11,dfc06f7f7eb83d2c0e2a1d67ecb95b520a406843,c30,PS,The Economics of Small Business Finance: The Roles of Private Equity and Debt Markets in the Financial Growth Cycle,"We examine the economics of financing small business in private equity and debt markets. Firms are viewed through a financial growth cycle paradigm in which different capital structures are optimal at different points in the cycle. We show the sources of small business finance, and how capital structure varies with firm size and age. The interconnectedness of small firm finance is discussed along with the impact of the macroeconomic environment. We also analyze a number of research and policy issues, review the literature, and suggest topics for future research.",poster,cp30
Economics,p2034,d11,72ba4a1054d7c55d187b5b53489cea9cf837173c,c59,Australian Software Engineering Conference,BUSINESS CONDITIONS AND EXPECTED RETURNS ON STOCKS AND BONDS,Abstract,poster,cp59
Economics,p2045,d11,bdadd9dd6a42ff7621ad1a5dd5b8b0c43640a713,c81,ACM Symposium on Applied Computing,Beyond the business case for corporate sustainability,"The article is intended as a contribution to the ongoing conceptual development of corporate sustainability. At the business level sustainability is often equated with eco-efficiency. However, such a reduction misses several important criteria that firms have to satisfy if they want to become truly sustainable.

This article discusses how the concept of sustainable development has evolved over the past three decades and particularly how it can be applied to the business level. It then goes on to describe the three types of capital relevant within the concept of corporate sustainability: economic, natural and social capital.

From this basis we shall then develop the six criteria managers aiming for corporate sustainability will have to satisfy: eco-efficiency, socio-efficiency, eco-effectiveness, socio-effectiveness, sufficiency and ecological equity. The article ends with a brief outlook towards future research. Copyright © 2002 John Wiley & Sons, Ltd. and ERP Environment",poster,cp81
Economics,p2051,d11,66936f2af365b1f0525692f22fdbd77edb7834c2,c56,International Conference on Automated Software Engineering,Entrepreneurial Orientation and Small Business Performance: A Configurational Approach,Abstract,poster,cp56
Economics,p2052,d11,deefd88cefb2d9a185685c02199c04389b3bb202,c89,Conference on Uncertainty in Artificial Intelligence,The Political Business Cycle,Abstract,poster,cp89
Economics,p2059,d11,e74f8912f5da9f4495da80733d02458d2ca93fd7,c6,Annual Conference on Genetic and Evolutionary Computation,Economic Growth and Business Cycles,Abstract,poster,cp6
Economics,p2064,d11,ee13a5550b3a683923fe66ab36d729fafd135050,c111,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,Towards the Sustainable Corporation: Win-Win-Win Business Strategies for Sustainable Development,"From the sustainable development policies of far-sighted governments to the increasing environmental awareness—and cynicism—of consumers, a range of pressures is being brought to bear on business to improve its environmental performance. This article traces the development of some of those pressures, highlighting industries in the firing line, and examining some of the concerns of consumers. It looks at the ways in which companies can turn the environment game into one in which they, their customers, and the environment are all winners. It also explores the rapidly expanding area of corporate environmental reporting, including forms of environmental disclosure, target audiences, and leading exponents of the field.",poster,cp111
Economics,p2066,d11,ae0ce68301de10b0d769972f6f2fb66aa965e3ba,c52,Workshop on Applied Computational Geometry,A Critical Assessment of Business Model Research,"Ever since the Internet boom of the mid-1990s, firms have been experimenting with new ways of doing business and achieving their goals, which has led to a branching of the scholarly literature on business models. Three interpretations of the meaning and function of “business models” have emerged from the management literature: (1) business models as attributes of real firms, (2) business models as cognitive/linguistic schemas, and (3) business models as formal conceptual representations of how a business functions. Relatedly, a provocative debate about the relationship between business models and strategy has fascinated many scholars. We offer a critical review of this now vast business model literature with the goal of organizing the literature and achieving greater understanding of the larger picture in this increasingly important research area. In addition to complementing and extending prior reviews, we also aim at a second and more important contribution: We aim at identifying the reasons behind the apparent lack of agreement in the interpretation of business models, and the relationship between business models and strategy. Whether strategy scholars consider business model research a new field may be due to the fact that the business model perspective may be challenging the assumptions of traditional theories of value creation and capture by focusing on value creation on the demand side and supply side, rather than focusing on value creation on the supply side only as these theories have done. We conclude by discussing how the business model perspective can contribute to research in different fields, offering future research directions.",poster,cp52
Economics,p2067,d11,5c2f6e55bb23f39f098eafa0974d2209dc5f8631,c70,Annual Meeting of the Association for Computational Linguistics,The entrepreneur's business model: toward a unified perspective,Abstract,poster,cp70
Economics,p2068,d11,e8081b45962783b7ca72dcbfe256be39045a32b4,c68,Symposium on Advances in Databases and Information Systems,Indivisible Labor and the Business Cycle,Abstract,poster,cp68
Economics,p2084,d11,9ebd1d80b50f7cb016a31d94d5a341d78bb04718,c8,Frontiers in Education Conference,On the Size Distribution of Business Firms,Abstract,poster,cp8
Economics,p2091,d11,7382e0bf8b27c0156f20c395e69f754c0b414e6e,c37,International Workshop on the Semantic Web,"Investment, Capacity Utilization and the Real Business Cycle","This paper adopts Keynes' view that shocks to the marginal efficiency of i nvestment are important for business fluctuations, but incorporates i t in a neoclassical framework with endogenous capacity utilization. I ncreases in the efficiency of newly produced investment goods stimula te the formation of ""new"" capital and more intensive utilization and accelerated depreciation of ""old"" capital. Theoretical and quantitat ive analysis suggests that the shocks and transmission mechanism stud ied here may be important elements of business cycles. Copyright 1988 by American Economic Association.",poster,cp37
Economics,p2092,d11,ea266f975ea79bd6223040cd4638032e88010974,c83,"International Convention on Information and Communication Technology, Electronics and Microelectronics",Circular Business Model Innovation: Inherent Uncertainties,"Circular business models based on remanufacturing and reuse promise significant cost savings as well as radical reductions in environmental impact. Variants of such business models have been suggested for decades, and there are notable success stories such as the Xerox product–service offering based on photocopiers that are remanufactured. Still, we are not seeing widespread adoption in industry. This paper examines causes for reluctance. Drawing on a hypothesis-testing framework of business model innovation, we show that circular business models imply significant challenges to proactive uncertainty reduction for the entrepreneur. Moreover, we show that many product–service system variants that facilitate return flow control in circular business models further aggravate the potential negative effects of failed uncertainty reduction because of increased capital commitments. Through a longitudinal action research study we also provide a counterexample to many of the challenges identified in previous studies, which could be overcome in the studied case. Copyright © 2015 John Wiley & Sons, Ltd and ERP Environment",poster,cp83
Economics,p2097,d11,f5e4a8d43d337e79f98a34d18c4465d95736e3e2,c71,International Joint Conference on Artificial Intelligence,Credit Spreads and Business Cycle Fluctuations,"This paper examines the evidence on the relationship between credit spreads and economic activity. Using an extensive data set of prices of outstanding corporate bonds trading in the secondary market, we construct a credit spread index that is--compared with the standard default-risk indicators--a considerably more powerful predictor of economic activity. Using an empirical framework, we decompose our index into a predictable component that captures the available firm-specific information on expected defaults and a residual component--the excess bond premium. Our results indicate that the predictive content of credit spreads is due primarily to movements in the excess bond premium. Innovations in the excess bond premium that are orthogonal to the current state of the economy are shown to lead to significant declines in economic activity and equity prices. We also show that during the 2007-09 financial crisis, a deterioration in the creditworthiness of broker-dealers--key financial intermediaries in the corporate cash market--led to an increase in the excess bond premium. These find- ings support the notion that a rise in the excess bond premium represents a reduction in the effective risk-bearing capacity of the financial sector and, as a result, a contraction in the supply of credit with significant adverse consequences for the macroeconomy.",poster,cp71
Economics,p2109,d11,916743e3f5d7ac258aa4af94b7b588414f0f0dd9,c59,Australian Software Engineering Conference,Servitization of business : Adding value by adding services,Abstract,poster,cp59
Economics,p2110,d11,36db80534269df1986a8e3c72b27edec9f0d43aa,j410,Journal of Political Economy,Emerging Market Business Cycles: The Cycle Is the Trend,"Emerging market business cycles exhibit strongly countercyclical current accounts, consumption volatility that exceeds income volatility, and “sudden stops” in capital inflows. These features contrast with developed small open economies. Nevertheless, we show that a standard model characterizes both types of markets. Motivated by the frequent policy regime switches observed in emerging markets, our premise is that these economies are subject to substantial volatility in trend growth. Our methodology exploits the information in consumption and net exports to identify the persistence of productivity. We find that shocks to trend growth—rather than transitory fluctuations around a stable trend—are the primary source of fluctuations in emerging markets. The key features of emerging market business cycles are then shown to be consistent with this underlying income process in an otherwise standard equilibrium model.",fullPaper,jv410
Economics,p2112,d11,a57ec5897b05262a8f2b0d897686dcda7e670adb,c51,International Conference on Engineering Education,Really Uncertain Business Cycles,"We propose uncertainty shocks as a new shock that drives business cycles. First, we demonstrate that microeconomic uncertainty is robustly countercyclical, rising sharply during recessions, particularly during the Great Recession of 2007-2009. Second, we quantify the impact of time-varying uncertainty on the economy in a dynamic stochastic general equilibrium model with heterogeneous firms. We find that reasonably calibrated uncertainty shocks can explain drops and rebounds in GDP of around 3%. Moreover, we show that increased uncertainty alters the relative impact of government policies, making them initially less effective and then subsequently more effective.",poster,cp51
Economics,p2118,d11,40e8f2c79c7be9dd40c3f6429b4c25b6954fc2ae,c71,International Joint Conference on Artificial Intelligence,Ride On! Mobility Business Models for the Sharing Economy,"The public perception of shared goods has changed substantially in the past few years. While co-owning properties has been widely accepted for a while (e.g., timeshares), the notion of sharing bikes, cars, or even rides on an on-demand basis is just now starting to gain widespread popularity. The emerging “sharing economy” is particularly interesting in the context of cities that struggle with population growth and increasing density. While sharing vehicles promises to reduce inner-city traffic, congestion, and pollution problems, the associated business models are not without problems themselves. Using agency theory, in this article we discuss existing shared mobility business models in an effort to unveil the optimal relationship between service providers (agents) and the local governments (principals) to achieve the common objective of sustainable mobility. Our findings show private or public models are fraught with conflicts, and point to a merit model as the most promising alignment of the strengths of agents and principals.",poster,cp71
Economics,p2119,d11,f767a9aee8734f8c5c0459c0816bf4b3587ae996,c79,IEEE Symposium on Computational Intelligence for Engineering Solutions,"The New Political Role of Business in a Globalized World: A Review of a New Perspective on CSR and its Implications for the Firm, Governance, and Democracy","Scholars in management and economics widely share the assumption that business firms focus on profits only, while it is the task of the state system to provide public goods. In this view business firms are conceived of as economic actors, and governments and their state agencies are considered the only political actors. We suggest that, under the conditions of globalization, the strict division of labour between private business and nation-state governance does not hold any more. Many business firms have started to assume social and political responsibilities that go beyond legal requirements and fill the regulatory vacuum in global governance. Our review of the literature shows that there are a growing number of publications from various disciplines that propose a politicized concept of corporate social responsibility. We consider the implications of this new perspective for theorizing about the business firm, governance, and democracy.",poster,cp79
Economics,p2121,d11,d7e453b7d044402f5c57ba0d3b0a2d63dff82e36,c108,IEEE International Conference on Multimedia and Expo,Understanding the small business sector,Abstract,poster,cp108
Economics,p2122,d11,b7057d3cdf3aedc24869e80aff1260b7b64ed454,c0,International Conference on Machine Learning,Divergent Capitalisms: The Social Structuring and Change of Business Systems,"The late twentieth century has witnessed the establishment of new forms of capitalism in East Asia as well as new market economies in Eastern Europe. Despite the growth of international investment and capital flows, these distinctive business systems remain different from each other and from those already developed in Europe and the Americas. This continued diversity of capitalism results from, and is reproduced by, significant differences in societal institutions and agencies such as the state, capital and labour markets, and dominant beliefs about trust, loyalty, and authority. This book presents the comparative business systems framework for describing and explaining the major differences in economic organization between market economies in the late twentieth century. This framework identifies the critical variations in coordination and control systems across forms of industrial capitalism, and shows how these are connected to major differences in their institutional contexts. Six major types of business system are identified and linked to different institutional arrangements. Significant differences in post-war East Asian business systems and the ways in which these are changing in the 1990s are analysed within this framework, which is also extended to compare the path-dependent nature of the new capitalisms emerging in Eastern Europe.",poster,cp0
Economics,p2128,d11,a20c5ef22df7a41e09cd74db706d1dfaa9c67fd3,j410,Journal of Political Economy,International Real Business Cycles,"We ask whether a two-country real business cycle model can account simultaneously for domestic and international aspects of business cycles. With this question in mind, we document a number of discrepancies between theory and data. The most striking discrepancy concerns the correlations of consumption and output across countries. In the data, outputs are generally more highly correlated across countries than consumptions. In the model we see the opposite.",fullPaper,jv410
Economics,p2145,d11,066954d31202b1b39b944786f53ec5cefc26c495,c87,International Conference on Big Data Research,The power of business models,Abstract,poster,cp87
Economics,p2148,d11,6e8624590bff085c6314b954ef93e159900f9a1c,j417,Journal of International Business Studies,Global platforms and ecosystems: Implications for international business theories,Abstract,fullPaper,jv417
Economics,p2151,d11,f8b49867eef3cf013af29e5e5dd64cd7498956e2,j418,Journal of the Knowledge Economy,The Role of Knowledge Economy in African Business,Abstract,fullPaper,jv418
Economics,p2158,d11,b29ab44fd1a0f7a048f7adec14112ef45be4d350,c72,Workshop on Research on Enterprise Networking,Business models and business model innovation: Between wicked and paradigmatic problems,Abstract,poster,cp72
Economics,p2159,d11,9cf0837ecb65622cda37abb078e1f573ac2de27d,c68,Symposium on Advances in Databases and Information Systems,Resuscitating Real Business Cycles,"The Real Business Cycle (RBC) research program has grown spectacularly over the last decade, as its concepts and methods have diffused into mainstream macroeconomics. Yet, there is increasing skepticism that technology shocks are a major source of business fluctuations. This chapter exposits the basic RBC model and shows that it requires large technology shocks to produce realistic business cycles. While Solow residuals are sufficiently volatile, these imply frequent technological regress. Productivity studies permitting unobserved factor variation find much smaller technology shocks, suggesting the imminent demise of real business cycles. However, we show that greater factor variation also dramatically amplifies shocks: a RBC model with varying capital utilization yields realistic business cycles from small, nonnegative changes in technology.",poster,cp68
Economics,p2160,d11,3b86b7019fbfea1d0f97e1d4ea7c0cd1d56c8470,c119,International Conference on Business Process Management,"CAPABILITIES, BUSINESS PROCESSES, AND COMPETITIVE ADVANTAGE: CHOOSING THE DEPENDENT VARIABLE IN EMPIRICAL TESTS OF THE RESOURCE-BASED VIEW","A growing body of empirical literature supports key assertions of the resource-based view. However, most of this work examines the impact of firm-specific resources on the overall performance of a firm. In this paper it is argued that, in some circumstances, adopting the effectiveness of business processes as a dependent variable may be more appropriate than adopting overall firm performance as a dependent variable. This idea is tested by examining the determinants of the effectiveness of the customer service business process in a sample of North American insurance companies. Results are consistent with resource-based expectations, and they show that distinctive advantages observable at the process level are not necessarily reflected in firm level performance. The implications of these findings for research and practice are discussed along with a discussion of the relationship between resources and capabilities, on the one hand, and business processes, activities, and routines, on the other. Copyright © 2003 John Wiley & Sons, Ltd.",poster,cp119
Economics,p2167,d11,d1b059dd8542e5f0cb1e50881b56bf4ab1c6175a,j410,Journal of Political Economy,Real Business Cycles,"In this paper we demonstrate how certain very ordinary economic principles lead maximizing individuals to choose consumption-production plans that display many of the characteristics commonly associated with business cycles. Our explanation is entirely consistent with (i) rational expectations, (ii) complete current information, (iii) stable preferences, (iv) no technological change, (v) no long-lived commodities, (vi) no frictions or adjustment costs, (vii) no government, (viii) no money, and (ix) no serial dependence in the stochastic elements of the environment. We also provide a completely worked out example of the type of artificial economy we have in mind. The time-series properties of the example exhibit some major features of observed business cycles. Although this type of model may not be capable of explaining all of the regularities in actual business cycles, we believe that it provides a useful, well-defined benchmark for assessing the relative importance of factors (e.g., monetary disturbances) that we have deliberately ignored.",fullPaper,jv410
Economics,p2168,d11,9c5ec9b42189f5e740e22ba02f2410a46aaa8258,c73,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,Business model innovation: it's not just about technology anymore,"Purpose – To innovate the company business model, executives must first understand what it is, and then examine what paths exist for them to improve on it. This article aims to examine this issue.Design/methodology/approach – The article provides a practical definition of business models and offers a Business Model Framework (BMF) that illuminates the opportunities for business model innovation.Findings – The article finds that BMF sequences possible business models from very basic (and not very valuable) models to far more advanced (and very valuable) models. Using the BMF, companies can assess where their current business model stands in relation to its potential and then define appropriate next steps for the further advancement of it.Practical implications – An organization must give a senior manager the resources and authority to define and launch business‐model experiments.Originality/value – The article provides a cogent model for assessing the potential for new business model innovation, a framewor...",poster,cp73
Economics,p2171,d11,560bbdd5ef274153e420fa8835acc8a7597e727d,c117,Very Large Data Bases Conference,Non-contractual relations in business: a preliminary study,"Most larger companies, and many smaller ones, attempt to plan carefully and completely. Important transactions not in the ordinary course of business are handled by a detailed contract. For example, recently the Empire State Building was sold for $65 million. More than 100 attorneys, representing thirty-four parties, produced a 400-page contract. Another example is found in the agreement of a major rubber company in the United States to give technical assistance to a Japanese firm. Several million dollars were involved and the contract consisted of eighty-eight provisions on seventeen pages. The twelve house counsel – lawyers who work for one corporation rather than many clients – interviewed said that all but the smallest businesses carefully planned most transactions of any significance. Corporations have procedures so that particular types of exchanges will be reviewed by their legal and financial departments. More routine transactions commonly are handled by what can be called standardized planning. A firm will have a set of terms and conditions for purchases, sales, or both printed on the business documents used in these exchanges. Thus the things to be sold and the price may be planned particularly for each transaction, but standard provisions will further elaborate the performances and cover the other subjects of planning. Typically, these terms and conditions are lengthy and printed in small type on the back of the forms. For example, twenty-four paragraphs in eight-point type are printed on the back of the purchase order form used by the Allis Chalmers Manufacturing Company.",poster,cp117
Economics,p2176,d11,3e183931d6291ad27135f46cd966a43bf45abc56,c39,Online World Conference on Soft Computing in Industrial Applications,The fit between product market strategy and business model: implications for firm performance,"We examine the fit between a firm's product market strategy and its business model. We develop a formal model in order to analyze the contingent effects of product market strategy and business model choices on firm performance. We investigate a unique, manually collected dataset, and find that novelty-centered business models—coupled with product market strategies that emphasize differentiation, cost leadership, or early market entry—can enhance firm performance. Our data suggest that business model and product market strategy are complements, not substitutes. Copyright © 2007 John Wiley & Sons, Ltd.",poster,cp39
Economics,p2188,d11,cff6ed8d129d73b9a499cf0e0f5f42d80f5153e8,c108,IEEE International Conference on Multimedia and Expo,Business Cycles in Emerging Economies: The Role of Interest Rates,Abstract,poster,cp108
Economics,p2197,d11,3d2de1842ceba26fd463ac2189db33a2c6553287,c31,Information Security Solutions Europe,Business sustainability: It is about time,"Sustainability is fast becoming fashionable in strategic management, and yet its meaning is often elusive. Some people restrict sustainability to environmental issues, and others use it synonymously with corporate social responsibility. In this essay, we return to the roots of its original meaning and argue that sustainability requires the consideration of time. Sustainability obliges firms to make intertemporal trade-offs to safeguard intergenerational equity. In this essay, we clarify the meaning of sustainability by showing that the notion of ‘time’ discriminates sustainability from responsibility and other similar concepts. We then argue that the omission of time from most strategic management has contributed to short-termism, which is the bane sustainability. We conclude with directions for future research that will integrate sustainability into strategy and contribute to a world in which both business and society can thrive for generations to come.",poster,cp31
Economics,p2208,d11,506d906aba127097eb5e887eb6fda02f0cda41dc,c79,IEEE Symposium on Computational Intelligence for Engineering Solutions,Theory ahead of business cycle measurement,"Recent developments in business cycle theory are reviewed. The principal finding is that the growth model, which was developed to account for the secular patterns in important economic aggregates, displays the business cycle phenomena once it incorporates the observed randomness in the rate of technological advance. The amplitudes and serial correlation properties of fluctuations in output and employment that the growth model predicts match those historically experienced in the United States. Further, the model continues to display the growth facts it was developed to explain. (This abstract was borrowed from another version of this item.)",poster,cp79
Economics,p2209,d11,c38dbd31202590682b02d7c84e049956849b1900,c34,International Conference on Data Warehousing and Knowledge Discovery,Estimating the performance effects of business groups in emerging markets,"Business groups—confederations of legally independent firms—are ubiquitous in emerging economies, yet very little is known about their effects on the performance of affiliated firms. We conceive of business groups as responses to market failures and high transaction costs. In doing so, we develop hypotheses about the effects of group affiliation on firm profitability: affiliation could either boost or depress firm profitability, and members of a group are likely to earn rates of return similar to other members of the same group. Using a unique data set compiled largely from local sources, we test for these effects in 14 emerging markets: Argentina, Brazil, Chile, India, Indonesia, Israel, Mexico, Peru, the Philippines, South Africa, South Korea, Taiwan, Thailand, and Turkey. We find evidence that business groups indeed affect the broad patterns of economic performance in 12 of the markets we examine. Group affiliation appears to have as profound an effect on profitability as does industry membership, yet strategy scholars have a much clearer grasp of industries than of groups. Moreover, membership in a group raises the profitability of the average group member in several of the markets we examine. This runs contrary to the wisdom, conventional in advanced economies, that unrelated diversification depresses profitability. Overall, our findings suggest that the roots of sustained differences in profitability may vary across institutional contexts; conclusions drawn in one context may well not apply to another. Copyright © 2001 John Wiley & Sons, Ltd.",poster,cp34
Economics,p2210,d11,1e198d3ef814f79923961fe4d0094cc033d62dc7,c71,International Joint Conference on Artificial Intelligence,Business models as models,Abstract,poster,cp71
Economics,p2211,d11,9807ef860267ac0fb0c9eeb575eb54a7e1b1dd3d,c86,International Conference on Big Data and Education,Business Model Innovation and Competitive Imitation: The Case of Sponsor-Based Business Models,"This paper provides the first formal model of business model innovation. Our analysis focuses on sponsor-based business model innovations where a firm monetizes its product through sponsors rather than setting prices to its customer base. We analyze strategic interactions between an innovative entrant and an incumbent where the incumbent may imitate the entrant's business model innovation once it is revealed. The results suggest that an entrant needs to strategically choose whether to reveal its innovation by competing through the new business model, or conceal it by adopting a traditional business model. We also show that the value of business model innovation may be so substantial that an incumbent may prefer to compete in a duopoly rather than to remain a monopolist. Copyright © 2012 John Wiley & Sons, Ltd.",poster,cp86
Economics,p2212,d11,ef66c9edbe3478d5cfca17d3b5fb3193bda6bff4,c37,International Workshop on the Semantic Web,Business Cycles and Labor-Market Search,"The quantitative implications of labor-market search for economic fluctuations are evaluated in the context of a real-business-cycle model. Incorporating labor-market search into the model is found to improve its empirical performance along several dimensions. In particular, hours now fluctuate substantially more than wages and the contemporaneous correlation between hours and productivity falls. In addition, the model replicates the observation that output growth displays positive autocorrelation at short horizons. Overall, the empirical results suggest that the labor-market-search environment embodies a quantitatively important propagation mechanism. Copyright 1996 by American Economic Association.",poster,cp37
Economics,p2213,d11,779df7fc21aafc70ddc26865468809b25d4c8177,c6,Annual Conference on Genetic and Evolutionary Computation,International Journal of Business and Management,Abstract,poster,cp6
Economics,p2218,d11,2ec08804538a439d83ab16ff5afeab459d73bd2c,c84,EUROCON Conference,Business Cases for Sustainability: The Role of Business Model Innovation for Corporate Sustainability,"A considerable body of literature deals with the creation of economic value while increasing corporate environmental and social performance. Some publications even focus on the business case for sustainability which aims at increasing corporate economic value through environmental or social measures. The existence of a business case for sustainability is, however, mostly seen as an ad hoc measure, a supplement to the core business, or simply a coincidence. As a contrast, this paper argues that business model innovations may be required to support a systematic, ongoing creation of business cases for sustainability. A framework for business model innovation is proposed as a means to strategically create business cases on a regular basis as an inherent, deeply integrated element of business activities.",poster,cp84
Economics,p2224,d11,12f3b50fbd64dd4153edd9b677382bc33e6d83b2,c89,Conference on Uncertainty in Artificial Intelligence,Uncertainty and Economic Activity: Evidence from Business Survey Data,"What is the impact of time-varying business uncertainty on economic activity? Using partly confidential business survey data from the U.S. and Germany in structural VARs, we find that positive innovations to business uncertainty lead to prolonged declines in economic activity. In contrast, their high-frequency impact is small. We find no evidence of the ""wait-and-see""-effect - large declines of economic activity on impact and subsequent fast rebounds - that the recent literature associates with positive uncertainty shocks. Rather, positive innovations to business uncertainty have effects similar to negative business confidence innovations. Once we control for their low-frequency effect, we find little statistically or economically significant impact of uncertainty innovations on activity. We argue that high uncertainty events are a mere epiphenomenon of bad economic times: recessions breed uncertainty.",poster,cp89
Economics,p2226,d11,946dff638c0d7b6eba37aaafce84cb1fff13ac01,j406,Journal of Business Ethics,Cryptocurrencies and Business Ethics,Abstract,fullPaper,jv406
Economics,p2231,d11,9338dd8eebc947c7bc62009c2c0d3362158a1f32,c42,IEEE Working Conference on Mining Software Repositories,Network Support and the Success of Newly Founded Business,Abstract,poster,cp42
Economics,p2232,d11,701849b99438d68806565939c4b4b3ea4f29c8ca,c61,"International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems",Growth and Business Cycles I. The Basic Neoclassical Model,Abstract,poster,cp61
Economics,p2233,d11,b9f5205df58691b7964dad01cd7951f712d1765d,c62,International Conference on Advanced Data and Information Engineering,Business Models: A Discovery Driven Approach,Abstract,poster,cp62
Economics,p2236,d11,4a10230aa087c83fb45ba86eb0d3555c3b4e704a,c68,Symposium on Advances in Databases and Information Systems,Survival Chances of Newly Founded Business Organizations,"Uses human capital theory and organizational ecology to explore the success of newly formed firms. Human capital focuses on the firm's founder and his/her background whereas organizational ecology considers the characteristics of the organization and its environmental conditions. Data used in the analysis were collected from 1,849 firm founders in Germany whose firms were formed in 1985-1986. Variables used were survival time, general and specific human capital of the founder, newness of the firm, initial size, organizational strategies, location, branch of industry, and market conditions. Of the firms considered, almost one-fourth had failed in the first two years, and 37% had failed within five years. The firms with founders who had more work experience and schooling improved their chances of survival. Those businesses that were novel were more likely to survive than those firms that were considered followers. Overall, the results show that all human capital variables considered have strong selection effects. (SRD)",poster,cp68
Economics,p2239,d11,cb18f41d47b7a53b421a8dfaf0b4ac063d2d0c22,c0,International Conference on Machine Learning,Entrepreneur Human Capital Inputs and Small Business Longevity,"Small business longevity is investigated utilizing a nationwide random sample of males who entered self-employment between 1976 and 1982. Highly educated entrepreneurs are most likely to create firms that remained in operation through 1986. Owner educational background, further, is a major determinant of the financial capital structure of small business startups. Financial capital endogeneity notwithstanding, firms with the larger financial investments at startup are consistently overrepresented in the survivor column. Firm leverage, finally, is trivial for delineating active from discontinued businesses. Reliance upon debt capital to finance business startup is clearly not associated with heightened risk of failure. Copyright 1990 by MIT Press.",poster,cp0
Economics,p2241,d11,ffb64ee02b9b768f4c16132bff7d4d1f8f08cfca,c47,EUROMICRO Conference on Software Engineering and Advanced Applications,Business and Social Networks in International Trade,"The first two main sections survey the roles of transnational networks in alleviating problems of contract enforcement and providing information about trading opportunities, respectively. The next section covers how domestic networks influence international trade through their impact on domestic market structure. Two overarching questions unify these sections: how do networks affect efficiency, and will networks grow or shrink in importance for international trade over time. The last main sections develop research agendas for two less studied areas: the role of intermediaries who can connect foreign agents to domestic networks and the ability of transnational production networks to facilitate technology transfer.",poster,cp47
Economics,p2244,d11,c48c2e32cf8bd108db43873f5407469365f5681b,c74,International Conference on Computational Linguistics,Theorising from case studies: Towards a pluralist future for international business research,Abstract,poster,cp74
Economics,p2246,d11,ab9417daa1824aa463b322cbc92bc53236ea6c92,c82,Symposium on Networked Systems Design and Implementation,Can News About the Future Drive the Business Cycle?,"We propose a model that generates an economic expansion in response to good news about future total factor productivity (TFP) or investment-specific technical change. The model has three key elements: variable capital utilization, adjustment costs to investment, and preferences that exhibit a weak short-run wealth effect on the labour supply. These preferences nest the two classes of utility functions most widely used in the business cycle literature as special cases. Our model can generate recessions that resemble those of the post-war U.S. economy without relying on negative productivity shocks. The recessions are caused not by contemporaneous negative shocks but rather by lackluster news about future TFP or investment-specific technical change.",poster,cp82
Economics,p2249,d11,df28bb9eb29efd2d12ee173b4b723c0aad9cae94,c119,International Conference on Business Process Management,Intellectual capital and business performance in Malaysian industries,"The purpose of this empirical study is to investigate the three elements of intellectual capital, i.e. human capital, structural capital, and customer capital, and their inter‐relationships within two industry sectors in Malaysia. The study was conducted using a psychometrically validated questionnaire which was originally administered in Canada. The main conclusions from this particular study are that: human capital is important regardless of industry type; human capital has a greater influence on how a business should be structured in non‐service industries compared to service industries; customer capital has a significant influence over structural capital irrespective of industry; and finally, the development of structural capital has a positive relationship with business performance regardless of industry. The final specified models in this study show a robust explanation of business performance variance within the Malaysian context which bodes well for future research in alternative contexts.",poster,cp119
Economics,p2253,d11,845b6079297f2b9aba8619c6b9b3980f6f516f30,c104,North American Chapter of the Association for Computational Linguistics,Search in the labor market and the real business cycle,Abstract,poster,cp104
Economics,p2261,d11,95abf3e9a9425c63954ff492dac218e8d54c0998,c55,Design Automation Conference,Conceptualizing a “Sustainability Business Model”,"According to one perspective, organizations will only be sustainable if the dominant neoclassical model of the firm is transformed, rather than supplemented, by social and environmental priorities. This article seeks to develop a “sustainability business model” (SBM)—a model where sustainability concepts shape the driving force of the firm and its decision making. The SBM is drawn from two case studies of organizations considered to be leaders in operationalizing sustainability and is informed by the ecological modernization perspective of sustainability. The analysis reveals that organizations adopting a SBM must develop internal structural and cultural capabilities to achieve firm-level sustainability and collaborate with key stakeholders to achieve sustainability for the system that an organization is part of.",poster,cp55
Economics,p2267,d11,3a78b9b57b0076ace1c394c44db9b0008acd48a6,c13,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,"Does Diversity Pay?: Race, Gender, and the Business Case for Diversity","The value-in-diversity perspective argues that a diverse workforce, relative to a homogeneous one, is generally beneficial for business, including but not limited to corporate profits and earnings. This is in contrast to other accounts that view diversity as either nonconsequential to business success or actually detrimental by creating conflict, undermining cohesion, and thus decreasing productivity. Using data from the 1996 to 1997 National Organizations Survey, a national sample of for-profit business organizations, this article tests eight hypotheses derived from the value-in-diversity thesis. The results support seven of these hypotheses: racial diversity is associated with increased sales revenue, more customers, greater market share, and greater relative profits. Gender diversity is associated with increased sales revenue, more customers, and greater relative profits. I discuss the implications of these findings relative to alternative views of diversity in the workplace.",poster,cp13
Economics,p2271,d11,d967052511f16f8ddde267191e9687bae3780c67,c11,European Conference on Modelling and Simulation,"Detrending, stylized facts and the business cycle","The stylized facts of macroeconomic time series can be presented by fitting structural time series models. Within this framework, we analyze the consequences of the widely used detrending technique popularized by Hodrick and Prescott (1980). It is shown that mechanical detrending based on the Hodrick-Prescott filter can lead investigators to report spurious cyclical behavior, and this point is illustrated with empirical examples. Structural time-series models also allow investigators to deal explicitly with seasonal and irregular movements that may distort estimated cyclical components. Finally, the structural framework provides a basis for exposing the limitations of ARIMA methodology and models based on a deterministic trend with a single break. Copyright 1993 by John Wiley & Sons, Ltd.",poster,cp11
Economics,p2273,d11,c5649d35e9c0bdb202af2cb3822280309f31f3dd,c109,Computer Vision and Pattern Recognition,The Financing of Business Start-Ups,Abstract,poster,cp109
Economics,p2275,d11,156e47bbb31d086b681e233041b9216eaa38e7a5,j426,IMF Economic Review,How Firms Respond to Business Cycles: The Role of Firm Age and Firm Size,Abstract,fullPaper,jv426
Economics,p2277,d11,afd31abd2c6fcda3ff895e664479394152382a0e,c55,Design Automation Conference,Investment Shocks and Business Cycles,"Shocks to the marginal efficiency of investment are the most important drivers of business cycle fluctuations in U.S. output and hours. Moreover, like a textbook demand shock, these disturbances drive prices higher in expansions. We reach these conclusions by estimating a dynamic stochastic general equilibrium (DSGE) model with several shocks and frictions. We also find that neutral technology shocks are not negligible, but their share in the variance of output is only around 25 percent and even lower for hours. Labor supply shocks explain a large fraction of the variation of hours at very low frequencies, but not over the business cycle. Finally, we show that imperfect competition and, to a lesser extent, technological frictions are the key to the transmission of investment shocks in the model.",poster,cp55
Economics,p2281,d11,f7ee394987b0a0e7d4b948c10a87b140c4ce5fd6,c69,Neural Information Processing Systems,"Small business performance: business, strategy and owner‐manager characteristics","Purpose: This paper aims to contribute to the understanding of the factors that influence small to medium-sized enterprise (SME) performance and particularly, growth. Design/methodology/approach: This paper utilises an original data set of 360 SMEs employing 5-249 people to run logit regression models of employment growth, turnover growth and profitability. The models include characteristics of the businesses, the owner-managers and their strategies. Findings: The results suggest that size and age of enterprise dominate performance and are more important than strategy and the entrepreneurial characteristics of the owner. Having a business plan was also found to be important. Research limitations/implications: The results contribute to the development of theoretical and knowledge bases, as well as offering results that will be of interest to research and policy communities. The results are limited to a single survey, using cross-sectional data. Practical implications: The findings have a bearing on business growth strategy for policy makers. The results suggest that policy measures that promote the take-up of business plans and are targeted at younger, larger-sized businesses may have the greatest impact in terms of helping to facilitate business growth. Originality/value: A novel feature of the models is the incorporation of entrepreneurial traits and whether there were any collaborative joint venture arrangements.",poster,cp69
Economics,p2284,d11,ea22328d41a062740e3164f5dc9ce450da316a31,c22,Grid Computing Environments,Agency Problems in Large Family Business Groups,"Greater managerial ownership in family firms need not mitigate agency problems, especially when each family controls a group of publicly traded and private firms, as is the case in most countries. Such structures give rise to their own set of agency problems, as managers act for the controlling family, but not for shareholders in general. For example, to avoid what we call “creative self–destruction,” a family might quash innovation in one firm to protect its obsolete investment in another. At present, we do not know whether these agency problems are more or less serious impediments to general prosperity than those afflicting widely held firms.",poster,cp22
Economics,p2290,d11,88d3cf3b59802170c34c8811ea5cd2e8404f1721,c70,Annual Meeting of the Association for Computational Linguistics,"Habit Persistence, Asset Returns, and the Business Cycle","Two modifications are introduced into the standard real-business-cycle model: habit preferences and a two-sector technology with limited intersectoral factor mobility. The model is consistent with the observed mean risk-free rate, equity premium, and Sharpe ratio on equity. In addition, its business-cycle implications represent a substantial improvement over the standard model. It accounts for persistence in output, comovement of employment across different sectors over the business cycle, the evidence of ""excess sensitivity"" of consumption growth to output growth, and the ""inverted leading-indicator property of interest rates,"" that interest rates are negatively correlated with future output.",poster,cp70
Economics,p2294,d11,532f7d60a06a4dccf94ad715d6a5dd7a248eaef3,c20,ACM Conference on Economics and Computation,On the Size Distribution of Business Firms,"This paper proposes a new theory of the size distributions of business firms. It postulates an underlying distribution of persons by managerial ""talent"" and then studies the division of persons into managers and employees and the allocation of productive factors across managers. The implications of the theory for secular changes in average firm size are developed and tested on U.S. time series.",poster,cp20
Economics,p2295,d11,427c62ecce53ea86a6ab37b94f38ec96f78174db,c43,European Conference on Machine Learning,UNDERSTANDING BUSINESS CYCLES,Abstract,poster,cp43
Economics,p2299,d11,e0af559aa1ffc8e01dcd0b146166eb4a8031b3fe,c115,International Conference on Information Integration and Web-based Applications & Services,Value-based Business Strategy,"This paper offers an exact definition of the value created by firms together with their suppliers and buyers. The ""added value"" of a firm is similarly defined, and shown under certain conditions to impose an upper bound on how much value the firm can capture. The key to a firm's achieving a positive added value is the existence of asymmetries between the firm and other firms. The paper identifies four routes (""value-based"" strategies) that lead to the creation of such asymmetries. Our analysis reveals the equal importance of a firm's supplier and buyer relations. Cooperative game theory provides the underpinnings of the analysis. Copyright 1996 The Massachusetts Institute of Technology.",poster,cp115
Economics,p2303,d11,9de447d9eb9bf7adc8c449abe2d416aaffc7694e,c75,International Conference on Predictive Models in Software Engineering,Tunneling or Value Addition? Evidence from Mergers by Korean Business Groups,"Business groups in emerging markets have the potential to create either value or agency problems. Using Korean mergers, we investigate the nature of business groups in emerging markets and examine whether Korean business groups (chaebols) add value to their member firms or provide the controlling shareholders with an opportunity for wealth transfer (tunneling). We show that chaebol-affiliated firms that performed well prior to the merger realize significantly negative announcement returns. We also find that chaebol bidders who acquired poorly performing targets within the same group and/or had concentrated equity ownership by owner-managers experience significantly negative abnormal returns. These types of mergers, however, have a significantly positive effect on the market value of the portfolio of other firms in the group. Our results support the tunneling view that firms belonging to business groups pay less attention to the maximization of individual firm value and make takeover decisions that are beneficial to only controlling shareholders.",poster,cp75
Economics,p2307,d11,2237adb3dc2bb568f5221a17526d0d2719cb1ce1,c15,Pacific Symposium on Biocomputing,Disaster Risk and Business Cycles,"To construct a business cycle model consistent with the observed behavior of asset prices, and study the effect of shocks to aggregate uncertainty, I introduce a small, time-varying risk of economic disaster in a standard real business cycle model. The paper establishes two simple theoretical results: first, when the probability of disaster is constant, the risk of disaster does not affect the path of macroeconomic aggregates - a ""separation theorem"" between macroeconomic quantities and asset prices in the spirit of Tallarini (2000). Second, shocks to the probability of disaster, which generate variation in risk premia over time, are observationally equivalent to preference shocks. An increase in the perceived probability of disaster leads to a collapse of investment and a recession, an increase in risk spreads, and a decrease in the yield on safe assets. To assess the empirical validity of the model, I infer the probability of disaster from observed asset prices and feed it into the model. The variation over time in this probability appears to account for a significant fraction of business cycle dynamics, especially sharp downturns in investment and output such as 2008-IV.",poster,cp15
Economics,p2311,d11,94cb9f9014b7c3f522e79eafbbab92cfb843b0cc,c40,European Conference on Computer Vision,Does business planning facilitate the development of new ventures,"Many prior researchers have criticized business planning, arguing that it interferes with the efforts of firm founders to undertake more valuable actions to develop their fledgling enterprises. In this paper, we challenge this negative view of business planning, arguing that business planning is an important precursor to action in new ventures. By helping firm founders to make decisions, to balance resource supply and demand, and to turn abstract goals into concrete operational steps, business planning reduces the likelihood of venture disbanding and accelerates product development and venture organizing activity. Empirically, we examine 223 new ventures initiated in the first 9 months of 1998 by a random sample of Swedish firm founders and provide support for our hypotheses. Copyright © 2003 John Wiley & Sons, Ltd.",poster,cp40
Economics,p2319,d11,027a340530c545490e85f34afce20b2fa491b292,c76,Group,Detrending and business cycle facts,Abstract,poster,cp76
Economics,p2320,d11,2be12a0c84aec1488712c03eb8ee3019b8c44f1b,c99,Symposium on the Theory of Computing,Qualitative research for international business,Abstract,poster,cp99
Economics,p2321,d11,8d6b828df043dd18517acefe2fc55229d699e80a,c61,"International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems",The Role of the Business Press as an Information Intermediary,"ABSTRACT This paper investigates whether the business press serves as an information intermediary. The press potentially shapes firms' information environments by packaging and disseminating information, as well as by creating new information through journalism activities. We find that greater press coverage reduces information asymmetry (i.e., lower spreads and greater depth) around earnings announcements, with broad dissemination of information having a bigger impact than the quantity or quality of press-generated information. These results are robust to controlling for firm-initiated disclosures, market reactions to the announcement, and other information intermediaries. Our findings suggest that the press helps reduce information problems around earnings announcements. Copyright (c), University of Chicago on behalf of the Accounting Research Center, 2009.",poster,cp61
Economics,p2327,d11,d73484367ab48b26c110ad7967f084d66cf3909f,c71,International Joint Conference on Artificial Intelligence,Business model innovation in entrepreneurship,Abstract,poster,cp71
Economics,p2333,d11,52a74e3cbdedcb45ca03904804bd8d6f3dcc1ab9,c75,International Conference on Predictive Models in Software Engineering,Frontiers of business cycle research,Abstract,poster,cp75
Economics,p2334,d11,65f951df1e7293a3a7708489ff6018581cc09dec,c13,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,A Framework for Integrated Risk Management in International Business,Abstract,poster,cp13
Economics,p2336,d11,24a9f6eb25683ffc3f399bd5a5d2a64b784eacb7,c63,International Conference on Evaluation & Assessment in Software Engineering,Why Companies Should Have Open Business Models,"Because of two trends ? rising R&D costs and decreased product revenues (due to shorter product life cycles) ? companies are finding it increasingly difficult to justify investments in innovation. Business models that embrace open innovation address both issues. The development costs of innovation are reduced by the greater use of external technology in a firm?s own R&D process. This saves time, as well as money. And the firm no longer restricts itself to the markets it serves directly. Now it participates in other segments through licensing fees, joint ventures and spinoffs, among other means. These different streams of income create more overall revenue from the innovation. 

To partake more fully in the benefits of open innovation, companies need to develop the ability to experiment with their business models, finding ways to open them up. Building that capability requires the creation of processes for conducting experiments and for assessing their results. Although that might seem obvious, many companies simply do not have such processes in place. In most organizations, no single person short of the CEO bears responsibility for the business model. Instead, business unit managers (who are usually posted to their jobs for just two to three years) tend to take the business model for granted.

To understand how an organization can open its business model, the author provides case examples of IBM, P&G and Air Products, three companies that operate in different industries with vastly different technologies and products. Each used to function with a very internally focused, closed business model. And each has since migrated to a business model that is substantially more open.",poster,cp63
Economics,p2342,d11,3ff1a98c2861e95c0e548a30a0ba2207d19af115,c121,International Conference on Interaction Sciences,Business Cycle Fluctuations in U.S. Macroeconomic Time Series,"This paper examines the empirical relationship in the postwar United States between the aggregate business cycle and various aspects of the macroeconomy, such as production, interest rates, prices, productivity, sectoral employment, investment, income, and consumption. This is done by examining the strength of the relationship between the aggregate cycle and the cyclical components of individual time series, whether individual series lead or lag the cycle, and whether individual series are useful in predicting aggregate fluctuations. The paper also reviews some additional empirical regularities in the U.S. economy, including the Phillips curve and some long-run relationships, in particular long-run money demand, long-run properties of interest rates and the yield curve, and the long-run properties of the shares in output of consumption, investment and government spending.",poster,cp121
Economics,p2346,d11,1658943bb5ae49fd91f9f5e0a3d368f3f36d424e,c6,Annual Conference on Genetic and Evolutionary Computation,Building a business case for diversity,"Executive Overview While most companies acknowledge the importance of making diversity a business consideration, diversity is often not a top business priority. Other business initiatives that present more compelling, factual evidence of payback on investment win out over diversity initiatives, which seem to offer less predictable and tangible benefits. As a result, many human resource executives revert to the argument that “it's the right thing to do” and trust that management will back their suggestions to promote a diversity-friendly work environment, then wonder why nothing happens or why well-intended initiatives fail. The presentation of a solid business case increases the likelihood of obtaining the leadership commitment and resources needed to successfully implement diversity initiatives.",poster,cp6
Economics,p2348,d11,e0db9cf68f6729c35752d6228eba0d99e03bc268,c21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,What's News in Business Cycles,"In this paper, we perform a structural Bayesian estimation of the contribution of anticipated shocks to business cycles in the postwar United States. Our theoretical framework is a real-business-cycle model augmented with four real rigidities: investment adjustment costs, variable capacity utilization, habit formation in consumption, and habit formation in leisure. Business cycles are assumed to be driven by permanent and stationary neutral productivity shocks, permanent investment-specific shocks, and government spending shocks. Each of these driving forces is buffeted by four types of structural innovations: unanticipated innovations and innovations anticipated one, two, and three quarters in advance. We find that anticipated shocks account for more than two thirds of predicted aggregate fluctuations.",poster,cp21
Economics,p2351,d11,e6bcf825b60de0d8897dd26462aa9775318a394a,c83,"International Convention on Information and Communication Technology, Electronics and Microelectronics",The resource-based view and international business:,Abstract,poster,cp83
Economics,p2357,d11,2a851e16673f72db684fdcf9ba1484d3e653a4b4,c55,Design Automation Conference,Bank Profitability and the Business Cycle,"An important element of the macro-prudential analysis is the study of the link between business cycle fluctuations and banking sector profitability and how this link is affected by institutional and structural characteristics. This work estimates a set of equations for net interest income, non-interest income, operating costs, provisions, and profit before taxes, for banks in the main industrialized countries and evaluates the effects on banking profitability of shocks to both macroeconomic and financial factors. Distinguishing mainly the euro area from Anglo-Saxon countries, the analysis also identifies differences in the resilience of the respective banking systems and relates them to the characteristics of their financial structure.",poster,cp55
Economics,p2359,d11,e060b334c3fcf868b4c72317aa98547c0194d93b,c52,Workshop on Applied Computational Geometry,Discriminant Analysis Of Predictors Of Business Failure,Abstract,poster,cp52
Economics,p2360,d11,dd24ef22f3a975085af0520c978d6fc4658d82f8,c77,Visualization for Computer Security,Cross-cultural competence in international business: toward a definition and a model,Abstract,poster,cp77
Economics,p2363,d11,f8492ca76de62647a874fb75c24626f6c56312bf,c80,International Symposium on Software Engineering for Adaptive and Self-Managing Systems,Diversification Discount or Premium? New Evidence from the Business Information Tracking Series,"I use the Business Information Tracking Series (BITS), a new census database that covers the whole U.S. economy at the establishment level, to examine whether the finding of a diversification discount is an artifact of segment data. BITS data allow me to construct business units that are more consistently and objectively defined than segments, and thus more comparable across firms. Using these data on a sample that yields a discount according to segment data, I find a diversification premium. The premium is robust to variations in the sample, business unit definition, and measures of excess value and diversification. Copyright 2004 by The American Finance Association.",poster,cp80
Economics,p2365,d11,12a55872b460a62216a07b9f459c77b29a3cf347,c81,ACM Symposium on Applied Computing,Medium Term Business Cycles,"Over the postwar, the U.S., Europe and Japan have experienced what may be thought of as medium frequency oscillations between persistent periods of robust growth and persistent periods of relative stagnation. These medium frequency movements, further, appear to bear some relation to the high frequency volatility of output. That is, periods of stagnation are often associated with significant recessions, while persistent booms typically are either free of recessions or are interrupted only by very modest downturns. In this paper we explore the idea of medium term cycles, which we define as reflecting the sum of the high and medium frequency variation in the data. We develop a methodology for identifying these kinds of fluctuations and then show that a number of important macroeconomic time series exhibit significant medium term cycles. The cycles feature strong procyclical movements in both disembodied and embodied technological change, research & development, and the efficiency of resource utilization. We then develop a model to explain the medium term cycle that features both disembodied and embodied endogenous technological change, along with countercyclical markups and variable factor utilization. The model is able to generate medium term fluctuations in output, technological change, and resource utilization that resemble the data, with a non-technological shock as the exogenous disturbance. In particular, the model offers a unified approach to explaining both high and medium frequency variation in aggregate business activity.",poster,cp81
Economics,p2373,d11,3ed3259b1aba93d795d4a2e5b1251fb54d977bbf,c89,Conference on Uncertainty in Artificial Intelligence,Private Global Business Regulation,"Regulations that govern the social and environmental impacts of global firms and markets without state enforcement are a relatively new dimension of global business regulation. The growth of such voluntary “civil regulations” reflects both the expansion of legitimate authority in the global economy outside the state and the increasing use of alternative regulatory instruments to govern firms, including self-regulation, market-based instruments, and soft laws. In response to global social activism, many firms have adopted voluntary regulatory standards to avoid additional regulation and/or to protect their reputations and brands. Activists have targeted highly visible firms and have been willing to work cooperatively with them. The most important civil regulations are multi-stockholder codes, whose governance is shared by firms and nongovernmental organizations (NGOs), and which rely on product and producer certifications. Such codes face the challenge of acquiring legitimacy and of persuading both firms a...",poster,cp89
Economics,p2376,d11,033cc9907ac3fd423df22e1deab608b896b633ca,c51,International Conference on Engineering Education,Redefining Business Success: Distinguishing Between Closure and Failure,Abstract,poster,cp51
Economics,p2377,d11,fac3a33e6d8ff538b93bdb0194da641d49e97b53,c24,International Conference on Data Technologies and Applications,Taxes and Business Strategy: A Planning Approach,"Chapter 1 Introduction to Tax Strategy Chapter 2 Tax Law Fundamentals Chapter 3 Returns on Alternative Savings Vehicles Chapter 4 Choosing the Optimal Organizational Form Chapter 5 Implicit Taxes and Clienteles, Arbitrage, Restrictions, and Frictions Chapter 6 Nontax Costs of Tax Planning Chapter 7 The Importance of Marginal Tax Rates and Dynamic Tax Planning Considerations Chapter 8 Compensation Planning Chapter 9 Pension and Retirement Planning Chapter 10 Multinational Tax Planning: Introduction and Investment Decisions Chapter 11 Multinational Tax Planning: Foreign Tax Credit Limitations and Income Shifting Chapter 12 Corporations: Formation, Operation, Capital Structure, and Liquidation Chapter 13 Introduction to Mergers, Acquisitions, and Divestitures Chapter 14 Taxable Acquisitions of Freestanding C Corporations Chapter 15 Taxable Acquisitions of S Corporations Chapter 16 Tax-Free Acquisitions of Freestanding C Corporations Chapter 17 Tax Planning for Divestitures Chapter 18 Estate and Gift Tax Planning",poster,cp24
Economics,p2381,d11,b13f60449ec02692356b7e1171e4d440a1793fcd,c65,International Symposium on Empirical Software Engineering and Measurement,Economic Development and Business Ownership: An Analysis Using Data of 23 OECD Countries in the Period 1976–1996,Abstract,poster,cp65
Economics,p2389,d11,f3df94823f1eec4b2a0a0e939cb9bc45d44559fd,c69,Neural Information Processing Systems,Financial Market Imperfections and Business Cycles,"This paper develops a simple model of macroeconomic behavior which incorporates the impact of financial market ""imperfections,"" such as those generated by asymmetric information in financial markets. These information asymmetries may lead to breakdowns in markets, like that for equity, in which risks arm shared. In particular, we analyze firm behavior in the presence of equity rationing and imperfect futures markets, in which there are lags in production. Aft a consequence, firms act in a risk-averse manner. We trace out the macroeconomic consequences, and show that they are able to account for many of the widely observed aspects of actual business cycles.",poster,cp69
Economics,p2396,d11,5825c075a8a8e5735251ecf54ccc7cfcc5f57f9c,c4,Conference on Innovative Data Systems Research,The Inflation Tax in a Real Business Cycle Model,"Money is incorporated into a real business cycle model using a cash-in-advance constraint. The model is used to analyze whether the business cycle is different in high-inflation and low-inflation economies and to analyze the impact of variability in the growth rate of money. The welfare cost of the inflation tax is measured and the steady-state properties of high and low inflation economies are compared. The welfare cost of a sustained (10 percent) inflation is estimated to be between 0.11 percent and 0.4 percent of GNP. The features of the business cycle are the same in high and low inflation economies, but the steady-state paths may be quite different. Copyright 1989 by American Economic Association.",poster,cp4
Economics,p2397,d11,48a42eb30eed922a0d0100f3c6309e54749b3a82,j410,Journal of Political Economy,Are Economic Time Series Asymmetric over the Business Cycle?,It has long been argued that major cyclical variables such as the unemployment rate display an asymmetric behavior over various phases of the business cycle. The paper provides a statistical test for this hypothesis. Using the framework of finite state Markov processes I implement a test to see if the behavior of the unemployment rate is characterized by sudden jumps and slower drops. It is argued that the framework provided in the paper can also be used to test other sample path properties of economic time series.,fullPaper,jv410
Economics,p2399,d11,a547431a3e5bcc09b08375c2a5ad09ca200e33c1,c73,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,Measurement of Business Economic Performance: An Examination of Method Convergence,"Strategic management researchers have measured business economic performance (BEP) through either perceptual assessments of senior executives or secondary data sources, but few explicitly evaluate the degree of convergence across methods. In an effort to examine method convergence, we collected data on three dimensions-sales growth, net income growth, and profitability (ROI) using both methods. Although convergent and discriminant validity were achieved using Campbell and Fiske's Multi Trait, Multi Method (MTMM) and Confiratory Factor Analysis (CFA), the approaches yielded different insights. The advantages of CFA over MTMM is demonstrated with implications for strategy research.",poster,cp73
Economics,p2400,d11,a547431a3e5bcc09b08375c2a5ad09ca200e33c1,c106,International Conference on Biometrics,Measurement of Business Economic Performance: An Examination of Method Convergence,"Strategic management researchers have measured business economic performance (BEP) through either perceptual assessments of senior executives or secondary data sources, but few explicitly evaluate the degree of convergence across methods. In an effort to examine method convergence, we collected data on three dimensions-sales growth, net income growth, and profitability (ROI) using both methods. Although convergent and discriminant validity were achieved using Campbell and Fiske's Multi Trait, Multi Method (MTMM) and Confiratory Factor Analysis (CFA), the approaches yielded different insights. The advantages of CFA over MTMM is demonstrated with implications for strategy research.",poster,cp106
Economics,p2409,d11,83cc11316cf027e017791d50ca68ba012ee6bda6,j429,NBER macroeconomics annual,Sources of Business Cycle Fluctuations,"What shocks account for the business cycle frequency and long-run movements of output and prices? This paper addresses this question using the identifying assumption that only supply shocks, such as shocks to technology, oil prices, and labor supply affect output in the long-run. Real and monetary aggregate demand shocks can affect output, but only in the short-run. This assumption sufficiently restricts the reduced form of key macroeconomic variables to allow estimation of the shocks and their effect on output and price at all frequencies. Aggregate demand shocks account for about 20 percent to 30 percent of output fluctuations at business cycle frequencies. Technological shocks account for about 1/4 of cyclical fluctuations, and about 1/3 of output's variance at low frequencies. Shocks to oil prices are important in explaining episodes in the 1970s and 1980s. Shocks that permanently affect labor input account for the balance of fluctuations in output, namely, about half of its variance at all frequencies.",fullPaper,jv429
Economics,p2419,d11,b8463dae9ea96a3d6e7236f73a0cf69cd4083a0c,j410,Journal of Political Economy,An Equilibrium Model of the Business Cycle,"This paper develops a theoretical example of a business cycle, that is, a model economy in which real output undergoes serially correlated movements about trend which are not explainable by movements in the availability of factors of production. The mechanism generating these movements involves unsystematic monetary-fiscal shocks, the effects of which are distributed through time due to information lags and an accelerator effect. Associated with these output movements are procyclical movements in prices, procyclical movements in the share of output devoted to investment, and, in a somewhat limited sense, procyclical movements in nominal rates of interest.",fullPaper,jv410
Economics,p2425,d11,1d5c700b8e948dc0169699fb7724986e97e44b03,c0,International Conference on Machine Learning,A Near-Rational Model of the Business Cycle,"This paper presents a model in which insignificantly suboptimal behavior causes aggregate demand shocks to have significant real effects. The individual loss to agents with inertial price-wage behavior is second-order in terms of the parameter describing the shock, while the effect on real economic variables is first-order. Thus, significant changes in business activity can be generated by anticipated money supply changes provided that some agents are willing to engage in nonmaximizing behavior which results in small losses.",poster,cp0
Economics,p2438,d11,c444fa682b01ecaca64e6133e5fd51b7d6be63c8,c3,Knowledge Discovery and Data Mining,Responsibility: The new business imperative,"Executive Overview Businesses today are experiencing profound pressures to reform and improve stakeholder-related practices and their impacts on stakeholders and the natural environment--in short, to manage responsibly as well as profitably. Pressures for expanding the emphasis on profits to managing responsibly derive from three general sources: primary stakeholders such as owners, employees, customers, and suppliers; secondary stakeholders such as non-governmental organizations (NGOs), activists, communities, and governments; and general societal trends and institutional forces. The latter include a proliferation of 'best of' rankings, the steady emergence and development of global principles and standards that are raising public expectations about corporate responsibility, and new reporting initiatives emphasizing the so-called triple bottom lines of economic, social, and environmental performance. To respond to these pressures, many multinational corporations (MNCs) in particular are developing what w...",poster,cp3
Economics,p2440,d11,248d9215e5745b768d67007dcbc0315650acb47f,c19,International Conference on Conceptual Structures,"Business Behavior, Value, and Growth",Abstract,poster,cp19
Economics,p2449,d11,b9d46245c23fd6ff9d7f0f72d2a43a2e4591eb57,j118,Review of Economics and Statistics,Entrepreneurship and the Business Cycle,"Abstract We find new empirical regularities in the business cycle in a cross-country panel of 22 OECD countries for the period 1972 to 2007; entrepreneurship Granger-causes the cycles of the world economy. Furthermore, the entrepreneurial cycle is positively affected by the national unemployment cycle. We discuss possible causes and implications of these findings.",fullPaper,jv118
Economics,p2455,d11,a8f049681b16766148b02183e81215a8f2881f14,c16,International Conference on Data Science and Advanced Analytics,"Production, growth and business cycles",Abstract,poster,cp16
Economics,p2456,d11,a4dfd17ddf4979f306bd64590c7d9b8b8abfdc3e,c60,Network and Distributed System Security Symposium,External Relationships and the Small Business: A Review of Small Business Alliance and Network Research**,"In order to thrive, small businesses are often advised to develop relationships with external organizations that have the potential to assist business development, survival, and growth. A focus on the external relationships of the small business underlines the vital importance of external resources in moving a small business toward increased success and profitability. Covering the period from 1990 to 2002, this paper reviews the small business literature as it relates to the use of these external relationships (such as organizational partnerships, networks, and alliances). In response to both academic and practitioner demand for further research in this area, an exhaustive analysis of the relevant literature was conducted and three “meta” research questions representing the connections within this literature were formed. The resource‐based view of the firm, resource dependency theory, and punctuated equilibrium theory are proposed as useful starting points for exploring these research questions and can give direction for moving forward in this research area.",poster,cp60
Economics,p2459,d11,98e20fd7f00efb041ee42774f0a96268f0eab125,c85,IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering,Business Model Innovation and Sources of Value Creation in Low-Income Markets,Abstract,poster,cp85
Economics,p2465,d11,5565cce74ebdf528b9ed7c10c995e5c875ff5149,c90,International Conference on Collaboration Technologies and Systems,International Business Cycles and the ERM: Is there a European Business Cycle?,"Successful fixed exchange rate regimes impose policy disciplines that are likely to lead to conformity in the business cycles of the participating countries. This conjecture is borne out in the present paper by the evidence that the business cycle affiliation of ERM member countries has shifted from the United States to Germany since the formation of the ERM. This effect is bolstered by growing links in trade and finance between the European countries. The United Kingdom is conspicuous among these in that its business cycle affiliation did not change in the period of study. Copyright @ 1997 by John Wiley & Sons, Ltd. All rights reserved.",poster,cp90
Economics,p2470,d11,cf1e1f530b5285bc7f2e5d52b9d5c8c8e7ad6a8a,c113,International Conference on Mobile Data Management,"Path-dependent and path-breaking change: reconfiguring business resources following acquisitions in the U.S. medical sector, 1978–1995","This paper studies how firms use acquisitions to achieve long-term business reconfiguration. We base the study in a routine-based perspective on business dynamics. We develop and test hypotheses concerning the relative extent of change by acquiring and non-acquiring businesses, focusing on product line addition, retention, and deletion as forms of changing resources. We develop and test hypotheses that compare and contrast resource-deepening and resource extension arguments. We test the hypotheses with data from more than 3000 firms that offered more than 200 product lines in the U.S. medical sector between 1978 and 1995. We find that acquisitions play a major role in business reconfiguration, offering opportunities for firms to both build on existing resources and obtain substantially different resources.",poster,cp113
Economics,p2471,d11,3c3e0037c6d7b064c40779ac13bb3f30434cb55f,c7,International Symposium on Intelligent Data Analysis,What do Investors Look for in a Business Plan?,"Most potential funders wish to see a business plan as a first step in deciding whether or not to invest. However, much of the literature on how to write a business plan fails to emphasize that different types of funder look at business plans from different perspectives. Using a real time methodology this article highlights the different investment criteria of bankers, venture capital fund managers and business angels. Bankers stress the financial aspects of the proposal and give little emphasis to market, entrepreneur or other issues. As equity investors, venture capital fund managers and business angels have a very different approach, emphasizing both market and finance issues. Business angels give more emphasis than venture capital fund managers to the entrepreneur and ‘investor fit’ considerations. The implication for entrepreneurs is that they must customize their business plan according to whether they are seeking funding from a bank, venture capital fund or business angel.",poster,cp7
Economics,p2474,d11,0dc1bba6c0c3913548c7cc27a3dbe4a14f47733a,c5,Technical Symposium on Computer Science Education,The Business Model: An Integrative Framework for Strategy Execution,"We have many useful frameworks for formulating business strategy, i.e., devising a theory of how to compete. Frameworks for strategy execution are comparatively fragmented and idiosyncratic. This paper proposes a business model framework to link the firm's theory about how to compete to its execution. The framework captures previous ideas about business models in a simple logical structure that reflects current thinking in strategy. The business model framework provides a consistent logical picture of the firm that is a useful tool for the strategist, for teaching, and potentially for research on business models in strategy.",poster,cp5
Economics,p2478,d11,35bea789e082a44ed13b7bd10529706cb253d8c7,c45,IEEE Symposium on Security and Privacy,Creating value through mutual commitment to business network relationships,A structural model of business relationship development in a business network context is formulated and tested on delta from the European International Marketing and Purchasing (IMP) project. The e ...,poster,cp45
Economics,p2480,d11,60302bd7477a10c5c9302234134561be76d2dbc3,c54,ACM SIGCOMM Symposium on Software Defined Networking Research,Real Business Cycles in Emerging Countries?,"We use more than a century of Argentine and Mexican data to estimate the structural parameters of a small-open-economy real-business-cycle model driven by nonstationary productivity shocks. We find that the RBC model does a poor job of explaining business cycles in emerging countries. We then estimate an augmented model that incorporates shocks to the country premium and financial frictions. We find that the estimated financial-friction model provides a remarkably good account of business cycles in emerging markets and, importantly, assigns a negligible role to nonstationary productivity shocks. (JEL E13, E32, E44, F43, O11, O16)",poster,cp54
Economics,p2485,d11,7ca0dfb132a81779b55043ba0c28fa772a48a267,c14,Hawaii International Conference on System Sciences,Cooperative Strategies in International Business,Abstract,poster,cp14
Economics,p2487,d11,80ea01115218b8361b802c591c5742cbd34427ad,c121,International Conference on Interaction Sciences,International Business Cycles: Theory and Evidence,"We review recent work comparing properties of international business cycles with those of dynamic general equilibrium models, emphasizing two discrepancies between theory and data that we refer to as anomalies. The first is the consumption/output/productivity anomaly: in the data we generally find that the correlation across countries of output fluctuations is larger than the analogous consumption and productivity correlations. In theoretical economies we find, for a wide range of parameter values, that the consumption correlation exceeds the productivity and output correlations. The second anomaly concerns relative price movements: the standard deviation of the terms of trade is considerably larger in the data than it is in theoretical economies. We speculate on changes in theoretical structure that might bring theory and data closer together.",poster,cp121
Economics,p2497,d11,74a0da9a7df829d220d0a7c9d3ef615a755f1825,c112,British Machine Vision Conference,Business cycle asymmetry: a deeper look,"This paper distinguishes two types of asymmetry in business cycles: deepness and steepness. Deepness is defined as the characteristic that troughs are further below trend than peaks are above. Most previous research has focused exclusively on steepness, which refers to cycles in which contractions are steeper than expansions. A test for deepness is proposed and applied to U.S. postwar quarterly unemployment, real GNP, and industrial production. Evidence of deepness is found for unemployment and industrial production, while the evidence for real GNP is weaker. Previous evidence of steepness in unemployment is confirmed. Copyright 1993 by Oxford University Press.",poster,cp112
Economics,p2498,d11,3bdd58bcfd1fd134ea5537d3530e95ea0d252301,j118,Review of Economics and Statistics,License to Sell: The Effect of Business Registration Reform on Entrepreneurial Activity in Mexico,"Abstract This paper estimates the economic effects of a recent reform that simplified business entry regulation in Mexico. The reform was introduced in different municipalities at different points in time. Using microlevel data, I find that the reform increased the number of registered businesses by 5%. This increase was due to former wage earners' opening businesses. Former unregistered business owners were not more likely to register their business after the reform. The reform also increased wage employment by 2.2%. Finally, the results imply that the competition from new entrants decreased the income of incumbent businesses by 3%.",fullPaper,jv118
History,p361,d12,7579330e89bffd736fee19d25359ab3ae65bf5f7,c39,Online World Conference on Soft Computing in Industrial Applications,rioja: Analysis of Quaternary Science Data,Abstract,poster,cp39
History,p2062,d12,94c23d3ef82b0fb5f43d467fc53fd5b561370ea7,c56,International Conference on Automated Software Engineering,"The core competence of the corporation’, Harvard Business Review, Vol. pp. .",Abstract,poster,cp56
History,p2473,d12,685c7a8c7ea5fd5b7dd5ee6e30c700daeba3b816,c37,International Workshop on the Semantic Web,Ethnic Entrepreneurs: Immigrant Business in Industrial Societies,"Foreword - John H Stanfield II Preface Opportunities, Group Characteristics, and Outcomes - Roger Waldinger, Howard Aldrich and Robin Ward Trends in Ethnic Business in the United States - Roger Waldinger and Howard Aldrich European Trends in Ethnic Business - Jochen Blaschke et al Spatial Dimensions of Opportunity Structures - Roger Waldinger, David McEvoy and Howard Aldrich Ethnic Entrepreneurs and Ethnic Strategies - Jeremy Boissevain et al Business on the Ragged Edge - Roger Waldinger, Mirjana Morokvasic and Annie Phizacklea Immigrant and Minority Business in the Garment Industries of Paris, London, and New York Conclusions and Policy Implications - Roger Waldinger et al",poster,cp37
Geography,p84,d13,702cd9a7a128706b8a6ec88e7424e06c326021e5,c20,ACM Conference on Economics and Computation,Upscaling urban data science for global climate solutions,"Non-technical summary Manhattan, Berlin and New Delhi all need to take action to adapt to climate change and to reduce greenhouse gas emissions. While case studies on these cities provide valuable insights, comparability and scalability remain sidelined. It is therefore timely to review the state-of-the-art in data infrastructures, including earth observations, social media data, and how they could be better integrated to advance climate change science in cities and urban areas. We present three routes for expanding knowledge on global urban areas: mainstreaming data collections, amplifying the use of big data and taking further advantage of computational methods to analyse qualitative data to gain new insights. These data-based approaches have the potential to upscale urban climate solutions and effect change at the global scale. Technical summary Cities have an increasingly integral role in addressing climate change. To gain a common understanding of solutions, we require adequate and representative data of urban areas, including data on related greenhouse gas emissions, climate threats and of socio-economic contexts. Here, we review the current state of urban data science in the context of climate change, investigating the contribution of urban metabolism studies, remote sensing, big data approaches, urban economics, urban climate and weather studies. We outline three routes for upscaling urban data science for global climate solutions: 1) Mainstreaming and harmonizing data collection in cities worldwide; 2) Exploiting big data and machine learning to scale solutions while maintaining privacy; 3) Applying computational techniques and data science methods to analyse published qualitative information for the systematization and understanding of first-order climate effects and solutions. Collaborative efforts towards a joint data platform and integrated urban services would provide the quantitative foundations of the emerging global urban sustainability science.",poster,cp20
Geography,p458,d13,43789305e5d2212da05f9c16b148e84aae5614b2,c72,Workshop on Research on Enterprise Networking,Citizen Science and Volunteered Geographic Information: Overview and Typology of Participation,Abstract,poster,cp72
Geography,p477,d13,9a7dfcd3c35ebfbce9e359a1a97d6892b83a37ec,c15,Pacific Symposium on Biocomputing,Citizen Science as an Ecological Research Tool: Challenges and Benefits,"Citizen science, the involvement of volunteers in research, has increased the scale of ecological field studies with continent-wide, centralized monitoring efforts and, more rarely, tapping of volunteers to conduct large, coordinated, field experiments. The unique benefit for the field of ecology lies in understanding processes occurring at broad geographic scales and on private lands, which are impossible to sample extensively with traditional field research models. Citizen science produces large, longitudinal data sets, whose potential for error and bias is poorly understood. Because it does not usually aim to uncover mechanisms underlying ecological patterns, citizen science is best viewed as complementary to more localized, hypothesis-driven research. In the process of addressing the impacts of current, global “experiments” altering habitat and climate, large-scale citizen science has led to new, quantitative approaches to emerging questions about the distribution and abundance of organisms across spa...",poster,cp15
Geography,p491,d13,8801ce73bea0c97f2d35f5e3bd4f4fdb49698461,c21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,Lessons from lady beetles: accuracy of monitoring data from US and UK citizen-science programs,"Citizen scientists have the potential to play a crucial role in the study of rapidly changing lady beetle (Coccinellidae) populations. We used data derived from three coccinellid-focused citizen-science programs to examine the costs and benefits of data collection from direct citizen-science (data used without verification) and verified citizen-science (observations verified by trained experts) programs. Data collated through direct citizen science overestimated species richness and diversity values in comparison to verified data, thereby influencing interpretation. The use of citizen scientists to collect data also influenced research costs; our analysis shows that verified citizen science was more cost effective than traditional science (in terms of data gathered per dollar). The ability to collect a greater number of samples through direct citizen science may compensate for reduced accuracy, depending on the type of data collected and the type(s) and extent of errors committed by volunteers.",poster,cp21
Geography,p861,d13,3a4fe629aaa6bfc2d07e651c6ec2e5cfc40bfb23,c80,International Symposium on Software Engineering for Adaptive and Self-Managing Systems,More Comprehensive and Inclusive Approaches to Demographic Data Collection,"Todd is a PhD Student in Engineering Education at Purdue University who's research is focused on en-trepreneurship education and entrepreneurship education as a component of modern engineering education efforts. research focuses what factors influence diverse students to choose engineering and stay in engineering through their careers and how different experiences within the practice and culture of engineering foster or hinder belongingness and identity development. Dr. Godwin graduated from Clemson University with a B. Dina Verdín is an Engineering Education graduate student at Purdue University. She completed her undergraduate degree in Industrial and Systems Engineering at San José State University. Her research interest focuses on the first-generation college student population, which includes changing the perspective of this population from a deficit base approach to an asset base approach. His research focuses on First Generation engineering college students' engineering identity, belonging-ness, and how they perceive their college experience.He is also on a National Science Foundation project looking at non-normative engineering students and how they may have differing paths to success. His education includes a B.S. His research focuses on the interactions between engineering cultures, student motivation, and their learning experiences. His projects involve the study of student perceptions, beliefs and attitudes towards becoming engineers, their problem solving processes, and cultural fit. His education includes a B. a joint appointment in Bioengineering. Her research focuses on the interactions between student motivation and their learning experiences. Her projects involve the study of student perceptions, beliefs and attitudes towards becoming engineers and scientists, and their problem solving processes. Other projects in the Benson group include effects of student-centered active learning, self-regulated learning, and incorporating engineering into secondary science and mathematics classrooms. Her education includes a B.S.",poster,cp80
Geography,p1066,d13,ed9e7821b3e51c7e59183300d6c8cf90c8de0f26,j17,Big Data & Society,COVID-19 is spatial: Ensuring that mobile Big Data is used for social good,"The mobility restrictions related to COVID-19 pandemic have resulted in the biggest disruption to individual mobilities in modern times. The crisis is clearly spatial in nature, and examining the geographical aspect is important in understanding the broad implications of the pandemic. The avalanche of mobile Big Data makes it possible to study the spatial effects of the crisis with spatiotemporal detail at the national and global scales. However, the current crisis also highlights serious limitations in the readiness to take the advantage of mobile Big Data for social good, both within and beyond the interests of health sector. We propose two strategical pathways for the future use of mobile Big Data for societal impact assessment, addressing access to both raw mobile Big Data as well as aggregated data products. Both pathways require careful considerations of privacy issues, harmonized and transparent methodologies, and attention to the representativeness, reliability and continuity of data. The goal is to be better prepared to use mobile Big Data in future crises.",fullPaper,jv17
Geography,p1484,d13,c49131dc223cdf72a63e02a996cec61b1f71e9dd,c95,Cyber ..,Managing data lakes in big data era: What's a data lake and why has it became popular in data management ecosystem,"The concept of a data lake is emerging as a popular way to organize and build the next generation of systems to master new big data challenges, but there are lots of concerns and questions for large enterprises to implement data lakes. The paper discusses the concept of data lakes and shares the author's thoughts and practices of data lakes.",fullPaper,cp95
Geography,p1610,d13,685db71ce0715ddf1127d72ee991c9ba9c8f89ba,c108,IEEE International Conference on Multimedia and Expo,Human Mortality Database,Abstract,poster,cp108
Geography,p1615,d13,d2272dd9ff850edc448f7fde86eef6bcd57af2cc,c114,Chinese Conference on Biometric Recognition,Development of a global land cover characteristics database and IGBP DISCover from 1 km AVHRR data,"Researchers from the U.S. Geological Survey, University of Nebraska-Lincoln and the European Commission's Joint Research Centre, Ispra, Italy produced a 1 km resolution global land cover characteristics database for use in a wide range of continental-to global-scale environmental studies. This database provides a unique view of the broad patterns of the biogeographical and ecoclimatic diversity of the global land surface, and presents a detailed interpretation of the extent of human development. The project was carried out as an International Geosphere-Biosphere Programme, Data and Information Systems (IGBP-DIS) initiative. The IGBP DISCover global land cover product is an integral component of the global land cover database. DISCover includes 17 general land cover classes defined to meet the needs of IGBP core science projects. A formal accuracy assessment of the DISCover data layer will be completed in 1998. The 1 km global land cover database was developed through a continent-by-continent unsupervised classification of 1 km monthly Advanced Very High Resolution Radiometer (AVHRR) Normalized Difference Vegetation Index (NDVI) composites covering 1992-1993. Extensive post-classification stratification was necessary to resolve spectral/temporal confusion between disparate land cover types. The complete global database consists of 961 seasonal land cover regions that capture patterns of land cover, seasonality and relative primary productivity. The seasonal land cover regions were aggregated to produce seven separate land cover data sets used for global environmental modelling and assessment. The data sets include IGBP DISCover, U.S. Geological Survey Anderson System, Simple Biosphere Model, Simple Biosphere Model 2, Biosphere-Atmosphere Transfer Scheme, Olson Ecosystems and Running Global Remote Sensing Land Cover. The database also includes all digital sources that were used in the classification. The complete database can be sourced from the website: http://edcwww.cr.usgs.gov/landdaac/glcc/glcc.html.",poster,cp114
Geography,p1662,d13,88fc814fc568fdfa4704ba2326eb623e95621bec,c69,Neural Information Processing Systems,Notes on CEPII’s Distances Measures: The GeoDist Database,"GeoDist makes available the exhaustive set of gravity variables used in Mayer and Zignago (2005). GeoDist provides several geographical variables, in particular bilateral distances measured using citylevel data to assess the geographic distribution of population inside each nation. We have calculated different measures of bilateral distances available for most countries across the world (225 countries in the current version of the database). For most of them, different calculations of “intra-national distances” are also available. The GeoDist webpage provides two distinct files: a country-specific one (geo_cepii)and a dyadic one (dist_cepii) including a set of different distance and common dummy variables used in gravity equations to identify particular links between countries such as colonial past, common languages, contiguity. We try to improve upon the existing similar datasets in terms of geographical coverage, quality of measurement and number of variables provided.",poster,cp69
Geography,p1669,d13,d5985ad8d754187dc36c4e2a221086f8530fe372,c59,Australian Software Engineering Conference,Completion of the 2006 National Land Cover Database for the conterminous United States.,Abstract,poster,cp59
Geography,p1701,d13,928e4178fca984b1d49da75c5120e05cff7a3fa9,c21,Research Conference of the South African Institute of Computer Scientists and Information Technologists,Development of a 2001 National land-cover database for the United States,"Multi-Resolution Land Characterization 2001 (MRLC 2001) is a second-generation Federal consortium designed to create an updated pool of nation-wide Landsat 5 and 7 imagery and derive a second-generation National Land Cover Database (NLCD 2001). The objectives of this multi-layer, multi-source database are two fold: first, to provide consistent land cover for all 50 States, and second, to provide a data framework which allows flexibility in developing and applying each independent data component to a wide variety of other applications. Components in the database include the following: (1) normalized imagery for three time periods per path/row, (2) ancillary data, including a 30 m Digital Elevation Model (DEM) derived into slope, aspect and slope position, (3) perpixel estimates of percent imperviousness and percent tree canopy (4) 29 classes of land cover data derived from the imagery, ancillary data, and derivatives, (5) classification rules, confidence estimates, and metadata from the land cover classification. This database is now being developed using a Mapping Zone approach, with 66 Zones in the continental United States and 23 Zones in Alaska. Results from three initial mapping Zones show single-pixel land cover accuracies ranging from 73 to 77 percent, imperviousness accuracies ranging from 83 to 91 percent, tree canopy accuracies ranging from 78 to 93 percent, and an estimated 50 percent increase in mapping efficiency over previous methods. The database has now entered the production phase and is being created using extensive partnering in the Federal government with planned completion by 2006.",poster,cp21
Geography,p1758,d13,8df1903824290707e09ee754f3c40d90b088d65b,c40,European Conference on Computer Vision,World Database on Protected Areas (WDPA),Abstract,poster,cp40
Geography,p1918,d13,17e6076b6761788684434d1e14e85e8877fc0146,c87,International Conference on Big Data Research,LandScan: A Global Population Database for Estimating Populations at Risk,"The LandScan Global Population Project produced a world-wide 1998 population database at a 30-by 30-second resolution for estimating ambient populations at risk. Best available census counts were distributed to cells based on probability coefficients which, in turn, were based on road proximity, slope, land cover, and nighttime lights, LandScan 1998 has been completed for the entire world. Verification and validation (V&V) studies were conducted routinely for all regions and more extensively for Israel, Germany, and the southwestern United States. Geographic information systems (GIS) were essential for conflation of diverse input variables, computation of probability coefficients, allocation of population to cells, and reconciliation of cell totals with aggregate (usually province) control totals. Remote sensing was an essential source of two input variables-land cover and nighttime lights-and one ancillary database-high-resolution panchromatic imagery-used in V&V of the population model and resulting LandScan database.",poster,cp87
Environmental Science,p87,d14,9d76e69f54bdf739fcd61d0fd25f92c7dd3923c2,c61,"International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems",Data Science for Wind Energy,Abstract,poster,cp61
Environmental Science,p729,d14,cc3137164f2d292a5255d5322abcd6eb95c087e3,c89,Conference on Uncertainty in Artificial Intelligence,Data-Driven Modeling: Using MATLAB® in Water Resources and Environmental Engineering,Abstract,poster,cp89
Environmental Science,p1535,d14,716000409a3a2e2c75801b3d58b9b17b68eeaef7,c45,IEEE Symposium on Security and Privacy,An improved method of constructing a database of monthly climate observations and associated high‐resolution grids,"A database of monthly climate observations from meteorological stations is constructed. The database includes six climate elements and extends over the global land surface. The database is checked for inhomogeneities in the station records using an automated method that refines previous methods by using incomplete and partially overlapping records and by detecting inhomogeneities with opposite signs in different seasons. The method includes the development of reference series using neighbouring stations. Information from different sources about a single station may be combined, even without an overlapping period, using a reference series. Thus, a longer station record may be obtained and fragmentation of records reduced. The reference series also enables 1961–90 normals to be calculated for a larger proportion of stations.",poster,cp45
Environmental Science,p1623,d14,3ff0d2c7621c40e6245c7ca0964b4b856255e0c2,c72,Workshop on Research on Enterprise Networking,"Development and validation of a global database of lakes, reservoirs and wetlands",Abstract,poster,cp72
Environmental Science,p1673,d14,87752bbb228c597e12cc7e86d9dba4539a04769e,c30,PS,An Overview of the China Meteorological Administration Tropical Cyclone Database,"AbstractThe China Meteorological Administration (CMA)’s tropical cyclone (TC) database includes not only the best-track dataset but also TC-induced wind and precipitation data. This article summarizes the characteristics and key technical details of the CMA TC database. In addition to the best-track data, other phenomena that occurred with the TCs are also recorded in the dataset, such as the subcenters, extratropical transitions, outer-range severe winds associated with TCs over the South China Sea, and coastal severe winds associated with TCs landfalling in China. These data provide additional information for researchers. The TC-induced wind and precipitation data, which map the distribution of severe wind and rainfall, are also helpful for investigating the impacts of TCs. The study also considers the changing reliability of the various data sources used since the database was created and the potential causes of temporal and spatial inhomogeneities within the datasets. Because of the greater number of ...",poster,cp30
Environmental Science,p1708,d14,d594c19d91c55a4e7f341fd1d45fc30c5fac0b1a,c40,European Conference on Computer Vision,Atlantic Hurricane Database Uncertainty and Presentation of a New Database Format,"Abstract“Best tracks” are National Hurricane Center (NHC) poststorm analyses of the intensity, central pressure, position, and size of Atlantic and eastern North Pacific basin tropical and subtropical cyclones. This paper estimates the uncertainty (average error) for Atlantic basin best track parameters through a survey of the NHC Hurricane Specialists who maintain and update the Atlantic hurricane database. A comparison is then made with a survey conducted over a decade ago to qualitatively assess changes in the uncertainties. Finally, the implications of the uncertainty estimates for NHC analysis and forecast products as well as for the prediction goals of the Hurricane Forecast Improvement Program are discussed.",poster,cp40
Environmental Science,p1738,d14,187f31d400d711e25fd3cddd195f0af730729912,c32,International Conference on Smart Data Services,Cloud-Screening and Quality Control Algorithms for the AERONET Database,Abstract,poster,cp32
Environmental Science,p1759,d14,afef030037e621538ac53e18d23a1a4660df79d8,c70,Annual Meeting of the Association for Computational Linguistics,Harmonized World Soil Database (version 1.2),"METIS-ID: 167825 The Harmonized World Soil Database is a 30 arc-second raster database with over 15000 different soil mapping units that combines existing regional and national updates of soil information worldwide (SOTER, ESD, Soil Map of China, ISRIC-WISE) with the information contained within the 1:5 000 000 scale FAO-UNESCO Soil Map of the World (FAO, 1971-1981). The resulting raster database consists of ... 21600 rows and 43200 columns, which are linked to harmonized soil property data. The use of a standardized structure allows for the linkage of the attribute data with the raster map to display or query the composition in terms of soil units and the characterization of selected soil parameters (organic Carbon, pH, water storage capacity, soil depth, cation exchange capacity of the soil and the clay fraction, total exchangeable nutrients, lime and gypsum contents, sodium exchange percentage, salinity, textural class and granulometry). Reliability of the information contained in the database is variable: the parts of the database that still make use of the Soil Map of the World such as North America, Australia, West Africa and South Asia are considered less reliable, while most of the areas covered by SOTER databases are considered to have the highest reliability (Southern Africa, Latin America and the Caribbean, Central and Eastern Europe). Further expansion and update of the HWSD is foreseen for the near future, notably with the excellent databases held in the USA (Natural Resources Conservation Service US General Soil Map, STATSGO), Canada (Agriculture and AgriFood Canada: The National Soil Database NSDB), and Australia (CSIRO, ACLEP, Nnatural Heritage Trust and National Land and Water Resources Audit: ASRIS), and with the recently released SOTER database for Central Africa (FAO/ISRIC/Univ. Gent, 2007)",poster,cp70
Environmental Science,p1890,d14,a7352bf3b88df27f0d0faa9d7ee7198a8b304c1d,c107,Annual Haifa Experimental Systems Conference,Development and use of a database of hydraulic properties of European soils,Abstract,poster,cp107
Environmental Science,p1908,d14,09d73eecceb080eb1f7cea71d7df1411c712baf6,c66,International Conference on Web and Social Media,The National Land Cover Database,Abstract,poster,cp66
Environmental Science,p1928,d14,075082cfbedfe3161d15354b31859ca59dfbeafb,c101,Interspeech,A global database of soil respiration data,"Abstract. Soil respiration – RS, the flux of CO2 from the soil to the atmosphere – is probably the least well constrained component of the terrestrial carbon cycle. Here we introduce the SRDB database, a near-universal compendium of published RS data, and make it available to the scientific community both as a traditional static archive and as a dynamic community database that may be updated over time by interested users. The database encompasses all published studies that report one of the following data measured in the field (not laboratory): annual RS, mean seasonal RS, a seasonal or annual partitioning of RS into its sources fluxes, RS temperature response (Q10), or RS at 10 °C. Its orientation is thus to seasonal and annual fluxes, not shorter-term or chamber-specific measurements. To date, data from 818 studies have been entered into the database, constituting 3379 records. The data span the measurement years 1961–2007 and are dominated by temperate, well-drained forests. We briefly examine some aspects of the SRDB data – its climate space coverage, mean annual RS fluxes and their correlation with other carbon fluxes, RS variability, temperature sensitivities, and the partitioning of RS source flux – and suggest some potential lines of research that could be explored using these data. The SRDB database is available online in a permanent archive as well as via a project-hosting repository; the latter source leverages open-source software technologies to encourage wider participation in the database's future development. Ultimately, we hope that the updating of, and corrections to, the SRDB will become a shared project, managed by the users of these data in the scientific community.",poster,cp101
Political Science,p93,d15,b85ac20631159ca3e370afa9c1f81a4618242b4f,j41,American Statistician,The Democratization of Data Science Education,"Abstract Over the last three decades, data have become ubiquitous and cheap. This transition has accelerated over the last five years and training in statistics, machine learning, and data analysis has struggled to keep up. In April 2014, we launched a program of nine courses, the Johns Hopkins Data Science Specialization, which has now had more than 4 million enrollments over the past five years. Here, the program is described and compared to standard data science curricula as they were organized in 2014 and 2015. We show that novel pedagogical and administrative decisions introduced in our program are now standard in online data science programs. The impact of the Data Science Specialization on data science education in the U.S. is also discussed. Finally, we conclude with some thoughts about the future of data science education in a data democratized world.",fullPaper,jv41
Political Science,p99,d15,08468bac470e5c2cbbd2b66e8e7cf2ab65f38e02,j44,Social Science Research Network,Data Science for Local Government,"The Data Science for Local Government project was about understanding how the growth of ‘data science’ is changing the way that local government works in the UK. We define data science as a dual shift which involves both bringing in new decision making and analytical techniques to local government work (e.g. machine learning and predictive analytics, artificial intelligence and A/B testing) and also expanding the types of data local government makes use of (for example, by repurposing administrative data, harvesting social media data, or working with mobile phone companies). The emergence of data science is facilitated by the growing availability of free, open-source tools for both collecting data and performing analysis. Based on extensive documentary review, a nationwide survey of local authorities, and in-depth interviews with over 30 practitioners, we have sought to produce a comprehensive guide to the different types of data science being undertaken in the UK, the types of opportunities and benefits created, and also some of the challenges and difficulties being encountered. Our aim was to provide a basis for people working in local government to start on their own data science projects, both by providing a library of dozens of ideas which have been tried elsewhere and also by providing hints and tips for overcoming key problems and challenges.",fullPaper,jv44
Political Science,p292,d15,4672c6cf14ce2e49cc39f9361b9e73d9442361c5,c72,Workshop on Research on Enterprise Networking,Data Quality in Citizen Science,Abstract,poster,cp72
Political Science,p368,d15,cebed64039064dfe950587b919ddc01dee7d871f,c101,Interspeech,From Little Science to Big Science,"In Little Science, Big Science (1963), Derek J. de Solla Price undertook a sociology of science that dealt with the growth and changing shape of scientific publishing and the social organization of science. The focus of Price’s work was on the long-term, gradual shift from “little science,” with the solo scientist, small laboratory, and minimal research funds, to “big science,” with collaborative research teams, large-scale research hardware, extensive funding, and corporate-political suitors of scientists. We extend Price’s focus on scientific publications by moving beyond his analysis of practices in physics and chemistry to examine a social science; namely, sociology. Specifically, we analyze 3,000 articles in four long-standing sociology journals over the fifty-year period from 1960-2010 to determine the gender of authors, the prestige of authors’ departments, length of articles, number of references, sources of data for studies, and patterns of funding for research. We find that sociology is not immune from the shift from “little science” to “big science.”",poster,cp101
Political Science,p369,d15,cf83811d697dc3419a52c9853807afb410eb3943,j122,American Journal of Political Science,Tree-Based Models for Political Science Data,"Political scientists often find themselves analyzing data sets with a large number of observations, a large number of variables, or both. Yet, traditional statistical techniques fail to take full advantage of the opportunities inherent in “big data,” as they are too rigid to recover nonlinearities and do not facilitate the easy exploration of interactions in high-dimensional data sets. In this article, we introduce a family of tree-based nonparametric techniques that may, in some circumstances, be more appropriate than traditional methods for confronting these data challenges. In particular, tree models are very effective for detecting nonlinearities and interactions, even in data sets with many (potentially irrelevant) covariates. We introduce the basic logic of tree-based models, provide an overview of the most prominent methods in the literature, and conduct three analyses that illustrate how the methods can be implemented while highlighting both their advantages and limitations. Replication Materials: The data, code, and any additional materials required to replicate all analyses in this article are available on the American Journal of Political Science Dataverse within the Harvard Dataverse Network at: https://doi.org/10.7910/DVN/8ZJBLI. Social science scholars often work with data sets containing a large number of observations, many potential covariates, or (increasingly) both. Indeed, political scientists now regularly analyze data with levels of complexity unimaginable just two decades ago. Widely used surveys, for instance, interview tens of thousands of respondents about hundreds of topics. Scholars of institutions can quickly assemble data sets with thousands of observations using resources like the Comparative Agendas Project. Moreover, new measurement methods, such as text analysis, have combined with data sources, such as Twitter, to generate databases of almost unmanageable sizes. It is clear that political science, like all areas of the social sciences, will increasingly have access to a deluge of data so vast that it will dwarf everything that has come before. What statistical methods are needed in this datasaturated world? Surely, there is no one correct answer. Yet, just as surely, traditional statistical models are not always equipped to take full advantage of new data sources. Traditional models—largely variants of linear regressions—are ideal for evaluating theories that imply specific functional forms relating outcomes to predictors. In particular, they excel in their ability to leverage assumptions about the data-generating process, or DGP (additivity, linearity in the parameters, homoskedasticity, Jacob M. Montgomery is Associate Professor, Department of Political Science, Washington University in St. Louis, Campus Box 1063, One Brookings Drive, St. Louis, MO 63130 (jacob.montgomery@wustl.edu). Santiago Olivella is Assistant Professor, Department of Political Science, University of North Carolina at Chapel Hill, Hamilton Hall 361, CB 3265, Chapel Hill, NC 27599 (olivella@unc.edu). etc.) to make valid inferences despite inherent data limitations. Although appropriate when testing theories that conform with these assumptions, standard models are often insufficiently flexible to capture nuances in the data—such as complex nonlinear functional forms and deep interactions—when no clear a priori expectations exist. In this article, we introduce a family of tree-based nonparametric techniques from the machine learning literature. We argue that, under specific circumstances, regression and classification tree models are an appropriate standard choice for analyzing high-dimensional data sets. In particular, past research has shown tree-based methods to be very useful for making accurate predictions when the underlying DGP includes nonlinearities, discontinuities, and interactions among many covariates. Further, tree models require few assumptions. Rather than imposing a presumed structure on the DGP, tree-based methods allow the data to “speak for themselves.” Thus, our goal in this article is to introduce political scientists to this promising family of methods, which are well suited for today’s data analysis demands. In the next sections, we discuss the promise and perils of high-dimensional, “large”-N data sets and introduce the basic logic of tree models. We then provide an overview of the most prominent methods in the literature. American Journal of Political Science, Vol. 62, No. 3, July 2018, Pp. 729–744 C ©2018, Midwest Political Science Association DOI: 10.1111/ajps.12361",fullPaper,jv122
Political Science,p370,d15,0b510ee69a507407008661aacb2345f73c70f8cb,c104,North American Chapter of the Association for Computational Linguistics,Strategies Employed by Citizen Science Programs to Increase the Credibility of Their Data,"The success of citizen science in producing important and unique data is attracting interest from scientists and resource managers. Nonetheless, questions remain about the credibility of citizen science data. Citizen science programs desire to meet the same standards of credibility as academic science, but they usually work within a different context, for example, training and managing significant numbers of volunteers with limited resources. We surveyed the credibility-building strategies of 30 citizen science programs that monitor environmental aspects of the California coast. We identified a total of twelve strategies: Three that are applied during training and planning; four that are applied during data collection; and five that are applied during data analysis and program evaluation. Variation in the application of these strategies by program is related to factors such as the number of participants, the focus on group or individual work, and the time commitment required of volunteers. The structure of each program and available resources require program designers to navigate tradeoffs in the choices of their credibility strategies. Our results illustrate those tradeoffs and provide a framework for the necessary discussions between citizen science programs and potential users of their data—including scientists and decision makers—about shared expectations for credibility and practical approaches for meeting those expectations. This article has been corrected here: http://dx.doi.org/10.5334/cstp.91",poster,cp104
Political Science,p388,d15,b9921fb4d1448058642897797e77bdaf8f444404,j130,Political Analysis,Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts,"Politics and political conflict often occur in the written and spoken word. Scholars have long recognized this, but the massive costs of analyzing even moderately sized collections of texts have hindered their use in political science research. Here lies the promise of automated text analysis: it substantially reduces the costs of analyzing large collections of text. We provide a guide to this exciting new area of research and show how, in many instances, the methods have already obtained part of their promise. But there are pitfalls to using automated methods—they are no substitute for careful thought and close reading and require extensive and problem-specific validation. We survey a wide range of new methods, provide guidance on how to validate the output of the models, and clarify misconceptions and errors in the literature. To conclude, we argue that for automated text methods to become a standard tool for political scientists, methodologists must contribute new methods and new methods of validation.",fullPaper,jv130
Political Science,p397,d15,ff7a79011e4ddba98474efe776432ac2b4431473,c73,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,Citizen science and the United Nations Sustainable Development Goals,Abstract,poster,cp73
Political Science,p438,d15,851a4c4e9d9bf8f023bc4cd29e023e4c43957b7d,c97,International Conference on Computational Logic,The Art and Science of Data-Driven Journalism,"Journalists have been using data in their stories for as long as the profession has existed. A revolution in computing in the 20th century created opportunities for data integration into investigations, as journalists began to bring technology into their work. In the 21st century, a revolution in connectivity is leading the media toward new horizons. The Internet, cloud computing, agile development, mobile devices, and open source software have transformed the practice of journalism, leading to the emergence of a new term: data journalism. Although journalists have been using data in their stories for as long as they have been engaged in reporting, data journalism is more than traditional journalism with more data. Decades after early pioneers successfully applied computer-assisted reporting and social science to investigative journalism, journalists are creating news apps and interactive features that help people understand data, explore it, and act upon the insights derived from it. New business models are emerging in which data is a raw material for profit, impact, and insight, co-created with an audience that was formerly reduced to passive consumption. Journalists around the world are grappling with the excitement and the challenge of telling compelling stories by harnessing the vast quantity of data that our increasingly networked lives, devices, businesses, and governments produce every day. While the potential of data journalism is immense, the pitfalls and challenges to its adoption throughout the media are similarly significant, from digital literacy to competition for scarce resources in newsrooms. Global threats to press freedom, digital security, and limited access to data create difficult working conditions for journalists in many countries. A combination of peer-to-peer learning, mentorship, online training, open data initiatives, and new programs at journalism schools rising to the challenge, however, offer reasons to be optimistic about more journalists learning to treat data as a source.",poster,cp97
Political Science,p465,d15,b7118fca8e7cd69d76090a5c145e89f303249eb8,c50,Conference on Emerging Network Experiment and Technology,The current state of citizen science as a tool for ecological research and public engagement,"Approaches to citizen science – an indispensable means of combining ecological research with environmental education and natural history observation – range from community-based monitoring to the use of the internet to “crowd-source” various scientific tasks, from data collection to discovery. With new tools and mechanisms for engaging learners, citizen science pushes the envelope of what ecologists can achieve, both in expanding the potential for spatial ecology research and in supplementing existing, but localized, research programs. The primary impacts of citizen science are seen in biological studies of global climate change, including analyses of phenology, landscape ecology, and macro-ecology, as well as in sub-disciplines focused on species (rare and invasive), disease, populations, communities, and ecosystems. Citizen science and the resulting ecological data can be viewed as a public good that is generated through increasingly collaborative tools and resources, while supporting public participation in science and Earth stewardship.",poster,cp50
Political Science,p488,d15,5952a9f10ef65983042794369d376e23d2682d7e,c30,PS,Openness in Political Science: Data Access and Research Transparency,"In 2012, the American Political Science Association (APSA) Council adopted new policies guiding data access and research transparency in political science. The policies appear as a revision to APSA's Guide to Professional Ethics in Political Science. The revisions were the product of an extended and broad consultation with a variety of APSA committees and the association's membership.",fullPaper,cp30
Political Science,p489,d15,a418d8fd1cc0abb34cf131d81723bc5da8817c93,c44,Italian National Conference on Sensors,Politicization of Science in the Public Sphere,"This study explores time trends in public trust in science in the United States from 1974 to 2010. More precisely, I test Mooney’s (2005) claim that conservatives in the United States have become increasingly distrustful of science. Using data from the 1974 to 2010 General Social Survey, I examine group differences in trust in science and group-specific change in these attitudes over time. Results show that group differences in trust in science are largely stable over the period, except for respondents identifying as conservative. Conservatives began the period with the highest trust in science, relative to liberals and moderates, and ended the period with the lowest. The patterns for science are also unique when compared to public trust in other secular institutions. Results show enduring differences in trust in science by social class, ethnicity, gender, church attendance, and region. I explore the implications of these findings, specifically, the potential for political divisions to emerge over the cultural authority of science and the social role of experts in the formation of public policy.",poster,cp44
Political Science,p1069,d15,0a829289a16ae48837cc2905635435db98bacc76,j262,Contemporary Sociology,"The Rise of Big Data Policing: Surveillance, Race, and the Future of Law Enforcement","The use of big data by the police is part of the very broad discussion of the use of big data in every aspect of our lives. It is the newest member of a small club—whose founding members were death and taxes—of things we cannot avoid. Even before the scandals concerning the alleged meddling of foreign governments in our elections and the revelations about Facebook’s targeted and politicized ads, we all really knew that we were being watched and monitored carefully. The interface between the Internet and GPS pretty much reveals our life story. And people who somehow find a way to disappear from the grid become persons of interest for precisely this reason. It’s why we should think twice about buying an airline ticket with cash. Trying to understand these issues is also a reminder that the day of discrete academic disciplines has gone. Andrew Ferguson’s The Rise of Big Data Policing: Surveillance, Race, and the Future of Law Enforcement has to be a multidisciplinary effort to explain something that is part social science, part engineering, and part law. It also resonates for sociologists, particularly sociological theorists, steeped in Michel Foucault’s work. He famously declared that around 1800 a new ‘‘episteme’’ emerged—a new set of institutions, ideas, and technologies that ushered in a new era of discipline and surveillance. Foucault’s appropriation of Bentham’s Panopticon led him to coin the term ‘‘panopticism’’ that anticipated many of the themes that inform Ferguson’s research. For Foucault, controlling criminals through extensive monitoring led all too easily to the control of entire populations of the assumed innocent and even to the people doing the monitoring itself. Ferguson provides us with an up-to-date, high-tech Foucault packaged in an engaging and timely book. As he makes clear in the early chapters, he is very aware of the general issues of big data and mundane surveillance, despite his primary interest in police practices. To this end, Ferguson distinguishes four issues: black, blue, bright, and no data. By ‘‘black data,’’ Ferguson not only highlights the racial inequities and legal concerns that are part of these new policing technologies but also the opacity of the technology itself. That is, the algorithms and computer programs that constitute big data policing are so complicated that accountability is compromised. As Ferguson puts it, ‘‘Policing is not a field that typically requires PhD-level technological expertise’’ (p. 136). Although Ferguson tacitly accepts the technical difficulty of the material, I wondered whether he could have done more to confront the technological opacity of black data. After all, molecular biology is also a tremendously difficult field, but there are ways of making its discoveries accessible. ‘‘Blue data’’ are quite different: they collect information about police practices and provide oversight. Efforts to police the police are the hallmark of blue data, and Ferguson credits the Obama administration with the creation of a collaboration between interested parties that has led to 90 data sets drawn from 50 local jurisdictions (p. 166). Advocates of ‘‘bright data’’ hope that big data can be used to make social programs more efficient by identifying and anticipating risk. Thus, a mother who is identified as someone who is struggling to feed her family could be sent information about food pantries. Ferguson also confronts the twin problems of missing data and expensive data. He estimates that at best big data policing makes use of only 50 percent of crime data and acknowledges that this opens the door to distortions and mistakes (p. 181). Ferguson also recognizes that both the costs and cost-benefits of big data are critical. However, here I was surprised by how little attention the topic actually received—just a page in the middle of the discussion of data concerns in Chapter Ten. Earlier Ferguson had noted very elegantly that ‘‘To find a guilty needle in the haystack, you have to collect a lot of innocent hay’’ (p. 110), and of course this raises the question 170 Reviews",fullPaper,jv262
Political Science,p1122,d15,d22e8ae8236a669053c4646b4859cdf1fadb64c2,j263,Television & New Media,Data Colonialism: Rethinking Big Data’s Relation to the Contemporary Subject,"We are often told that data are the new oil. But unlike oil, data are not a substance found in nature. It must be appropriated. The capture and processing of social data unfolds through a process we call data relations, which ensures the “natural” conversion of daily life into a data stream. The result is nothing less than a new social order, based on continuous tracking, and offering unprecedented new opportunities for social discrimination and behavioral influence. We propose that this process is best understood through the history of colonialism. Thus, data relations enact a new form of data colonialism, normalizing the exploitation of human beings through data, just as historic colonialism appropriated territory and resources and ruled subjects for profit. Data colonialism paves the way for a new stage of capitalism whose outlines we only glimpse: the capitalization of life without limit.",fullPaper,jv263
Political Science,p1270,d15,2b9d65579c65c2e29310632079accbe7be2fd5ab,c98,Vision,Big Data in the Public Sector: Lessons for Practitioners and Scholars,"In this essay, we consider the role of Big Data in the public sector. Motivating our work is the recognition that Big Data is still in its infancy and many important questions regarding the true value of Big Data remain unanswered. The question we consider is as follows: What are the limits, or potential, of Big Data in the public sector? By reviewing the literature and summarizing insights from a series of interviews from public sector Chief Information Officers (CIOs), we offer a scholarly foundation for both practitioners and researchers interested in understanding Big Data in the public sector.",poster,cp98
Political Science,p1409,d15,6d6beed0fd1ddd3221b0f28bc34ebf2e8c7a4ad7,c16,International Conference on Data Science and Advanced Analytics,Big Data in Public Affairs,"This article offers an overview of the conceptual, substantive, and practical issues surrounding “big data” to provide one perspective on how the field of public affairs can successfully cope with the big data revolution. Big data in public affairs refers to a combination of administrative data collected through traditional means and large-scale data sets created by sensors, computer networks, or individuals as they use the Internet. In public affairs, new opportunities for real-time insights into behavioral patterns are emerging but are bound by safeguards limiting government reach through the restriction of the collection and analysis of these data. To address both the opportunities and challenges of this emerging phenomenon, the authors first review the evolving canon of big data articles across related fields. Second, they derive a working definition of big data in public affairs. Third, they review the methodological and analytic challenges of using big data in public affairs scholarship and practice. The article concludes with implications for public affairs.",poster,cp16
Political Science,p1487,d15,fbc1c0375b33a868cb662548bbde23e840167fc3,j104,Science,The Human Face of Big Data,"Sandy Smolan, director; Rick Smolan, executive producer Premieres February 24, 2016, on PBS; check local listings Combining beautifully animated data visualizations with commentary from scientists, futurists, tech creators, and other experts, this 1-hour documentary highlights how researchers are using big data to tackle everything from malaria outbreaks and traffic jams to poverty and recidivism.",fullPaper,jv104
Political Science,p1881,d15,250718af678dedb2938775f9ca6fb8601749778a,c29,ACM-SIAM Symposium on Discrete Algorithms,Social Conflict in Africa: A New Database,"We describe the Social Conflict in Africa Database (SCAD), a new event dataset for conducting research and analysis on various forms of social and political unrest in Africa. SCAD contains information on over 7,200 instances of protests, riots, strikes, government repression, communal violence, and other forms of unrest for 47 African countries from 1990–2010. SCAD includes information on event dates, actors and targets, lethality, georeferenced location information, and other conflict attributes. This article gives an overview of the data collection process, presents descriptive statistics and trends across the continent, and compares SCAD to the widely used Banks event data. We believe that SCAD will be a useful resource for scholars across multiple disciplines as well as for the policy community.",poster,cp29
Political Science,p2070,d15,bc6ab7733fc6498dc0c7cfc93d3d6c5859a9279b,c95,Cyber ..,Ethical Theory and Business,"Found in this Section: 1. Overview of Changes 2. Chapter-by-Chapter Changes 1. Overview of Changes * The 9th edition is thoroughly updated to reflect the best new work in the field and important new trends in business ethics. * New case studies on recent business ethics scandals such as the financial crisis, Wall Street ethics, gay and lesbian rights at work, and the BP Deepwater Horizon disaster are included. (ex. p. 196) * International business ethics issues have been woven throughout the chapters. (ex. p. 536) * MySearchLab with eText can be packaged with this text. * MySearchLab provides engaging experiences that personalize learning, and comes from a trusted partner with educational expertise and a deep commitment to helping students and instructors achieve their goals. * eText - Just like the printed text, you can highlight and add notes to the eText or download it to your iPad. * Assessment - Chapter quizzes and flashcards offer immediate feedback and report directly to the gradebook. * Writing and Research - A wide range of writing, grammar and research tools and access to a variety of academic journals, census data, Associated Press newsfeeds, and discipline-specific readings help you hone your writing and research skills. * Ebsco's ContentSelect - With Ebsco's ContentSelect, students get 24-hour access to abstracts and full-text articles from thousands of scholarly and popular periodicals, including Newsweek, National Review, and USA Today's Magazine - all grouped and organized by subject area. * Research Tutorial - When students click on the research button in MySearchLab, they get a step-by-step tutorial for the entire research process, including understanding the assignment, finding a topic, creating effective notes, how to form a paradigm, and understand and finding source material. * YouDecide - YouDecide activities provide opposing viewpoints in a series of issue-centered debates with accompanying video resources. * Create a Custom Text: For enrollments of at least 25, create your own textbook by combining chapters from best-selling Pearson textbooks and/or reading selections in the sequence you want. To begin building your custom text, visit www.pearsoncustomlibrary.com. You may also work with a dedicated Pearson Custom editor to create your ideal text-publishing your own original content or mixing and matching Pearson content. Contact your Pearson Publisher's Representative to get started. 2. Chapter-by-Chapter Changes Chapter 1: Ethical Theory and Business Practice * This introductory chapter has been thoroughly updated. * Three new case studies facilitate discussion of ethical theories. Chapter 2: The Purpose of the Corporation * New coverage of ethical cultures and moral responsibility is included. * New case on BP and the Deepwater Horizon disaster is featured. Chapter 3: Ethical Treatment of Employees * New coverage of whistle-blowing is included in this chapter. Chapter 4: Diversity and Discrimination in the Workplace * The coverage of affirmative action and diversity has been updated. * A new case on gay and lesbian rights at work is featured. Chapter 5: Marketing and the Disclosure of Information * This chapter features expanded coverage of pharmaceutical marketing. Chapter 6: Ethical Issues in Finance and Accounting * New coverage of Wall Street ethics, insider trading, and predatory lending and the financial crisis is included. Chapter 7: Ethical Issues Regarding Emerging Technologies * New coverage of social media and internet privacy is included. Chapter 8: Environmental Sustainability * Three new articles on environmental sustainability are featured. * New cases on best practices in sustainability are included. Chapter 9: Ethical Issues in International Business * Updated articles and cases are included. Chapter 10: Social and Economic Justice * New coverage of the financial crisis and executive competition is included. * New case studies on base of the pyramid strategies are featured.",poster,cp95
Political Science,p2173,d15,419ea302cfb779b7c4c4df3eacad7f994eea3cfc,j419,Business Ethics Quarterly,Multi-Stakeholder Initiatives on Sustainability: A Cross-Disciplinary Review and Research Agenda for Business Ethics,"ABSTRACT: Although the literature on multi-stakeholder initiatives for sustainability has grown in recent years, it is scattered across several academic fields, making it hard to ascertain how individual disciplines, such as business ethics, can further contribute to the debate. Based on an extensive review of the literature on certification and principle-based MSIs for sustainability (n = 293 articles), we show that the scholarly debate rests on three broad themes (the “3Is”): the input into creating and governing MSIs; the institutionalization of MSIs; and the impact that relevant initiatives create. While our discussion reveals the theoretical underpinnings of the 3Is, it also shows that a number of research challenges related to business ethics remain unaddressed. We unpack these challenges and suggest how scholars can utilize theoretical insights in business ethics to push the boundaries of the field. Finally, we also discuss what business ethics research can gain from theory development in the MSI field.",fullPaper,jv419
Political Science,p2177,d15,62f78a7ceb32194aad0cab80371d75b98357a6c1,c110,Biometrics and Identity Management,The Death of Competition: Leadership and Strategy in the Age of Business Ecosystems,"The death of competition an ecological metaphor leading business ecosystems the stages of a business ecosystem, co-evolution and cars - stages in action stage I - the terrain of opportunities stage II - the revolution spreads stage III - the red queen effect stage IV - renewal or death the paradox of powerless activism.",poster,cp110
Political Science,p2219,d15,c7babb794c5e70d7db23c45a679c09e349327ac1,c92,International Symposium on Computer Architecture,The Case for and Against Business Assumption of Social Responsibilities,"There are many sound reasons both for and against business’s assumption of social responsibilities. Because of the increasing amount of rhetoric which exists on this subject, it is appropriate to e...",poster,cp92
Political Science,p2220,d15,20ed3a64667fd93b5ce2fd7774ae76b6f22e6862,j421,World Politics,"American Business, Public Policy, Case-Studies, and Political Theory","Case-Studies of the policy-making process constitute one of the more important methods of political science analysis. Beginning with Schattschneider, Herring, and others in the 1930's, case-studies have been conducted on a great variety of decisions. They have varied in subject-matter and format, in scope and rigor, but they form a distinguishable body of literature which continues to grow year by year. The most recent addition, a book-length study by Raymond Bauer and his associates, stands with Robert A. Dahl's prize-winning Who Governs? (New Haven 1961) as the best yet to appear. With its publication a new level of sophistication has been reached. The standards of research its authors have set will indeed be difficult to uphold in the future. American Business and Public Policy is an analysis of political relationships within the context of a single, well-defined issue—foreign trade. It is an analysis of business attitudes, strategies, communications and, through these, business relationships in politics. The analysis makes use of the best behavioral research techniques without losing sight of the rich context of policies, traditions, and institutions. Thus, it does not, in Dahl's words, exchange relevance for rigor; rather it is standing proof that the two—relevance and rigor—are not mutually exclusive goals.",fullPaper,jv421
Political Science,p2242,d15,76961b68475f45c2807396956bf17c4864a57df1,c32,International Conference on Smart Data Services,Qualitative Methods in Business Research,PART ONE: THE BUSINESS OF QUALITATIVE RESEARCH Introduction Research Philosophy Research Design and Process Focus and Frame Access and Relationships Ethics in Research Qualitative Research Materials Electronic Research PART TWO: METHODS IN QUALITATIVE BUSINESS RESEARCH Case Study Research Ethnographic Research Grounded Theory Research Focus Group Research Action Research Narrative Research Discourse Analysis Feminist Research Critical Research PART THREE: WRITING AND EVALUATING QUALITATIVE BUSINESS RESEARCH Writing Process Qualitative Research Evaluation Closing Up,poster,cp32
Political Science,p2257,d15,cdc89021383700e4702d8fed73d470adffd80d36,c29,ACM-SIAM Symposium on Discrete Algorithms,Research Methodology for the Business and Administrative Sciences,"This well regarded text provides students and researchers with practical guidelines on how to conduct research. Definitions, constructs and ideas are dealt with in a learner-centred approach using case studies, examples, activities, and self-evaluation questions. A brief summary with multiple choice questions concludes each chapter. The whole book is written with outcomes-based education principles in mind. An in-depth overview of the research processes using South African examples is provided.",poster,cp29
Political Science,p2265,d15,ebd00186957ab28fb056d59c86c37659eec1e6af,c87,International Conference on Big Data Research,Reengineering the Corporation: A Manifesto for Business Revolution,"The article reviews the book “Reengineering the Corporation: A Manifesto for Business Revolution,” by Michael Hammer and James Champy.",poster,cp87
Political Science,p2423,d15,a51f9d980ab9daca5ec3a3e79a33152804fc1d86,c47,EUROMICRO Conference on Software Engineering and Advanced Applications,Principals and Agents: The Structure of Business,"Eight essays address the inherent problems of the agency relationship, such as monitoring performance and the careful design of incentives. By applying agency theory to actual practices in areas as diverse as tax-sheltered programs and transfer pricing, the contributors present conceptual tools and practical ideas for shaping agency structures to serve the best interests of both parties. A research colloquium book.",poster,cp47
Political Science,p2428,d15,ad2b5e8c5c482b8c47b367eeada3c6c2caffc705,c22,Grid Computing Environments,Lambert Review of Business-University Collaboration: Final Report,"This review of business-university collaborationin the United Kingdom has three objectives: (1) to illustrate opportunitiesarising from changes in ways that business undertakes RD (2) to recognizebusinesses already collaborating with university departments, which are rolemodels for those without university links; and (3) to offer ideas andrecommendations to shape policy. Points out two new trends shaping business world-wide: Firms are doing lessresearch and development (RD and business RD (2) that government mustbetter support business-university collaboration; and (3) that business mustlearn to exploit innovations being developed by universities. Comparatively, UK businesses have low research intensity. Analyzesthedemand for research from business, identifying the need to raisebusiness demand for research from all sources. Some proposals for buildingnetworks among research-intensive businesses and supporting business-universitycollaboration are offered. Also offers recommendations for encouragingcommunication between business people and academics, to support knowledgetransfer. Universities have the potential to transfer knowledge from their strongscience base to business in the form of intellectual property (IP). However, anumber of barriers to commercializing IP are discussed. First is lack ofclarity of IP ownership in collaborations; second is variable quality ofuniversity technology transfer offices; third, more emphasis should be put onlicensing technology and less on developing university spinouts. Because universities play increasing roles in regional economic development,this analysisrecommends English Regional Development Agencies shouldchange their targets to creating relationships between business anduniversities across regions and nations. The current dual support system of university funding had strengths andweaknesses. The analysissuggests that this system offersdisincentives to business-university cooperation. A number of principles toencourage business research are presented, and the UK government is urged toconsider the balance and fill the research funding gap. A voluntary code of governance for university management is suggested, asare strategies for improving professional and entrepreneurial skills forstudents. Overall, the report concludes that despite much good collaborativework, more remains to be done. Four appendixes identify the goals of thereport, give the draft code of governance, summarize its recommendations, andlist contributors to the report. (TNM)",poster,cp22
Political Science,p2439,d15,3dd6bbce2c63649e168f9c5ad7ab7c48c5d301b4,c37,International Workshop on the Semantic Web,The Business of Talk: Organizations in Action,"Preface and Acknowledgements. Introduction. 1. Talk and Organization. 2. Organizations in Action. 3. Talk as Social Action. 4. The Interaction Order of Organizations. 5. Conversational Procedures and Organizational Practices. 6. Information, Interaction and Institution. 7. Organizational Agendas. 8. Local Logic and Organizational Rationality. 9. The Business of Talk. Appendix I: Notes on Transcription Conventions. Bibliography. . Index.",poster,cp37
Political Science,p2476,d15,3a37dc3fd05ee24c0da7b962ff616ef25b4313b4,c8,Frontiers in Education Conference,Islamic Ethics and the Implications for Business,Abstract,poster,cp8
Political Science,p2481,d15,4b061afc1025523373101c6ad11962204572108b,c4,Conference on Innovative Data Systems Research,Methodology for Creating Business Knowledge,PART ONE: INTRODUCTION TO RESEARCH METHODOLOGY The Language of Methodology The Act of Creating Knowledge To Become a Knowledge-Creator PART TWO: THREE METHODOLOGICAL VIEWS The Analytical View The Systems View The Actors View PART THREE: METHODOLOGY Methodical Procedures Methods in Language and Action Methodics PART FOUR: APPROACHING METHODOLOGY The Analytical Approach The Systems Approach The Actors Approach PART FIVE: METHODOLOGY OF COMPLEMENTARITY The Views as Transformative Operators Three Cases - Knowledge of Complementarity Methodology as Business Creating Intelligence,poster,cp4
Geology,p279,d16,425744cb05e854071d06af0da2b8ef2d677f33d5,j83,EOS,Harnessing the GPS Data Explosion for Interdisciplinary Science,"More GPS stations, faster data delivery, and better data processing provide an abundance of information for all kinds of Earth scientists.",fullPaper,jv83
Geology,p492,d16,3954e2d220d9a7b7a46f9561cafb6251524d8ee5,c3,Knowledge Discovery and Data Mining,Mars Reconnaissance Orbiter's High Resolution Imaging Science Experiment (HiRISE),"[1] The HiRISE camera features a 0.5 m diameter primary mirror, 12 m effective focal length, and a focal plane system that can acquire images containing up to 28 Gb (gigabits) of data in as little as 6 seconds. HiRISE will provide detailed images (0.25 to 1.3 m/pixel) covering ∼1% of the Martian surface during the 2-year Primary Science Phase (PSP) beginning November 2006. Most images will include color data covering 20% of the potential field of view. A top priority is to acquire ∼1000 stereo pairs and apply precision geometric corrections to enable topographic measurements to better than 25 cm vertical precision. We expect to return more than 12 Tb of HiRISE data during the 2-year PSP, and use pixel binning, conversion from 14 to 8 bit values, and a lossless compression system to increase coverage. HiRISE images are acquired via 14 CCD detectors, each with 2 output channels, and with multiple choices for pixel binning and number of Time Delay and Integration lines. HiRISE will support Mars exploration by locating and characterizing past, present, and future landing sites, unsuccessful landing sites, and past and potentially future rover traverses. We will investigate cratering, volcanism, tectonism, hydrology, sedimentary processes, stratigraphy, aeolian processes, mass wasting, landscape evolution, seasonal processes, climate change, spectrophotometry, glacial and periglacial processes, polar geology, and regolith properties. An Internet Web site (HiWeb) will enable anyone in the world to suggest HiRISE targets on Mars and to easily locate, view, and download HiRISE data products.",poster,cp3
Geology,p498,d16,832139bd87f51f0a173b5bd9255944748bc31a96,c117,Very Large Data Bases Conference,Global multi-resolution terrain elevation data 2010 (GMTED2010),"For more information on the USGS—the Federal source for science about the Earth, its natural and living resources, natural hazards, and the environment, visit http://www.usgs.gov or call 1–888–ASK–USGS. For an overview of USGS information products, including maps, imagery, and publications, Any use of trade, product, or firm names is for descriptive purposes only and does not imply endorsement by the U.S. Government. Although this report is in the public domain, permission must be secured from the individual copyright owners to reproduce any copyrighted materials contained within this report. 10. Diagram showing the GMTED2010 layer extents (minimum and maximum latitude and longitude) are a result of the coordinate system inherited from the 1-arc-second SRTM",poster,cp117
Geology,p820,d16,c1ad6123d168525cacf262f2fd69f88f689381b4,c74,International Conference on Computational Linguistics,An Introduction to Geotechnical Engineering,"This manual presents data on soil behaviour, with emphasis on practical and empirical knowledge, required by geotechnical engineers for the design and construction of foundations and embankments. It deals with: index and classification properties of soils; soil classification; clay minerals and soil structure; compaction; water in soils (capillarity, shrinkage, swelling, frost action, permeability, seepage, effective stress); consolidation and consolidation settlements; time rate of consolidation; the Mohr circle, failure theories, and stress paths; shear strength of sands and clays. Four appendices deal with the following: application of the ""SI"" system of units to getechnical engineering; derivation of Laplace's equation; derivation and solution of Terzaghi's one-dimensional consolidation theory; pore pressure parameters. (TRRL)",poster,cp74
Geology,p962,d16,5ed1d6261984992fea16a4a35ec331c0916bdb3c,c58,Extreme Science and Engineering Discovery Environment,Interpolation and Approximation,Abstract,poster,cp58
Geology,p984,d16,a3cb05dd21c57751af99ecfc2b2e387099cd51ce,c46,Ideal,Engineering cyber infrastructure for U‐Pb geochronology: Tripoli and U‐Pb_Redux,"In the past decade, major advancements in precision and accuracy of U‐Pb geochronology, which stem from improved sample pretreatment and refined measurement techniques, have revealed previously unresolvable discrepancies among analyses from different laboratories. One solution to evaluating and resolving many of these discrepancies is the adoption of a common software platform that standardizes data‐processing protocols, enabling robust interlaboratory comparisons. We present the results of a collaboration to develop cyber infrastructure for high‐precision U‐Pb geochronology based on analyzing accessory minerals by isotope dilution‐thermal ionization mass spectrometry. This cyber infrastructure implements an architecture specifying the workflows of data acquisition, statistical filtering, analysis and interpretation, publication, community‐based archiving, and the compilation and comparison of data from different laboratories. The backbone of the cyber infrastructure consists of two open‐source software programs: Tripoli and U‐Pb_Redux. Tripoli interfaces with commercially available mass spectrometers using standardized protocols, statistical filtering, and interactive visualizations to aid the analyst in preparing raw data for analysis in U‐Pb_Redux. U‐Pb_Redux implements the architecture by orchestrating the analyst's workflow with interactive visualizations and provides data reduction and uncertainty propagation that support data interpretations. Finally, U‐Pb_Redux enables production of publication‐ready graphics and data tables, the archiving of results, and the comparative compilation of archived results to support cooperative science.",poster,cp46
Geology,p1541,d16,dc52d1ede1b90bf9d296bc5b34c9310b7eaa99a2,c41,IEEE International Conference on Data Engineering,The mnist database of handwritten digits,Disclosed is an improved articulated bar flail having shearing edges for efficiently shredding materials. An improved shredder cylinder is disclosed with a plurality of these flails circumferentially spaced and pivotally attached to the periphery of a rotatable shaft. Also disclosed is an improved shredder apparatus which has a pair of these shredder cylinders mounted to rotate about spaced parallel axes which cooperates with a conveyer apparatus which has a pair of inclined converging conveyer belts with one of the belts mounted to move with respect to the other belt to allow the transport of articles of various sizes therethrough.,poster,cp41
Geology,p1666,d16,fa154947515cd58c0a273bb170bc6dc8a8c99847,j165,Bulletin of Earthquake Engineering,Active fault database of Turkey,Abstract,fullPaper,jv165
Geology,p1682,d16,d1de096375c58d18e167b0f2324f0288750b7411,c106,International Conference on Biometrics,NGA-West2 Database,"The NGA-West2 project database expands on its predecessor to include worldwide ground motion data recorded from shallow crustal earthquakes in active tectonic regimes post-2000 and a set of small-to-moderate-magnitude earthquakes in California between 1998 and 2011. The database includes 21,336 (mostly) three-component records from 599 events. The parameter space covered by the database is M 3.0 to M 7.9, closest distance of 0.05 to 1,533 km, and site time-averaged shear-wave velocity in the top 30 m of VS30 = 94 m/s to 2,100 m/s (although data becomes sparse for distances >400 km and VS30 > 1,200 m/s or <150 m/s). The database includes uniformly processed time series and response spectral ordinates for 111 periods ranging from 0.01 s to 20 s at 11 damping ratios. Ground motions and metadata for source, path, and site conditions were subject to quality checks by ground motion prediction equation developers and topical working groups.",poster,cp106
Geology,p1991,d16,89e3fae32bf72b61834fc2ae60b1f8508e714e38,c96,Human Language Technology - The Baltic Perspectiv,World Ocean Database,"The U.S. National Oceanic and Atmospheric Administration's (NOAA) World Ocean Database 2009, released in November as an update to the 2005 version, provides about 9.1 million temperature profiles and 3.5 million salinity reports, with some information dating as far back as 1800. The updated database includes scientific information about the oceans that can be sorted in various ways, including geographically or by year. 
 
“There is now more data about the global oceans than ever before,” according to Sydney Levitus, director of the World Data Center for Oceanography, part of NOAA's National Oceanographic Data Center. “Previous databases have shown the world ocean has warmed during the last 53 years, and it's crucial we have reliable, accurate monitoring of our oceans into the future,” he said. The database is a part of the Integrated Ocean Observing System and the Global Earth Observation System of Systems.",poster,cp96
Art,p223,d17,e12b5363078a6d435bfba80e9d5cbab6b2cac897,c64,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",Data Science and Digital Art History,"I present a number of core concepts from data science that are relevant to digital art history and the use of quantitative methods to study any cultural artifacts or processes in general. These concepts are objects, features, data, feature space, and dimension reduction. These concepts enable computational exploration of both large and small visual cultural data. We can analyze relations between works on a single artist, many artists, all digitized production from a whole historical period, holdings in museum collections, collection metadata, or writings about art. The same concepts allow us to study contemporary vernacular visual media using massive social media content. (In our lab, we analyzed works by van Gogh, Mondrian, and Rothko, 6000 paintings by French Impressionists, 20,000 photographs from MoMA photo­graphy collection, one million manga pages from manga books, one million artworks of contemporary non-professional artists, and over 13 million Instagram images from 16 global cities.) While data science techniques do not replace other art historical methods, they allow us to see familiar art historical material in new ways, and also to study contemporary digital visual culture.",poster,cp64
